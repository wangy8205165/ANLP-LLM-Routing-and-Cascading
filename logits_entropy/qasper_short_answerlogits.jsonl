{"generated_text": "A very small lexicon consisting of 15 positive words and 15 negative words.", "avg_logp": -0.15785199403762817, "avg_entropy_nats": 0.32395458221435547, "avg_entropy_bits": 0.46736764907836914}
{"generated_text": "A very small lexicon consisting of 15 positive words and 15 negative words.", "avg_logp": -0.15785199403762817, "avg_entropy_nats": 0.32395458221435547, "avg_entropy_bits": 0.46736764907836914}
{"generated_text": "The proposed method performed well even with a minimal amount of supervision, and the models in the top block performed considerably better than the random baselines, with the BiGRU encoder trained with the AL+CA+CO dataset achieving the highest score.", "avg_logp": -0.15250471234321594, "avg_entropy_nats": 0.37058794498443604, "avg_entropy_bits": 0.5346454381942749}
{"generated_text": "The relations of Cause and Concession are used to propagate polarity.", "avg_logp": -0.22609162330627441, "avg_entropy_nats": 0.5225696563720703, "avg_entropy_bits": 0.7539086937904358}
{"generated_text": "The relations of Cause and Concession are used to propagate polarity.", "avg_logp": -0.22609162330627441, "avg_entropy_nats": 0.5225696563720703, "avg_entropy_bits": 0.7539086937904358}
{"generated_text": "The Japanese data consists of a corpus of about 100 million sentences.", "avg_logp": -0.2963492274284363, "avg_entropy_nats": 0.6506589651107788, "avg_entropy_bits": 0.9387024641036987}
{"generated_text": "The Japanese data consists of a corpus of about 100 million sentences.", "avg_logp": -0.2963492274284363, "avg_entropy_nats": 0.6506589651107788, "avg_entropy_bits": 0.9387024641036987}
{"generated_text": "The ACP Corpus.", "avg_logp": -0.5890800952911377, "avg_entropy_nats": 1.1979432106018066, "avg_entropy_bits": 1.728266716003418}
{"generated_text": "The results show that the proposed approach yields a gain over a purely supervised baseline when labeled data are small.", "avg_logp": -0.29623308777809143, "avg_entropy_nats": 0.7484627962112427, "avg_entropy_bits": 1.079803466796875}
{"generated_text": "It depends on discourse relation-based label propagation from a small seed lexicon.", "avg_logp": -0.3249281048774719, "avg_entropy_nats": 0.8063086271286011, "avg_entropy_bits": 1.1632574796676636}
{"generated_text": "The seed lexicon consists of 15 positive words and 15 negative words.", "avg_logp": -0.06534653902053833, "avg_entropy_nats": 0.1875934600830078, "avg_entropy_bits": 0.27064016461372375}
{"generated_text": "about 100 million sentences.", "avg_logp": -0.09522754698991776, "avg_entropy_nats": 0.26686638593673706, "avg_entropy_bits": 0.38500678539276123}
{"generated_text": "unanswerable", "avg_logp": -0.10984059423208237, "avg_entropy_nats": 0.29701533913612366, "avg_entropy_bits": 0.42850252985954285}
{"generated_text": "unanswerable", "avg_logp": -0.10984059423208237, "avg_entropy_nats": 0.29701533913612366, "avg_entropy_bits": 0.42850252985954285}
{"generated_text": "The annotation experiment is evaluated through agreement statistics, including heatmaps, and Cohen's κ scores, as well as comparison of expert and crowdsourced annotations.", "avg_logp": -0.39033636450767517, "avg_entropy_nats": 0.8885639309883118, "avg_entropy_bits": 1.2819267511367798}
{"generated_text": "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.", "avg_logp": -0.004148625768721104, "avg_entropy_nats": 0.021172959357500076, "avg_entropy_bits": 0.030546121299266815}
{"generated_text": "Yes.", "avg_logp": -0.32915544509887695, "avg_entropy_nats": 0.6537865996360779, "avg_entropy_bits": 0.9432146549224854}
{"generated_text": "Yes.", "avg_logp": -0.32915544509887695, "avg_entropy_nats": 0.6537865996360779, "avg_entropy_bits": 0.9432146549224854}
{"generated_text": "They manifest in systematic patterns that vary according to the type of community, with distinctive communities exhibiting different engagement patterns than generic communities.", "avg_logp": -0.47021105885505676, "avg_entropy_nats": 1.0036287307739258, "avg_entropy_bits": 1.4479302167892456}
{"generated_text": "They observe that user engagement varies systematically with a community's distinctiveness and dynamicity, with dynamic communities having higher rates of user retention and engagement, and distinctive communities having higher short-term engagement but lower long-term retention.", "avg_logp": -0.2966611087322235, "avg_entropy_nats": 0.6543156504631042, "avg_entropy_bits": 0.9439778923988342}
{"generated_text": "They selected almost 300 Reddit communities by filtering a dataset of all subreddits from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of history.", "avg_logp": -0.28835269808769226, "avg_entropy_nats": 0.5429854989051819, "avg_entropy_bits": 0.7833623886108398}
{"generated_text": "They selected almost 300 Reddit communities by filtering a dataset of all subreddits from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of history.", "avg_logp": -0.28835269808769226, "avg_entropy_nats": 0.5429854989051819, "avg_entropy_bits": 0.7833623886108398}
{"generated_text": "The authors measure a community's temporal dynamicity by computing the pointwise mutual information (PMI) of a word's frequency in a particular community at a given time period relative to its frequency in the entire history of the community.", "avg_logp": -0.21448108553886414, "avg_entropy_nats": 0.4655520021915436, "avg_entropy_bits": 0.6716495752334595}
{"generated_text": "The authors measure how distinctive a community is by calculating the average specificity of all utterances in the community, where specificity is defined as the average specificity of each word in the utterance.", "avg_logp": -0.13281434774398804, "avg_entropy_nats": 0.29399290680885315, "avg_entropy_bits": 0.42414209246635437}
{"generated_text": "Chinese general corpus.", "avg_logp": -0.12145139276981354, "avg_entropy_nats": 0.22551894187927246, "avg_entropy_bits": 0.32535505294799805}
{"generated_text": "Chinese general corpus.", "avg_logp": -0.12145139276981354, "avg_entropy_nats": 0.22551894187927246, "avg_entropy_bits": 0.32535505294799805}
{"generated_text": "QANet and BERT-Base.", "avg_logp": -0.08313310891389847, "avg_entropy_nats": 0.14788024127483368, "avg_entropy_bits": 0.21334610879421234}
{"generated_text": "QANet and BERT-Base.", "avg_logp": -0.08313310891389847, "avg_entropy_nats": 0.14788024127483368, "avg_entropy_bits": 0.21334610879421234}
{"generated_text": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text.", "avg_logp": -0.06627067178487778, "avg_entropy_nats": 0.16569699347019196, "avg_entropy_bits": 0.2390502244234085}
{"generated_text": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text.", "avg_logp": -0.06627067178487778, "avg_entropy_nats": 0.16569699347019196, "avg_entropy_bits": 0.2390502244234085}
{"generated_text": "Clinical text structuring tasks.", "avg_logp": -0.19685612618923187, "avg_entropy_nats": 0.4669943153858185, "avg_entropy_bits": 0.6737304329872131}
{"generated_text": "Clinical text structuring tasks.", "avg_logp": -0.19685612618923187, "avg_entropy_nats": 0.4669943153858185, "avg_entropy_bits": 0.6737304329872131}
{"generated_text": "The dataset contains 17,833 sentences, but it is not specified that all of them are questions.", "avg_logp": -0.40188464522361755, "avg_entropy_nats": 0.9028154015541077, "avg_entropy_bits": 1.3024871349334717}
{"generated_text": "17,833 sentences contain 2,714 question-answer pairs.", "avg_logp": -0.13975302875041962, "avg_entropy_nats": 0.42590421438217163, "avg_entropy_bits": 0.6144499182701111}
{"generated_text": "The tasks evaluated are clinical text structuring, question answering, and named entity recognition.", "avg_logp": -0.2641311287879944, "avg_entropy_nats": 0.6847288012504578, "avg_entropy_bits": 0.9878548383712769}
{"generated_text": "unanswerable", "avg_logp": -0.1672077476978302, "avg_entropy_nats": 0.49349069595336914, "avg_entropy_bits": 0.7119566202163696}
{"generated_text": "They introduce clinical named entity information into the model.", "avg_logp": -0.11593606323003769, "avg_entropy_nats": 0.28459957242012024, "avg_entropy_bits": 0.41059041023254395}
{"generated_text": "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.", "avg_logp": -0.07606740295886993, "avg_entropy_nats": 0.16330426931381226, "avg_entropy_bits": 0.23559825122356415}
{"generated_text": "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.", "avg_logp": -0.11090392619371414, "avg_entropy_nats": 0.24001693725585938, "avg_entropy_bits": 0.34627124667167664}
{"generated_text": "BERT-Base and QANet.", "avg_logp": -0.10979332029819489, "avg_entropy_nats": 0.2642272710800171, "avg_entropy_bits": 0.38119935989379883}
{"generated_text": "Perplexity, R@3 in next-word prediction, latency, and energy usage.", "avg_logp": -0.1221718043088913, "avg_entropy_nats": 0.2925306558609009, "avg_entropy_bits": 0.42203250527381897}
{"generated_text": "Kneser–Ney smoothing and five-gram model.", "avg_logp": -0.20873649418354034, "avg_entropy_nats": 0.4729524850845337, "avg_entropy_bits": 0.6823262572288513}
{"generated_text": "Perplexity and R@3 (Recall at 3).", "avg_logp": -0.2964281737804413, "avg_entropy_nats": 0.6702380180358887, "avg_entropy_bits": 0.9669489860534668}
{"generated_text": "Perplexity and R@3 (Recall at 3).", "avg_logp": -0.2964281737804413, "avg_entropy_nats": 0.6702380180358887, "avg_entropy_bits": 0.9669489860534668}
{"generated_text": "The Yelp Challenge dataset.", "avg_logp": -0.16376876831054688, "avg_entropy_nats": 0.43327653408050537, "avg_entropy_bits": 0.6250858902931213}
{"generated_text": "The Yelp Challenge dataset.", "avg_logp": -0.16376876831054688, "avg_entropy_nats": 0.43327653408050537, "avg_entropy_bits": 0.6250858902931213}
{"generated_text": "No, they train their own NMT model using the Yelp Challenge dataset.", "avg_logp": -0.25945383310317993, "avg_entropy_nats": 0.6749625205993652, "avg_entropy_bits": 0.9737651348114014}
{"generated_text": "No, they train their own NMT model using the Yelp Challenge dataset.", "avg_logp": -0.25945383310317993, "avg_entropy_nats": 0.6749625205993652, "avg_entropy_bits": 0.9737651348114014}
{"generated_text": "By defining a clear-text sequence of context for each generated fake review, including the review rating, restaurant name, city, state, and food tags.", "avg_logp": -0.15699860453605652, "avg_entropy_nats": 0.3319949507713318, "avg_entropy_bits": 0.47896745800971985}
{"generated_text": "AdaBoost with 200 shallow decision trees.", "avg_logp": -0.07298951596021652, "avg_entropy_nats": 0.2800333499908447, "avg_entropy_bits": 0.40400272607803345}
{"generated_text": "Yes.", "avg_logp": -0.5866988897323608, "avg_entropy_nats": 1.0249392986297607, "avg_entropy_bits": 1.4786748886108398}
{"generated_text": "unanswerable", "avg_logp": -0.3244422674179077, "avg_entropy_nats": 0.7177236080169678, "avg_entropy_bits": 1.0354561805725098}
{"generated_text": "They compared the results of their model to the results of See et al. See2017.", "avg_logp": -0.3066607415676117, "avg_entropy_nats": 0.6821140646934509, "avg_entropy_bits": 0.9840825200080872}
{"generated_text": "They compared the results of their model to the results of See et al. See2017.", "avg_logp": -0.3066607415676117, "avg_entropy_nats": 0.6821140646934509, "avg_entropy_bits": 0.9840825200080872}
{"generated_text": "There is one attention layer in their model, which is part of the Bahdanau's attention mechanism in the decoder.", "avg_logp": -0.33748024702072144, "avg_entropy_nats": 0.708652913570404, "avg_entropy_bits": 1.0223701000213623}
{"generated_text": "The truthfulness of the attributions from the saliency maps is not guaranteed and needs to be rigorously checked, as they may not always highlight the important input features that the network attended to during prediction.", "avg_logp": -0.3437401056289673, "avg_entropy_nats": 0.7398505210876465, "avg_entropy_bits": 1.0673787593841553}
{"generated_text": "geometric bias, neighborhood bias, and RIPA metric.", "avg_logp": -0.27159690856933594, "avg_entropy_nats": 0.6089348793029785, "avg_entropy_bits": 0.8785074353218079}
{"generated_text": "The probabilistic observations that contribute to the more robust algorithm are the conditional probabilities of word pairs, ensuring that the probability of prediction or outcome cannot depend on a protected class such as gender.", "avg_logp": -0.18350836634635925, "avg_entropy_nats": 0.5468399524688721, "avg_entropy_bits": 0.7889232635498047}
{"generated_text": "In a low-resourced setting, the quality of the data is more critical than the volume of data.", "avg_logp": -0.23325173556804657, "avg_entropy_nats": 0.4501131772994995, "avg_entropy_bits": 0.6493760347366333}
{"generated_text": "In a low-resourced setting, the quality of the data is more critical than the volume of data.", "avg_logp": -0.23325173556804657, "avg_entropy_nats": 0.4501131772994995, "avg_entropy_bits": 0.6493760347366333}
{"generated_text": "The model is improved more by quality than by quantity, as shown by the results where high-quality data helps, but not the addition of more noisy data.", "avg_logp": -0.6006989479064941, "avg_entropy_nats": 1.254223108291626, "avg_entropy_bits": 1.8094613552093506}
{"generated_text": "Character Word Embedding (CWE) and fastText.", "avg_logp": -0.07887542247772217, "avg_entropy_nats": 0.2375192940235138, "avg_entropy_bits": 0.3426678776741028}
{"generated_text": "Brazilian Portuguese.", "avg_logp": -0.2108810991048813, "avg_entropy_nats": 0.46706366539001465, "avg_entropy_bits": 0.6738304495811462}
{"generated_text": "Brazilian Portuguese.", "avg_logp": -0.2108810991048813, "avg_entropy_nats": 0.46706366539001465, "avg_entropy_bits": 0.6738304495811462}
{"generated_text": "A large Portuguese corpus from several sources, making it a multi-genre corpus representative of the Portuguese language.", "avg_logp": -0.16946129500865936, "avg_entropy_nats": 0.4070627987384796, "avg_entropy_bits": 0.5872673988342285}
{"generated_text": "Portuguese word2vec embeddings.", "avg_logp": -0.16051268577575684, "avg_entropy_nats": 0.3739142417907715, "avg_entropy_bits": 0.5394442677497864}
{"generated_text": "Yes.", "avg_logp": -0.5217162370681763, "avg_entropy_nats": 1.0449042320251465, "avg_entropy_bits": 1.50747811794281}
{"generated_text": "Yes.", "avg_logp": -0.5217162370681763, "avg_entropy_nats": 1.0449042320251465, "avg_entropy_bits": 1.50747811794281}
{"generated_text": "The quality of the citation is measured through the F1 measure, which is evaluated in the initial recognition of the text spans and their classification.", "avg_logp": -0.33730581402778625, "avg_entropy_nats": 0.8105562925338745, "avg_entropy_bits": 1.1693854331970215}
{"generated_text": "The dataset contains 237,723 court decisions.", "avg_logp": -0.2625836431980133, "avg_entropy_nats": 0.5629562735557556, "avg_entropy_bits": 0.8121742010116577}
{"generated_text": "unanswerable", "avg_logp": -0.22873912751674652, "avg_entropy_nats": 0.58970707654953, "avg_entropy_bits": 0.8507674932479858}
{"generated_text": "unanswerable", "avg_logp": -0.33547866344451904, "avg_entropy_nats": 0.6552968621253967, "avg_entropy_bits": 0.9453935623168945}
{"generated_text": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to the Dryhootch manual.", "avg_logp": -0.15161895751953125, "avg_entropy_nats": 0.34313443303108215, "avg_entropy_bits": 0.49503833055496216}
{"generated_text": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to the Dryhootch manual.", "avg_logp": -0.15161895751953125, "avg_entropy_nats": 0.34313443303108215, "avg_entropy_bits": 0.49503833055496216}
{"generated_text": "LIWC is incorporated into the system by using its WordStat dictionary format for text analysis, and its language models (ULM and CLM) to measure the probability that a word or string of characters was generated by the same underlying process as the training data.", "avg_logp": -0.28909218311309814, "avg_entropy_nats": 0.6143558621406555, "avg_entropy_bits": 0.8863282203674316}
{"generated_text": "LIWC is incorporated into the system by using its WordStat dictionary format for text analysis, and its language models (ULM and CLM) to measure the probability that a word or string of characters was generated by the same underlying process as the training data.", "avg_logp": -0.28909218311309814, "avg_entropy_nats": 0.6143558621406555, "avg_entropy_bits": 0.8863282203674316}
{"generated_text": "210 users are surveyed using the clinically validated survey.", "avg_logp": -0.17619656026363373, "avg_entropy_nats": 0.502021849155426, "avg_entropy_bits": 0.7242644429206848}
{"generated_text": "The clinically validated survey tools used are the Domain-Specific Risk-Taking (DOSPERT) Scale, The Berlin Social Support Scales (BSSS), and the Values In Action Scale (VIAS).", "avg_logp": -0.061139121651649475, "avg_entropy_nats": 0.15596085786819458, "avg_entropy_bits": 0.22500395774841309}
{"generated_text": "unanswerable", "avg_logp": -0.24028418958187103, "avg_entropy_nats": 0.6387443542480469, "avg_entropy_bits": 0.9215133190155029}
{"generated_text": "unanswerable", "avg_logp": -0.3609501123428345, "avg_entropy_nats": 0.7408664226531982, "avg_entropy_bits": 1.0688443183898926}
{"generated_text": "unanswerable", "avg_logp": -0.3609501123428345, "avg_entropy_nats": 0.7408664226531982, "avg_entropy_bits": 1.0688443183898926}
{"generated_text": "They list 75 fine-grained named entity types.", "avg_logp": -0.21770605444908142, "avg_entropy_nats": 0.4789033830165863, "avg_entropy_bits": 0.6909114718437195}
{"generated_text": "Yes.", "avg_logp": -0.13596658408641815, "avg_entropy_nats": 0.3947819173336029, "avg_entropy_bits": 0.5695499181747437}
{"generated_text": "Yes.", "avg_logp": -0.13596658408641815, "avg_entropy_nats": 0.3947819173336029, "avg_entropy_bits": 0.5695499181747437}
{"generated_text": "They are annotated/labelled manually in the case of some languages, but for others, they are induced automatically from clusters.", "avg_logp": -0.6788118481636047, "avg_entropy_nats": 1.5061464309692383, "avg_entropy_bits": 2.17290997505188}
{"generated_text": "Yes.", "avg_logp": -0.2680589556694031, "avg_entropy_nats": 0.6994273066520691, "avg_entropy_bits": 1.009060263633728}
{"generated_text": "Yes.", "avg_logp": -0.36926794052124023, "avg_entropy_nats": 0.7643877267837524, "avg_entropy_bits": 1.1027783155441284}
{"generated_text": "unanswerable", "avg_logp": -0.38115194439888, "avg_entropy_nats": 0.7670361995697021, "avg_entropy_bits": 1.1065993309020996}
{"generated_text": "unanswerable", "avg_logp": -0.38115194439888, "avg_entropy_nats": 0.7670361995697021, "avg_entropy_bits": 1.1065993309020996}
{"generated_text": "The accuracy reported by state-of-the-art methods varies, with some achieving up to 96.0% (Bartz et al.) and others achieving 95.4% (the proposed method).", "avg_logp": -0.31855595111846924, "avg_entropy_nats": 0.7415391206741333, "avg_entropy_bits": 1.069814920425415}
{"generated_text": "The proposed approach outperforms previous state-of-the-art vision-based methods, including CNN-mean and CNN-avgmax, with 11.6% and 6.7% absolute gains on P@1 on the BERGSMA and MMID dataset respectively.", "avg_logp": -0.10869510471820831, "avg_entropy_nats": 0.277951717376709, "avg_entropy_bits": 0.4009995758533478}
{"generated_text": "The Adam algorithm is applied to optimize the model with learning rate of 0.0001 and batch size of 128.", "avg_logp": -0.17768247425556183, "avg_entropy_nats": 0.4203202724456787, "avg_entropy_bits": 0.6063939332962036}
{"generated_text": "English, German, French, and Japanese.", "avg_logp": -0.11659618467092514, "avg_entropy_nats": 0.21732917428016663, "avg_entropy_bits": 0.3135397434234619}
{"generated_text": "English, German, French, and Japanese.", "avg_logp": -0.11659618467092514, "avg_entropy_nats": 0.21732917428016663, "avg_entropy_bits": 0.3135397434234619}
{"generated_text": "yes", "avg_logp": -0.7908366322517395, "avg_entropy_nats": 1.4789671897888184, "avg_entropy_bits": 2.1336984634399414}
{"generated_text": "They did not explicitly compare to previous research models, citing that most existing works use smaller data or pre-date BERT.", "avg_logp": -0.35258540511131287, "avg_entropy_nats": 0.7452831268310547, "avg_entropy_bits": 1.0752161741256714}
{"generated_text": "The datasets used in training are Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, and the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task.", "avg_logp": -0.13286378979682922, "avg_entropy_nats": 0.35031384229660034, "avg_entropy_bits": 0.5053960084915161}
{"generated_text": "The datasets used in training are Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, and the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task.", "avg_logp": -0.13286378979682922, "avg_entropy_nats": 0.35031384229660034, "avg_entropy_bits": 0.5053960084915161}
{"generated_text": "The authors use two GAN models: weGAN and deGAN.", "avg_logp": -0.35107913613319397, "avg_entropy_nats": 0.7485788464546204, "avg_entropy_bits": 1.0799709558486938}
{"generated_text": "The authors use two GAN models: weGAN and deGAN.", "avg_logp": -0.35107913613319397, "avg_entropy_nats": 0.7485788464546204, "avg_entropy_bits": 1.0799709558486938}
{"generated_text": "unanswerable", "avg_logp": -0.10702896863222122, "avg_entropy_nats": 0.3026837110519409, "avg_entropy_bits": 0.43668028712272644}
{"generated_text": "The CNN data set and the 20 Newsgroups data set.", "avg_logp": -0.1486498862504959, "avg_entropy_nats": 0.3472405970096588, "avg_entropy_bits": 0.5009622573852539}
{"generated_text": "Yes.", "avg_logp": -0.37125977873802185, "avg_entropy_nats": 0.7442450523376465, "avg_entropy_bits": 1.0737186670303345}
{"generated_text": "The authors do not explicitly define 'incorrect words', but they exemplify them through the use of Twitter data with grammatical errors and speech-to-text errors.", "avg_logp": -0.4783245325088501, "avg_entropy_nats": 1.038292407989502, "avg_entropy_bits": 1.4979393482208252}
{"generated_text": "The article does not explicitly state the number of vanilla transformers used after applying an embedding layer.", "avg_logp": -0.2736065983772278, "avg_entropy_nats": 0.6079006791114807, "avg_entropy_bits": 0.8770151734352112}
{"generated_text": "no.", "avg_logp": -0.7242013812065125, "avg_entropy_nats": 1.1178460121154785, "avg_entropy_bits": 1.6127108335494995}
{"generated_text": "no.", "avg_logp": -0.7242013812065125, "avg_entropy_nats": 1.1178460121154785, "avg_entropy_bits": 1.6127108335494995}
{"generated_text": "no.", "avg_logp": -0.7683168649673462, "avg_entropy_nats": 1.2519489526748657, "avg_entropy_bits": 1.806180715560913}
{"generated_text": "no.", "avg_logp": -0.7683168649673462, "avg_entropy_nats": 1.2519489526748657, "avg_entropy_bits": 1.806180715560913}
{"generated_text": "They outperform other models by 6% to 8% in the sentiment classification task and by 0.94% to 1.89% in the intent classification task with STT error.", "avg_logp": -0.13588696718215942, "avg_entropy_nats": 0.33510568737983704, "avg_entropy_bits": 0.4834553003311157}
{"generated_text": "34,432 user conversations.", "avg_logp": -0.1497311145067215, "avg_entropy_nats": 0.3919599652290344, "avg_entropy_bits": 0.5654786229133606}
{"generated_text": "34,432 user conversations.", "avg_logp": -0.1497311145067215, "avg_entropy_nats": 0.3919599652290344, "avg_entropy_bits": 0.5654786229133606}
{"generated_text": "The metrics to measure user engagement are overall rating, mean number of turns, and per-utterance word count.", "avg_logp": -0.22927117347717285, "avg_entropy_nats": 0.4775242805480957, "avg_entropy_bits": 0.6889219284057617}
{"generated_text": "The metrics to measure user engagement are overall rating, mean number of turns, and per-utterance word count.", "avg_logp": -0.22927117347717285, "avg_entropy_nats": 0.4775242805480957, "avg_entropy_bits": 0.6889219284057617}
{"generated_text": "The system designs introduced are a multi-step language understanding module, a novel dialog act scheme, and a fact/opinion interleaving strategy.", "avg_logp": -0.2913397550582886, "avg_entropy_nats": 0.7485606670379639, "avg_entropy_bits": 1.0799447298049927}
{"generated_text": "unanswerable", "avg_logp": -0.4023727774620056, "avg_entropy_nats": 0.7148597240447998, "avg_entropy_bits": 1.0313246250152588}
{"generated_text": "Yes.", "avg_logp": -0.18707899749279022, "avg_entropy_nats": 0.4133894145488739, "avg_entropy_bits": 0.5963948369026184}
{"generated_text": "The number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction.", "avg_logp": -0.0852026641368866, "avg_entropy_nats": 0.22758671641349792, "avg_entropy_bits": 0.32833823561668396}
{"generated_text": "No, they have multiple baselines including ROUGE, WordNCE, WordPPL, and others.", "avg_logp": -0.40414148569107056, "avg_entropy_nats": 0.9086175560951233, "avg_entropy_bits": 1.3108580112457275}
{"generated_text": "No, they have multiple baselines including ROUGE, WordNCE, WordPPL, and others.", "avg_logp": -0.40414148569107056, "avg_entropy_nats": 0.9086175560951233, "avg_entropy_bits": 1.3108580112457275}
{"generated_text": "They use recurrent neural networks (RNNs) and LSTM LMs (Long Short-Term Memory Language Models).", "avg_logp": -0.20790979266166687, "avg_entropy_nats": 0.46121832728385925, "avg_entropy_bits": 0.6653974056243896}
{"generated_text": "They ask human judges to rate the fluency of system-generated compressions on an ordinal scale from 1 (disfluent) through 3 (fluent).", "avg_logp": -0.09164391458034515, "avg_entropy_nats": 0.2271442860364914, "avg_entropy_bits": 0.3276999592781067}
{"generated_text": "The model \"gets lost\" and overrides the information brought by the text-based annotations if it loses track of the objects in the picture.", "avg_logp": -0.20093320310115814, "avg_entropy_nats": 0.46691855788230896, "avg_entropy_bits": 0.6736210584640503}
{"generated_text": "The model \"gets lost\" and overrides the information brought by the text-based annotations if it loses track of the objects in the picture.", "avg_logp": -0.20093320310115814, "avg_entropy_nats": 0.46691855788230896, "avg_entropy_bits": 0.6736210584640503}
{"generated_text": "The baseline used is the multimodal baseline of Calixto et al. (2017).", "avg_logp": -0.21280021965503693, "avg_entropy_nats": 0.6243886351585388, "avg_entropy_bits": 0.9008024334907532}
{"generated_text": "Soft attention, hard stochastic attention, and local attention.", "avg_logp": -0.1936948150396347, "avg_entropy_nats": 0.4639882743358612, "avg_entropy_bits": 0.6693936586380005}
{"generated_text": "unanswerable", "avg_logp": -0.14253714680671692, "avg_entropy_nats": 0.544207751750946, "avg_entropy_bits": 0.7851258516311646}
{"generated_text": "unanswerable", "avg_logp": -0.14253714680671692, "avg_entropy_nats": 0.544207751750946, "avg_entropy_bits": 0.7851258516311646}
{"generated_text": "It is not explicitly stated in the article how much their system outperforms the lexicon-based models.", "avg_logp": -0.2273450791835785, "avg_entropy_nats": 0.5134848356246948, "avg_entropy_bits": 0.7408021092414856}
{"generated_text": "It is not explicitly stated in the article how much their system outperforms the lexicon-based models.", "avg_logp": -0.2273450791835785, "avg_entropy_nats": 0.5134848356246948, "avg_entropy_bits": 0.7408021092414856}
{"generated_text": "TF-IDF, LDA, and NVDM.", "avg_logp": -0.11060559749603271, "avg_entropy_nats": 0.3091898560523987, "avg_entropy_bits": 0.446066677570343}
{"generated_text": "4.5 million human comments were used in the candidate set.", "avg_logp": -0.23244507610797882, "avg_entropy_nats": 0.5816221237182617, "avg_entropy_bits": 0.8391034007072449}
{"generated_text": "198,112 news articles.", "avg_logp": -0.08502265810966492, "avg_entropy_nats": 0.29044437408447266, "avg_entropy_bits": 0.41902264952659607}
{"generated_text": "The Tencent News dataset with millions of real comments.", "avg_logp": -0.2414376586675644, "avg_entropy_nats": 0.5285462737083435, "avg_entropy_bits": 0.762531042098999}
{"generated_text": "up to four percentage points in accuracy.", "avg_logp": -0.13002677261829376, "avg_entropy_nats": 0.25369250774383545, "avg_entropy_bits": 0.36600086092948914}
{"generated_text": "The 2019 GermEval shared task dataset on hierarchical text classification.", "avg_logp": -0.20686201751232147, "avg_entropy_nats": 0.355063796043396, "avg_entropy_bits": 0.5122488141059875}
{"generated_text": "The 2019 GermEval shared task dataset on hierarchical text classification.", "avg_logp": -0.20686201751232147, "avg_entropy_nats": 0.355063796043396, "avg_entropy_bits": 0.5122488141059875}
{"generated_text": "They combine text representations with knowledge graph embeddings by concatenating the three representations (text, metadata, and author embeddings) and passing them into a multilayer perceptron (MLP) with two layers.", "avg_logp": -0.16441777348518372, "avg_entropy_nats": 0.39380958676338196, "avg_entropy_bits": 0.5681471824645996}
{"generated_text": "The traditional methods to identifying important attributes include using linguistically motivated patterns, Bayes model, part-of-speech tagging patterns, frequency metrics, and ranking process.", "avg_logp": -0.31156229972839355, "avg_entropy_nats": 0.7143077254295349, "avg_entropy_bits": 1.03052818775177}
{"generated_text": "The traditional methods to identifying important attributes include using linguistically motivated patterns, Bayes model, part-of-speech tagging patterns, frequency metrics, and ranking process.", "avg_logp": -0.31156229972839355, "avg_entropy_nats": 0.7143077254295349, "avg_entropy_bits": 1.03052818775177}
{"generated_text": "FastText.", "avg_logp": -0.197612002491951, "avg_entropy_nats": 0.528274416923523, "avg_entropy_bits": 0.762138843536377}
{"generated_text": "User-generated text data from product enquiries on the Alibaba.com platform is used.", "avg_logp": -0.3347585201263428, "avg_entropy_nats": 0.6813400983810425, "avg_entropy_bits": 0.9829660058021545}
{"generated_text": "Yes.", "avg_logp": -0.49856337904930115, "avg_entropy_nats": 1.0863924026489258, "avg_entropy_bits": 1.5673329830169678}
{"generated_text": "The Stanford Sentiment Treebank (SST-2) and the Snips dataset.", "avg_logp": -0.16089794039726257, "avg_entropy_nats": 0.33055204153060913, "avg_entropy_bits": 0.4768857955932617}
{"generated_text": "The Stanford Sentiment Treebank (SST-2) and the Snips dataset.", "avg_logp": -0.16089794039726257, "avg_entropy_nats": 0.33055204153060913, "avg_entropy_bits": 0.4768857955932617}
{"generated_text": "unanswerable", "avg_logp": -0.2556111216545105, "avg_entropy_nats": 0.6185582280158997, "avg_entropy_bits": 0.8923909068107605}
{"generated_text": "Wealth, population, democracy, and official development assistance (ODA) are the country-specific drivers of international development rhetoric, with varying relationships and fluctuations observed across different levels of these factors.", "avg_logp": -0.33015182614326477, "avg_entropy_nats": 0.7496678233146667, "avg_entropy_bits": 1.0815420150756836}
{"generated_text": "unanswerable", "avg_logp": -0.18576836585998535, "avg_entropy_nats": 0.4832060933113098, "avg_entropy_bits": 0.6971189975738525}
{"generated_text": "unanswerable", "avg_logp": -0.18576836585998535, "avg_entropy_nats": 0.4832060933113098, "avg_entropy_bits": 0.6971189975738525}
{"generated_text": "The main international development topics that states raise are identified through the application of natural language processing (NLP) to countries' speeches in the UN General Debate, specifically through structural topic models (STMs).", "avg_logp": -0.1840847134590149, "avg_entropy_nats": 0.4031643271446228, "avg_entropy_bits": 0.5816431641578674}
{"generated_text": "The authors present evaluations and insights from QnAMaker usage, including performance metrics and trends in knowledge base creation and usage.", "avg_logp": -0.6976880431175232, "avg_entropy_nats": 1.3726001977920532, "avg_entropy_bits": 1.980243444442749}
{"generated_text": "The conversation layer is a persona-based chit-chat layer that can handle queries like \"hi\", \"thank you\", and \"what's up\" and can be grounded in a specific personality, such as professional, witty, friendly, caring, or enthusiastic.", "avg_logp": -0.22619189321994781, "avg_entropy_nats": 0.5217981338500977, "avg_entropy_bits": 0.7527956366539001}
{"generated_text": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, and QnAMaker WebApp.", "avg_logp": -0.07398518174886703, "avg_entropy_nats": 0.15578855574131012, "avg_entropy_bits": 0.22475537657737732}
{"generated_text": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, and QnAMaker WebApp.", "avg_logp": -0.07398518174886703, "avg_entropy_nats": 0.15578855574131012, "avg_entropy_bits": 0.22475537657737732}
{"generated_text": "unanswerable", "avg_logp": -0.18361395597457886, "avg_entropy_nats": 0.5600239038467407, "avg_entropy_bits": 0.8079437017440796}
{"generated_text": "unanswerable", "avg_logp": -0.18361395597457886, "avg_entropy_nats": 0.5600239038467407, "avg_entropy_bits": 0.8079437017440796}
{"generated_text": "unanswerable", "avg_logp": -0.5303151607513428, "avg_entropy_nats": 0.7971175909042358, "avg_entropy_bits": 1.1499977111816406}
{"generated_text": "The experiments with large-scale features involve tuning a hierarchical phrase-based (HPB) model on NIST MT 2002 and testing on MT 2004 and 2005, and reranking an N-best list from a HPB system with 7491 features.", "avg_logp": -0.18722178041934967, "avg_entropy_nats": 0.41222789883613586, "avg_entropy_bits": 0.5947191119194031}
{"generated_text": "unanswerable", "avg_logp": -0.24580621719360352, "avg_entropy_nats": 0.6382890939712524, "avg_entropy_bits": 0.9208564758300781}
{"generated_text": "unanswerable", "avg_logp": -0.1357317864894867, "avg_entropy_nats": 0.5105221271514893, "avg_entropy_bits": 0.736527681350708}
{"generated_text": "The work is evaluated on a dataset of $\\sim$ 8.7M annotated anonymised user utterances derived from requests across 23 domains.", "avg_logp": -0.20114591717720032, "avg_entropy_nats": 0.4459981620311737, "avg_entropy_bits": 0.6434392929077148}
{"generated_text": "The work is evaluated on a dataset of $\\sim$ 8.7M annotated anonymised user utterances derived from requests across 23 domains.", "avg_logp": -0.20114591717720032, "avg_entropy_nats": 0.4459981620311737, "avg_entropy_bits": 0.6434392929077148}
{"generated_text": "Developing additional performance metrics based on topological properties.", "avg_logp": -0.22906611859798431, "avg_entropy_nats": 0.42934709787368774, "avg_entropy_bits": 0.6194168925285339}
{"generated_text": "Class Membership Tests, Class Distinction Test, and Word Equivalence Test.", "avg_logp": -0.11132664978504181, "avg_entropy_nats": 0.2150222361087799, "avg_entropy_bits": 0.3102114796638489}
{"generated_text": "Class Membership Tests, Class Distinction Test, and Word Equivalence Test.", "avg_logp": -0.11132664978504181, "avg_entropy_nats": 0.2150222361087799, "avg_entropy_bits": 0.3102114796638489}
{"generated_text": "The validation loss consistently increases after about 15 epochs when using less than 50% of the available data.", "avg_logp": -0.13521836698055267, "avg_entropy_nats": 0.29858240485191345, "avg_entropy_bits": 0.430763304233551}
{"generated_text": "Visual and textual multimodality.", "avg_logp": -0.4490200877189636, "avg_entropy_nats": 0.8651002645492554, "avg_entropy_bits": 1.2480758428573608}
{"generated_text": "Visual and textual multimodality.", "avg_logp": -0.4490200877189636, "avg_entropy_nats": 0.8651002645492554, "avg_entropy_bits": 1.2480758428573608}
{"generated_text": "Hasty Student and Impatient Reader are previously reported models.", "avg_logp": -0.18527665734291077, "avg_entropy_nats": 0.4241296350955963, "avg_entropy_bits": 0.6118897795677185}
{"generated_text": "The new model achieves state-of-the-art results compared to other neural models.", "avg_logp": -0.2269587218761444, "avg_entropy_nats": 0.4882161617279053, "avg_entropy_bits": 0.7043470144271851}
{"generated_text": "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then scoring each candidate word from two perspectives: the possibility of the candidate word being a legal word and the rationality of the link that the candidate word directly follows previous segmentation history.", "avg_logp": -0.20533239841461182, "avg_entropy_nats": 0.3847762942314148, "avg_entropy_bits": 0.5551148056983948}
{"generated_text": "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then scoring each candidate word from two perspectives: the possibility of the candidate word being a legal word and the rationality of the link that the candidate word directly follows previous segmentation history.", "avg_logp": -0.20533239841461182, "avg_entropy_nats": 0.3847762942314148, "avg_entropy_bits": 0.5551148056983948}
{"generated_text": "The active learning model works by employing a CRF-based segmenter as the learning engine and a scoring model based on information entropy and neural network as the selection engine, which selects samples for annotators to relabel and updates the training set to continuously improve the accuracy of the classifier.", "avg_logp": -0.20541204512119293, "avg_entropy_nats": 0.4560498595237732, "avg_entropy_bits": 0.6579408645629883}
{"generated_text": "Gated neural network, LSTM, and CRF are employed.", "avg_logp": -0.4278216063976288, "avg_entropy_nats": 0.8791516423225403, "avg_entropy_bits": 1.2683477401733398}
{"generated_text": "script knowledge guides the expectation of the reader, supports coreference resolution, common-sense knowledge inference, and enables the embedding of the current sentence into the larger context.", "avg_logp": -0.12167501449584961, "avg_entropy_nats": 0.24323216080665588, "avg_entropy_bits": 0.3509098291397095}
{"generated_text": "The annotators' inter-annotator agreement was found to be sufficiently high, with 90.5% of coreference chain pairs annotated by at least 3 annotators.", "avg_logp": -0.19150273501873016, "avg_entropy_nats": 0.43102389574050903, "avg_entropy_bits": 0.6218360066413879}
{"generated_text": "The annotators' inter-annotator agreement was found to be sufficiently high, with 90.5% of coreference chain pairs annotated by at least 3 annotators.", "avg_logp": -0.19150273501873016, "avg_entropy_nats": 0.43102389574050903, "avg_entropy_bits": 0.6218360066413879}
{"generated_text": "4 undergraduate students of computational linguistics.", "avg_logp": -0.26514267921447754, "avg_entropy_nats": 0.5588463544845581, "avg_entropy_bits": 0.8062449097633362}
{"generated_text": "WN18 and YAGO3-10.", "avg_logp": -0.060299139469861984, "avg_entropy_nats": 0.1863764375448227, "avg_entropy_bits": 0.2688843607902527}
{"generated_text": "WN18 and YAGO3-10.", "avg_logp": -0.060299139469861984, "avg_entropy_nats": 0.1863764375448227, "avg_entropy_bits": 0.2688843607902527}
{"generated_text": "The approach is used to detect incorrect facts by identifying the triple that results in the least change in the model's prediction when removed from the graph.", "avg_logp": -0.1534704566001892, "avg_entropy_nats": 0.40240493416786194, "avg_entropy_bits": 0.5805476307868958}
{"generated_text": "unanswerable", "avg_logp": -0.28928735852241516, "avg_entropy_nats": 0.5290255546569824, "avg_entropy_bits": 0.7632224559783936}
{"generated_text": "The proposed model has several advantages, including the ability to jointly model words in documents as arising from a mixture of topics, account for annotators' biases and different levels of expertise, and scale to large datasets using an efficient stochastic variational inference algorithm.", "avg_logp": -0.15296949446201324, "avg_entropy_nats": 0.3588802218437195, "avg_entropy_bits": 0.5177547931671143}
{"generated_text": "Supervised topic models are the state-of-the-art approaches for predicting target variables associated with complex high-dimensional data.", "avg_logp": -0.08281783759593964, "avg_entropy_nats": 0.18488365411758423, "avg_entropy_bits": 0.26673075556755066}
{"generated_text": "The 20-Newsgroups, Reuters-21578, and LabelMe datasets were used.", "avg_logp": -0.0863635465502739, "avg_entropy_nats": 0.202249675989151, "avg_entropy_bits": 0.2917846143245697}
{"generated_text": "The 20-Newsgroups, Reuters-21578, and LabelMe datasets were used.", "avg_logp": -0.0863635465502739, "avg_entropy_nats": 0.202249675989151, "avg_entropy_bits": 0.2917846143245697}
{"generated_text": "The dataset was collected through a human-to-human dialogue setting, where 90 well-trained workers were paired to converse according to given goals in a synchronous chatroom setting.", "avg_logp": -0.24188944697380066, "avg_entropy_nats": 0.5690367221832275, "avg_entropy_bits": 0.8209463953971863}
{"generated_text": "The dataset was collected through a human-to-human dialogue setting, where 90 well-trained workers were paired to converse according to given goals in a synchronous chatroom setting.", "avg_logp": -0.24188944697380066, "avg_entropy_nats": 0.5690367221832275, "avg_entropy_bits": 0.8209463953971863}
{"generated_text": "Benchmark models are provided for different components of a pipelined task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG).", "avg_logp": -0.07039620727300644, "avg_entropy_nats": 0.141017884016037, "avg_entropy_bits": 0.20344580709934235}
{"generated_text": "The corpus was annotated using a combination of automatic rules and manual annotation by three experts, with a high quality of annotation achieved, as shown by the average dialogue act F1 of 95.36% and average state accuracy of 94.95%.", "avg_logp": -0.3173219859600067, "avg_entropy_nats": 0.6397581100463867, "avg_entropy_bits": 0.9229758977890015}
{"generated_text": "Attentive Mimicking.", "avg_logp": -0.05744902789592743, "avg_entropy_nats": 0.23159153759479523, "avg_entropy_bits": 0.3341159522533417}
{"generated_text": "An absolute improvement of up to 24% over a BERT baseline.", "avg_logp": -0.1582796573638916, "avg_entropy_nats": 0.3596617877483368, "avg_entropy_bits": 0.5188822746276855}
{"generated_text": "MNLI, AG's News, and DBPedia.", "avg_logp": -0.009350212290883064, "avg_entropy_nats": 0.05004594475030899, "avg_entropy_bits": 0.07220103591680527}
{"generated_text": "MNLI, AG's News, and DBPedia.", "avg_logp": -0.009350212290883064, "avg_entropy_nats": 0.05004594475030899, "avg_entropy_bits": 0.07220103591680527}
{"generated_text": "WNLaMPro dataset.", "avg_logp": -0.1354963630437851, "avg_entropy_nats": 0.2861872911453247, "avg_entropy_bits": 0.4128809869289398}
{"generated_text": "The model achieves the state-of-the-art results compared to baselines.", "avg_logp": -0.24992398917675018, "avg_entropy_nats": 0.5324183702468872, "avg_entropy_bits": 0.7681173086166382}
{"generated_text": "The article does not provide a specific numerical value for the performance difference between this method and the baseline, but it mentions that the proposed method achieves the state-of-the-art results and outperforms previous methods on several datasets.", "avg_logp": -0.3184739053249359, "avg_entropy_nats": 0.6915310621261597, "avg_entropy_bits": 0.9976683855056763}
{"generated_text": "AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI.", "avg_logp": -0.08461424708366394, "avg_entropy_nats": 0.1695443093776703, "avg_entropy_bits": 0.24460074305534363}
{"generated_text": "AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI.", "avg_logp": -0.08461424708366394, "avg_entropy_nats": 0.1695443093776703, "avg_entropy_bits": 0.24460074305534363}
{"generated_text": "The mentioned cues are contextual words and feature values such as the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context, etc.", "avg_logp": -0.18809008598327637, "avg_entropy_nats": 0.39382490515708923, "avg_entropy_bits": 0.5681692361831665}
{"generated_text": "The author's work ranked among the top submissions, with reinforcement learning approaches receiving good human evaluation results, and in two batches, they were the best.", "avg_logp": -0.43407267332077026, "avg_entropy_nats": 1.0319061279296875, "avg_entropy_bits": 1.4887256622314453}
{"generated_text": "Classification and regression approaches, as well as deep learning models.", "avg_logp": -0.39434587955474854, "avg_entropy_nats": 0.8251240849494934, "avg_entropy_bits": 1.1904025077819824}
{"generated_text": "Classification and regression approaches, as well as deep learning models.", "avg_logp": -0.39434587955474854, "avg_entropy_nats": 0.8251240849494934, "avg_entropy_bits": 1.1904025077819824}
{"generated_text": "The classification approaches experimented for this task include using a Support Vector Classification (SVC) to predict the target label (\"summary\" or \"not summary\") of the input sentence.", "avg_logp": -0.22167082130908966, "avg_entropy_nats": 0.451395720243454, "avg_entropy_bits": 0.6512263417243958}
{"generated_text": "Yes.", "avg_logp": -0.24427597224712372, "avg_entropy_nats": 0.4813326299190521, "avg_entropy_bits": 0.6944161653518677}
{"generated_text": "missing values, language-specific attributes, and multiword expressions.", "avg_logp": -0.12815789878368378, "avg_entropy_nats": 0.31366437673568726, "avg_entropy_bits": 0.45252206921577454}
{"generated_text": "yes", "avg_logp": -0.9379690885543823, "avg_entropy_nats": 1.5935537815093994, "avg_entropy_bits": 2.2990119457244873}
{"generated_text": "Yes.", "avg_logp": -0.5079699754714966, "avg_entropy_nats": 0.8672233819961548, "avg_entropy_bits": 1.2511389255523682}
{"generated_text": "They validate on 31 languages with both UD and UniMorph data.", "avg_logp": -0.13311298191547394, "avg_entropy_nats": 0.3243637979030609, "avg_entropy_bits": 0.4679580330848694}
{"generated_text": "They validate on 31 languages with both UD and UniMorph data.", "avg_logp": -0.13311298191547394, "avg_entropy_nats": 0.3243637979030609, "avg_entropy_bits": 0.4679580330848694}
{"generated_text": "The paper does not explicitly mention any adjustments to improve the prediction accuracy of face and audio features, but it does suggest that personalization of the recognition algorithm (e.g., mean and standard deviation normalization) could help to adapt the classification for specific speakers and reduce bias in the acoustic signal.", "avg_logp": -0.24688532948493958, "avg_entropy_nats": 0.6000453233718872, "avg_entropy_bits": 0.8656823635101318}
{"generated_text": "The face and audio data analysis is evaluated using confusion matrices and F1 scores, with macro-averaged F1 scores of 33% for facial expressions and 29% for audio signals.", "avg_logp": -0.20407512784004211, "avg_entropy_nats": 0.5021974444389343, "avg_entropy_bits": 0.724517822265625}
{"generated_text": "The baseline model is a neural network with an embedding layer, a bidirectional LSTM, and two dense layers followed by a soft max output layer, inspired by a previous study.", "avg_logp": -0.1957641988992691, "avg_entropy_nats": 0.40727394819259644, "avg_entropy_bits": 0.5875720977783203}
{"generated_text": "Off-the-shelf tools are used for emotion recognition from facial expressions and audio signals.", "avg_logp": -0.27781641483306885, "avg_entropy_nats": 0.5515537261962891, "avg_entropy_bits": 0.7957239151000977}
{"generated_text": "Off-the-shelf tools are used for emotion recognition from facial expressions and audio signals.", "avg_logp": -0.27781641483306885, "avg_entropy_nats": 0.5515537261962891, "avg_entropy_bits": 0.7957239151000977}
{"generated_text": "30k and 90k subwords.", "avg_logp": -0.2944084107875824, "avg_entropy_nats": 0.6389676928520203, "avg_entropy_bits": 0.9218355417251587}
{"generated_text": "30k and 90k subwords.", "avg_logp": -0.2944084107875824, "avg_entropy_nats": 0.6389676928520203, "avg_entropy_bits": 0.9218355417251587}
{"generated_text": "Their experimental results in the low-resource dataset showed that the optimized NMT system outperformed the PBSMT system across all data settings, achieving a BLEU score of 9.4 in the ultra-low data condition and 32.8 in the full IWSLT 14 training corpus.", "avg_logp": -0.16355286538600922, "avg_entropy_nats": 0.4033178687095642, "avg_entropy_bits": 0.5818646550178528}
{"generated_text": "PBSMT and the baseline NMT system.", "avg_logp": -0.3283614218235016, "avg_entropy_nats": 0.75033038854599, "avg_entropy_bits": 1.0824979543685913}
{"generated_text": "Lack of system adaptation to low-resource settings, using hyperparameters typical for high-resource settings, and not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions.", "avg_logp": -0.22148503363132477, "avg_entropy_nats": 0.5430915355682373, "avg_entropy_bits": 0.783515453338623}
{"generated_text": "unanswerable", "avg_logp": -0.3270080089569092, "avg_entropy_nats": 0.621293306350708, "avg_entropy_bits": 0.8963367938995361}
{"generated_text": "unanswerable", "avg_logp": -0.3270080089569092, "avg_entropy_nats": 0.621293306350708, "avg_entropy_bits": 0.8963367938995361}
{"generated_text": "unanswerable", "avg_logp": -0.3389510214328766, "avg_entropy_nats": 0.6902509927749634, "avg_entropy_bits": 0.9958215951919556}
{"generated_text": "The Penelope semantic frame extractor is used for text analysis and mining, specifically to extract causation frames from the corpus of comments on The Guardian.", "avg_logp": -0.2709779143333435, "avg_entropy_nats": 0.5679713487625122, "avg_entropy_bits": 0.8194094300270081}
{"generated_text": "Causal mapping methods employed include the use of cognitive mapping, which involves graphically representing networks of causes and effects, and the Penelope semantic frame extractor, which automatically mines cause and effect concepts from texts using semantic frames.", "avg_logp": -0.31946665048599243, "avg_entropy_nats": 0.6947410106658936, "avg_entropy_bits": 1.0022993087768555}
{"generated_text": "Ternary Trans-CNN model.", "avg_logp": -0.2404668629169464, "avg_entropy_nats": 0.41999682784080505, "avg_entropy_bits": 0.6059273481369019}
{"generated_text": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.", "avg_logp": -0.16317664086818695, "avg_entropy_nats": 0.39709386229515076, "avg_entropy_bits": 0.5728853940963745}
{"generated_text": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.", "avg_logp": -0.16317664086818695, "avg_entropy_nats": 0.39709386229515076, "avg_entropy_bits": 0.5728853940963745}
{"generated_text": "3189 rows of text messages with an average length of 116 words.", "avg_logp": -0.1613224893808365, "avg_entropy_nats": 0.33846962451934814, "avg_entropy_bits": 0.4883084297180176}
{"generated_text": "3189 rows of text messages with an average length of 116 words.", "avg_logp": -0.1613224893808365, "avg_entropy_nats": 0.33846962451934814, "avg_entropy_bits": 0.4883084297180176}
{"generated_text": "The dataset is collected from a labelled dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study conducted by Davidson et al.", "avg_logp": -0.28160446882247925, "avg_entropy_nats": 0.6207977533340454, "avg_entropy_bits": 0.8956218361854553}
{"generated_text": "No, the text augmentation techniques (Synonym Replacement, Random Insertion, Random Swap, and Random Deletion) were used together, not individually.", "avg_logp": -0.22843725979328156, "avg_entropy_nats": 0.5667268633842468, "avg_entropy_bits": 0.8176140189170837}
{"generated_text": "Transfer Learning on a Ternary Trans-CNN model, a Hybrid multi-channel CNN and LSTM model, and a Fully Connected dense network.", "avg_logp": -0.3001002073287964, "avg_entropy_nats": 0.6640013456344604, "avg_entropy_bits": 0.9579513669013977}
{"generated_text": "Yes.", "avg_logp": -0.5126340389251709, "avg_entropy_nats": 1.0843942165374756, "avg_entropy_bits": 1.5644501447677612}
{"generated_text": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.", "avg_logp": -0.16317664086818695, "avg_entropy_nats": 0.39709386229515076, "avg_entropy_bits": 0.5728853940963745}
{"generated_text": "They demonstrate this by showing that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool, even with a large parallel corpus.", "avg_logp": -0.1624981313943863, "avg_entropy_nats": 0.34825411438941956, "avg_entropy_bits": 0.5024245381355286}
{"generated_text": "They demonstrate this by showing that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool, even with a large parallel corpus.", "avg_logp": -0.1624981313943863, "avg_entropy_nats": 0.34825411438941956, "avg_entropy_bits": 0.5024245381355286}
{"generated_text": "The language-specific and language-neutral components are not disjunctive, as the authors assume that the language-specific component is similar across all sentences in the language.", "avg_logp": -0.2666872441768646, "avg_entropy_nats": 0.5983642339706421, "avg_entropy_bits": 0.863257110118866}
{"generated_text": "They hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way.", "avg_logp": -0.05751682072877884, "avg_entropy_nats": 0.1211487427353859, "avg_entropy_bits": 0.1747806966304779}
{"generated_text": "The challenge of removing the language-specific information from the representations while preserving the semantic cross-lingual properties.", "avg_logp": -0.6008583307266235, "avg_entropy_nats": 1.2346689701080322, "avg_entropy_bits": 1.7812507152557373}
{"generated_text": "They propose two approaches to defining and evaluating faithfulness: evaluating faithfulness across models and tasks, and evaluating faithfulness across input space.", "avg_logp": -0.23563215136528015, "avg_entropy_nats": 0.5534736514091492, "avg_entropy_bits": 0.7984936833381653}
{"generated_text": "They propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful, in practice.", "avg_logp": -0.1685137003660202, "avg_entropy_nats": 0.44163841009140015, "avg_entropy_bits": 0.6371495127677917}
{"generated_text": "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption.", "avg_logp": -0.10969708114862442, "avg_entropy_nats": 0.2230944186449051, "avg_entropy_bits": 0.321857213973999}
{"generated_text": "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption.", "avg_logp": -0.10969708114862442, "avg_entropy_nats": 0.2230944186449051, "avg_entropy_bits": 0.321857213973999}
{"generated_text": "The key points in guidelines for faithfulness evaluation are: \n\n1. Be explicit in what you evaluate, distinguishing between plausability and faithfulness.\n2. Faithfulness evaluation should not involve human-judgement on the quality of interpretation.\n3. Faithfulness evaluation should not involve human-provided gold labels.\n4. Do not trust “inherent interpretability” claims.\n5. Faithfulness evaluation of IUI systems should not rely on user performance.", "avg_logp": -0.09372993558645248, "avg_entropy_nats": 0.22499337792396545, "avg_entropy_bits": 0.3245968222618103}
{"generated_text": "yes", "avg_logp": -0.5551418662071228, "avg_entropy_nats": 1.2495388984680176, "avg_entropy_bits": 1.802703619003296}
{"generated_text": "unanswerable", "avg_logp": -0.19551140069961548, "avg_entropy_nats": 0.545181930065155, "avg_entropy_bits": 0.7865312695503235}
{"generated_text": "unanswerable", "avg_logp": -0.19551140069961548, "avg_entropy_nats": 0.545181930065155, "avg_entropy_bits": 0.7865312695503235}
{"generated_text": "Two.", "avg_logp": -0.8847836852073669, "avg_entropy_nats": 1.766035556793213, "avg_entropy_bits": 2.5478506088256836}
{"generated_text": "unanswerable", "avg_logp": -0.040417127311229706, "avg_entropy_nats": 0.1846109926700592, "avg_entropy_bits": 0.2663373649120331}
{"generated_text": "unanswerable", "avg_logp": -0.4911540150642395, "avg_entropy_nats": 0.9908304810523987, "avg_entropy_bits": 1.4294661283493042}
{"generated_text": "The adaptation model achieves a 1.73% improvement in EM score and 0.08% improvement in F1 score over the baseline model.", "avg_logp": -0.28889065980911255, "avg_entropy_nats": 0.7050637006759644, "avg_entropy_bits": 1.017191767692566}
{"generated_text": "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components.", "avg_logp": -0.09595497697591782, "avg_entropy_nats": 0.20324918627738953, "avg_entropy_bits": 0.29322659969329834}
{"generated_text": "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components.", "avg_logp": -0.09595497697591782, "avg_entropy_nats": 0.20324918627738953, "avg_entropy_bits": 0.29322659969329834}
{"generated_text": "The exact performance on SQuAD is 68.73% EM score and 77.39% F1 score.", "avg_logp": -0.11637257039546967, "avg_entropy_nats": 0.33899080753326416, "avg_entropy_bits": 0.4890604317188263}
{"generated_text": "The correlation results are high or very high for all quality scores in all datasets, apart from Q2 in DUC-05 where it is only moderate.", "avg_logp": -0.19653981924057007, "avg_entropy_nats": 0.4547441601753235, "avg_entropy_bits": 0.6560571193695068}
{"generated_text": "The datasets used in the experiments are from the NIST DUC shared tasks, specifically DUC-05, DUC-06, and DUC-07.", "avg_logp": -0.14802202582359314, "avg_entropy_nats": 0.31218063831329346, "avg_entropy_bits": 0.45038142800331116}
{"generated_text": "BiGRU with attention and ROUGE.", "avg_logp": -0.29941505193710327, "avg_entropy_nats": 0.6489481925964355, "avg_entropy_bits": 0.9362342953681946}
{"generated_text": "BiGRU with attention and ROUGE.", "avg_logp": -0.29941505193710327, "avg_entropy_nats": 0.6489481925964355, "avg_entropy_bits": 0.9362342953681946}
{"generated_text": "The five linguistic quality criteria addressed are given in Figure FIGREF2, which are: $\\mathcal {Q}1$ (Grammaticality), $\\mathcal {Q}2$ (Relevance), $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus), and $\\mathcal {Q}5$ (Structure & Coherence).", "avg_logp": -0.11725659668445587, "avg_entropy_nats": 0.28424084186553955, "avg_entropy_bits": 0.41007277369499207}
{"generated_text": "WN18RR, FB15k-237, and YAGO3-10.", "avg_logp": -0.016875941306352615, "avg_entropy_nats": 0.05813780054450035, "avg_entropy_bits": 0.08387511223554611}
{"generated_text": "WN18RR, FB15k-237, and YAGO3-10.", "avg_logp": -0.016875941306352615, "avg_entropy_nats": 0.05813780054450035, "avg_entropy_bits": 0.08387511223554611}
{"generated_text": "TransE, DistMult, ComplEx, ConvE, and RotatE.", "avg_logp": -0.12098387628793716, "avg_entropy_nats": 0.3137536942958832, "avg_entropy_bits": 0.45265087485313416}
{"generated_text": "HAKE significantly and consistently outperforms several state-of-the-art methods on the benchmark datasets.", "avg_logp": -0.11214860528707504, "avg_entropy_nats": 0.2096528559923172, "avg_entropy_bits": 0.30246514081954956}
{"generated_text": "Entities are mapped onto the polar coordinate system by combining the modulus part and the phase part, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information.", "avg_logp": -0.06901498883962631, "avg_entropy_nats": 0.21365045011043549, "avg_entropy_bits": 0.3082324266433716}
{"generated_text": "phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficially incorporated in the future.", "avg_logp": -0.2862187325954437, "avg_entropy_nats": 0.5889054536819458, "avg_entropy_bits": 0.8496109247207642}
{"generated_text": "phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficially incorporated in the future.", "avg_logp": -0.2862187325954437, "avg_entropy_nats": 0.5889054536819458, "avg_entropy_bits": 0.8496109247207642}
{"generated_text": "The training parallel corpus used in the study contains 18805 lines of annotated code.", "avg_logp": -0.32784515619277954, "avg_entropy_nats": 0.6698904037475586, "avg_entropy_bits": 0.966447651386261}
{"generated_text": "The training parallel corpus used in the study contains 18805 lines of annotated code.", "avg_logp": -0.32784515619277954, "avg_entropy_nats": 0.6698904037475586, "avg_entropy_bits": 0.966447651386261}
{"generated_text": "unanswerable", "avg_logp": -0.17091424763202667, "avg_entropy_nats": 0.5263018608093262, "avg_entropy_bits": 0.7592930793762207}
{"generated_text": "The architecture of the system is a neural network based machine translation model using a Long Short-Term Memory (LSTM) model with an encoder layer, a decoder layer, and an output layer.", "avg_logp": -0.2216980755329132, "avg_entropy_nats": 0.5083494186401367, "avg_entropy_bits": 0.7333933115005493}
{"generated_text": "finite and small.", "avg_logp": -0.5250360369682312, "avg_entropy_nats": 1.0639214515686035, "avg_entropy_bits": 1.5349140167236328}
{"generated_text": "phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficial.", "avg_logp": -0.16231316328048706, "avg_entropy_nats": 0.3719741106033325, "avg_entropy_bits": 0.5366451740264893}
{"generated_text": "Python.", "avg_logp": -0.09405092895030975, "avg_entropy_nats": 0.3281240165233612, "avg_entropy_bits": 0.4733828902244568}
{"generated_text": "The validation data from the source corpus.", "avg_logp": -0.2668417990207672, "avg_entropy_nats": 0.5934566855430603, "avg_entropy_bits": 0.856177031993866}
{"generated_text": "The text-to-image synthesis is trained in a supervised manner in the early stages of research, but later generative models are trained in an unsupervised manner.", "avg_logp": -0.3734864592552185, "avg_entropy_nats": 0.9650997519493103, "avg_entropy_bits": 1.3923447132110596}
{"generated_text": "The text-to-image synthesis is trained in a supervised manner in the early stages of research, but later generative models are trained in an unsupervised manner.", "avg_logp": -0.3734864592552185, "avg_entropy_nats": 0.9650997519493103, "avg_entropy_bits": 1.3923447132110596}
{"generated_text": "unanswerable", "avg_logp": -0.4924585223197937, "avg_entropy_nats": 1.2779572010040283, "avg_entropy_bits": 1.8437025547027588}
{"generated_text": "unanswerable", "avg_logp": -0.21364998817443848, "avg_entropy_nats": 0.5916295051574707, "avg_entropy_bits": 0.8535408973693848}
{"generated_text": "The scalar gate mechanism.", "avg_logp": -0.8020150065422058, "avg_entropy_nats": 1.6127647161483765, "avg_entropy_bits": 2.3267273902893066}
{"generated_text": "A BiLSTM with max pooling.", "avg_logp": -0.20654140412807465, "avg_entropy_nats": 0.4027058184146881, "avg_entropy_bits": 0.5809816718101501}
{"generated_text": "They evaluate on 11 downstream transfer tasks, including sentiment classification, subjectivity classification, question-type classification, recognizing textual entailment, estimating semantic relatedness, and measuring textual semantic similarity.", "avg_logp": -0.09759917110204697, "avg_entropy_nats": 0.3044184446334839, "avg_entropy_bits": 0.43918293714523315}
{"generated_text": "WS353, WS353R, WS353S, SimLex999, SimVerb3500, MEN, MTurk287, MTurk771, RG, RW.", "avg_logp": -0.09588893502950668, "avg_entropy_nats": 0.17681251466274261, "avg_entropy_bits": 0.2550865113735199}
{"generated_text": "WS353, WS353R, WS353S, SimLex999, SimVerb3500, MEN, MTurk287, MTurk771, RG, RW.", "avg_logp": -0.09588893502950668, "avg_entropy_nats": 0.17681251466274261, "avg_entropy_bits": 0.2550865113735199}
{"generated_text": "Yes, the New York Times (NYT) corpus is used for experiments, with two versions (NYT24 and NYT29) having 24 and 29 relations respectively.", "avg_logp": -0.25333529710769653, "avg_entropy_nats": 0.5480151772499084, "avg_entropy_bits": 0.7906188368797302}
{"generated_text": "WordDecoding (WDec) model performed better in experiments, achieving F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively.", "avg_logp": -0.13355597853660583, "avg_entropy_nats": 0.27829161286354065, "avg_entropy_bits": 0.40148991346359253}
{"generated_text": "Previous work authors referred to are BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF9, BIBREF11, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, BIBREF36, and BIBREF37.", "avg_logp": -0.04702737554907799, "avg_entropy_nats": 0.12169448286294937, "avg_entropy_bits": 0.17556804418563843}
{"generated_text": "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively.", "avg_logp": -0.043841052800416946, "avg_entropy_nats": 0.11753436177968979, "avg_entropy_bits": 0.16956624388694763}
{"generated_text": "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively.", "avg_logp": -0.043841052800416946, "avg_entropy_nats": 0.11753436177968979, "avg_entropy_bits": 0.16956624388694763}
{"generated_text": "The baselines were: (1) Rank by the number of times a citation is mentioned in the document, (2) Rank by the number of times the citation is cited in the literature (citation impact), (3) Rank using Google Scholar Related Articles, (4) Rank by the TF*IDF weighted cosine similarity, and (5) Rank using a learning-to-rank model trained on text similarity rankings.", "avg_logp": -0.04640358313918114, "avg_entropy_nats": 0.10967710614204407, "avg_entropy_bits": 0.15823063254356384}
{"generated_text": "The baselines were: (1) Rank by the number of times a citation is mentioned in the document, (2) Rank by the number of times the citation is cited in the literature (citation impact), (3) Rank using Google Scholar Related Articles, (4) Rank by the TF*IDF weighted cosine similarity, and (5) Rank using a learning-to-rank model trained on text similarity rankings.", "avg_logp": -0.04640358313918114, "avg_entropy_nats": 0.10967710614204407, "avg_entropy_bits": 0.15823063254356384}
{"generated_text": "They developed a supervised model using SVMRank, a point-wise learning-to-rank algorithm, to reorder the results returned from a search engine query.", "avg_logp": -0.2812252938747406, "avg_entropy_nats": 0.6215812563896179, "avg_entropy_bits": 0.8967520594596863}
{"generated_text": "unanswerable", "avg_logp": -0.2615034878253937, "avg_entropy_nats": 0.6484200954437256, "avg_entropy_bits": 0.9354723691940308}
{"generated_text": "Amazon Mechanical Turk (AMT) and Google Adwords (GA) are mentioned as crowdsourcing platforms used in the article.", "avg_logp": -0.24895481765270233, "avg_entropy_nats": 0.5888595581054688, "avg_entropy_bits": 0.8495447635650635}
{"generated_text": "CNN outperformed autoencoders and MLP with the highest achieved accuracy of 82.6%.", "avg_logp": -0.09315912425518036, "avg_entropy_nats": 0.25373438000679016, "avg_entropy_bits": 0.3660613000392914}
{"generated_text": "CNN outperformed autoencoders and MLP with the highest achieved accuracy of 82.6%.", "avg_logp": -0.09315912425518036, "avg_entropy_nats": 0.25373438000679016, "avg_entropy_bits": 0.3660613000392914}
{"generated_text": "unanswerable", "avg_logp": -0.2802122235298157, "avg_entropy_nats": 0.691608190536499, "avg_entropy_bits": 0.9977796077728271}
{"generated_text": "The proposed CNN model achieved the highest accuracy of 82.6%.", "avg_logp": -0.20291796326637268, "avg_entropy_nats": 0.4331910312175751, "avg_entropy_bits": 0.6249625086784363}
{"generated_text": "The dataset used in this work was collected manually and includes Persian movie reviews from 2014 to 2016, with a total of 60% used for training and the rest (40%) for testing and validation.", "avg_logp": -0.22546999156475067, "avg_entropy_nats": 0.5130776166915894, "avg_entropy_bits": 0.7402145862579346}
{"generated_text": "Yes.", "avg_logp": -0.24930620193481445, "avg_entropy_nats": 0.5044436454772949, "avg_entropy_bits": 0.7277583479881287}
{"generated_text": "Yes.", "avg_logp": -0.24930620193481445, "avg_entropy_nats": 0.5044436454772949, "avg_entropy_bits": 0.7277583479881287}
{"generated_text": "The dataset was collected through crowd-sourcing on Amazon Mechanical Turk, using the ParlAI interface to render 360 images and display neighborhood maps.", "avg_logp": -0.2109995186328888, "avg_entropy_nats": 0.4783467948436737, "avg_entropy_bits": 0.690108597278595}
{"generated_text": "natural language.", "avg_logp": -0.22367605566978455, "avg_entropy_nats": 0.4718914031982422, "avg_entropy_bits": 0.680795431137085}
{"generated_text": "The authors looked at F1 scores, accuracy, and localization performance metrics, including mean predicted location.", "avg_logp": -0.4734276831150055, "avg_entropy_nats": 1.0916922092437744, "avg_entropy_bits": 1.5749791860580444}
{"generated_text": "The Talk The Walk dataset, which consists of over 10k successful dialogues collected from Amazon Mechanical Turk.", "avg_logp": -0.19782471656799316, "avg_entropy_nats": 0.4554382562637329, "avg_entropy_bits": 0.6570584774017334}
{"generated_text": "unanswerable", "avg_logp": -0.2570531964302063, "avg_entropy_nats": 0.5929543375968933, "avg_entropy_bits": 0.855452299118042}
{"generated_text": "unanswerable", "avg_logp": -0.2570531964302063, "avg_entropy_nats": 0.5929543375968933, "avg_entropy_bits": 0.855452299118042}
{"generated_text": "The accuracy of the system is measured by a formula that rewards clustering which matches the article clustering and the total number of claims clustered, with parameters to be tuned.", "avg_logp": -0.19757802784442902, "avg_entropy_nats": 0.5233861804008484, "avg_entropy_bits": 0.7550866603851318}
{"generated_text": "The incoming claim is used to retrieve similar factchecked claims by running a Louvain Community Detection over a graph of all claims, where the connections between them are added when the distance between them is determined to be less than a certain threshold.", "avg_logp": -0.24654828011989594, "avg_entropy_nats": 0.5504024624824524, "avg_entropy_bits": 0.7940628528594971}
{"generated_text": "The Quora duplicate question dataset.", "avg_logp": -0.09465321898460388, "avg_entropy_nats": 0.275967001914978, "avg_entropy_bits": 0.3981361985206604}
{"generated_text": "The factchecking algorithm consists of claim detection, claim clustering, and automated retrieval of existing factchecks.", "avg_logp": -0.28967753052711487, "avg_entropy_nats": 0.7075429558753967, "avg_entropy_bits": 1.020768642425537}
{"generated_text": "The baseline is a simple, transparent, and interpretable model called Path Ranking-based Knowledge Graph Completion (PRKGC).", "avg_logp": -0.19200381636619568, "avg_entropy_nats": 0.4676768481731415, "avg_entropy_bits": 0.6747150421142578}
{"generated_text": "WikiHop.", "avg_logp": -0.10334810614585876, "avg_entropy_nats": 0.2761240601539612, "avg_entropy_bits": 0.39836281538009644}
{"generated_text": "Yes.", "avg_logp": -0.24010837078094482, "avg_entropy_nats": 0.44676733016967773, "avg_entropy_bits": 0.6445490121841431}
{"generated_text": "Yes.", "avg_logp": -0.24010837078094482, "avg_entropy_nats": 0.44676733016967773, "avg_entropy_bits": 0.6445490121841431}
{"generated_text": "The dataset was annotated through a crowdsourcing framework, where workers were asked to judge whether a statement could be derived from given articles, and then select justification explanations for the statement.", "avg_logp": -0.3196585476398468, "avg_entropy_nats": 0.745568573474884, "avg_entropy_bits": 1.0756280422210693}
{"generated_text": "The proposed dataset is derived from WikiHop, an entity-based multi-hop QA dataset.", "avg_logp": -0.22957584261894226, "avg_entropy_nats": 0.46594005823135376, "avg_entropy_bits": 0.6722093224525452}
{"generated_text": "Two labels.", "avg_logp": -0.5952138304710388, "avg_entropy_nats": 1.340498924255371, "avg_entropy_bits": 1.9339311122894287}
{"generated_text": "unanswerable", "avg_logp": -0.16891539096832275, "avg_entropy_nats": 0.4046115279197693, "avg_entropy_bits": 0.5837310552597046}
{"generated_text": "The experts are the political pundits of the Washington Post.", "avg_logp": -0.23852595686912537, "avg_entropy_nats": 0.5046247243881226, "avg_entropy_bits": 0.7280194759368896}
{"generated_text": "The experts are the political pundits of the Washington Post.", "avg_logp": -0.23852595686912537, "avg_entropy_nats": 0.5046247243881226, "avg_entropy_bits": 0.7280194759368896}
{"generated_text": "The crowd in these experiments refers to the users of social media, specifically Twitter, whose sentiments and opinions are being analyzed.", "avg_logp": -0.2525613307952881, "avg_entropy_nats": 0.5722320675849915, "avg_entropy_bits": 0.8255563378334045}
{"generated_text": "The answer is based on the Washington Post's expert opinions.", "avg_logp": -0.509709894657135, "avg_entropy_nats": 1.1632972955703735, "avg_entropy_bits": 1.6782832145690918}
{"generated_text": "The proposed method significantly outperforms state-of-the-art methods in experiments.", "avg_logp": -0.10196244716644287, "avg_entropy_nats": 0.31036636233329773, "avg_entropy_bits": 0.44776397943496704}
{"generated_text": "Further analysis is done using t-SNE tool to visualize the learned embedding and test accuracy with increasing epoch, embedding size, and training data proportions.", "avg_logp": -0.24001993238925934, "avg_entropy_nats": 0.5181124806404114, "avg_entropy_bits": 0.7474783062934875}
{"generated_text": "TransE, TransR, TransH, PTransE, ALL-PATHS, R-GCN, and KR-EAR.", "avg_logp": -0.05084963142871857, "avg_entropy_nats": 0.15451443195343018, "avg_entropy_bits": 0.22291722893714905}
{"generated_text": "DBP24K, Game30K, and FB24K.", "avg_logp": -0.04437142610549927, "avg_entropy_nats": 0.17363795638084412, "avg_entropy_bits": 0.25050660967826843}
{"generated_text": "DBP24K, Game30K, and FB24K.", "avg_logp": -0.04437142610549927, "avg_entropy_nats": 0.17363795638084412, "avg_entropy_bits": 0.25050660967826843}
{"generated_text": "KANE captures both high-order structural and attribute information of KGs through recursive embedding propagation based on relation triples and multi-head attention-based aggregation.", "avg_logp": -0.0885913297533989, "avg_entropy_nats": 0.20175538957118988, "avg_entropy_bits": 0.29107150435447693}
{"generated_text": "Recent works on knowledge graph embeddings mentioned include TransH, TransR, and several path-based methods such as PTransE and ALL-PATHS, as well as attribute-incorporated methods like R-GCN and KR-EAR.", "avg_logp": -0.16521787643432617, "avg_entropy_nats": 0.3719139099121094, "avg_entropy_bits": 0.5365583896636963}
{"generated_text": "Yes.", "avg_logp": -0.29575952887535095, "avg_entropy_nats": 0.6511496305465698, "avg_entropy_bits": 0.9394102096557617}
{"generated_text": "Yes.", "avg_logp": -0.6779828071594238, "avg_entropy_nats": 1.1716606616973877, "avg_entropy_bits": 1.6903489828109741}
{"generated_text": "SVM (Support Vector Machine) classifiers.", "avg_logp": -0.2104167938232422, "avg_entropy_nats": 0.4864821135997772, "avg_entropy_bits": 0.7018452882766724}
{"generated_text": "Spelling errors, capitalisation, POS ratios, length, sentiment ratios, and repeated characters.", "avg_logp": -0.3037979304790497, "avg_entropy_nats": 0.7228847146034241, "avg_entropy_bits": 1.0429022312164307}
{"generated_text": "Spelling errors, capitalisation, POS ratios, length, sentiment ratios, and repeated characters.", "avg_logp": -0.3037979304790497, "avg_entropy_nats": 0.7228847146034241, "avg_entropy_bits": 1.0429022312164307}
{"generated_text": "No, the data acquired under hashtag-based supervision is not verified by humans at any stage.", "avg_logp": -0.3244730532169342, "avg_entropy_nats": 0.8202848434448242, "avg_entropy_bits": 1.1834207773208618}
{"generated_text": "The hashtags used for distant supervision are not explicitly mentioned in the article, but it is stated that hashtags used to indicate drunk or sober tweets are removed so that they provide labels, but do not act as features.", "avg_logp": -0.1904289722442627, "avg_entropy_nats": 0.49196895956993103, "avg_entropy_bits": 0.7097612023353577}
{"generated_text": "The authors equate drunk tweeting with drunk texting.", "avg_logp": -0.35202980041503906, "avg_entropy_nats": 0.8300313949584961, "avg_entropy_bits": 1.1974821090698242}
{"generated_text": "The text corpora (S) from aristo2016:combining.", "avg_logp": -0.09853765368461609, "avg_entropy_nats": 0.31860238313674927, "avg_entropy_bits": 0.459646075963974}
{"generated_text": "The text corpora (S) from aristo2016:combining.", "avg_logp": -0.09853765368461609, "avg_entropy_nats": 0.31860238313674927, "avg_entropy_bits": 0.459646075963974}
{"generated_text": "The proposed technique, TupleInf, achieves an accuracy of 61.4% on the 4th grade set and outperforms the state-of-the-art system, TableILP, by 11.8%.", "avg_logp": -0.18733233213424683, "avg_entropy_nats": 0.4257131516933441, "avg_entropy_bits": 0.6141741871833801}
{"generated_text": "The answer is yes.", "avg_logp": -0.34621497988700867, "avg_entropy_nats": 0.7618380784988403, "avg_entropy_bits": 1.0990999937057495}
{"generated_text": "no", "avg_logp": -0.9541540145874023, "avg_entropy_nats": 1.494094967842102, "avg_entropy_bits": 2.1555233001708984}
{"generated_text": "The method used to generate the OpenIE extractions is Open IE v4.", "avg_logp": -0.23211318254470825, "avg_entropy_nats": 0.5403140783309937, "avg_entropy_bits": 0.7795084118843079}
{"generated_text": "The answer is yes.", "avg_logp": -0.3343390226364136, "avg_entropy_nats": 0.7089965343475342, "avg_entropy_bits": 1.0228657722473145}
{"generated_text": "The text corpora (S) from aristo2016:combining.", "avg_logp": -0.1326085478067398, "avg_entropy_nats": 0.3458212614059448, "avg_entropy_bits": 0.49891459941864014}
{"generated_text": "Open IE v4.", "avg_logp": -0.27084678411483765, "avg_entropy_nats": 0.504180371761322, "avg_entropy_bits": 0.7273784875869751}
{"generated_text": "The answer is yes.", "avg_logp": -0.19059890508651733, "avg_entropy_nats": 0.43810147047042847, "avg_entropy_bits": 0.6320468783378601}
{"generated_text": "The primary reason for the dense approach outperforming the sparse one is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.", "avg_logp": -0.0594029501080513, "avg_entropy_nats": 0.1722983568906784, "avg_entropy_bits": 0.24857395887374878}
{"generated_text": "The evaluation is conducted using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation.", "avg_logp": -0.04112972691655159, "avg_entropy_nats": 0.1579877883195877, "avg_entropy_bits": 0.22792820632457733}
{"generated_text": "The synsets are induced in an unsupervised way by the Watset[CWnolog, MCL] method and the semantic relations from dictionaries such as Wiktionary, referred to as Joint Exp SWN.", "avg_logp": -0.17843982577323914, "avg_entropy_nats": 0.450174480676651, "avg_entropy_bits": 0.64946448802948}
{"generated_text": "Cosine similarity.", "avg_logp": -0.18746474385261536, "avg_entropy_nats": 0.37817007303237915, "avg_entropy_bits": 0.5455840229988098}
{"generated_text": "The Lucene retrieval system was used for baselines.", "avg_logp": -0.2973105311393738, "avg_entropy_nats": 0.6723878383636475, "avg_entropy_bits": 0.9700505137443542}
{"generated_text": "The word embeddings used were created by Kyubyong Park and Edouard Grave et al, with two different methods: fastText and word2vec by Kyubyong Park (100-dimensional vector, 10k words) and fastText by Edouard Grave et al (300-dimensional vector, 200k words).", "avg_logp": -0.2165556252002716, "avg_entropy_nats": 0.46957769989967346, "avg_entropy_bits": 0.677457332611084}
{"generated_text": "No extraction, No annotation, Wrong range, Wrong tag, and Wrong range and tag.", "avg_logp": -0.03662116453051567, "avg_entropy_nats": 0.11724049597978592, "avg_entropy_bits": 0.16914226114749908}
{"generated_text": "unanswerable", "avg_logp": -0.35299497842788696, "avg_entropy_nats": 0.7269048690795898, "avg_entropy_bits": 1.0487020015716553}
{"generated_text": "The supplemental tasks used for multitask learning are tasks A and the other auxiliary tasks.", "avg_logp": -0.33070576190948486, "avg_entropy_nats": 0.739014208316803, "avg_entropy_bits": 1.0661721229553223}
{"generated_text": "unanswerable", "avg_logp": -0.2876802980899811, "avg_entropy_nats": 0.7727572321891785, "avg_entropy_bits": 1.114853024482727}
{"generated_text": "unanswerable", "avg_logp": -0.30255773663520813, "avg_entropy_nats": 0.7799937725067139, "avg_entropy_bits": 1.1252930164337158}
{"generated_text": "A strong feature-based method is a system that utilizes rich non-textual features such as answer's profile.", "avg_logp": -0.29958227276802063, "avg_entropy_nats": 0.7703418731689453, "avg_entropy_bits": 1.1113684177398682}
{"generated_text": "The answer is yes.", "avg_logp": -0.3846827745437622, "avg_entropy_nats": 0.9601902961730957, "avg_entropy_bits": 1.3852617740631104}
{"generated_text": "Yes.", "avg_logp": -0.15963605046272278, "avg_entropy_nats": 0.5031638145446777, "avg_entropy_bits": 0.7259118556976318}
{"generated_text": "unanswerable", "avg_logp": -0.2754693627357483, "avg_entropy_nats": 0.7499600052833557, "avg_entropy_bits": 1.0819635391235352}
{"generated_text": "Their model is different from BERT in that it eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words.", "avg_logp": -0.15118493139743805, "avg_entropy_nats": 0.3586607277393341, "avg_entropy_bits": 0.5174380540847778}
{"generated_text": "unanswerable", "avg_logp": -0.17839549481868744, "avg_entropy_nats": 0.6221712827682495, "avg_entropy_bits": 0.8976033926010132}
{"generated_text": "9", "avg_logp": -0.7942432165145874, "avg_entropy_nats": 1.6099662780761719, "avg_entropy_bits": 2.32269024848938}
{"generated_text": "Bag-of-words-based benchmarks, including Classification with TF-IDF + Linear SVM, Classification with Depeche++ Emotion lexicons + Linear SVM, Classification with NRC Emotion lexicons + Linear SVM, and Combination of TF-IDF and NRC Emotion lexicons + Linear SVM.", "avg_logp": -0.10302134603261948, "avg_entropy_nats": 0.19852282106876373, "avg_entropy_bits": 0.2864078879356384}
{"generated_text": "The dataset contains a total of 9710 passages.", "avg_logp": -0.11053697764873505, "avg_entropy_nats": 0.324323832988739, "avg_entropy_bits": 0.4679003953933716}
{"generated_text": "Three unique annotators for each passage, plus in-house annotators who are experts in narrative content.", "avg_logp": -0.3277788758277893, "avg_entropy_nats": 0.6940405368804932, "avg_entropy_bits": 1.001288890838623}
{"generated_text": "unanswerable", "avg_logp": -0.2956143021583557, "avg_entropy_nats": 0.6580034494400024, "avg_entropy_bits": 0.9492983818054199}
{"generated_text": "The syntactic analysis shows that the proposed method enables control of the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them.", "avg_logp": -0.1188996285200119, "avg_entropy_nats": 0.23508240282535553, "avg_entropy_bits": 0.3391522467136383}
{"generated_text": "Fine-grained syntactic analysis.", "avg_logp": -0.15218698978424072, "avg_entropy_nats": 0.3246240019798279, "avg_entropy_bits": 0.4683333933353424}
{"generated_text": "It is demonstrated through a fine-grained syntactic analysis of the translations generated using the method, which shows that the system successfully injects the correct gender and number information.", "avg_logp": -0.2765651047229767, "avg_entropy_nats": 0.5999225378036499, "avg_entropy_bits": 0.8655052185058594}
{"generated_text": "Google's machine translation system (GMT), accessed through its Cloud API.", "avg_logp": -0.09351858496665955, "avg_entropy_nats": 0.2138727903366089, "avg_entropy_bits": 0.30855318903923035}
{"generated_text": "The components of the black-box context injection system include pre-defined textual hints about the gender and number of the speaker and the audience, which are concatenated to a given input sentence.", "avg_logp": -0.1641523391008377, "avg_entropy_nats": 0.3661832809448242, "avg_entropy_bits": 0.5282908082008362}
{"generated_text": "Cepstral mean normalization (CMN), variance with mean normalization (CMVN), and no other normalization techniques are mentioned.", "avg_logp": -0.2536974549293518, "avg_entropy_nats": 0.6109646558761597, "avg_entropy_bits": 0.8814356327056885}
{"generated_text": "They experiment with 40 mel-scaled log filterbank energies (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector), and also try spectrogram features.", "avg_logp": -0.12406998872756958, "avg_entropy_nats": 0.26872310042381287, "avg_entropy_bits": 0.38768547773361206}
{"generated_text": "A 6-layer bidirectional LSTM with 1024 hidden units.", "avg_logp": -0.09870506078004837, "avg_entropy_nats": 0.21888047456741333, "avg_entropy_bits": 0.31577780842781067}
{"generated_text": "Conversational speech.", "avg_logp": -0.17145124077796936, "avg_entropy_nats": 0.3076786398887634, "avg_entropy_bits": 0.4438864588737488}
{"generated_text": "The previous model, MTMSN, used a two-part approach: training a categorical variable to predict the number of spans to extract and generalizing the single-span head method using non-maximum suppression (NMS) algorithm.", "avg_logp": -0.1698545664548874, "avg_entropy_nats": 0.3961345851421356, "avg_entropy_bits": 0.5715014338493347}
{"generated_text": "They use sequence tagging with a categorical tag to predict the number of spans to extract and then apply non-maximum suppression to find the most probable set of non-overlapping spans.", "avg_logp": -0.24987295269966125, "avg_entropy_nats": 0.5747163891792297, "avg_entropy_bits": 0.8291404843330383}
{"generated_text": "The proposed model surpasses the NABERT+ baseline in every metric on other question types, with most of the improvement coming from better preprocessing.", "avg_logp": -0.18714585900306702, "avg_entropy_nats": 0.4049498736858368, "avg_entropy_bits": 0.5842191576957703}
{"generated_text": "The proposed model slightly eclipses the current state-of-the-art results on the entire DROP dataset.", "avg_logp": -0.04110777750611305, "avg_entropy_nats": 0.11635579913854599, "avg_entropy_bits": 0.16786593198776245}
{"generated_text": "MTMSN.", "avg_logp": -0.1255647838115692, "avg_entropy_nats": 0.39200127124786377, "avg_entropy_bits": 0.5655382871627808}
{"generated_text": "The model trained using XR loss has access to a large collection of un-annotated text from the same distribution, whereas the fully supervised model only has access to a small-scale annotated data.", "avg_logp": -0.16357563436031342, "avg_entropy_nats": 0.44921836256980896, "avg_entropy_bits": 0.6480849981307983}
{"generated_text": "unanswerable", "avg_logp": -0.3534681499004364, "avg_entropy_nats": 0.7142281532287598, "avg_entropy_bits": 1.0304133892059326}
{"generated_text": "The aspect-based sentiment classifier trained only using the XR loss achieves a F1 score of 35.28 when using prior distributions of aspects, but improves to 53.81, 58.84, 61.81, and 65.58 with 100, 1000, 5000, and 10000 labeled sentences respectively.", "avg_logp": -0.1691846400499344, "avg_entropy_nats": 0.3853481113910675, "avg_entropy_bits": 0.5559398531913757}
{"generated_text": "The expectation regularization loss is defined as the KL-divergence between the model's posterior distribution over a set and the expected label distribution, which is composed of two parts: the cross-entropy loss and the entropy of the model's posterior distribution.", "avg_logp": -0.22106927633285522, "avg_entropy_nats": 0.5949426293373108, "avg_entropy_bits": 0.8583207726478577}
{"generated_text": "The Lemming model, a log-linear model that performs joint morphological tagging and lemmatization, and a state-of-the-art neural model that also performs joint morphological tagging and lemmatization.", "avg_logp": -0.11679315567016602, "avg_entropy_nats": 0.32261407375335693, "avg_entropy_bits": 0.4654337465763092}
{"generated_text": "The NLU-Benchmark (NLU-BM) dataset and the ROMULUS dataset.", "avg_logp": -0.14073313772678375, "avg_entropy_nats": 0.28334400057792664, "avg_entropy_bits": 0.40877896547317505}
{"generated_text": "Intent F1, span F1, Exact Match (EM) accuracy, and the EM Combined score.", "avg_logp": -0.3349805474281311, "avg_entropy_nats": 0.7898242473602295, "avg_entropy_bits": 1.1394755840301514}
{"generated_text": "No.", "avg_logp": -0.6806243658065796, "avg_entropy_nats": 1.1507720947265625, "avg_entropy_bits": 1.6602132320404053}
{"generated_text": "SQuAD and NewsQA.", "avg_logp": -0.13395220041275024, "avg_entropy_nats": 0.28896841406822205, "avg_entropy_bits": 0.4168933033943176}
{"generated_text": "They train the question answerer with supervised learning using a Negative Log-Likelihood (NLL) loss, and the agent with the Rainbow algorithm, a variant of Deep Q-Learning.", "avg_logp": -0.29822778701782227, "avg_entropy_nats": 0.5739486217498779, "avg_entropy_bits": 0.8280328512191772}
{"generated_text": "Ctrl+F (search for token) and stop.", "avg_logp": -0.15761204063892365, "avg_entropy_nats": 0.4045303761959076, "avg_entropy_bits": 0.5836139917373657}
{"generated_text": "The authors propose three multimodal models: Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), and Textual Kernels Model (TKM).", "avg_logp": -0.06618335098028183, "avg_entropy_nats": 0.15973716974258423, "avg_entropy_bits": 0.2304520308971405}
{"generated_text": "unanswerable", "avg_logp": -0.19496212899684906, "avg_entropy_nats": 0.4738897383213043, "avg_entropy_bits": 0.683678388595581}
{"generated_text": "150,000 tweets.", "avg_logp": -0.1843700110912323, "avg_entropy_nats": 0.3896833062171936, "avg_entropy_bits": 0.5621941089630127}
{"generated_text": "The proposed multimodal models do not outperform unimodal textual models.", "avg_logp": -0.15212653577327728, "avg_entropy_nats": 0.27025169134140015, "avg_entropy_bits": 0.38989073038101196}
{"generated_text": "The author attributes the inability of multimodal models to outperform textual models to noisy data, complexity and diversity of multimodal relations, and a small set of multimodal examples.", "avg_logp": -0.165217787027359, "avg_entropy_nats": 0.32212918996810913, "avg_entropy_bits": 0.46473419666290283}
{"generated_text": "F-score, Area Under the ROC Curve (AUC), and mean accuracy (ACC).", "avg_logp": -0.07219649851322174, "avg_entropy_nats": 0.1444215029478073, "avg_entropy_bits": 0.20835618674755096}
{"generated_text": "The data is collected using the Twitter API.", "avg_logp": -0.11204569786787033, "avg_entropy_nats": 0.42722341418266296, "avg_entropy_bits": 0.6163531541824341}
{"generated_text": "Yes.", "avg_logp": -0.48040148615837097, "avg_entropy_nats": 0.9582857489585876, "avg_entropy_bits": 1.3825139999389648}
{"generated_text": "The unimodal detection models used were a CNN for image features extraction and a single layer LSTM for tweet text classification.", "avg_logp": -0.28975605964660645, "avg_entropy_nats": 0.6855792999267578, "avg_entropy_bits": 0.989081859588623}
{"generated_text": "The Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM) were proposed.", "avg_logp": -0.0853809118270874, "avg_entropy_nats": 0.16435441374778748, "avg_entropy_bits": 0.23711329698562622}
{"generated_text": "hate or not hate.", "avg_logp": -0.3264179229736328, "avg_entropy_nats": 0.7802790403366089, "avg_entropy_bits": 1.1257046461105347}
{"generated_text": "The evaluation metrics used were accuracy (ACC) and the normalized mutual information metric (NMI).", "avg_logp": -0.0857301652431488, "avg_entropy_nats": 0.18683037161827087, "avg_entropy_bits": 0.2695392668247223}
{"generated_text": "The proposed approach, STC, achieved significantly better performance in short text clustering tasks, outperforming various baseline methods across three datasets.", "avg_logp": -0.5104314088821411, "avg_entropy_nats": 0.95526522397995, "avg_entropy_bits": 1.3781564235687256}
{"generated_text": "They achieved a significantly better performance compared to other methods.", "avg_logp": -0.38073402643203735, "avg_entropy_nats": 0.8250980377197266, "avg_entropy_bits": 1.1903648376464844}
{"generated_text": "K-means, Skip-thought Vectors, Recursive Neural Network, Paragraph Vector, Laplacian Eigenmaps (LE), Locality Preserving Indexing (LPI), and bidirectional RNN.", "avg_logp": -0.07123348116874695, "avg_entropy_nats": 0.15596814453601837, "avg_entropy_bits": 0.22501449286937714}
{"generated_text": "They used three public short text datasets: SearchSnippets, StackOverflow, and Biomedical.", "avg_logp": -0.05513766035437584, "avg_entropy_nats": 0.1394653469324112, "avg_entropy_bits": 0.20120596885681152}
{"generated_text": "no.", "avg_logp": -0.6737265586853027, "avg_entropy_nats": 0.9976065158843994, "avg_entropy_bits": 1.439241886138916}
{"generated_text": "Three configurations of Transformer networks are explored: a small to medium-sized network with 4 Transformer layers, a small network with 2 Transformer layers, and a minimal network with 1 Transformer layer.", "avg_logp": -0.10734467953443527, "avg_entropy_nats": 0.2661746144294739, "avg_entropy_bits": 0.3840087950229645}
{"generated_text": "unanswerable", "avg_logp": -0.3210139274597168, "avg_entropy_nats": 0.6602020859718323, "avg_entropy_bits": 0.9524703025817871}
{"generated_text": "The problem is evaluated using sparse categorical cross-entropy and BLEU-2 scores.", "avg_logp": -0.2566620707511902, "avg_entropy_nats": 0.5767079591751099, "avg_entropy_bits": 0.8320137858390808}
{"generated_text": "The authors use four individual datasets: AI2, CC, IL, and MAWPS.", "avg_logp": -0.10701894760131836, "avg_entropy_nats": 0.2521834075450897, "avg_entropy_bits": 0.3638237714767456}
{"generated_text": "User-satisfaction metrics such as user interruption rate, reuse of this and other VVA skills, and number of active dialogs, AUC-ROC, accuracy, and top-1 accuracy were used.", "avg_logp": -0.10525138676166534, "avg_entropy_nats": 0.25074905157089233, "avg_entropy_bits": 0.3617544174194336}
{"generated_text": "unanswerable", "avg_logp": -0.15546458959579468, "avg_entropy_nats": 0.5107913017272949, "avg_entropy_bits": 0.7369160652160645}
{"generated_text": "five-minute reuse and one-day return.", "avg_logp": -0.2404281347990036, "avg_entropy_nats": 0.4351344704627991, "avg_entropy_bits": 0.6277663707733154}
{"generated_text": "Vector space representation using TF-IDF (Term Frequency-Inverse Document Frequency) weights.", "avg_logp": -0.2213899791240692, "avg_entropy_nats": 0.4791368544101715, "avg_entropy_bits": 0.69124835729599}
{"generated_text": "A dataset of 14 TDs with subjects on aliens, stories, law, and news.", "avg_logp": -0.21021710336208344, "avg_entropy_nats": 0.447679340839386, "avg_entropy_bits": 0.6458646655082703}
{"generated_text": "They evaluate knowledge extraction performance by comparing the similarity values produced by different similarity measures, such as CS, SRCC, and PCC, on a dataset of 14 textual documents.", "avg_logp": -0.22495043277740479, "avg_entropy_nats": 0.4680899679660797, "avg_entropy_bits": 0.6753110289573669}
{"generated_text": "The Masked Language Modeling (MLM) task on the French OSCAR corpus.", "avg_logp": -0.20033791661262512, "avg_entropy_nats": 0.4376921057701111, "avg_entropy_bits": 0.6314562559127808}
{"generated_text": "unanswerable", "avg_logp": -0.342435359954834, "avg_entropy_nats": 0.9516146779060364, "avg_entropy_bits": 1.372889757156372}
{"generated_text": "CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches.", "avg_logp": -0.036993373185396194, "avg_entropy_nats": 0.13874724507331848, "avg_entropy_bits": 0.20016998052597046}
{"generated_text": "CamemBERT outperformed previous models by a large margin for POS tagging and dependency parsing, and achieved state-of-the-art results for NER and NLI.", "avg_logp": -0.32873690128326416, "avg_entropy_nats": 0.8058784008026123, "avg_entropy_bits": 1.1626367568969727}
{"generated_text": "yes", "avg_logp": -0.8790935277938843, "avg_entropy_nats": 1.2549333572387695, "avg_entropy_bits": 1.810486078262329}
{"generated_text": "100k steps during 17 hours.", "avg_logp": -0.2921949028968811, "avg_entropy_nats": 0.5973677635192871, "avg_entropy_bits": 0.8618195056915283}
{"generated_text": "The French OSCAR corpus, a pre-classified and pre-filtered version of the November 2018 Common Crawl snapshot.", "avg_logp": -0.11313378810882568, "avg_entropy_nats": 0.2413407266139984, "avg_entropy_bits": 0.3481811285018921}
{"generated_text": "The state-of-the-art measures for controversy detection are based on the structure of the endorsement graph, but the proposed method equates in accuracy to the one developed by Garimella et al. and improves considerably computing time and robustness.", "avg_logp": -0.3152400255203247, "avg_entropy_nats": 0.730634331703186, "avg_entropy_bits": 1.0540823936462402}
{"generated_text": "netanyahu, ukraine, @mauriciomacri 1-11 Jan, Kavanaugh 3 Oct, @mauriciomacri 11-18 Mar, Bolsonaro 27 Oct, sxsw, germanwings, onedirection, ultralive, nepal, mothersday.", "avg_logp": -0.04288911074399948, "avg_entropy_nats": 0.11181606352329254, "avg_entropy_bits": 0.16131646931171417}
{"generated_text": "They used thirty different discussions that took place between March 2015 and June 2019, in four languages (English, Portuguese, Spanish, and French), and five regions over the world, with half of them being controversial and half non-controversial.", "avg_logp": -0.16545043885707855, "avg_entropy_nats": 0.370465487241745, "avg_entropy_bits": 0.5344686508178711}
{"generated_text": "Twitter.", "avg_logp": -0.09516334533691406, "avg_entropy_nats": 0.3380912244319916, "avg_entropy_bits": 0.487762451171875}
{"generated_text": "Four languages: English, Portuguese, Spanish, and French.", "avg_logp": -0.09477542340755463, "avg_entropy_nats": 0.2375670075416565, "avg_entropy_bits": 0.34273678064346313}
{"generated_text": "unanswerable.", "avg_logp": -0.3823314607143402, "avg_entropy_nats": 0.7140856981277466, "avg_entropy_bits": 1.030207872390747}
{"generated_text": "The difficulties of sentiment analysis on Twitter include its informal language and style, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.", "avg_logp": -0.15198881924152374, "avg_entropy_nats": 0.3250398337841034, "avg_entropy_bits": 0.4689333140850067}
{"generated_text": "unanswerable", "avg_logp": -0.41090333461761475, "avg_entropy_nats": 1.0249590873718262, "avg_entropy_bits": 1.478703260421753}
{"generated_text": "unanswerable", "avg_logp": -0.3646475076675415, "avg_entropy_nats": 0.9597612619400024, "avg_entropy_bits": 1.3846427202224731}
{"generated_text": "The dataset COSTRA 1.0 contains 293 annotations from 12 annotators, including 15 types of sentence transformations and 150 seed sentences.", "avg_logp": -0.3662954568862915, "avg_entropy_nats": 0.9071370363235474, "avg_entropy_bits": 1.3087221384048462}
{"generated_text": "They are represented as distinct paraphrases of the original sentence.", "avg_logp": -0.48227018117904663, "avg_entropy_nats": 1.0911071300506592, "avg_entropy_bits": 1.5741348266601562}
{"generated_text": "The 15 types of modifications illustrated in the dataset are not explicitly listed in the provided snippet, but they are described as follows: \n\n- dadaistic swap of roles\n- paraphrase\n- minimal change\n- nonsense\n- past\n- future\n- ban\n- different meaning (using the same set of words)\n- generalization\n- simplification\n- and 8 other types of modifications that are not specified in the provided snippet.", "avg_logp": -0.3778156638145447, "avg_entropy_nats": 0.8579394221305847, "avg_entropy_bits": 1.237744927406311}
{"generated_text": "yes, the corpus is freely available at the following link: http://hdl.handle.net/11234/1-3123.", "avg_logp": -0.11960387974977493, "avg_entropy_nats": 0.2547726035118103, "avg_entropy_bits": 0.36755916476249695}
{"generated_text": "unanswerable", "avg_logp": -0.33973032236099243, "avg_entropy_nats": 0.7572160959243774, "avg_entropy_bits": 1.0924317836761475}
{"generated_text": "They do not explicitly analyze how the modifications changed the starting set of sentences, but they do mention that the minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence.", "avg_logp": -0.3223270773887634, "avg_entropy_nats": 0.747025191783905, "avg_entropy_bits": 1.0777294635772705}
{"generated_text": "They introduce language variation by using two sources of Czech data, Global Voices and OpenSubtitles, to select seed sentences with different styles, both journalistic and common spoken language.", "avg_logp": -0.24968834221363068, "avg_entropy_nats": 0.5909091830253601, "avg_entropy_bits": 0.8525018095970154}
{"generated_text": "Yes.", "avg_logp": -0.4638785719871521, "avg_entropy_nats": 0.9615554213523865, "avg_entropy_bits": 1.3872312307357788}
{"generated_text": "unanswerable", "avg_logp": -0.11558536440134048, "avg_entropy_nats": 0.44210758805274963, "avg_entropy_bits": 0.6378263831138611}
{"generated_text": "unrelated.", "avg_logp": -0.593123197555542, "avg_entropy_nats": 1.1847915649414062, "avg_entropy_bits": 1.7092928886413574}
{"generated_text": "unanswerable", "avg_logp": -0.46954256296157837, "avg_entropy_nats": 0.9612250924110413, "avg_entropy_bits": 1.3867546319961548}
{"generated_text": "The proposed model achieved the best prediction accuracy of 86.63 for the STS Corpus, outperformed the model of BIBREF14 for the Sanders dataset, and outperformed the model of BIBREF14 for the HCR dataset.", "avg_logp": -0.12866061925888062, "avg_entropy_nats": 0.30286431312561035, "avg_entropy_bits": 0.43694087862968445}
{"generated_text": "unanswerable", "avg_logp": -0.24392952024936676, "avg_entropy_nats": 0.6783472299575806, "avg_entropy_bits": 0.9786481857299805}
{"generated_text": "The datasets used were Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset.", "avg_logp": -0.14118652045726776, "avg_entropy_nats": 0.2565491199493408, "avg_entropy_bits": 0.37012213468551636}
{"generated_text": "yes", "avg_logp": -0.48851293325424194, "avg_entropy_nats": 1.0707926750183105, "avg_entropy_bits": 1.5448272228240967}
{"generated_text": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR).", "avg_logp": -0.05242549255490303, "avg_entropy_nats": 0.11565451323986053, "avg_entropy_bits": 0.16685418784618378}
{"generated_text": "The proposed model uses five semantic rules, which are displayed in Table TABREF15, to handle negation and specific parts of speech particles.", "avg_logp": -0.40009805560112, "avg_entropy_nats": 0.919251561164856, "avg_entropy_bits": 1.3261997699737549}
{"generated_text": "link prediction and triplet classification.", "avg_logp": -0.15217702090740204, "avg_entropy_nats": 0.24173957109451294, "avg_entropy_bits": 0.34875646233558655}
{"generated_text": "Yes, they evaluate their LAN approach by comparing it with different scoring functions and on datasets with varying sample rates.", "avg_logp": -0.4407154619693756, "avg_entropy_nats": 0.9574679732322693, "avg_entropy_bits": 1.3813343048095703}
{"generated_text": "Yes.", "avg_logp": -0.17089107632637024, "avg_entropy_nats": 0.5126235485076904, "avg_entropy_bits": 0.739559531211853}
{"generated_text": "unanswerable", "avg_logp": -0.19901752471923828, "avg_entropy_nats": 0.4939616620540619, "avg_entropy_bits": 0.7126359939575195}
{"generated_text": "The datasets used to train the model include the movie sentence polarity dataset, laptop dataset, and restaurant dataset, which are either clean-labeled or noisy-labeled.", "avg_logp": -0.32599249482154846, "avg_entropy_nats": 0.7057775855064392, "avg_entropy_bits": 1.0182218551635742}
{"generated_text": "The proposed model, NetAb, outperforms a wide range of baselines markedly in sentence-level sentiment classification learning for data with noisy labels.", "avg_logp": -0.1545192450284958, "avg_entropy_nats": 0.3234531283378601, "avg_entropy_bits": 0.46664413809776306}
{"generated_text": "Yes.", "avg_logp": -0.28341352939605713, "avg_entropy_nats": 0.5027867555618286, "avg_entropy_bits": 0.7253679633140564}
{"generated_text": "yes", "avg_logp": -0.5312617421150208, "avg_entropy_nats": 1.2286473512649536, "avg_entropy_bits": 1.7725634574890137}
{"generated_text": "No, the article states that the authors propose the Semantic Text Exchange Score (STES) as a new metric for evaluating overall performance on semantic text exchange.", "avg_logp": -0.282046377658844, "avg_entropy_nats": 0.6438480615615845, "avg_entropy_bits": 0.9288763403892517}
{"generated_text": "NWN-STEM, GWN-STEM, and W2V-STEM.", "avg_logp": -0.04911767318844795, "avg_entropy_nats": 0.10485679656267166, "avg_entropy_bits": 0.1512763798236847}
{"generated_text": "The performance of both the i-vector system and the x-vector system on CN-Celeb(E) was much worse than on SITW and SITW(S).", "avg_logp": -0.1288854330778122, "avg_entropy_nats": 0.3484572768211365, "avg_entropy_bits": 0.5027176141738892}
{"generated_text": "unconstrained conditions.", "avg_logp": -0.45413193106651306, "avg_entropy_nats": 0.9065424799919128, "avg_entropy_bits": 1.3078641891479492}
{"generated_text": "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement.", "avg_logp": -0.02906890958547592, "avg_entropy_nats": 0.06935479491949081, "avg_entropy_bits": 0.10005781799554825}
{"generated_text": "unanswerable", "avg_logp": -0.3494179844856262, "avg_entropy_nats": 0.7692671418190002, "avg_entropy_bits": 1.1098178625106812}
{"generated_text": "unanswerable\n\nThe article does not directly compare the two models on CN-Celeb, but rather presents results for each model on CN-Celeb(E) and SITW(S), and also compares the performance of the models on SITW(S) and CN-Celeb(E) separately.", "avg_logp": -0.43156859278678894, "avg_entropy_nats": 0.9515649080276489, "avg_entropy_bits": 1.372818112373352}
{"generated_text": "The performance on CN-Celeb is much worse than the performance on VoxCeleb, as indicated by the significantly higher equal error rate (EER) on CN-Celeb compared to VoxCeleb.", "avg_logp": -0.2592020630836487, "avg_entropy_nats": 0.5713043808937073, "avg_entropy_bits": 0.8242180347442627}
{"generated_text": "Six benchmark classification datasets: SST, Subj, MPQA, RT, TREC, and Text classification.", "avg_logp": -0.18367235362529755, "avg_entropy_nats": 0.38912615180015564, "avg_entropy_bits": 0.5613903999328613}
{"generated_text": "They measure performance using accuracy on various classification tasks.", "avg_logp": -0.5861129760742188, "avg_entropy_nats": 1.0739097595214844, "avg_entropy_bits": 1.5493243932724}
{"generated_text": "Yes.", "avg_logp": -0.5080059766769409, "avg_entropy_nats": 0.9046654105186462, "avg_entropy_bits": 1.3051562309265137}
{"generated_text": "No, the article does not mention the evaluation of other pre-trained language models for contextual augmentation.", "avg_logp": -0.24182124435901642, "avg_entropy_nats": 0.5235932469367981, "avg_entropy_bits": 0.7553853988647461}
{"generated_text": "unanswerable", "avg_logp": -0.1898484081029892, "avg_entropy_nats": 0.4124656915664673, "avg_entropy_bits": 0.595062255859375}
{"generated_text": "unanswerable", "avg_logp": -0.11433552205562592, "avg_entropy_nats": 0.28469064831733704, "avg_entropy_bits": 0.4107217788696289}
{"generated_text": "unanswerable.", "avg_logp": -0.2182472050189972, "avg_entropy_nats": 0.5542575120925903, "avg_entropy_bits": 0.7996245622634888}
{"generated_text": "Yes.", "avg_logp": -0.36073538661003113, "avg_entropy_nats": 0.6681112051010132, "avg_entropy_bits": 0.9638806581497192}
{"generated_text": "unanswerable.", "avg_logp": -0.17482738196849823, "avg_entropy_nats": 0.29179081320762634, "avg_entropy_bits": 0.42096516489982605}
{"generated_text": "The learning paradigms covered in this survey include the change of learning paradigm, the broadening of the input spectrum, and the generation of deep questions.", "avg_logp": -0.12232817709445953, "avg_entropy_nats": 0.3481934070587158, "avg_entropy_bits": 0.502336859703064}
{"generated_text": "Textual inputs, knowledge bases, and images.", "avg_logp": -0.19270217418670654, "avg_entropy_nats": 0.44561004638671875, "avg_entropy_bits": 0.6428794264793396}
{"generated_text": "No.", "avg_logp": -0.42823326587677, "avg_entropy_nats": 0.9371180534362793, "avg_entropy_bits": 1.351975679397583}
{"generated_text": "The hypersphere model, which represents the distribution of named entity embeddings as a hypersphere in a high-dimensional space.", "avg_logp": -0.44583195447921753, "avg_entropy_nats": 0.8739749789237976, "avg_entropy_bits": 1.260879397392273}
{"generated_text": "Yes.", "avg_logp": -0.29370176792144775, "avg_entropy_nats": 0.5368391275405884, "avg_entropy_bits": 0.7744951248168945}
{"generated_text": "The baseline model, namely Distant Supervision, and Alec Go's model using Naive Bayes, SVM, and MaxEnt classifiers.", "avg_logp": -0.20616091787815094, "avg_entropy_nats": 0.49568819999694824, "avg_entropy_bits": 0.7151268720626831}
{"generated_text": "The Effective Word Score (EFWS) of a word x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x.", "avg_logp": -0.057839974761009216, "avg_entropy_nats": 0.1123967245221138, "avg_entropy_bits": 0.1621541976928711}
{"generated_text": "Using TextBlob and Opinion Finder Tool, with TextBlob providing a direct subjectivity level and Opinion Finder providing the information of which parts of the text are subjective and which are objective.", "avg_logp": -0.3403955101966858, "avg_entropy_nats": 0.6643314957618713, "avg_entropy_bits": 0.9584277868270874}
{"generated_text": "It is not necessary for DMN, as the new DMN+ model learns to select important facts from a larger set without requiring labeled supporting facts during training.", "avg_logp": -0.2318289577960968, "avg_entropy_nats": 0.5522967576980591, "avg_entropy_bits": 0.7967958450317383}
{"generated_text": "Supporting fact supervision refers to the requirement that relevant facts for answering a question are labeled during training.", "avg_logp": -0.23680828511714935, "avg_entropy_nats": 0.6160260438919067, "avg_entropy_bits": 0.8887377381324768}
{"generated_text": "They proposed a new input module with a two-level encoder, a sentence reader, and an input fusion layer to allow for information flow between sentences.", "avg_logp": -0.13989010453224182, "avg_entropy_nats": 0.32678937911987305, "avg_entropy_bits": 0.4714573919773102}
{"generated_text": "They proposed a new input module with a two-level encoder and input fusion layer, modified the memory module with a novel attention-based GRU, and introduced a new input module to represent images.", "avg_logp": -0.2282952070236206, "avg_entropy_nats": 0.5054572224617004, "avg_entropy_bits": 0.7292205691337585}
{"generated_text": "The model learns to select the important facts from a larger set without requiring labeled supporting facts during training.", "avg_logp": -0.13320507109165192, "avg_entropy_nats": 0.3392731547355652, "avg_entropy_bits": 0.48946768045425415}
{"generated_text": "yes", "avg_logp": -0.6736648082733154, "avg_entropy_nats": 0.9445067644119263, "avg_entropy_bits": 1.3626351356506348}
{"generated_text": "Yes.", "avg_logp": -0.30966922640800476, "avg_entropy_nats": 0.5637047290802002, "avg_entropy_bits": 0.8132540583610535}
{"generated_text": "They perform manual evaluation by having three English Literature PhD candidates score reference sentences, reconstructed sentences, and transferred sentences on a 0-5 scale for fluency, and also have them classify transferred sentences by style.", "avg_logp": -0.25646471977233887, "avg_entropy_nats": 0.5885111093521118, "avg_entropy_bits": 0.8490419387817383}
{"generated_text": "BLEU scores and perplexity.", "avg_logp": -0.18045058846473694, "avg_entropy_nats": 0.3651321530342102, "avg_entropy_bits": 0.5267742872238159}
{"generated_text": "They are identified as words that are not tagged as NOUN, VERB, or ADJ.", "avg_logp": -0.23863908648490906, "avg_entropy_nats": 0.5356488823890686, "avg_entropy_bits": 0.7727780342102051}
{"generated_text": "They model style as a suite of low-level linguistic controls, such as frequency of personal pronouns, prepositions, and syntactic features like the number of SBAR non-terminals in its constituency parse.", "avg_logp": -0.07247435301542282, "avg_entropy_nats": 0.2337425798177719, "avg_entropy_bits": 0.3372192680835724}
{"generated_text": "unanswerable", "avg_logp": -0.43152546882629395, "avg_entropy_nats": 0.7653622627258301, "avg_entropy_bits": 1.1041842699050903}
{"generated_text": "The article provides insights into the relationship between demographics, such as age and gender, and mental health, specifically depression, through the analysis of social media data, including visual and textual content.", "avg_logp": -0.19503195583820343, "avg_entropy_nats": 0.5416316986083984, "avg_entropy_bits": 0.7814093232154846}
{"generated_text": "unanswerable", "avg_logp": -0.08583590388298035, "avg_entropy_nats": 0.32656145095825195, "avg_entropy_bits": 0.47112858295440674}
{"generated_text": "The framework facilitates demographic inference from social media by integrating a heterogeneous set of features from different modalities, including aesthetic features from posted images, choice of profile picture, language features from textual content and profile descriptions, and sociability from ego-network and user engagement.", "avg_logp": -0.14349624514579773, "avg_entropy_nats": 0.3292367160320282, "avg_entropy_bits": 0.4749881625175476}
{"generated_text": "The article does not explicitly state the types of features used from each data type, but it mentions the following features from each modality: \n\n- Aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness)\n- Choice of profile picture (for gender, age, and facial expression)\n- Screen name\n- Language features from textual content and profile description (n-gram, emotion, sentiment)\n- Sociability from ego-network\n- User engagement\n- Demographic features from profile/posted images (facial presentation, emotion from facial expression)\n- General image features (color, saturation, hue, brightness, contrast, luminance)\n- Textual content features (LIWC, psycholinguistic cues, language style, thinking style, emotional tone)\n- Qualitative language analysis (analytical thinking, clout, authenticity, emotional tone, descriptors categories, informal language markers, linguistic aspects)", "avg_logp": -0.21209701895713806, "avg_entropy_nats": 0.47429728507995605, "avg_entropy_bits": 0.6842663288116455}
{"generated_text": "The data is annotated through a gold standard dataset of 1061 users who disclose their age, and a lexicon of depression symptoms consisting of 1500 depression-indicative terms created with the help of a psychologist clinician.", "avg_logp": -0.2038804292678833, "avg_entropy_nats": 0.5045097470283508, "avg_entropy_bits": 0.7278537750244141}
{"generated_text": "Self-reported data, electronic health records, and crowdsourced signals from Twitter users.", "avg_logp": -0.609253466129303, "avg_entropy_nats": 1.2377489805221558, "avg_entropy_bits": 1.7856943607330322}
{"generated_text": "Twitter.", "avg_logp": -0.41096970438957214, "avg_entropy_nats": 0.7983438372612, "avg_entropy_bits": 1.1517667770385742}
{"generated_text": "The source of the textual data is Twitter.", "avg_logp": -0.2148088961839676, "avg_entropy_nats": 0.5877453088760376, "avg_entropy_bits": 0.8479372262954712}
{"generated_text": "Twitter.", "avg_logp": -0.15205982327461243, "avg_entropy_nats": 0.531273365020752, "avg_entropy_bits": 0.7664653658866882}
{"generated_text": "unanswerable", "avg_logp": -0.0341586172580719, "avg_entropy_nats": 0.14699077606201172, "avg_entropy_bits": 0.21206286549568176}
{"generated_text": "unanswerable", "avg_logp": -0.1108139157295227, "avg_entropy_nats": 0.3874509334564209, "avg_entropy_bits": 0.558973491191864}
{"generated_text": "unanswerable", "avg_logp": -0.08591920882463455, "avg_entropy_nats": 0.32423102855682373, "avg_entropy_bits": 0.4677664339542389}
{"generated_text": "A sememe is a unit of meaning in a word, as annotated according to HowNet BIBREF5.", "avg_logp": -0.3756718039512634, "avg_entropy_nats": 0.8050686120986938, "avg_entropy_bits": 1.161468505859375}
{"generated_text": "The Dutch section of the OSCAR corpus, a large multilingual corpus obtained by language classification in the Common Crawl corpus.", "avg_logp": -0.06020857393741608, "avg_entropy_nats": 0.19896788895130157, "avg_entropy_bits": 0.2870499789714813}
{"generated_text": "The state of the art in Dutch NLP tasks is achieved by RobBERT, a Dutch RoBERTa-based pre-trained language model.", "avg_logp": -0.2761254906654358, "avg_entropy_nats": 0.6304880380630493, "avg_entropy_bits": 0.9096020460128784}
{"generated_text": "They experimented on sentiment analysis, die/dat disambiguation, and possibly other Dutch language tasks.", "avg_logp": -0.31024011969566345, "avg_entropy_nats": 0.7439040541648865, "avg_entropy_bits": 1.0732266902923584}
{"generated_text": "The results suggest that NLP representations are more robust to task-nuisances, allow for better transfer, and achieve higher performance in complex tasks.", "avg_logp": -0.16135245561599731, "avg_entropy_nats": 0.2947235703468323, "avg_entropy_bits": 0.42519617080688477}
{"generated_text": "The performance of natural language based agents is better, achieving higher performance in complex tasks and outperforming vision-based representations.", "avg_logp": -0.33427760004997253, "avg_entropy_nats": 0.6608791947364807, "avg_entropy_bits": 0.9534471035003662}
{"generated_text": "unanswerable", "avg_logp": -0.11034280061721802, "avg_entropy_nats": 0.3717881441116333, "avg_entropy_bits": 0.5363768935203552}
{"generated_text": "The authors perform experiments on several tasks in the VizDoom environment, comparing the performance of natural language representation against visual-based and feature representations.", "avg_logp": -0.230783149600029, "avg_entropy_nats": 0.5077747106552124, "avg_entropy_bits": 0.7325640916824341}
{"generated_text": "The state is represented via natural language by converting the list of available labels into a string that describes the state, using directions and relative positions to describe objects and their locations.", "avg_logp": -0.4607420861721039, "avg_entropy_nats": 1.0147905349731445, "avg_entropy_bits": 1.4640332460403442}
{"generated_text": "The TL-TranSum method outperforms the Maximal Relevance Minimal Redundancy (MRMR) method by 7% of ROUGE-SU4 score.", "avg_logp": -0.10090214014053345, "avg_entropy_nats": 0.21951153874397278, "avg_entropy_bits": 0.31668820977211}
{"generated_text": "The paper mentions that existing research on moral sentiment change is scarce and that there is no existing systematic and formal treatment of this topic, but it does not discuss previous models specifically applied to the same task.", "avg_logp": -0.4264494478702545, "avg_entropy_nats": 0.914192259311676, "avg_entropy_bits": 1.31890070438385}
{"generated_text": "The datasets used in the paper include Google N-grams, COHA, MFD, WordNet, and a dataset of approximately 14,000 English words with emotional valence ratings collected by BIBREF28.", "avg_logp": -0.22816169261932373, "avg_entropy_nats": 0.5000622868537903, "avg_entropy_bits": 0.7214373350143433}
{"generated_text": "The parameter-free model, specifically the Centroid model, summarizes each set of seed words by its expected vector in embedding space and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule.", "avg_logp": -0.06675667315721512, "avg_entropy_nats": 0.19646035134792328, "avg_entropy_bits": 0.28343236446380615}
{"generated_text": "They quantify moral relevance by using a set of seed words, including both morally relevant and morally irrelevant words, and calculating the probability of a query concept being associated with the morally relevant class.", "avg_logp": -0.3717789053916931, "avg_entropy_nats": 0.8571410179138184, "avg_entropy_bits": 1.2365931272506714}
{"generated_text": "Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation.", "avg_logp": -0.03918575868010521, "avg_entropy_nats": 0.0874532014131546, "avg_entropy_bits": 0.12616831064224243}
{"generated_text": "The Google N-grams corpus and the Pew Research Center's 2013 Global Attitudes survey, as well as the Google N-grams corpus for diachronic embeddings, and the Moral Foundations Dictionary.", "avg_logp": -0.3457542359828949, "avg_entropy_nats": 0.7823110222816467, "avg_entropy_bits": 1.128636121749878}
{"generated_text": "The system performed well, with the neural approach being generally preferred by participants in both genres, especially in the fairy-tale genre.", "avg_logp": -0.3714663088321686, "avg_entropy_nats": 0.795854389667511, "avg_entropy_bits": 1.1481751203536987}
{"generated_text": "The information is extracted using a neural question-answering technique (AskBERT) and a rule-based information extraction technique (OpenIE5).", "avg_logp": -0.19214990735054016, "avg_entropy_nats": 0.45940643548965454, "avg_entropy_bits": 0.6627833247184753}
{"generated_text": "The article does not explicitly provide guidelines for writing input vernacular to inspire better poems, but it mentions that human evaluation shows that the model can generate high-quality poems when the input vernacular is carefully written.", "avg_logp": -0.3580915033817291, "avg_entropy_nats": 0.7806408405303955, "avg_entropy_bits": 1.1262266635894775}
{"generated_text": "unanswerable", "avg_logp": -0.47160032391548157, "avg_entropy_nats": 1.058577537536621, "avg_entropy_bits": 1.5272045135498047}
{"generated_text": "A corpus of poems and a corpus of vernacular literature from online resources, including 163K quatrain poems and 337K short paragraphs from 281 famous books.", "avg_logp": -0.1518511176109314, "avg_entropy_nats": 0.2930103838443756, "avg_entropy_bits": 0.4227246046066284}
{"generated_text": "BLEU and the Micro Entity F1.", "avg_logp": -0.09626102447509766, "avg_entropy_nats": 0.19115489721298218, "avg_entropy_bits": 0.27577823400497437}
{"generated_text": "Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, and DSR.", "avg_logp": -0.025538459420204163, "avg_entropy_nats": 0.08102791756391525, "avg_entropy_bits": 0.1168985590338707}
{"generated_text": "InCar dataset and CamRest dataset.", "avg_logp": -0.1179187074303627, "avg_entropy_nats": 0.1885681003332138, "avg_entropy_bits": 0.2720462679862976}
{"generated_text": "unanswerable", "avg_logp": -0.2494921088218689, "avg_entropy_nats": 0.6578150987625122, "avg_entropy_bits": 0.9490265846252441}
{"generated_text": "equi-distant segments of the video.", "avg_logp": -0.17785830795764923, "avg_entropy_nats": 0.5165041089057922, "avg_entropy_bits": 0.7451578974723816}
{"generated_text": "Yes.", "avg_logp": -0.5195859670639038, "avg_entropy_nats": 0.902873158454895, "avg_entropy_bits": 1.3025705814361572}
{"generated_text": "yes", "avg_logp": -0.6548557281494141, "avg_entropy_nats": 1.2139298915863037, "avg_entropy_bits": 1.7513304948806763}
{"generated_text": "unanswerable", "avg_logp": -0.37322694063186646, "avg_entropy_nats": 0.8205974102020264, "avg_entropy_bits": 1.1838717460632324}
{"generated_text": "unanswerable", "avg_logp": -0.11134741455316544, "avg_entropy_nats": 0.37030333280563354, "avg_entropy_bits": 0.5342347621917725}
{"generated_text": "A multi-class Naive Bayes classifier is used for emergency categorization.", "avg_logp": -0.10812841355800629, "avg_entropy_nats": 0.2561749219894409, "avg_entropy_bits": 0.36958229541778564}
{"generated_text": "The Support Vector Machine (SVM) classifier is used for emergency detection in the first stage of classification.", "avg_logp": -0.20965197682380676, "avg_entropy_nats": 0.44850602746009827, "avg_entropy_bits": 0.6470574140548706}
{"generated_text": "unanswerable", "avg_logp": -0.28816431760787964, "avg_entropy_nats": 0.6664673089981079, "avg_entropy_bits": 0.961509108543396}
{"generated_text": "unanswerable", "avg_logp": -0.3219790458679199, "avg_entropy_nats": 0.9921767115592957, "avg_entropy_bits": 1.431408405303955}
{"generated_text": "unanswerable", "avg_logp": -0.10574876517057419, "avg_entropy_nats": 0.39166495203971863, "avg_entropy_bits": 0.5650531053543091}
{"generated_text": "The tweets are specific to a region, as the system currently collects tweets with a location filter for the city of \"Mumbai\".", "avg_logp": -0.2470235377550125, "avg_entropy_nats": 0.5910292863845825, "avg_entropy_bits": 0.8526751399040222}
{"generated_text": "unanswerable", "avg_logp": -0.2645072042942047, "avg_entropy_nats": 0.6973748803138733, "avg_entropy_bits": 1.0060992240905762}
{"generated_text": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.", "avg_logp": -0.042291175574064255, "avg_entropy_nats": 0.14544835686683655, "avg_entropy_bits": 0.20983760058879852}
{"generated_text": "Upward reasoning is defined as a context that allows an inference from a more specific phrase to a more general phrase, while downward reasoning is defined as a context that allows an inference from a more general phrase to a more specific phrase.", "avg_logp": -0.12271270155906677, "avg_entropy_nats": 0.30402329564094543, "avg_entropy_bits": 0.4386129081249237}
{"generated_text": "Monotonicity reasoning is a type of reasoning based on word replacement that requires the ability to capture the interaction between lexical and syntactic structures.", "avg_logp": -0.09320509433746338, "avg_entropy_nats": 0.2562260031700134, "avg_entropy_bits": 0.36965593695640564}
{"generated_text": "Several key relationships were found between dialogue acts and emotions, including Accept/Agree with Joy, Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc.", "avg_logp": -0.16380314528942108, "avg_entropy_nats": 0.3435404896736145, "avg_entropy_bits": 0.4956241846084595}
{"generated_text": "The ensemble annotator extracts the final label by giving priority to labels that are perfectly matching in all models, then to context-based models, and finally by ranking labels with their confidence values and selecting the top one or two matching labels.", "avg_logp": -0.28345608711242676, "avg_entropy_nats": 0.6320101022720337, "avg_entropy_bits": 0.911797821521759}
{"generated_text": "Defined using the Dialogue Act Markup in Several Layers (DAMSL) tag set.", "avg_logp": -0.12267030775547028, "avg_entropy_nats": 0.2839793264865875, "avg_entropy_bits": 0.409695565700531}
{"generated_text": "Five neural models were used.", "avg_logp": -0.24570605158805847, "avg_entropy_nats": 0.7258168458938599, "avg_entropy_bits": 1.0471323728561401}
{"generated_text": "No, the article does not compare their neural network against any other model, but it does mention SyncNet, a previous model that achieved 99% accuracy on lip video synchronisation.", "avg_logp": -0.33432096242904663, "avg_entropy_nats": 0.7538017630577087, "avg_entropy_bits": 1.0875059366226196}
{"generated_text": "They use an existing dataset, UltraSuite, which is a repository of ultrasound and acoustic data gathered from child speech therapy sessions.", "avg_logp": -0.16346560418605804, "avg_entropy_nats": 0.38841521739959717, "avg_entropy_bits": 0.5603646636009216}
{"generated_text": "No, their neural network predicts a set of candidate offsets and selects the one with the smallest average distance.", "avg_logp": -0.2875886559486389, "avg_entropy_nats": 0.6019034385681152, "avg_entropy_bits": 0.8683631420135498}
{"generated_text": "A two-stream neural network architecture.", "avg_logp": -0.05296102911233902, "avg_entropy_nats": 0.184353306889534, "avg_entropy_bits": 0.2659655809402466}
{"generated_text": "Aspects are identified in aspect extraction through a combination of deep learning and linguistics, using techniques such as convolutional neural networks (CNNs) and linguistic patterns (LPs), as well as features such as word embeddings, part-of-speech (POS) tags, and syntactic relations.", "avg_logp": -0.22190390527248383, "avg_entropy_nats": 0.5062891840934753, "avg_entropy_bits": 0.7304208278656006}
{"generated_text": "The Ritter NER corpus, the UMBC Twitter corpus, and the Making Sense of Microposts (MSM) 2013 corpus.", "avg_logp": -0.14732350409030914, "avg_entropy_nats": 0.2985844612121582, "avg_entropy_bits": 0.43076637387275696}
{"generated_text": "The full 81 hours of the WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them.", "avg_logp": -0.03248779475688934, "avg_entropy_nats": 0.10829099267721176, "avg_entropy_bits": 0.15623091161251068}
{"generated_text": "The encoder network has 5 convolutional layers, and the context network has 7 convolutional layers.", "avg_logp": -0.14826665818691254, "avg_entropy_nats": 0.32566967606544495, "avg_entropy_bits": 0.4698420464992523}
{"generated_text": "They explore that pre-training on more data leads to better accuracy on the WSJ benchmark, and that pre-training on the full Librispeech corpus reduces WER by 32% when only about eight hours of transcribed data is available.", "avg_logp": -0.21102698147296906, "avg_entropy_nats": 0.516063392162323, "avg_entropy_bits": 0.7445220947265625}
{"generated_text": "The character representations from various languages are joint through shared character embeddings.", "avg_logp": -0.4576061964035034, "avg_entropy_nats": 0.8930346965789795, "avg_entropy_bits": 1.2883766889572144}
{"generated_text": "The Universal Dependencies (UD) treebanks.", "avg_logp": -0.1437099128961563, "avg_entropy_nats": 0.38070276379585266, "avg_entropy_bits": 0.5492379665374756}
{"generated_text": "yes", "avg_logp": -0.5684734582901001, "avg_entropy_nats": 1.149245262145996, "avg_entropy_bits": 1.658010482788086}
{"generated_text": "The in-house dataset includes 6 languages with 56 entity types and 53 relation types, while the ACE05 dataset includes 3 languages with 7 entity types and 6 relation types.", "avg_logp": -0.09367341548204422, "avg_entropy_nats": 0.2164640724658966, "avg_entropy_bits": 0.31229159235954285}
{"generated_text": "They experiment on 7 target languages: German, Spanish, Portuguese, Chinese, Italian, Japanese, and Arabic.", "avg_logp": -0.12517066299915314, "avg_entropy_nats": 0.2801585793495178, "avg_entropy_bits": 0.40418338775634766}
{"generated_text": "The in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset.", "avg_logp": -0.07885673642158508, "avg_entropy_nats": 0.20465171337127686, "avg_entropy_bits": 0.2952500283718109}
{"generated_text": "Auto-completion performs better when using both language and vision than only language.", "avg_logp": -0.13039538264274597, "avg_entropy_nats": 0.29898780584335327, "avg_entropy_bits": 0.4313482642173767}
{"generated_text": "The data provided by this research includes 16k images and 740k region descriptions from the Visual Genome dataset, and 9k images and 54k referring expressions from the ReferIt dataset.", "avg_logp": -0.12853176891803741, "avg_entropy_nats": 0.34172379970550537, "avg_entropy_bits": 0.4930031895637512}
{"generated_text": "They complete a user query prefix conditioned upon an image by using a modified FactorCell LSTM language model that incorporates image features extracted from a pre-trained CNN.", "avg_logp": -0.20249590277671814, "avg_entropy_nats": 0.4065955877304077, "avg_entropy_bits": 0.5865933895111084}
{"generated_text": "unanswerable", "avg_logp": -0.0938638225197792, "avg_entropy_nats": 0.2995460033416748, "avg_entropy_bits": 0.43215349316596985}
{"generated_text": "The model outperformed the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.", "avg_logp": -0.1475909948348999, "avg_entropy_nats": 0.39012351632118225, "avg_entropy_bits": 0.5628292560577393}
{"generated_text": "They compared their model with a two-step baseline approach for interpreting navigation instructions, and a sequence-to-sequence model that does not consider the behavioral graph.", "avg_logp": -0.11086877435445786, "avg_entropy_nats": 0.22434504330158234, "avg_entropy_bits": 0.3236614763736725}
{"generated_text": "The proposed model \"Ours with Mask and Ordered Triplets\" outperformed the Baseline and Ablation models on all metrics in previously seen environments, with a 35% increase in Exact Match and 25% increase in Goal Match accuracy compared to the Baseline model.", "avg_logp": -0.12724563479423523, "avg_entropy_nats": 0.2799071669578552, "avg_entropy_bits": 0.403820663690567}
{"generated_text": "The evaluation metrics used are Exact Match (EM), harmonic average of precision and recall (F1), edit distance (ED), and Goal Match (GM).", "avg_logp": -0.1438426375389099, "avg_entropy_nats": 0.35084623098373413, "avg_entropy_bits": 0.5061641335487366}
{"generated_text": "The authors used Mechanical Turk for collecting a dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans.", "avg_logp": -0.22290819883346558, "avg_entropy_nats": 0.39321091771125793, "avg_entropy_bits": 0.5672834515571594}
{"generated_text": "The navigation instructions were collected through Mechanical Turk using 100 simulated environments with a corresponding topological map.", "avg_logp": -0.052170220762491226, "avg_entropy_nats": 0.1911422461271286, "avg_entropy_bits": 0.2757599651813507}
{"generated_text": "unanswerable", "avg_logp": -0.26666733622550964, "avg_entropy_nats": 0.6106406450271606, "avg_entropy_bits": 0.8809682130813599}
{"generated_text": "bag-of-words, length of stay, number of previous admissions, clinically positive and negative phenomena within each risk factor domain, EEG biomarkers, structural brain anatomy, social and role functioning assessments, personality assessment, and various symptom scales.", "avg_logp": -0.12688380479812622, "avg_entropy_nats": 0.28053784370422363, "avg_entropy_bits": 0.40473055839538574}
{"generated_text": "The initial results show a Fleiss's Kappa of 0.575, indicating moderate to substantial agreement among annotators.", "avg_logp": -0.2685398459434509, "avg_entropy_nats": 0.627597987651825, "avg_entropy_bits": 0.9054324626922607}
{"generated_text": "The authors used two datasets: a corpus of 240,000 paragraphs extracted from Meditech, and a corpus of approximately 100,000 paragraphs from the Research Patient Data Registry (RPDR).", "avg_logp": -0.21289889514446259, "avg_entropy_nats": 0.49301958084106445, "avg_entropy_bits": 0.7112769484519958}
{"generated_text": "unanswerable", "avg_logp": -0.15147759020328522, "avg_entropy_nats": 0.6365981698036194, "avg_entropy_bits": 0.9184169769287109}
{"generated_text": "The morphology knowledge is implemented by segmenting words into morpheme units and then applying BPE on the stem units.", "avg_logp": -0.3279930651187897, "avg_entropy_nats": 0.6360765099525452, "avg_entropy_bits": 0.9176644086837769}
{"generated_text": "The word segmentation method works by segmenting complex words into simple and effective morpheme units, which can be achieved through morpheme segmentation, BPE, or a combination of both, and adding specific symbols to assist the NMT model in identifying morpheme boundaries and capturing semantic information.", "avg_logp": -0.26592040061950684, "avg_entropy_nats": 0.5953952670097351, "avg_entropy_bits": 0.8589738607406616}
{"generated_text": "unanswerable", "avg_logp": -0.17098113894462585, "avg_entropy_nats": 0.5567610859870911, "avg_entropy_bits": 0.8032364845275879}
{"generated_text": "unanswerable", "avg_logp": -0.0977814719080925, "avg_entropy_nats": 0.3781239986419678, "avg_entropy_bits": 0.54551762342453}
{"generated_text": "24 layers.", "avg_logp": -0.6113376617431641, "avg_entropy_nats": 1.3773045539855957, "avg_entropy_bits": 1.9870305061340332}
{"generated_text": "No, they calculate the total objective function by summing up the auxiliary loss and the main loss at each specified intermediate layer.", "avg_logp": -0.4641854465007782, "avg_entropy_nats": 1.011386513710022, "avg_entropy_bits": 1.4591221809387207}
{"generated_text": "unanswerable", "avg_logp": -0.35255908966064453, "avg_entropy_nats": 0.7787913084030151, "avg_entropy_bits": 1.1235584020614624}
{"generated_text": "Yes, agglutinative languages are used in the prediction of both prefixing and suffixing languages.", "avg_logp": -0.1714164763689041, "avg_entropy_nats": 0.40934911370277405, "avg_entropy_bits": 0.5905659794807434}
{"generated_text": "Navajo is an example of a strongly prefixing language.", "avg_logp": -0.12871624529361725, "avg_entropy_nats": 0.28317829966545105, "avg_entropy_bits": 0.40853992104530334}
{"generated_text": "The performance on the task is evaluated using test accuracy, as shown in Table TABREF18, and validation set accuracy, as shown in Table TABREF19.", "avg_logp": -0.20170779526233673, "avg_entropy_nats": 0.4711835980415344, "avg_entropy_bits": 0.6797741651535034}
{"generated_text": "English, Spanish, and Zulu.", "avg_logp": -0.011659715324640274, "avg_entropy_nats": 0.05693848803639412, "avg_entropy_bits": 0.08214487135410309}
{"generated_text": "The baselines are residual-connected models.", "avg_logp": -0.19506631791591644, "avg_entropy_nats": 0.5883747935295105, "avg_entropy_bits": 0.848845362663269}
{"generated_text": "yes", "avg_logp": -0.49078866839408875, "avg_entropy_nats": 0.8581129312515259, "avg_entropy_bits": 1.2379951477050781}
{"generated_text": "German-English, Turkish-English, and English-German.", "avg_logp": -0.045695945620536804, "avg_entropy_nats": 0.16024215519428253, "avg_entropy_bits": 0.23118054866790771}
{"generated_text": "The datasets used were IWSLT14 German-English, Turkish-English, and WMT14 English-German, as well as the SETimes corpus and the WMT14 English-German dataset.", "avg_logp": -0.23728322982788086, "avg_entropy_nats": 0.5226197838783264, "avg_entropy_bits": 0.7539809346199036}
{"generated_text": "The order of binomials is tracked across time through metrics such as movement, which measures the change in ordinality of a list over time.", "avg_logp": -0.21506695449352264, "avg_entropy_nats": 0.5014060735702515, "avg_entropy_bits": 0.7233760356903076}
{"generated_text": "News publications, Reddit, and sports communities.", "avg_logp": -0.2821253836154938, "avg_entropy_nats": 0.9340863227844238, "avg_entropy_bits": 1.3476017713546753}
{"generated_text": "Yes, trinomials show a tendency for the last word to keep the same position in different orderings.", "avg_logp": -0.22230598330497742, "avg_entropy_nats": 0.5782524347305298, "avg_entropy_bits": 0.8342418670654297}
{"generated_text": "A directed graph where an edge from i to j means that i tends to come before j in binomials.", "avg_logp": -0.09004102647304535, "avg_entropy_nats": 0.20622754096984863, "avg_entropy_bits": 0.29752349853515625}
{"generated_text": "The most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general.", "avg_logp": -0.03730231150984764, "avg_entropy_nats": 0.15071333944797516, "avg_entropy_bits": 0.2174333930015564}
{"generated_text": "Panini's Law, which claims that words with fewer syllables come first, and Cooper and Ross's work, which expands to a large list of rules, many overlapping, and suggests that they can compound.", "avg_logp": -0.22020220756530762, "avg_entropy_nats": 0.5142005085945129, "avg_entropy_bits": 0.7418345808982849}
{"generated_text": "News publications, wine reviews, and Reddit.", "avg_logp": -0.1350354701280594, "avg_entropy_nats": 0.35298267006874084, "avg_entropy_bits": 0.5092463493347168}
{"generated_text": "They model a city description using embeddings by computing the TF-IDF weighted element-wise mean of the token-level representations from pretrained language models (ELMo, BERT, and GloVe).", "avg_logp": -0.10854082554578781, "avg_entropy_nats": 0.27658170461654663, "avg_entropy_bits": 0.39902305603027344}
{"generated_text": "They collect human judgments through a crowd-sourced experiment where workers are asked to identify the intruder in a triplet of cities.", "avg_logp": -0.35785624384880066, "avg_entropy_nats": 0.7704029083251953, "avg_entropy_bits": 1.1114565134048462}
{"generated_text": "A simple clustering algorithm that iteratively proposes random exchanges of memberships, only accepting proposals when the cluster strength increases, until convergence.", "avg_logp": -0.16752438247203827, "avg_entropy_nats": 0.4132247567176819, "avg_entropy_bits": 0.5961572527885437}
{"generated_text": "The approach performs better in the multi-domain setting, achieving a joint goal accuracy of 45.72% on the MultiWoZ test set, which is significantly better than most previous models.", "avg_logp": -0.19334647059440613, "avg_entropy_nats": 0.4533848464488983, "avg_entropy_bits": 0.6540960669517517}
{"generated_text": "Joint goal accuracy, joint domain accuracy, joint domain-slot accuracy, and cross-entropy loss.", "avg_logp": -0.23392291367053986, "avg_entropy_nats": 0.47185444831848145, "avg_entropy_bits": 0.680742084980011}
{"generated_text": "The WoZ2.0 dataset and the MultiWoZ dataset.", "avg_logp": -0.12563380599021912, "avg_entropy_nats": 0.23333773016929626, "avg_entropy_bits": 0.33663517236709595}
{"generated_text": "unanswerable", "avg_logp": -0.08943827450275421, "avg_entropy_nats": 0.35023295879364014, "avg_entropy_bits": 0.5052793025970459}
{"generated_text": "The article does not explicitly state why the model cannot learn first-order logic on natural language, but it implies that the complexity and variability of natural language may be a challenge.", "avg_logp": -0.32892313599586487, "avg_entropy_nats": 0.8261020183563232, "avg_entropy_bits": 1.191813349723816}
{"generated_text": "unanswerable", "avg_logp": -0.4612096846103668, "avg_entropy_nats": 0.9083423614501953, "avg_entropy_bits": 1.3104610443115234}
{"generated_text": "STORIES.", "avg_logp": -0.33875805139541626, "avg_entropy_nats": 0.8131191730499268, "avg_entropy_bits": 1.1730830669403076}
{"generated_text": "The answer is no.", "avg_logp": -0.465798556804657, "avg_entropy_nats": 0.8429985046386719, "avg_entropy_bits": 1.2161897420883179}
{"generated_text": "Because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement.", "avg_logp": -0.05929626151919365, "avg_entropy_nats": 0.16803470253944397, "avg_entropy_bits": 0.24242281913757324}
{"generated_text": "They measure grammaticality by computing the log ratio of the grammatical phrase over the ungrammatical phrase.", "avg_logp": -0.13789451122283936, "avg_entropy_nats": 0.35895609855651855, "avg_entropy_bits": 0.5178641676902771}
{"generated_text": "The inflection model of D18-1473.", "avg_logp": -0.12897922098636627, "avg_entropy_nats": 0.3272106945514679, "avg_entropy_bits": 0.472065269947052}
{"generated_text": "unanswerable \n\nThe article discusses the performance of several models, but it does not provide a clear answer to what the performance achieved by the model described in the paper is.", "avg_logp": -0.5038974285125732, "avg_entropy_nats": 1.0427199602127075, "avg_entropy_bits": 1.5043269395828247}
{"generated_text": "unanswerable", "avg_logp": -0.4555252194404602, "avg_entropy_nats": 0.9064360857009888, "avg_entropy_bits": 1.307710886001587}
{"generated_text": "The size of the datasets employed varies, including a relatively small corpus for the two-layer recurrent neural language models, a random subset of 4 million sentences (138 million word tokens) for the French frWaC dataset, and a large corpus for the pre-trained English language models (1 billion word tokens for the Billion Word benchmark and English Wikipedia).", "avg_logp": -0.3093889653682709, "avg_entropy_nats": 0.7125621438026428, "avg_entropy_bits": 1.0280097723007202}
{"generated_text": "Recurrent Neural Network (RNN) and Generative Recurrent Neural Network Grammars (RNNG) are the baseline models.", "avg_logp": -0.15689584612846375, "avg_entropy_nats": 0.41314929723739624, "avg_entropy_bits": 0.5960484147071838}
{"generated_text": "BLEU and ROUGE (1, 2, L) scores with multiple references.", "avg_logp": -0.0077677033841609955, "avg_entropy_nats": 0.03705137223005295, "avg_entropy_bits": 0.05345383659005165}
{"generated_text": "The E2E NLG challenge dataset and the official training and testing sets of the dataset.", "avg_logp": -0.255801260471344, "avg_entropy_nats": 0.6337496638298035, "avg_entropy_bits": 0.9143074154853821}
{"generated_text": "The model's better performance is attributed to its richer and deeper representation enhancement, as well as the use of pre-trained ELMo encoder and subword-level embeddings.", "avg_logp": -0.35764777660369873, "avg_entropy_nats": 0.8759578466415405, "avg_entropy_bits": 1.263739824295044}
{"generated_text": "unanswerable", "avg_logp": -0.4773741662502289, "avg_entropy_nats": 0.8785954713821411, "avg_entropy_bits": 1.267545223236084}
{"generated_text": "unanswerable \n\nThe article does not mention the exact F1 score and recall values, but it mentions that the model prioritized recall over precision and achieved a high recall.", "avg_logp": -0.5696921348571777, "avg_entropy_nats": 1.1223207712173462, "avg_entropy_bits": 1.6191667318344116}
{"generated_text": "unanswerable", "avg_logp": -0.2580343186855316, "avg_entropy_nats": 0.8219783306121826, "avg_entropy_bits": 1.1858640909194946}
{"generated_text": "The English Wikipedia dataset and a dump of 15 million English news articles published in the past 4 years.", "avg_logp": -0.16348028182983398, "avg_entropy_nats": 0.33136507868766785, "avg_entropy_bits": 0.478058785200119}
{"generated_text": "The dataset used is around 3 million news articles, with around 150 million sentences, 3 billion words, and 5 billion tokens.", "avg_logp": -0.2681121230125427, "avg_entropy_nats": 0.5641882419586182, "avg_entropy_bits": 0.8139515519142151}
{"generated_text": "The BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), the BIBREF6 system from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).", "avg_logp": -0.06740755587816238, "avg_entropy_nats": 0.15451644361019135, "avg_entropy_bits": 0.22292008996009827}
{"generated_text": "The proposed model achieves state-of-the-art performances on both datasets with significant improvements, specifically improving Ques. Match and Int. Match by 10.6 and 5.4 points, respectively, on SParC.", "avg_logp": -0.20420199632644653, "avg_entropy_nats": 0.4123159945011139, "avg_entropy_bits": 0.594846248626709}
{"generated_text": "SParC and CoSQL.", "avg_logp": -0.035884030163288116, "avg_entropy_nats": 0.12753614783287048, "avg_entropy_bits": 0.18399576842784882}
{"generated_text": "Concat, Turn, Gate, Action Copy, Tree Copy, and SQL Attn are evaluated.", "avg_logp": -0.17233119904994965, "avg_entropy_nats": 0.39447712898254395, "avg_entropy_bits": 0.5691102743148804}
{"generated_text": "SParC and CoSQL.", "avg_logp": -0.0347781702876091, "avg_entropy_nats": 0.12276919931173325, "avg_entropy_bits": 0.17711851000785828}
{"generated_text": "unanswerable", "avg_logp": -0.3881942927837372, "avg_entropy_nats": 0.8525712490081787, "avg_entropy_bits": 1.230000376701355}
{"generated_text": "Yes.", "avg_logp": -0.17632293701171875, "avg_entropy_nats": 0.44232258200645447, "avg_entropy_bits": 0.638136625289917}
{"generated_text": "yes", "avg_logp": -0.4208347201347351, "avg_entropy_nats": 0.9215165376663208, "avg_entropy_bits": 1.3294672966003418}
{"generated_text": "unanswerable", "avg_logp": -0.39292651414871216, "avg_entropy_nats": 0.7420231103897095, "avg_entropy_bits": 1.0705130100250244}
{"generated_text": "Kappa value.", "avg_logp": -0.30656346678733826, "avg_entropy_nats": 0.5901108980178833, "avg_entropy_bits": 0.8513500094413757}
{"generated_text": "The inter-annotator agreement is high, with a Kappa value of 0.99 for argument order, 0.88 for the scope of arguments, and 0.83 for second- and third-level senses.", "avg_logp": -0.1803687959909439, "avg_entropy_nats": 0.47773900628089905, "avg_entropy_bits": 0.6892316937446594}
{"generated_text": "The resources are adapted to properties of Chinese text by removing AltLexC and adding Progression into the sense hierarchy.", "avg_logp": -0.14692601561546326, "avg_entropy_nats": 0.4125295579433441, "avg_entropy_bits": 0.5951544046401978}
{"generated_text": "The results show that incorporating context information gives significant improvement in predicting argument impact, with the flat representation of the context achieving the highest F1 score.", "avg_logp": -0.20318353176116943, "avg_entropy_nats": 0.5092020034790039, "avg_entropy_bits": 0.7346232533454895}
{"generated_text": "The Majority baseline and SVM with RBF kernel, as well as FastText and BiLSTM with Attention.", "avg_logp": -0.3459128439426422, "avg_entropy_nats": 0.6381213665008545, "avg_entropy_bits": 0.92061448097229}
{"generated_text": "The dataset includes claims with both impact votes and the corresponding context of the argument, which is structured as an argument path consisting of claims and their relationships.", "avg_logp": -0.4659420847892761, "avg_entropy_nats": 1.0170361995697021, "avg_entropy_bits": 1.4672728776931763}
{"generated_text": "Impact votes and argument context.", "avg_logp": -0.5748611688613892, "avg_entropy_nats": 1.1535255908966064, "avg_entropy_bits": 1.6641854047775269}
{"generated_text": "The dataset used for training/testing ranges from 2,557 days for France to 2,922 days for the UK.", "avg_logp": -0.2062762975692749, "avg_entropy_nats": 0.5337092280387878, "avg_entropy_bits": 0.7699796557426453}
{"generated_text": "Yes, the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$, showing geometric properties translating linguistic similarities.", "avg_logp": -0.11716517060995102, "avg_entropy_nats": 0.32238075137138367, "avg_entropy_bits": 0.4650970697402954}
{"generated_text": "The embeddings display geometric properties pertaining to the behavior of the time series, such as words being grouped by context or influence on electricity consumption, and the norm of a word vector reflecting its significance.", "avg_logp": -0.2649342119693756, "avg_entropy_nats": 0.5920462012290955, "avg_entropy_bits": 0.8541420102119446}
{"generated_text": "Achieving less than 5% of Mean Absolute Percentage Error (MAPE) was deemed impressive by expert electricity forecasters.", "avg_logp": -0.07074779272079468, "avg_entropy_nats": 0.19716593623161316, "avg_entropy_bits": 0.28445032238960266}
{"generated_text": "Our proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis on the Stance Sentiment Emotion Corpus, achieving an improvement of 5 F-Score points over the existing state-of-the-art system for emotion analysis.", "avg_logp": -0.11466945707798004, "avg_entropy_nats": 0.2645261585712433, "avg_entropy_bits": 0.3816305696964264}
{"generated_text": "They improved the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.", "avg_logp": -0.10568077862262726, "avg_entropy_nats": 0.21529611945152283, "avg_entropy_bits": 0.31060662865638733}
{"generated_text": "SemEval 2016 Task 6 and the systems proposed by BIBREF15.", "avg_logp": -0.11365959793329239, "avg_entropy_nats": 0.26375579833984375, "avg_entropy_bits": 0.38051918148994446}
{"generated_text": "The multi-tasking is performed by jointly performing sentiment analysis and emotion analysis using a two-layered multi-task attention based neural network.", "avg_logp": -0.23387093842029572, "avg_entropy_nats": 0.5418513417243958, "avg_entropy_bits": 0.7817263007164001}
{"generated_text": "The datasets used for training are the SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC).", "avg_logp": -0.07780598104000092, "avg_entropy_nats": 0.18764719367027283, "avg_entropy_bits": 0.27071768045425415}
{"generated_text": "unanswerable", "avg_logp": -0.08751511573791504, "avg_entropy_nats": 0.3322378993034363, "avg_entropy_bits": 0.47931796312332153}
{"generated_text": "unanswerable", "avg_logp": -0.41987085342407227, "avg_entropy_nats": 0.8031997084617615, "avg_entropy_bits": 1.1587722301483154}
{"generated_text": "The previous state-of-the-art performance for sentiment analysis was 3.2 F-score points, and for emotion analysis, it was 5 F-Score points.", "avg_logp": -0.20639435946941376, "avg_entropy_nats": 0.5259700417518616, "avg_entropy_bits": 0.7588143944740295}
{"generated_text": "The classifier can significantly speed up the annotation process, making large-scale content analysis more feasible, by pre-filtering tweets for manual annotation.", "avg_logp": -0.15120846033096313, "avg_entropy_nats": 0.34487131237983704, "avg_entropy_bits": 0.4975440800189972}
{"generated_text": "The model needs to be improved by applying reasoning or irony detection methods to avoid category II and III errors.", "avg_logp": -0.3836607038974762, "avg_entropy_nats": 0.8366495966911316, "avg_entropy_bits": 1.2070300579071045}
{"generated_text": "Category I, II, and III errors, which are defined as errors that can be classified by humans with or without background knowledge, and errors that require correct interpretation of the tweet authors' intention.", "avg_logp": -0.36999812722206116, "avg_entropy_nats": 0.8529820442199707, "avg_entropy_bits": 1.230592966079712}
{"generated_text": "A convolutional neural network (CNN) is used.", "avg_logp": -0.18591657280921936, "avg_entropy_nats": 0.37941643595695496, "avg_entropy_bits": 0.5473822355270386}
{"generated_text": "The hashtag-based baseline uses hashtags with the highest pointwise mutual information (pmi) between a hashtag and a class.", "avg_logp": -0.20245814323425293, "avg_entropy_nats": 0.4736189842224121, "avg_entropy_bits": 0.6832877993583679}
{"generated_text": "unanswerable", "avg_logp": -0.32398778200149536, "avg_entropy_nats": 0.6967660784721375, "avg_entropy_bits": 1.0052210092544556}
{"generated_text": "The MH17 Twitter dataset introduced by BIBREF4.", "avg_logp": -0.07327093929052353, "avg_entropy_nats": 0.18602336943149567, "avg_entropy_bits": 0.2683749794960022}
{"generated_text": "Manual labeling of content, distant annotations (e.g., URL domain, hashtags), and Natural Language Processing (NLP) models.", "avg_logp": -0.2911621034145355, "avg_entropy_nats": 0.6866421699523926, "avg_entropy_bits": 0.99061518907547}
{"generated_text": "The HMM models pick up on spaces, indentation, and special characters in the data, such as comment symbols in Linux data and punctuation on the Shakespeare data.", "avg_logp": -0.18616946041584015, "avg_entropy_nats": 0.4105578362941742, "avg_entropy_bits": 0.5923097729682922}
{"generated_text": "The HMMs learn information about spaces, indentation, and special characters in the data, such as comment symbols, that the LSTMs don't.", "avg_logp": -0.2556205093860626, "avg_entropy_nats": 0.5707778930664062, "avg_entropy_bits": 0.8234583735466003}
{"generated_text": "The authors use visualizations to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.", "avg_logp": -0.09419983625411987, "avg_entropy_nats": 0.2288685441017151, "avg_entropy_bits": 0.33018749952316284}
{"generated_text": "The gap in performance between the HMMs and the LSTMs is relatively small, with the hybrid algorithm performing a bit better than the standalone LSTM on all text data sets.", "avg_logp": -0.25982043147087097, "avg_entropy_nats": 0.6368180513381958, "avg_entropy_bits": 0.9187341928482056}
{"generated_text": "yes", "avg_logp": -0.6233550310134888, "avg_entropy_nats": 1.13498854637146, "avg_entropy_bits": 1.6374423503875732}
{"generated_text": "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset.", "avg_logp": -0.18362940847873688, "avg_entropy_nats": 0.3673356771469116, "avg_entropy_bits": 0.5299533605575562}
{"generated_text": "300-dimensional Glove Common Crawl Embeddings (840B Token) are used, and then fine-tuned.", "avg_logp": -0.26513731479644775, "avg_entropy_nats": 0.5220708847045898, "avg_entropy_bits": 0.7531890273094177}
{"generated_text": "840B Token Common Crawl.", "avg_logp": -0.27702492475509644, "avg_entropy_nats": 0.6041956543922424, "avg_entropy_bits": 0.8716700077056885}
{"generated_text": "100k parameters (excluding embedding weights) for their model, compared to 250k parameters for BIBREF8's model.", "avg_logp": -0.2757732570171356, "avg_entropy_nats": 0.6034348011016846, "avg_entropy_bits": 0.8705723881721497}
{"generated_text": "100k parameters, excluding the embedding weights.", "avg_logp": -0.17063577473163605, "avg_entropy_nats": 0.5021326541900635, "avg_entropy_bits": 0.7244243025779724}
{"generated_text": "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset were used.", "avg_logp": -0.15126676857471466, "avg_entropy_nats": 0.31799882650375366, "avg_entropy_bits": 0.4587753117084503}
{"generated_text": "The system's F1 performance outperformed previous results by as much as 12 F1 points, establishing a new state of the art for classifying hate speech.", "avg_logp": -0.1198127493262291, "avg_entropy_nats": 0.26385965943336487, "avg_entropy_bits": 0.38066908717155457}
{"generated_text": "A logistic regression baseline model using character n-grams and word unigrams with TF*IDF weighting.", "avg_logp": -0.12779927253723145, "avg_entropy_nats": 0.2563154697418213, "avg_entropy_bits": 0.3697851002216339}
{"generated_text": "The datasets used were KFTT (a collection of Wikipedia articles about the city of Kyoto) and BTEC (a travel conversation corpus), as well as the larger Japanese-English ASPEC dataset.", "avg_logp": -0.12344175577163696, "avg_entropy_nats": 0.2855510413646698, "avg_entropy_bits": 0.4119630753993988}
{"generated_text": "English-to-Japanese.", "avg_logp": -0.17041748762130737, "avg_entropy_nats": 0.36340782046318054, "avg_entropy_bits": 0.5242866277694702}
{"generated_text": "25% more roles.", "avg_logp": -0.2871195673942566, "avg_entropy_nats": 0.650564432144165, "avg_entropy_bits": 0.9385662078857422}
{"generated_text": "Coverage was measured by evaluating against expertly annotated data and comparison with PropBank.", "avg_logp": -0.21811233460903168, "avg_entropy_nats": 0.5377695560455322, "avg_entropy_bits": 0.7758374810218811}
{"generated_text": "The quality of the dataset was measured using F1 scores, specifically 79.8 F1 for individual worker-vs-worker agreement and 84.1 F1 for overall consistency of the dataset.", "avg_logp": -0.27107152342796326, "avg_entropy_nats": 0.6211842894554138, "avg_entropy_bits": 0.8961796164512634}
{"generated_text": "The corpus was obtained by crowdsourcing, with 133K verbs annotated by a single QA-generating worker and validated by two others.", "avg_logp": -0.2855314612388611, "avg_entropy_nats": 0.6529682278633118, "avg_entropy_bits": 0.9420340061187744}
{"generated_text": "The workers are trained through a short training procedure with extensive personal feedback, and 1 out of 3 participants are selected after exhibiting good performance, tested against expert annotations.", "avg_logp": -0.28745585680007935, "avg_entropy_nats": 0.6236404180526733, "avg_entropy_bits": 0.8997229337692261}
{"generated_text": "The improved annotation protocol involves systematically screening workers, providing concise guidelines, and performing a short training procedure, as well as employing two independent workers plus an additional consolidator for each predicate.", "avg_logp": -0.24704568088054657, "avg_entropy_nats": 0.6217042207717896, "avg_entropy_bits": 0.8969295620918274}
{"generated_text": "The previous dataset was annotated by non-expert workers after a brief training procedure, and later by crowdsourced workers with a validation step.", "avg_logp": -0.3213123679161072, "avg_entropy_nats": 0.7435877919197083, "avg_entropy_bits": 1.0727704763412476}
{"generated_text": "The dataset size is not explicitly stated in the article, but it is mentioned that the 2018 dataset contains 133K verbs with 2.0 QA pairs per verb on average.", "avg_logp": -0.25622862577438354, "avg_entropy_nats": 0.6876899003982544, "avg_entropy_bits": 0.9921268820762634}
{"generated_text": "unanswerable", "avg_logp": -0.07778017222881317, "avg_entropy_nats": 0.2772224545478821, "avg_entropy_bits": 0.39994746446609497}
{"generated_text": "unanswerable", "avg_logp": -0.2002573311328888, "avg_entropy_nats": 0.5286511182785034, "avg_entropy_bits": 0.7626824378967285}
{"generated_text": "unanswerable", "avg_logp": -0.2813475430011749, "avg_entropy_nats": 0.8773965239524841, "avg_entropy_bits": 1.2658156156539917}
{"generated_text": "2014.", "avg_logp": -0.3466378152370453, "avg_entropy_nats": 0.7519768476486206, "avg_entropy_bits": 1.0848733186721802}
{"generated_text": "97.6% when using the object's color to uniquely identify it, and 96.0% when using the object's shape.", "avg_logp": -0.26011204719543457, "avg_entropy_nats": 0.513361394405365, "avg_entropy_bits": 0.7406238913536072}
{"generated_text": "A simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command.", "avg_logp": -0.10202936083078384, "avg_entropy_nats": 0.22247818112373352, "avg_entropy_bits": 0.3209681510925293}
{"generated_text": "Supervised learning manner.", "avg_logp": -0.28386154770851135, "avg_entropy_nats": 0.6339932680130005, "avg_entropy_bits": 0.914658784866333}
{"generated_text": "Yes.", "avg_logp": -0.48453378677368164, "avg_entropy_nats": 1.0166220664978027, "avg_entropy_bits": 1.4666755199432373}
{"generated_text": "unanswerable", "avg_logp": -0.3150489628314972, "avg_entropy_nats": 0.6523735523223877, "avg_entropy_bits": 0.9411760568618774}
{"generated_text": "unanswerable", "avg_logp": -0.44092437624931335, "avg_entropy_nats": 0.8729732632637024, "avg_entropy_bits": 1.2594342231750488}
{"generated_text": "unanswerable", "avg_logp": -0.40576374530792236, "avg_entropy_nats": 0.7798689603805542, "avg_entropy_bits": 1.1251130104064941}
{"generated_text": "unanswerable", "avg_logp": -0.14320826530456543, "avg_entropy_nats": 0.40015071630477905, "avg_entropy_bits": 0.5772954821586609}
{"generated_text": "unanswerable", "avg_logp": -0.15002799034118652, "avg_entropy_nats": 0.4100759029388428, "avg_entropy_bits": 0.5916144251823425}
{"generated_text": "Non-contextual properties of a word.", "avg_logp": -0.16035601496696472, "avg_entropy_nats": 0.3543259799480438, "avg_entropy_bits": 0.5111842751502991}
{"generated_text": "The random model, which predicts 1 with 15% probability and 0 otherwise.", "avg_logp": -0.15835481882095337, "avg_entropy_nats": 0.3769949674606323, "avg_entropy_bits": 0.5438887476921082}
{"generated_text": "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.", "avg_logp": -0.0375552773475647, "avg_entropy_nats": 0.11665113270282745, "avg_entropy_bits": 0.16829200088977814}
{"generated_text": "The overall baseline results on this new task are that all models outperform the random baseline by a wide margin, with the best F1 score in content words more than doubling that of the random baseline (0.286 vs. 0.116).", "avg_logp": -0.16259421408176422, "avg_entropy_nats": 0.40048328042030334, "avg_entropy_bits": 0.577775239944458}
{"generated_text": "The F1 score is used as the evaluation metric, especially since the problem is imbalanced.", "avg_logp": -0.20483043789863586, "avg_entropy_nats": 0.44243183732032776, "avg_entropy_bits": 0.6382942199707031}
{"generated_text": "The authors provide an intriguing U-shape in the scatter plot, where words that are most likely to be echoed are either unusually frequent or unusually rare.", "avg_logp": -0.2704446613788605, "avg_entropy_nats": 0.6099415421485901, "avg_entropy_bits": 0.8799595832824707}
{"generated_text": "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.", "avg_logp": -0.02835678867995739, "avg_entropy_nats": 0.10479448735713959, "avg_entropy_bits": 0.1511864811182022}
{"generated_text": "The SQUAD dataset and a random subset of 70,484 instances from it.", "avg_logp": -0.22099873423576355, "avg_entropy_nats": 0.5308364033699036, "avg_entropy_bits": 0.7658349871635437}
{"generated_text": "The performance of this system is measured using the BLEU metric for the VQG model and human evaluation for the chatbot model.", "avg_logp": -0.16163673996925354, "avg_entropy_nats": 0.38789424300193787, "avg_entropy_bits": 0.559613049030304}
{"generated_text": "15,000 images with 5 questions per image, adding a total of 75,000 questions.", "avg_logp": -0.08046791702508926, "avg_entropy_nats": 0.30403250455856323, "avg_entropy_bits": 0.438626229763031}
{"generated_text": "Yes, the VQG model is similar to image captioning ML systems, as it uses a CNN to learn image features and a decoder to generate a question from the image.", "avg_logp": -0.32511845231056213, "avg_entropy_nats": 0.7211223244667053, "avg_entropy_bits": 1.0403594970703125}
{"generated_text": "The dataset used for training the system consists of 15,000 images with 75,000 questions from MS COCO, Bing, and Flickr datasets, and 162,064 utterances over 10,907 dialogues from the Persona-chat dataset, and 304,713 utterances over 220,579 conversational exchanges from the Cornell-movie corpus.", "avg_logp": -0.11657209694385529, "avg_entropy_nats": 0.30618590116500854, "avg_entropy_bits": 0.44173288345336914}
{"generated_text": "They obtain word lattices from words by treating all possible substrings of a sentence as vertices and connecting them by directed edges according to their positions in the original sentence.", "avg_logp": -0.20790067315101624, "avg_entropy_nats": 0.4540911018848419, "avg_entropy_bits": 0.6551149487495422}
{"generated_text": "P@1, MAP, and MRR for DBQA, and P@1 and MRR for KBRE.", "avg_logp": -0.09169640392065048, "avg_entropy_nats": 0.19527384638786316, "avg_entropy_bits": 0.2817206084728241}
{"generated_text": "DBQA and KBRE.", "avg_logp": -0.24346019327640533, "avg_entropy_nats": 0.4636310935020447, "avg_entropy_bits": 0.6688783168792725}
{"generated_text": "unanswerable", "avg_logp": -0.2980385422706604, "avg_entropy_nats": 0.7589386105537415, "avg_entropy_bits": 1.0949169397354126}
{"generated_text": "yes", "avg_logp": -0.7528993487358093, "avg_entropy_nats": 1.3544294834136963, "avg_entropy_bits": 1.954028606414795}
{"generated_text": "They propose using recurrent architectures, evaluating the combination of audio and ultrasound signals, and extending the classification to more fine-grained place of articulation.", "avg_logp": -0.32156819105148315, "avg_entropy_nats": 0.7265916466712952, "avg_entropy_bits": 1.0482500791549683}
{"generated_text": "The dataset, known as the Ultrax Typically Developing dataset (UXTD), contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, with 31 females and 27 males.", "avg_logp": -0.134874626994133, "avg_entropy_nats": 0.2973916828632355, "avg_entropy_bits": 0.4290454685688019}
{"generated_text": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification.", "avg_logp": -0.039508264511823654, "avg_entropy_nats": 0.1371983289718628, "avg_entropy_bits": 0.1979353427886963}
{"generated_text": "Yes.", "avg_logp": -0.28848618268966675, "avg_entropy_nats": 0.7128696441650391, "avg_entropy_bits": 1.0284535884857178}
{"generated_text": "The dataset has a total of 10700 training examples.", "avg_logp": -0.30944082140922546, "avg_entropy_nats": 0.6905443072319031, "avg_entropy_bits": 0.9962448477745056}
{"generated_text": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used to classify phonetic segments.", "avg_logp": -0.09653019160032272, "avg_entropy_nats": 0.2624797224998474, "avg_entropy_bits": 0.37867823243141174}
{"generated_text": "58 speakers.", "avg_logp": -0.4108673334121704, "avg_entropy_nats": 0.9348732233047485, "avg_entropy_bits": 1.3487370014190674}
{"generated_text": "Significant improvements (with p-value < 0.05)", "avg_logp": -0.3151179254055023, "avg_entropy_nats": 0.6936682462692261, "avg_entropy_bits": 1.0007518529891968}
{"generated_text": "The multi-turn dialog system learns by directly from the data, without relying on human-defined heuristic rules.", "avg_logp": -0.3142629861831665, "avg_entropy_nats": 0.6755015850067139, "avg_entropy_bits": 0.9745427966117859}
{"generated_text": "Human evaluation is performed by recruiting English-speaking students to rate the responses generated by the models based on three criteria: grammatical correctness, contextual coherence, and emotional appropriateness.", "avg_logp": -0.0728968009352684, "avg_entropy_nats": 0.2235075980424881, "avg_entropy_bits": 0.3224532902240753}
{"generated_text": "Yes, human evaluation metrics such as grammatical correctness, contextual coherence, and emotional appropriateness are also measured.", "avg_logp": -0.1939634084701538, "avg_entropy_nats": 0.40136343240737915, "avg_entropy_bits": 0.5790449976921082}
{"generated_text": "The vanilla sequence-to-sequence model (S2S) and the Hierarchical Recurrent Attention Network (HRAN).", "avg_logp": -0.08433175832033157, "avg_entropy_nats": 0.1611984223127365, "avg_entropy_bits": 0.2325601726770401}
{"generated_text": "unanswerable", "avg_logp": -0.12982334196567535, "avg_entropy_nats": 0.40180420875549316, "avg_entropy_bits": 0.5796809196472168}
{"generated_text": "unanswerable", "avg_logp": -0.23739570379257202, "avg_entropy_nats": 0.5872697830200195, "avg_entropy_bits": 0.8472511768341064}
{"generated_text": "The main reason syntactic parsing is important for semantic role labelling for interlanguages is that it generates correct syntactic analyses for partial grammatical fragments in L2 texts, providing crucial information for SRL.", "avg_logp": -0.11918409913778305, "avg_entropy_nats": 0.257651686668396, "avg_entropy_bits": 0.3717128336429596}
{"generated_text": "Two senior students majoring in Applied Linguistics.", "avg_logp": -0.09053459018468857, "avg_entropy_nats": 0.21029353141784668, "avg_entropy_bits": 0.30338940024375916}
{"generated_text": "They outperform existing state-of-the-art VQA models by a large margin in terms of visual grounding correlation, with a 36.4% increase in rank-correlation when evaluated in VQA-HAT dataset and a 7.7% increase when evaluated in VQA-X.", "avg_logp": -0.18165552616119385, "avg_entropy_nats": 0.3946070075035095, "avg_entropy_bits": 0.5692976713180542}
{"generated_text": "They measure the correlation using rank-correlation.", "avg_logp": -0.17598192393779755, "avg_entropy_nats": 0.39825233817100525, "avg_entropy_bits": 0.5745566487312317}
{"generated_text": "They obtain region descriptions and object annotations from the Visual Genome dataset.", "avg_logp": -0.06979470700025558, "avg_entropy_nats": 0.20636624097824097, "avg_entropy_bits": 0.29772353172302246}
{"generated_text": "SNLI + MultiNLI.", "avg_logp": -0.18086719512939453, "avg_entropy_nats": 0.49395471811294556, "avg_entropy_bits": 0.7126259803771973}
{"generated_text": "BERT.", "avg_logp": -0.23410192131996155, "avg_entropy_nats": 0.6391129493713379, "avg_entropy_bits": 0.922045111656189}
{"generated_text": "six high-performing models covering sentence encoding models, cross-sentence attention models, and fine-tuned pre-trained language models.", "avg_logp": -0.12994064390659332, "avg_entropy_nats": 0.26288366317749023, "avg_entropy_bits": 0.37926095724105835}
{"generated_text": "SNLI, MultiNLI, and SICK datasets were used.", "avg_logp": -0.13533742725849152, "avg_entropy_nats": 0.2788150906562805, "avg_entropy_bits": 0.4022451341152191}
{"generated_text": "unanswerable", "avg_logp": -0.13077887892723083, "avg_entropy_nats": 0.46233469247817993, "avg_entropy_bits": 0.6670079231262207}
{"generated_text": "yes", "avg_logp": -0.5440292358398438, "avg_entropy_nats": 0.9045806527137756, "avg_entropy_bits": 1.3050340414047241}
{"generated_text": "They use a stacking ensemble model with a simple full-connection model as the ensemble model.", "avg_logp": -0.3247557580471039, "avg_entropy_nats": 0.7233688235282898, "avg_entropy_bits": 1.0436005592346191}
{"generated_text": "unanswerable", "avg_logp": -0.054251961410045624, "avg_entropy_nats": 0.23882117867469788, "avg_entropy_bits": 0.3445461094379425}
{"generated_text": "unanswerable", "avg_logp": -0.06700161099433899, "avg_entropy_nats": 0.2738605737686157, "avg_entropy_bits": 0.3950972557067871}
{"generated_text": "The dataset used is the VLSP 2019 HSD dataset, which was crawled from various sources including Facebook, and also includes data from VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, and text crawled from Facebook.", "avg_logp": -0.24719692766666412, "avg_entropy_nats": 0.5587589740753174, "avg_entropy_bits": 0.8061187863349915}
{"generated_text": "unanswerable \n\nThe article does not explicitly mention the language of the Twitter data used in the study.", "avg_logp": -0.2336663156747818, "avg_entropy_nats": 0.5864808559417725, "avg_entropy_bits": 0.8461130857467651}
{"generated_text": "The article does not explicitly state the other interesting correlations observed, but it mentions that \"we observe interesting hidden correlation in data\" and specifically notes the correlation between \"Yoga-Veganism\" and \"Women-Yoga\".", "avg_logp": -0.3221331238746643, "avg_entropy_nats": 0.7723102569580078, "avg_entropy_bits": 1.1142079830169678}
{"generated_text": "The baselines were a simple averaging model and a BM25 feature combined with evidence averaging.", "avg_logp": -0.4060603678226471, "avg_entropy_nats": 0.9875388741493225, "avg_entropy_bits": 1.4247173070907593}
{"generated_text": "The state of the art for ranking MC Test answers is the HABCNN-TE model.", "avg_logp": -0.195716992020607, "avg_entropy_nats": 0.48735448718070984, "avg_entropy_bits": 0.7031038999557495}
{"generated_text": "The size of the introduced dataset is not explicitly stated in the article, but it is mentioned that the final size of the dataset is outlined in Fig. FIGREF8.", "avg_logp": -0.2793673276901245, "avg_entropy_nats": 0.6957774758338928, "avg_entropy_bits": 1.003794550895691}
{"generated_text": "The Argus dataset, AI2-8grade/CK12 dataset, and MCTest dataset.", "avg_logp": -0.10139397531747818, "avg_entropy_nats": 0.26842001080513, "avg_entropy_bits": 0.3872481882572174}
{"generated_text": "The BLEU metric is adopted to evaluate the model performance during evaluation.", "avg_logp": -0.03505321592092514, "avg_entropy_nats": 0.10565824806690216, "avg_entropy_bits": 0.1524326354265213}
{"generated_text": "The WMT 2014 English-French translation dataset, WMT 2014 English-German translation dataset, IWSLT 2014 German-English translation dataset, and IWSLT 2015 English-Vietnamese translation dataset.", "avg_logp": -0.06560495495796204, "avg_entropy_nats": 0.12281657755374908, "avg_entropy_bits": 0.17718686163425446}
{"generated_text": "WMT 2014 English-French translation, WMT 2014 English-German translation, and IWSLT 2014 German-English translation.", "avg_logp": -0.09491752833127975, "avg_entropy_nats": 0.2066810578107834, "avg_entropy_bits": 0.29817771911621094}
{"generated_text": "2.2 BLEU gains in En-Fr translation.", "avg_logp": -0.17501746118068695, "avg_entropy_nats": 0.4325637221336365, "avg_entropy_bits": 0.6240574717521667}
{"generated_text": "They determine the opinion summary by applying a bi-linear term to calculate the association score between the opinion representation and each aspect, and then obtaining the improved opinion summary via a weighted sum of the opinion representations.", "avg_logp": -0.2476198524236679, "avg_entropy_nats": 0.5484949946403503, "avg_entropy_bits": 0.7913109064102173}
{"generated_text": "Yes.", "avg_logp": -0.24573950469493866, "avg_entropy_nats": 0.5811359286308289, "avg_entropy_bits": 0.8384019732475281}
{"generated_text": "The datasets used to train the model are not explicitly stated as the training datasets, but the authors mention that they use the standard train-test split in the SemEval challenge for evaluation, and the datasets used for evaluation are listed as SemEval 2014 (laptop domain), SemEval 2014 (restaurant domain), SemEval 2015 (restaurant domain), and SemEval 2016 (restaurant domain).", "avg_logp": -0.2907453179359436, "avg_entropy_nats": 0.6885120272636414, "avg_entropy_bits": 0.9933128356933594}
{"generated_text": "5.0%, 1.6%, 1.4%, 1.3% absolute gains on the four datasets.", "avg_logp": -0.14503787457942963, "avg_entropy_nats": 0.3370763957500458, "avg_entropy_bits": 0.48629847168922424}
{"generated_text": "About 23.", "avg_logp": -0.22339701652526855, "avg_entropy_nats": 0.4543033540248871, "avg_entropy_bits": 0.655421257019043}
{"generated_text": "state-of-the-art neural seq2seq architectures, including n-gram, convolution, LSTM, Transformer, and GPT-2 models.", "avg_logp": -0.15002571046352386, "avg_entropy_nats": 0.30592143535614014, "avg_entropy_bits": 0.4413513243198395}
{"generated_text": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations.", "avg_logp": -0.009179632179439068, "avg_entropy_nats": 0.03360630199313164, "avg_entropy_bits": 0.04848363995552063}
{"generated_text": "unanswerable", "avg_logp": -0.5086092948913574, "avg_entropy_nats": 0.9236395359039307, "avg_entropy_bits": 1.3325300216674805}
{"generated_text": "The traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text because they only connect adjacent words.", "avg_logp": -0.15342557430267334, "avg_entropy_nats": 0.3632437288761139, "avg_entropy_bits": 0.5240498781204224}
{"generated_text": "The use of word embeddings does not replace some previous features of the model, but rather establishes additional \"virtual\" edges to connect semantically similar nodes.", "avg_logp": -0.29272758960723877, "avg_entropy_nats": 0.6219083666801453, "avg_entropy_bits": 0.8972241282463074}
{"generated_text": "The previous co-occurrence networks are based on a simple word adjacency network model and a word co-occurrence network model.", "avg_logp": -0.3591778874397278, "avg_entropy_nats": 0.7515605688095093, "avg_entropy_bits": 1.0842726230621338}
{"generated_text": "BLEU score was initially considered, but the authors found it to be inappropriate, and instead, they used manual evaluation to assess the relevance of the generated explanations.", "avg_logp": -0.5701960325241089, "avg_entropy_nats": 1.0870403051376343, "avg_entropy_bits": 1.568267583847046}
{"generated_text": "unanswerable", "avg_logp": -0.22661471366882324, "avg_entropy_nats": 0.666025698184967, "avg_entropy_bits": 0.9608719348907471}
{"generated_text": "One explanation for each instance in the training set of SNLI, and three explanations for each instance in the validation and testing sets of SNLI.", "avg_logp": -0.19407571852207184, "avg_entropy_nats": 0.4202114939689636, "avg_entropy_bits": 0.6062370538711548}
{"generated_text": "The performance difference is 0.48% improvement.", "avg_logp": -0.4530189633369446, "avg_entropy_nats": 1.1645307540893555, "avg_entropy_bits": 1.6800626516342163}
{"generated_text": "neutral class with approximately 31% errors.", "avg_logp": -0.407121866941452, "avg_entropy_nats": 0.7842355966567993, "avg_entropy_bits": 1.1314127445220947}
{"generated_text": "A dump of the Italian Wikipedia (dated 2019.04.01), the main categories of Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).", "avg_logp": -0.06012798473238945, "avg_entropy_nats": 0.12884974479675293, "avg_entropy_bits": 0.18589088320732117}
{"generated_text": "Yes.", "avg_logp": -0.21245799958705902, "avg_entropy_nats": 0.5440714359283447, "avg_entropy_bits": 0.7849291563034058}
{"generated_text": "Yes.", "avg_logp": -0.21221774816513062, "avg_entropy_nats": 0.5427903532981873, "avg_entropy_bits": 0.7830809354782104}
{"generated_text": "The dataset used to train Word2Vec for the Italian Language is composed of 2.6 GB of raw text, including 421,829,960 words divided into 17,305,401 sentences.", "avg_logp": -0.06847745180130005, "avg_entropy_nats": 0.18571946024894714, "avg_entropy_bits": 0.2679365277290344}
{"generated_text": "Different parameter settings, such as the number of epochs, window size, and negative sampling value, significantly impact the performance and semantic capacity of the resulting model, with some configurations leading to oscillatory trends and others resulting in improved accuracy.", "avg_logp": -0.2523503005504608, "avg_entropy_nats": 0.5230094194412231, "avg_entropy_bits": 0.7545431852340698}
{"generated_text": "unanswerable", "avg_logp": -0.2487233281135559, "avg_entropy_nats": 0.6207866668701172, "avg_entropy_bits": 0.8956059217453003}
{"generated_text": "The dataset used for training Word2Vec in Italian language was obtained from a dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).", "avg_logp": -0.08750829845666885, "avg_entropy_nats": 0.21811899542808533, "avg_entropy_bits": 0.31467917561531067}
{"generated_text": "The auxiliary signals from the morphology table are incorporated in the decoder through an attention module, which assigns different weights to the affixes in the table to exclude irrelevant ones and amplify relevant ones.", "avg_logp": -0.18112295866012573, "avg_entropy_nats": 0.4441640377044678, "avg_entropy_bits": 0.6407932639122009}
{"generated_text": "affixes.", "avg_logp": -0.5709740519523621, "avg_entropy_nats": 1.0876073837280273, "avg_entropy_bits": 1.569085717201233}
{"generated_text": "Yes.", "avg_logp": -0.6516196727752686, "avg_entropy_nats": 1.061446189880371, "avg_entropy_bits": 1.5313432216644287}
{"generated_text": "Yes.", "avg_logp": -0.25527483224868774, "avg_entropy_nats": 0.6789844036102295, "avg_entropy_bits": 0.9795674681663513}
{"generated_text": "The machine learning models used include RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, and SVM with linear kernel.", "avg_logp": -0.1296529769897461, "avg_entropy_nats": 0.28791505098342896, "avg_entropy_bits": 0.4153735935688019}
{"generated_text": "The methodology used to compensate for limited labelled data is the assumption that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets.", "avg_logp": -0.18176515400409698, "avg_entropy_nats": 0.46003037691116333, "avg_entropy_bits": 0.6636834740638733}
{"generated_text": "The five natural disasters examined were the East Coast Bomb Cyclone, the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, and the California Camp Fires.", "avg_logp": -0.038896966725587845, "avg_entropy_nats": 0.08888363838195801, "avg_entropy_bits": 0.1282319873571396}
{"generated_text": "Twitter.", "avg_logp": -0.11873944848775864, "avg_entropy_nats": 0.3072442412376404, "avg_entropy_bits": 0.44325971603393555}
{"generated_text": "They used a refined collection of tweets gathered from Twitter, which contains 8,257 tweets and 12,784 entities in total.", "avg_logp": -0.18084782361984253, "avg_entropy_nats": 0.4066905379295349, "avg_entropy_bits": 0.586730420589447}
{"generated_text": "unanswerable", "avg_logp": -0.24450740218162537, "avg_entropy_nats": 0.6531196236610413, "avg_entropy_bits": 0.9422523975372314}
{"generated_text": "Significant patterns were discovered, including that harassment occurred more frequently at night, conductors and drivers were top harassers, and strong correlations existed between age of perpetrators and location, single/multiple harasser(s) and location, and age and single/multiple harasser(s).", "avg_logp": -0.2203255146741867, "avg_entropy_nats": 0.4355328679084778, "avg_entropy_bits": 0.6283410787582397}
{"generated_text": "Yes.", "avg_logp": -0.30398309230804443, "avg_entropy_nats": 0.5987357497215271, "avg_entropy_bits": 0.8637930750846863}
{"generated_text": "They are experimenting with slot-filling tasks for four different apps: booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.", "avg_logp": -0.1568387746810913, "avg_entropy_nats": 0.41019564867019653, "avg_entropy_bits": 0.5917872190475464}
{"generated_text": "unanswerable", "avg_logp": -0.23635536432266235, "avg_entropy_nats": 0.7042140960693359, "avg_entropy_bits": 1.0159661769866943}
{"generated_text": "They select answer candidates by searching for (concept $_2$, relation, $_2$1) and (concept $_2$0, relation, $_2$1) in ConceptNet to select distractors that share the same concept $_2$ or concept $_1$ and the relation.", "avg_logp": -0.18610888719558716, "avg_entropy_nats": 0.46390920877456665, "avg_entropy_bits": 0.6692795157432556}
{"generated_text": "They use statistical measures, mathematical and probabilistic frameworks, text mining procedures, and natural language processing tools to extract causality from text.", "avg_logp": -0.20240114629268646, "avg_entropy_nats": 0.47064080834388733, "avg_entropy_bits": 0.6789911985397339}
{"generated_text": "A 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API.", "avg_logp": -0.11935892701148987, "avg_entropy_nats": 0.339755117893219, "avg_entropy_bits": 0.4901629686355591}
{"generated_text": "High certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication.", "avg_logp": -0.19956809282302856, "avg_entropy_nats": 0.5105783939361572, "avg_entropy_bits": 0.7366089224815369}
{"generated_text": "They use a combination of crowdsourcing (for sentiment scores) and automatic methods (such as the Stanford CoreNLP toolkit and Latent Dirichlet Allocation) to analyze the corpora.", "avg_logp": -0.3044302463531494, "avg_entropy_nats": 0.6502975225448608, "avg_entropy_bits": 0.9381809830665588}
{"generated_text": "They collect a comparable corpus by gathering the same number of statements selected at random, but controlling for time of year.", "avg_logp": -0.23173996806144714, "avg_entropy_nats": 0.48614829778671265, "avg_entropy_bits": 0.701363742351532}
{"generated_text": "They select control documents that do not contain any of the causal words (`caused', `causing', or `causes') or bidirectional words, and match them temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013, then select them randomly.", "avg_logp": -0.1813228875398636, "avg_entropy_nats": 0.38392508029937744, "avg_entropy_bits": 0.5538867712020874}
{"generated_text": "The experiment is conducted on the WebQA dataset, which is a Chinese-English bilingual dataset.", "avg_logp": -0.6540406346321106, "avg_entropy_nats": 1.3844311237335205, "avg_entropy_bits": 1.9973121881484985}
{"generated_text": "MemN2N and Attentive and Impatient Readers.", "avg_logp": -0.07738380879163742, "avg_entropy_nats": 0.16545610129833221, "avg_entropy_bits": 0.2387026846408844}
{"generated_text": "The type and correctness of all the question answer pairs were verified by at least two annotators.", "avg_logp": -0.10968863219022751, "avg_entropy_nats": 0.24955669045448303, "avg_entropy_bits": 0.36003419756889343}
{"generated_text": "unanswerable", "avg_logp": -0.18082547187805176, "avg_entropy_nats": 0.47015640139579773, "avg_entropy_bits": 0.6782922744750977}
{"generated_text": "no", "avg_logp": -0.7524345517158508, "avg_entropy_nats": 1.4958429336547852, "avg_entropy_bits": 2.1580450534820557}
{"generated_text": "Resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots, \\pi _n\\rbrace $ are introduced to distinguish mentions resolved by different categories of information, defined as $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $, indicating string-matching, precise-construct, or attribute-matching modes.", "avg_logp": -0.16380545496940613, "avg_entropy_nats": 0.3173583447933197, "avg_entropy_bits": 0.45785123109817505}
{"generated_text": "The model presented in the paper outperforms several state-of-the-art supervised coreference resolution systems, but its performance is compared to other unsupervised systems.", "avg_logp": -0.4301694333553314, "avg_entropy_nats": 0.9432216286659241, "avg_entropy_bits": 1.360781192779541}
{"generated_text": "unanswerable", "avg_logp": -0.3611750602722168, "avg_entropy_nats": 0.8111259937286377, "avg_entropy_bits": 1.1702073812484741}
{"generated_text": "unanswerable", "avg_logp": -0.22990682721138, "avg_entropy_nats": 0.5693739056587219, "avg_entropy_bits": 0.821432888507843}
{"generated_text": "They mention collecting external data for training and developing in task 1, and releasing 11 examples of complete user intent and 3 data files for participants to build their dialogue systems in task 2.", "avg_logp": -0.21322475373744965, "avg_entropy_nats": 0.5480042099952698, "avg_entropy_bits": 0.79060298204422}
{"generated_text": "unanswerable", "avg_logp": -0.313064843416214, "avg_entropy_nats": 0.7611560225486755, "avg_entropy_bits": 1.0981160402297974}
{"generated_text": "unanswerable", "avg_logp": -0.33312758803367615, "avg_entropy_nats": 0.656028151512146, "avg_entropy_bits": 0.946448564529419}
{"generated_text": "The metrics used in the evaluation are F1-score, task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, and guidance ability for out of scope input.", "avg_logp": -0.10539592057466507, "avg_entropy_nats": 0.20627284049987793, "avg_entropy_bits": 0.2975887954235077}
{"generated_text": "They measure the quality of summaries using Rouge-L.", "avg_logp": -0.28040367364883423, "avg_entropy_nats": 0.590478241443634, "avg_entropy_bits": 0.8518800139427185}
{"generated_text": "yes", "avg_logp": -0.5754320025444031, "avg_entropy_nats": 1.0505332946777344, "avg_entropy_bits": 1.5155991315841675}
{"generated_text": "They refer to the different formats or ways in which an answer can be generated, such as concise phrases or well-formed sentences that make sense without the context of the question.", "avg_logp": -0.271556556224823, "avg_entropy_nats": 0.5171384811401367, "avg_entropy_bits": 0.7460730671882629}
{"generated_text": "no.", "avg_logp": -0.8166931867599487, "avg_entropy_nats": 1.3447535037994385, "avg_entropy_bits": 1.9400691986083984}
{"generated_text": "MCAN and state-of-the-art extractive approaches.", "avg_logp": -0.1818290650844574, "avg_entropy_nats": 0.39844810962677, "avg_entropy_bits": 0.5748391151428223}
{"generated_text": "unanswerable", "avg_logp": -0.21787047386169434, "avg_entropy_nats": 0.5195438861846924, "avg_entropy_bits": 0.7495434284210205}
{"generated_text": "The answer style refers to the format or manner in which an answer is presented, such as a concise phrase, a well-formed sentence, or a specific length.", "avg_logp": -0.23940719664096832, "avg_entropy_nats": 0.5766840577125549, "avg_entropy_bits": 0.8319791555404663}
{"generated_text": "LipCH-Net.", "avg_logp": -0.1414344310760498, "avg_entropy_nats": 0.34453099966049194, "avg_entropy_bits": 0.49705320596694946}
{"generated_text": "sequence-to-sequence learning.", "avg_logp": -0.265066921710968, "avg_entropy_nats": 0.6003009080886841, "avg_entropy_bits": 0.8660510778427124}
{"generated_text": "The visible movements of the neck, head, and mouth.", "avg_logp": -0.09119096398353577, "avg_entropy_nats": 0.20193469524383545, "avg_entropy_bits": 0.2913301885128021}
{"generated_text": "unanswerable", "avg_logp": -0.20890069007873535, "avg_entropy_nats": 0.5002355575561523, "avg_entropy_bits": 0.7216873168945312}
{"generated_text": "They demonstrate the robustness of their results by simulating the effects of an overly context-sensitive classifier in a cross-document setting through randomized-context and no-context experiments.", "avg_logp": -0.15738371014595032, "avg_entropy_nats": 0.3929493725299835, "avg_entropy_bits": 0.5669060945510864}
{"generated_text": "The baseline and classification systems used in experiments are not explicitly mentioned in the article, but the article does mention that the features and variations of them have been used successfully in several successive works (cf. BIBREF1, BIBREF16, BIBREF15).", "avg_logp": -0.26610860228538513, "avg_entropy_nats": 0.6565930247306824, "avg_entropy_bits": 0.9472633600234985}
{"generated_text": "The EAU text spans are annotated with sentiment scores assigned to nodes in constituency parse trees.", "avg_logp": -0.25862276554107666, "avg_entropy_nats": 0.6888032555580139, "avg_entropy_bits": 0.9937329888343811}
{"generated_text": "They are defined as units that share great similarity with elementary discourse units (EDUs) in RST.", "avg_logp": -0.30316418409347534, "avg_entropy_nats": 0.7185328006744385, "avg_entropy_bits": 1.0366237163543701}
{"generated_text": "The Twitter corpus from 2015/07/21 till 2015/08/04.", "avg_logp": -0.19811557233333588, "avg_entropy_nats": 0.3723610043525696, "avg_entropy_bits": 0.5372033715248108}
{"generated_text": "The proposed word embeddings surpass SdfastText in the intrinsic evaluation matrix and have a more comprehensive vocabulary.", "avg_logp": -0.23018543422222137, "avg_entropy_nats": 0.5070955157279968, "avg_entropy_bits": 0.7315842509269714}
{"generated_text": "Yes.", "avg_logp": -0.28760379552841187, "avg_entropy_nats": 0.6729063391685486, "avg_entropy_bits": 0.970798671245575}
{"generated_text": "unanswerable", "avg_logp": -0.19065019488334656, "avg_entropy_nats": 0.5442321300506592, "avg_entropy_bits": 0.785161018371582}
{"generated_text": "The data is collected from multiple web resources, including news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board, and tweets regarding news and sports from Twitter.", "avg_logp": -0.03741542622447014, "avg_entropy_nats": 0.11210241913795471, "avg_entropy_bits": 0.1617296189069748}
{"generated_text": "The trends found in musical preferences are that audiences want more contemporary, intense, and novel music, but less mellow and unpretentious over time.", "avg_logp": -0.22372864186763763, "avg_entropy_nats": 0.5174825191497803, "avg_entropy_bits": 0.7465693354606628}
{"generated_text": "The decades they looked at were the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, and 1990s.", "avg_logp": -0.07997278869152069, "avg_entropy_nats": 0.16813935339450836, "avg_entropy_bits": 0.2425737828016281}
{"generated_text": "77 genres.", "avg_logp": -0.31530600786209106, "avg_entropy_nats": 0.8104473352432251, "avg_entropy_bits": 1.1692283153533936}
{"generated_text": "Yes.", "avg_logp": -0.21376127004623413, "avg_entropy_nats": 0.521140456199646, "avg_entropy_bits": 0.7518466711044312}
{"generated_text": "The CRF model achieved an F1 score of 89.60 on the development set, 87.82 on the test set, and 71.49 on the supplemental test set.", "avg_logp": -0.08321531116962433, "avg_entropy_nats": 0.200832799077034, "avg_entropy_bits": 0.2897404730319977}
{"generated_text": "Yes.", "avg_logp": -0.3889182507991791, "avg_entropy_nats": 0.6365305781364441, "avg_entropy_bits": 0.9183194041252136}
{"generated_text": "The handcrafted features used are: Bias feature, Token feature, Uppercase feature, Titlecase feature, Character trigram feature, Quotation feature, Word suffix feature, POS tag, Word shape, and Word embedding.", "avg_logp": -0.0578257218003273, "avg_entropy_nats": 0.1165001168847084, "avg_entropy_bits": 0.1680741310119629}
{"generated_text": "unanswerable", "avg_logp": -0.25851184129714966, "avg_entropy_nats": 0.7471665143966675, "avg_entropy_bits": 1.077933430671692}
{"generated_text": "They outperform state-of-the-art in terms of BLEU between output and human-written reformulations.", "avg_logp": -0.061933375895023346, "avg_entropy_nats": 0.18381890654563904, "avg_entropy_bits": 0.2651946246623993}
{"generated_text": "The three new proposed architectures are: \n\n1. An extension of the baseline architecture with a special dedicated discriminator to control that the latent representation does not contain stylistic information.\n2. A shifted autoencoder (SAE) that feeds the \"soft\" generated sentence into the encoder and checks how close is the representation to the original representation in terms of the cosine distance.\n3. A combination of both the above approaches.", "avg_logp": -0.13490580022335052, "avg_entropy_nats": 0.34555956721305847, "avg_entropy_bits": 0.4985370635986328}
{"generated_text": "Up to 5 percentage points for accuracy and up to 8 points for BLEU.", "avg_logp": -0.09784999489784241, "avg_entropy_nats": 0.23248125612735748, "avg_entropy_bits": 0.3353995680809021}
{"generated_text": "The standard parametrized attention and the non-attention baseline are used as baseline methods.", "avg_logp": -0.24391445517539978, "avg_entropy_nats": 0.5850145220756531, "avg_entropy_bits": 0.8439974784851074}
{"generated_text": "unanswerable (the article mentions BLEU scores, but does not provide a specific value)", "avg_logp": -0.40455004572868347, "avg_entropy_nats": 0.8872352838516235, "avg_entropy_bits": 1.2800098657608032}
{"generated_text": "The datasets used in experiments are the toy data, WMT'17, and four large machine translation datasets: English-Czech, English-German, English-Finnish, and English-Turkish.", "avg_logp": -0.2404155731201172, "avg_entropy_nats": 0.5074051022529602, "avg_entropy_bits": 0.7320308089256287}
{"generated_text": "Cycle consistency loss.", "avg_logp": -0.3821992576122284, "avg_entropy_nats": 0.7437729835510254, "avg_entropy_bits": 1.0730375051498413}
{"generated_text": "The proposed model significantly outperforms competitive baselines, obtaining the best published results.", "avg_logp": -0.1417837142944336, "avg_entropy_nats": 0.40581509470939636, "avg_entropy_bits": 0.5854674577713013}
{"generated_text": "Our model significantly outperforms competitive baselines, obtaining the best published results.", "avg_logp": -0.033811114728450775, "avg_entropy_nats": 0.10148389637470245, "avg_entropy_bits": 0.14641031622886658}
{"generated_text": "unanswerable", "avg_logp": -0.20895665884017944, "avg_entropy_nats": 0.56540846824646, "avg_entropy_bits": 0.8157119750976562}
{"generated_text": "unanswerable", "avg_logp": -0.5052387714385986, "avg_entropy_nats": 0.9048682451248169, "avg_entropy_bits": 1.3054488897323608}
{"generated_text": "Adversarial training BIBREF17, BIBREF10, BIBREF18, BIBREF11, matching the distributions of source and target word embeddings through generative adversarial networks BIBREF19.", "avg_logp": -0.10844375193119049, "avg_entropy_nats": 0.2805953323841095, "avg_entropy_bits": 0.4048134684562683}
{"generated_text": "The training set size is not explicitly mentioned in the article.", "avg_logp": -0.36618849635124207, "avg_entropy_nats": 0.9126613736152649, "avg_entropy_bits": 1.3166919946670532}
{"generated_text": "The baseline FEVER system, which ran the AllenNLP implementation of Decomposable Attention.", "avg_logp": -0.3357013761997223, "avg_entropy_nats": 0.7001754641532898, "avg_entropy_bits": 1.0101397037506104}
{"generated_text": "The pre-trained transformer used is one released by OpenAI.", "avg_logp": -0.24420297145843506, "avg_entropy_nats": 0.5507437586784363, "avg_entropy_bits": 0.7945552468299866}
{"generated_text": "The FEVER task is a large-scale challenge that tests a combination of retrieval and textual entailment capabilities.", "avg_logp": -0.08005386590957642, "avg_entropy_nats": 0.21527671813964844, "avg_entropy_bits": 0.31057867407798767}
{"generated_text": "unanswerable", "avg_logp": -0.17759934067726135, "avg_entropy_nats": 0.5401288270950317, "avg_entropy_bits": 0.779241144657135}
{"generated_text": "unanswerable", "avg_logp": -0.24633020162582397, "avg_entropy_nats": 0.6481525897979736, "avg_entropy_bits": 0.9350864887237549}
{"generated_text": "Yes.", "avg_logp": -0.18400445580482483, "avg_entropy_nats": 0.4679357707500458, "avg_entropy_bits": 0.6750885844230652}
{"generated_text": "The MuST-C corpus and a mix of public and proprietary data for English-Italian and WMT14 sentence pairs for English-German.", "avg_logp": -0.16199728846549988, "avg_entropy_nats": 0.3403863310813904, "avg_entropy_bits": 0.4910736680030823}
{"generated_text": "They enrich the positional embedding with length information by computing the distance from every position to the end of the sentence or by representing the proportion of the sentence that has been covered at a given position.", "avg_logp": -0.15704013407230377, "avg_entropy_nats": 0.3524036407470703, "avg_entropy_bits": 0.5084109902381897}
{"generated_text": "They condition the output to a given target-source class by prepending a length token to the source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$) at training time.", "avg_logp": -0.11689792573451996, "avg_entropy_nats": 0.294437438249588, "avg_entropy_bits": 0.4247834384441376}
{"generated_text": "English-Italian and English-German.", "avg_logp": -0.22411973774433136, "avg_entropy_nats": 0.36093857884407043, "avg_entropy_bits": 0.5207242369651794}
{"generated_text": "The MuST-C corpus, a mix of public and proprietary data, and WMT14 sentence pairs.", "avg_logp": -0.3289841413497925, "avg_entropy_nats": 0.6163266897201538, "avg_entropy_bits": 0.8891714811325073}
{"generated_text": "Yes.", "avg_logp": -0.1841467022895813, "avg_entropy_nats": 0.5015616416931152, "avg_entropy_bits": 0.7236005067825317}
{"generated_text": "The proposed approach is compared against several state-of-the-art models, including the baseline tracker, the focus tracker, the HWU tracker, the HWU tracker with \"original\" flag set to (HWU+), a maximum entropy (MaxEnt) type of discriminative model, and a deep neural network (DNN) architecture.", "avg_logp": -0.13058194518089294, "avg_entropy_nats": 0.29897740483283997, "avg_entropy_bits": 0.4313332140445709}
{"generated_text": "unanswerable", "avg_logp": -0.21333900094032288, "avg_entropy_nats": 0.5658358931541443, "avg_entropy_bits": 0.8163286447525024}
{"generated_text": "The library is framework agnostic, as it is designed to be integrated into a deep learning framework, with a distributional API similar to both TensorFlow and PyTorch.", "avg_logp": -0.38724568486213684, "avg_entropy_nats": 0.9001948833465576, "avg_entropy_bits": 1.2987067699432373}
{"generated_text": "The self-critical baseline.", "avg_logp": -0.22421714663505554, "avg_entropy_nats": 0.4124125838279724, "avg_entropy_bits": 0.5949856638908386}
{"generated_text": "Parallel scan inference, vectorized parsing, and semiring matrix operations.", "avg_logp": -0.11387605220079422, "avg_entropy_nats": 0.26349887251853943, "avg_entropy_bits": 0.3801485002040863}
{"generated_text": "The baseline is the Sent baseline, which assumes all targets in each sentence are of the same polarity.", "avg_logp": -0.2981830835342407, "avg_entropy_nats": 0.6968076229095459, "avg_entropy_bits": 1.0052809715270996}
{"generated_text": "unanswerable", "avg_logp": -0.2919578552246094, "avg_entropy_nats": 0.7074861526489258, "avg_entropy_bits": 1.0206868648529053}
{"generated_text": "The input representation of OpenIE tuples into the model is a concatenation of word embedding and another embedding indicating whether this word is a predicate.", "avg_logp": -0.1344934105873108, "avg_entropy_nats": 0.30622103810310364, "avg_entropy_bits": 0.4417835772037506}
{"generated_text": "The VIST dataset contains 40,071 training, 4,988 validation, and 5,050 testing stories.", "avg_logp": -0.09018225222826004, "avg_entropy_nats": 0.20298579335212708, "avg_entropy_bits": 0.29284659028053284}
{"generated_text": "FHVAE-based disentangled speech representation learning is more prominent than speaker adversarial training in improving the robustness of subword modeling towards speaker variation.", "avg_logp": -0.09708262979984283, "avg_entropy_nats": 0.24433977901935577, "avg_entropy_bits": 0.3525077700614929}
{"generated_text": "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect.", "avg_logp": -0.028585925698280334, "avg_entropy_nats": 0.0659957155585289, "avg_entropy_bits": 0.0952116847038269}
{"generated_text": "Intra-sentential and intra-word code-mixed puns.", "avg_logp": -0.12457408010959625, "avg_entropy_nats": 0.26388102769851685, "avg_entropy_bits": 0.3806998133659363}
{"generated_text": "The dialogue is guided by providing the Wizard with a restricted list of valid and relevant dialogue task actions that change dynamically based on the context.", "avg_logp": -0.16629120707511902, "avg_entropy_nats": 0.3560248911380768, "avg_entropy_bits": 0.5136353373527527}
{"generated_text": "The part of dialogue that is guided is the Wizard's dialogue options, which are provided by the Intelligent Wizard Interface, allowing the Wizard to make intelligent, relevant choices without intensive training.", "avg_logp": -0.4412606358528137, "avg_entropy_nats": 1.0545326471328735, "avg_entropy_bits": 1.5213690996170044}
{"generated_text": "The article mentions that the authors performed a data collection using the CRWIZ framework and compared the results to a similar dataset collected in a controlled lab setting with a single Wizard.", "avg_logp": -0.2570269703865051, "avg_entropy_nats": 0.4737197160720825, "avg_entropy_bits": 0.6834329962730408}
{"generated_text": "The framework ensures compliance with procedures by providing a restricted list of valid and relevant dialogue task actions that change dynamically based on the context.", "avg_logp": -0.20768815279006958, "avg_entropy_nats": 0.45407834649086, "avg_entropy_bits": 0.6550966501235962}
{"generated_text": "They combine the models using two strategies: Max Score Ensemble and Average Score Ensemble.", "avg_logp": -0.13295944035053253, "avg_entropy_nats": 0.3420199155807495, "avg_entropy_bits": 0.49343040585517883}
{"generated_text": "The logistic regression model with character-level n-gram features is presented as a strong baseline for comparison.", "avg_logp": -0.10522492974996567, "avg_entropy_nats": 0.2709220349788666, "avg_entropy_bits": 0.39085787534713745}
{"generated_text": "The context used includes the user screen name, all comments in the same thread, and the news article the comment is written for.", "avg_logp": -0.13461758196353912, "avg_entropy_nats": 0.3261541426181793, "avg_entropy_bits": 0.47054097056388855}
{"generated_text": "The language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.", "avg_logp": -0.030371859669685364, "avg_entropy_nats": 0.08515402674674988, "avg_entropy_bits": 0.12285128980875015}
{"generated_text": "The neural network model mainly consists of three parallel LSTM layers.", "avg_logp": -0.15457652509212494, "avg_entropy_nats": 0.3855772912502289, "avg_entropy_bits": 0.5562704205513}
{"generated_text": "The model consumes human interaction through user input, which is lower-cased and tokenized to match the model training data via spaCy, and then processed through the Storyline Planner and Story Writer modules.", "avg_logp": -0.34351253509521484, "avg_entropy_nats": 0.7554755806922913, "avg_entropy_bits": 1.0899208784103394}
{"generated_text": "They evaluate generated stories through a questionnaire completed by Mechanical Turk workers, rating story quality, engagement, and satisfaction on a five-point scale, and also through independent ratings by a separate set of Turkers.", "avg_logp": -0.35692068934440613, "avg_entropy_nats": 0.7579362392425537, "avg_entropy_bits": 1.0934706926345825}
{"generated_text": "unanswerable", "avg_logp": -0.0710063949227333, "avg_entropy_nats": 0.22345788776874542, "avg_entropy_bits": 0.3223815858364105}
{"generated_text": "The baseline is a computer-only story generation system and a turn-taking baseline that mimics the interaction of previous work.", "avg_logp": -0.2532142400741577, "avg_entropy_nats": 0.6231865286827087, "avg_entropy_bits": 0.8990680575370789}
{"generated_text": "The spelling features described in BIBREF12.", "avg_logp": -0.24789221584796906, "avg_entropy_nats": 0.5020430684089661, "avg_entropy_bits": 0.7242951393127441}
{"generated_text": "The contextual features used are the frequency of a token in the whole corpus, the frequency of contextual keywords from the windowed portions of the texts centering on the token in the whole corpus, and the size of the window.", "avg_logp": -0.1211451068520546, "avg_entropy_nats": 0.33664149045944214, "avg_entropy_bits": 0.4856710135936737}
{"generated_text": "The cybersecurity articles used in the model are sourced from 35 cybersecurity blogs published from 2001 to 2018.", "avg_logp": -0.19799184799194336, "avg_entropy_nats": 0.45901474356651306, "avg_entropy_bits": 0.6622182130813599}
{"generated_text": "human-crafted features that heavily rely on specific language knowledge such as dependency structure.", "avg_logp": -0.07872703671455383, "avg_entropy_nats": 0.19528719782829285, "avg_entropy_bits": 0.28173989057540894}
{"generated_text": "Yes.", "avg_logp": -0.13856124877929688, "avg_entropy_nats": 0.41465049982070923, "avg_entropy_bits": 0.5982141494750977}
{"generated_text": "The data in RAFAEL is labelled through an annotation process, described in section SECREF12.", "avg_logp": -0.20930181443691254, "avg_entropy_nats": 0.46086397767066956, "avg_entropy_bits": 0.6648862361907959}
{"generated_text": "They handle polysemous words by taking only the first word sense, usually the most common one, when extracting WordNet synsets.", "avg_logp": -0.23805208504199982, "avg_entropy_nats": 0.5096344351768494, "avg_entropy_bits": 0.7352469563484192}
{"generated_text": "The fluctuation in the sense of the word and its neighbors is measured by the concept of surrounding uniformity.", "avg_logp": -0.10764200240373611, "avg_entropy_nats": 0.28413620591163635, "avg_entropy_bits": 0.4099218547344208}
{"generated_text": "Mere fine-tuning reaches state-of-the-art results for factoid questions.", "avg_logp": -0.15529440343379974, "avg_entropy_nats": 0.32009512186050415, "avg_entropy_bits": 0.46179962158203125}
{"generated_text": "The architecture of the model is a neural sequence-to-sequence Spanish-English speech-to-text (ST) model, which is pre-trained using English ASR data from the Switchboard Telephone speech corpus and then fine-tuned for topic modeling using Nonnegative Matrix Factorization (NMF).", "avg_logp": -0.2504240572452545, "avg_entropy_nats": 0.5231818556785583, "avg_entropy_bits": 0.7547919154167175}
{"generated_text": "Spanish.", "avg_logp": -0.2847912907600403, "avg_entropy_nats": 0.539185643196106, "avg_entropy_bits": 0.7778803706169128}
{"generated_text": "The vocabulary comes from the training corpus, with the word frequency being ranked from greatest to least.", "avg_logp": -0.5188050270080566, "avg_entropy_nats": 1.0750234127044678, "avg_entropy_bits": 1.5509308576583862}
{"generated_text": "Character level granularity.", "avg_logp": -0.5821840763092041, "avg_entropy_nats": 1.3257654905319214, "avg_entropy_bits": 1.912675142288208}
{"generated_text": "LDC corpus and NIST datasets (MT03, MT04, MT05, MT06, MT08).", "avg_logp": -0.13900278508663177, "avg_entropy_nats": 0.3158208727836609, "avg_entropy_bits": 0.4556332230567932}
{"generated_text": "They use the classic information retrieval metrics of precision (P), recall (R), and F1, which is the harmonic mean of P and R.", "avg_logp": -0.19048020243644714, "avg_entropy_nats": 0.3414994180202484, "avg_entropy_bits": 0.4926794767379761}
{"generated_text": "Yes.", "avg_logp": -0.14333966374397278, "avg_entropy_nats": 0.46146994829177856, "avg_entropy_bits": 0.6657603979110718}
{"generated_text": "gAnswer, QAKiS, Platypus, and WDAqua.", "avg_logp": -0.04133465141057968, "avg_entropy_nats": 0.10023561120033264, "avg_entropy_bits": 0.14460942149162292}
{"generated_text": "unanswerable", "avg_logp": -0.2523290514945984, "avg_entropy_nats": 0.48985224962234497, "avg_entropy_bits": 0.7067074179649353}
{"generated_text": "Yes.", "avg_logp": -0.2440737783908844, "avg_entropy_nats": 0.5112564563751221, "avg_entropy_bits": 0.7375870943069458}
{"generated_text": "The Europarl corpus contains 2,333,816 sentences and 53,487,257 words, while the SoNaR corpus contains more than 500 million words.", "avg_logp": -0.11873594671487808, "avg_entropy_nats": 0.30017879605293274, "avg_entropy_bits": 0.4330664277076721}
{"generated_text": "The scores for predicting perceived musical hardness and darkness were drawn from a random sample of 503 songs.", "avg_logp": -0.16708241403102875, "avg_entropy_nats": 0.4695917069911957, "avg_entropy_bits": 0.6774775981903076}
{"generated_text": "unanswerable", "avg_logp": -0.10142746567726135, "avg_entropy_nats": 0.3796490728855133, "avg_entropy_bits": 0.547717809677124}
{"generated_text": "Topics present in the metal genre include sadness, death, freedom, nature, occultism, unpleasant/disgusting objects, harshness, gloominess, dystopia, and satanism.", "avg_logp": -0.1433398723602295, "avg_entropy_nats": 0.35368674993515015, "avg_entropy_bits": 0.5102620720863342}
{"generated_text": "SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics by a big margin.", "avg_logp": -0.15729984641075134, "avg_entropy_nats": 0.33383673429489136, "avg_entropy_bits": 0.481624573469162}
{"generated_text": "ROUGE and CIC for automatic evaluation, and relevance, conciseness, and readability, as well as a ranking evaluation, for human evaluation.", "avg_logp": -0.21957767009735107, "avg_entropy_nats": 0.42376452684402466, "avg_entropy_bits": 0.6113629341125488}
{"generated_text": "unanswerable", "avg_logp": -0.1307103931903839, "avg_entropy_nats": 0.3806758522987366, "avg_entropy_bits": 0.5491991639137268}
{"generated_text": "The answer is yes.", "avg_logp": -0.6191117763519287, "avg_entropy_nats": 1.2930150032043457, "avg_entropy_bits": 1.8654264211654663}
{"generated_text": "SPNet incorporates the speaker role, semantic slot, and dialog domain annotations as semantic scaffolds to improve abstractive summary quality.", "avg_logp": -0.22232882678508759, "avg_entropy_nats": 0.428933322429657, "avg_entropy_bits": 0.6188198924064636}
{"generated_text": "Pointer-Generator and Transformer.", "avg_logp": -0.19389046728610992, "avg_entropy_nats": 0.5737975239753723, "avg_entropy_bits": 0.8278148770332336}
{"generated_text": "The new evaluation metric, CIC (Critical Information Completeness), considers critical informative entities by calculating the recall of semantic slot information between a candidate summary and a reference summary.", "avg_logp": -0.07932103425264359, "avg_entropy_nats": 0.21415242552757263, "avg_entropy_bits": 0.3089565932750702}
{"generated_text": "Yes.", "avg_logp": -0.30065590143203735, "avg_entropy_nats": 0.7534403204917908, "avg_entropy_bits": 1.0869845151901245}
{"generated_text": "The annotators were educated to rank results by their coverage, fluency, and plausibility in daily life.", "avg_logp": -0.16324520111083984, "avg_entropy_nats": 0.34404462575912476, "avg_entropy_bits": 0.49635136127471924}
{"generated_text": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore.", "avg_logp": -0.012868261896073818, "avg_entropy_nats": 0.04647111892700195, "avg_entropy_bits": 0.06704365462064743}
{"generated_text": "no", "avg_logp": -1.0387763977050781, "avg_entropy_nats": 1.6618292331695557, "avg_entropy_bits": 2.397512912750244}
{"generated_text": "no", "avg_logp": -1.0813454389572144, "avg_entropy_nats": 1.6545064449310303, "avg_entropy_bits": 2.3869481086730957}
{"generated_text": "no, the sentences in the dataset were written by humans who were shown the concept-sets only, not the images or videos.", "avg_logp": -0.5642175674438477, "avg_entropy_nats": 1.2598497867584229, "avg_entropy_bits": 1.8175792694091797}
{"generated_text": "They are sampled from several large corpora of image/video captions.", "avg_logp": -0.13732674717903137, "avg_entropy_nats": 0.32629716396331787, "avg_entropy_bits": 0.47074729204177856}
{"generated_text": "at least 7% in absolute accuracy for all MCQA datasets, and at least 16% in absolute accuracy for all datasets except SemEval.", "avg_logp": -0.31159502267837524, "avg_entropy_nats": 0.6362547278404236, "avg_entropy_bits": 0.9179215431213379}
{"generated_text": "MultiNLI and SNLI.", "avg_logp": -0.0949525535106659, "avg_entropy_nats": 0.23197799921035767, "avg_entropy_bits": 0.33467352390289307}
{"generated_text": "BERT-based models and RoBERTa models.", "avg_logp": -0.38445231318473816, "avg_entropy_nats": 0.8422771692276001, "avg_entropy_bits": 1.2151490449905396}
{"generated_text": "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.", "avg_logp": -0.02830151841044426, "avg_entropy_nats": 0.052186500281095505, "avg_entropy_bits": 0.07528920471668243}
{"generated_text": "unanswerable", "avg_logp": -0.32523995637893677, "avg_entropy_nats": 0.6553601622581482, "avg_entropy_bits": 0.9454848766326904}
{"generated_text": "Ambiguity in labelled PIO data, integration of training data from different tasks and sources, and assessing the model's capacity for transfer learning and domain adaptation.", "avg_logp": -0.15480802953243256, "avg_entropy_nats": 0.3802589178085327, "avg_entropy_bits": 0.5485976338386536}
{"generated_text": "They measure the diversity of inferences using the number of distinct n-gram, normalized to [0, 1] by dividing the total number of generated tokens.", "avg_logp": -0.09427643567323685, "avg_entropy_nats": 0.22170470654964447, "avg_entropy_bits": 0.3198522925376892}
{"generated_text": "unanswerable", "avg_logp": -0.3396609127521515, "avg_entropy_nats": 0.7252779006958008, "avg_entropy_bits": 1.0463547706604004}
{"generated_text": "RNN-based Seq2Seq model.", "avg_logp": -0.15555842220783234, "avg_entropy_nats": 0.37029680609703064, "avg_entropy_bits": 0.5342252850532532}
{"generated_text": "It learns event background information by being trained on an auxiliary dataset with rich event background knowledge in the pretrain stage.", "avg_logp": -0.22083131968975067, "avg_entropy_nats": 0.48060572147369385, "avg_entropy_bits": 0.693367600440979}
{"generated_text": "The Atomic dataset scales up the size of the Event2Mind dataset and expands the scope to nine types of inference dimensions.", "avg_logp": -0.11684216558933258, "avg_entropy_nats": 0.30725452303886414, "avg_entropy_bits": 0.4432746171951294}
{"generated_text": "a large commercial vendor-based transcription pipeline used by Microsoft for production data.", "avg_logp": -0.35330668091773987, "avg_entropy_nats": 0.7119784355163574, "avg_entropy_bits": 1.027167797088623}
{"generated_text": "Significant improvement.", "avg_logp": -0.4724145829677582, "avg_entropy_nats": 1.089842677116394, "avg_entropy_bits": 1.572310447692871}
{"generated_text": "2-layer LSTMs.", "avg_logp": -0.3299577832221985, "avg_entropy_nats": 0.5216248035430908, "avg_entropy_bits": 0.7525454759597778}
{"generated_text": "They appended the domain tag “<2domain>\" to the source sentences of the respective corpora.", "avg_logp": -0.13360875844955444, "avg_entropy_nats": 0.29553794860839844, "avg_entropy_bits": 0.42637118697166443}
{"generated_text": "The potential for ongoing mixed-initiative multi-turn dialogues that do not follow a particular plan or pursue a particular fixed information need.", "avg_logp": -0.068583182990551, "avg_entropy_nats": 0.2348870038986206, "avg_entropy_bits": 0.3388702869415283}
{"generated_text": "The reaction delay, defined as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred.", "avg_logp": -0.08733735233545303, "avg_entropy_nats": 0.18094253540039062, "avg_entropy_bits": 0.2610448896884918}
{"generated_text": "The model architecture is a linguistically-infused neural network that combines a text sequence sub-network with a vector representation sub-network, using a \"late fusion\" approach.", "avg_logp": -0.1965646594762802, "avg_entropy_nats": 0.4026449918746948, "avg_entropy_bits": 0.5808939337730408}
{"generated_text": "answer, appreciation, elaboration, humor, negative reaction, question, agreement, disagreement, or \"other\" (no label was predicted).", "avg_logp": -0.21457889676094055, "avg_entropy_nats": 0.5061536431312561, "avg_entropy_bits": 0.7302253842353821}
{"generated_text": "The input features are 39-dimensional MFCCs.", "avg_logp": -0.22048115730285645, "avg_entropy_nats": 0.45904237031936646, "avg_entropy_bits": 0.6622580885887146}
{"generated_text": "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061.", "avg_logp": -0.03804035484790802, "avg_entropy_nats": 0.1615740805864334, "avg_entropy_bits": 0.23310211300849915}
{"generated_text": "The Switchboard conversational English corpus.", "avg_logp": -0.035164400935173035, "avg_entropy_nats": 0.13059398531913757, "avg_entropy_bits": 0.188407301902771}
{"generated_text": "Sizable improvements in average precision.", "avg_logp": -0.12974083423614502, "avg_entropy_nats": 0.3827644884586334, "avg_entropy_bits": 0.5522123575210571}
{"generated_text": "Frege's holistic and functional approach to meaning, as expressed in his sentence holism, is related to the general distributional hypothesis, as both views emphasize that the meaning of a word is derived from its relations to other words and contexts.", "avg_logp": -0.37274450063705444, "avg_entropy_nats": 0.8086268305778503, "avg_entropy_bits": 1.1666018962860107}
{"generated_text": "Meaning is defined as truth-value potential.", "avg_logp": -0.22146673500537872, "avg_entropy_nats": 0.555050253868103, "avg_entropy_bits": 0.8007681965827942}
{"generated_text": "Yes.", "avg_logp": -0.414531946182251, "avg_entropy_nats": 0.9065377116203308, "avg_entropy_bits": 1.3078575134277344}
{"generated_text": "1000 paraphrases are generated per question.", "avg_logp": -0.33615782856941223, "avg_entropy_nats": 0.8855146765708923, "avg_entropy_bits": 1.2775276899337769}
{"generated_text": "latent states that provide some contextual information about each node in a given derivation.", "avg_logp": -0.1836707890033722, "avg_entropy_nats": 0.39635705947875977, "avg_entropy_bits": 0.5718223452568054}
{"generated_text": "GraphParser without paraphrases and a monolingual machine translation based model for paraphrase generation.", "avg_logp": -0.08429175615310669, "avg_entropy_nats": 0.21574047207832336, "avg_entropy_bits": 0.31124770641326904}
{"generated_text": "Yes.", "avg_logp": -0.2761838138103485, "avg_entropy_nats": 0.5420860052108765, "avg_entropy_bits": 0.7820648550987244}
{"generated_text": "The strongest correlation among the topics was determined to be between exercise and obesity, but there was also a notable correlation between exercise and diabetes.", "avg_logp": -0.22114741802215576, "avg_entropy_nats": 0.5484105348587036, "avg_entropy_bits": 0.7911890745162964}
{"generated_text": "Using a topic modeling approach with Latent Dirichlet Allocation (LDA) and a lexicon-based approach with Linguistic Inquiry and Word Count (LIWC).", "avg_logp": -0.13363111019134521, "avg_entropy_nats": 0.34794700145721436, "avg_entropy_bits": 0.5019814372062683}
{"generated_text": "The Swissmetro dataset and other datasets not mentioned for the sake of clarity and replicability.", "avg_logp": -0.30985650420188904, "avg_entropy_nats": 0.6871195435523987, "avg_entropy_bits": 0.9913039207458496}
{"generated_text": "They train their embeddings by learning the new embeddings using the embeddings train set for each variable in the encoding set simultaneously.", "avg_logp": -0.2191367745399475, "avg_entropy_nats": 0.5032335519790649, "avg_entropy_bits": 0.7260125875473022}
{"generated_text": "They model travel behavior using a method called Travel Behavior Embeddings, which is based on the concept of word embeddings from natural language processing.", "avg_logp": -0.25209540128707886, "avg_entropy_nats": 0.6333316564559937, "avg_entropy_bits": 0.9137044548988342}
{"generated_text": "They interpret the coefficients by projecting them back into the original dummy variable space, following the well-known rules of normal random variables.", "avg_logp": -0.21639448404312134, "avg_entropy_nats": 0.5029931664466858, "avg_entropy_bits": 0.725665807723999}
{"generated_text": "They outperform the previous state-of-the-art model, Human Trafficking Deep Network (HTDN), by a substantial margin in weighted binary classification accuracy (Wt. Acc.).", "avg_logp": -0.21496042609214783, "avg_entropy_nats": 0.49834856390953064, "avg_entropy_bits": 0.7189649343490601}
{"generated_text": "Yes.", "avg_logp": -0.3378084599971771, "avg_entropy_nats": 0.7671747207641602, "avg_entropy_bits": 1.1067991256713867}
{"generated_text": "The lexicon of trafficking flags is expanded through a data-driven approach using word embeddings learned from raw text data, particularly by analyzing the vectors for emojis and mapping them to a 2-dimensional space using t-SNE.", "avg_logp": -0.21526257693767548, "avg_entropy_nats": 0.46569913625717163, "avg_entropy_bits": 0.6718618273735046}
{"generated_text": "Yes.", "avg_logp": -0.23008283972740173, "avg_entropy_nats": 0.6330148577690125, "avg_entropy_bits": 0.9132474660873413}
{"generated_text": "They use human annotators who were trained on snippets of conversations to label the four aspects of the task.", "avg_logp": -0.4123569130897522, "avg_entropy_nats": 0.8547623753547668, "avg_entropy_bits": 1.2331613302230835}
{"generated_text": "The lack of background information about a comment, such as a reference to a specific website like Stormfront, can make it difficult to classify as trolling or not.", "avg_logp": -0.5376200079917908, "avg_entropy_nats": 1.046492576599121, "avg_entropy_bits": 1.5097694396972656}
{"generated_text": "The potential solutions suggested include including additional knowledge from anthologies along with sentiment or polarity, using deeper semantics to represent comments and sentences in their logical form, jointly learning classifiers to predict variables simultaneously, and using the temporal sequence of response comments as input features for later comments.", "avg_logp": -0.1763753592967987, "avg_entropy_nats": 0.41887572407722473, "avg_entropy_bits": 0.6043099164962769}
{"generated_text": "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens.", "avg_logp": -0.034787867218256, "avg_entropy_nats": 0.12486719340085983, "avg_entropy_bits": 0.1801452487707138}
{"generated_text": "unanswerable", "avg_logp": -0.0903719812631607, "avg_entropy_nats": 0.3585001826286316, "avg_entropy_bits": 0.5172064304351807}
{"generated_text": "Strong.", "avg_logp": -0.16944608092308044, "avg_entropy_nats": 0.6349324584007263, "avg_entropy_bits": 0.9160138964653015}
{"generated_text": "The novel method (MCD splits) achieves a significantly higher compound divergence at a similar atom divergence compared to other approaches.", "avg_logp": -0.18705014884471893, "avg_entropy_nats": 0.41231223940849304, "avg_entropy_bits": 0.5948407649993896}
{"generated_text": "The authors justify that the question answering dataset presented is realistic by stating that it is \"simple yet realistic\" and that it is designed to measure compositional generalization using the DBCA method.", "avg_logp": -0.29810047149658203, "avg_entropy_nats": 0.7059542536735535, "avg_entropy_bits": 1.0184767246246338}
{"generated_text": "The three baseline ML architectures analyzed are not explicitly stated in the article, but they are referred to as \"three baseline ML architectures\" throughout the text.", "avg_logp": -0.307464599609375, "avg_entropy_nats": 0.7549889087677002, "avg_entropy_bits": 1.0892187356948853}
{"generated_text": "239,357 English question-answer pairs.", "avg_logp": -0.034196995198726654, "avg_entropy_nats": 0.1632395088672638, "avg_entropy_bits": 0.23550482094287872}
{"generated_text": "Other approaches to creating compositional generalization benchmarks include using specific splits between train and test data, such as query patterns, input patterns, and output patterns, as well as using datasets like clevr, sqoop, and MetaQA.", "avg_logp": -0.3301965296268463, "avg_entropy_nats": 0.8343728184700012, "avg_entropy_bits": 1.2037454843521118}
{"generated_text": "The problem of training a CSKS algorithm with a small amount of labeled keyword instances.", "avg_logp": -0.2930346429347992, "avg_entropy_nats": 0.6911996006965637, "avg_entropy_bits": 0.9971901774406433}
{"generated_text": "Honk and DeepSpeech-finetune.", "avg_logp": -0.0728655606508255, "avg_entropy_nats": 0.20650382339954376, "avg_entropy_bits": 0.2979219853878021}
{"generated_text": "English and Hindi.", "avg_logp": -0.13097642362117767, "avg_entropy_nats": 0.3745485544204712, "avg_entropy_bits": 0.5403593182563782}
{"generated_text": "unanswerable", "avg_logp": -0.2626415193080902, "avg_entropy_nats": 0.6744922399520874, "avg_entropy_bits": 0.9730866551399231}
{"generated_text": "The proposed method, FlowSeq, obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality compared to the state-of-the-art contemporaneous work, CMLM with 10 iterations of refinement.", "avg_logp": -0.1351710706949234, "avg_entropy_nats": 0.27195993065834045, "avg_entropy_bits": 0.3923552334308624}
{"generated_text": "WMT2014, WMT2016, and IWSLT-2014.", "avg_logp": -0.2189532369375229, "avg_entropy_nats": 0.4964695870876312, "avg_entropy_bits": 0.7162542343139648}
{"generated_text": "WMT2014 DE-EN, WMT2016 RO-EN, and IWSLT2014 DE-EN.", "avg_logp": -0.010456941090524197, "avg_entropy_nats": 0.040937479585409164, "avg_entropy_bits": 0.05906030163168907}
{"generated_text": "The visual attention is very sparse, with just one source encoding being attended to in most cases.", "avg_logp": -0.12519991397857666, "avg_entropy_nats": 0.4334140717983246, "avg_entropy_bits": 0.6252843141555786}
{"generated_text": "The visual features in the How2 dataset are much more dominated by \"common\" dimensions, indicating they are less discriminative among individual sentences and vocabulary elements at the token level.", "avg_logp": -0.2422279268503189, "avg_entropy_nats": 0.49956417083740234, "avg_entropy_bits": 0.7207186818122864}
{"generated_text": "The Ranker and the Reasoner.", "avg_logp": -0.26541730761528015, "avg_entropy_nats": 0.43024730682373047, "avg_entropy_bits": 0.6207156777381897}
{"generated_text": "The Ranker and Reasoner models cooperate in a cooperative game, where the Ranker selects passages and the Reasoner predicts the linking entities to improve the selection process.", "avg_logp": -0.2690950930118561, "avg_entropy_nats": 0.5450635552406311, "avg_entropy_bits": 0.7863604426383972}
{"generated_text": "unanswerable", "avg_logp": -0.2208823263645172, "avg_entropy_nats": 0.6215038895606995, "avg_entropy_bits": 0.8966405391693115}
{"generated_text": "HotpotQA and MedHop are created.", "avg_logp": -0.4056781828403473, "avg_entropy_nats": 0.7177333235740662, "avg_entropy_bits": 1.0354702472686768}
{"generated_text": "They reference several empirical investigations, including BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF26, BIBREF27, BIBREF28, and BIBREF29.", "avg_logp": -0.07253614068031311, "avg_entropy_nats": 0.20156855881214142, "avg_entropy_bits": 0.2908019721508026}
{"generated_text": "Chinese to English and English to Czech, and also Chinese to English and English to Chinese.", "avg_logp": -0.42677730321884155, "avg_entropy_nats": 1.0282014608383179, "avg_entropy_bits": 1.4833811521530151}
