{
    "llama3_pred": [
        "A seed lexicon consisting of 15 positive words and 15 negative words.",
        "A seed lexicon consisting of 15 positive words and 15 negative words.",
        "The proposed method performed well, even with a minimal amount of supervision, and achieved high accuracy, with the BiGRU encoder trained with the AL+CA+CO dataset obtaining the highest score.",
        "The answer is through the exploitation of discourse relations, specifically Cause and Concession.",
        "The relations are used to propagate polarity through label propagation, where the polarity of an event is determined based on its discourse relation with another event.",
        "The Japanese corpus used in the experiment contained about 100 million sentences.",
        "The Japanese data consists of a corpus of about 100 million sentences.",
        "The labels available in the dataset for supervision are positive and negative labels, as shown in Table TABREF19.",
        "The results show a significant gain over a purely supervised baseline when labeled data were small.",
        "The model learns using mostly raw data by exploiting discourse relation-based label propagation from a small seed lexicon and a large raw corpus.",
        "15 positive words and 15 negative words.",
        "about 100 million sentences.",
        "unanswerable",
        "unanswerable",
        "The annotation experiment is evaluated through agreement statistics, including heatmaps, Cohen's kappa scores, and the use of bootstrap resampling to compare the agreement of different annotator groups.",
        "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.",
        "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, implying that their results are based on English data.",
        "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, resulting in 283 English-speaking communities for their analysis.",
        "They manifest in systematically varying patterns across different types of communities.",
        "They observe that user engagement varies systematically with a community's distinctiveness and dynamicity, with dynamic communities exhibiting higher rates of user retention and engagement, and distinctive communities fostering strong, focused interest from users at one particular point in time.",
        "They were selected from a dataset of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate their measures, in at least 4 months of the subreddit's history.",
        "They selected the 300 Reddit communities for comparison from all subreddits on Reddit from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of activity.",
        "The authors measure a community's temporal dynamicity by computing the pointwise mutual information (PMI) of words in a community relative to its entire history, indicating how frequently a word occurs more in a small window of time than in the entire history, which they call \"volatility.\"",
        "The authors measure how distinctive a community is by calculating the average specificity of all utterances in the community, using a framework based on pointwise mutual information (PMI).",
        "Chinese general corpus.",
        "Chinese general corpus.",
        "BERT-Base and QANet.",
        "BERT-Base and QANet.",
        "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is a result of the query term according to the paragraph text.",
        "Clinical text structuring is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text.",
        "Clinical text structuring tasks.",
        "The specific tasks being unified are different clinical text structuring tasks.",
        "There are unrelated sentences in between questions.",
        "2,714 question-answer pairs.",
        "The tasks evaluated are clinical text structuring (CTS) and question answering based clinical text structuring (QA-CTS).",
        "unanswerable\n\nHowever, the article mentions that labeling necessary amount of data for training neural network requires expensive labor cost, implying that there may be concerns about data privacy and security, but it does not explicitly address the issue.",
        "They introduce clinical named entity information into the model.",
        "The QA-CTS task dataset contains 17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
        "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
        "QANet and BERT-Base.",
        "Word-level perplexity, R@3 in next-word prediction, latency, and energy usage.",
        "Kneser-Ney smoothing and Skip-LSTM are mentioned as classic language models.",
        "Perplexity and log perplexity are commonly used evaluation metrics for language models, with log perplexity being more human-understandable in terms of recall.",
        "Perplexity is a commonly used evaluation metric for language models, but the article also mentions that log perplexity provides a more human-understandable indicator of R@3.",
        "The Yelp Challenge dataset.",
        "The Yelp Challenge dataset.",
        "unanswerable \n\nThe article does not explicitly mention using a pretrained NMT model, but it mentions using the Yelp Challenge dataset to train the NMT model.",
        "unanswerable \n\nThe article does not mention the use of a pretrained NMT model for review generation.",
        "The context is a clear-text sequence of: the review rating, restaurant name, city, state, and food tags.",
        "AdaBoost with 200 shallow decision trees.",
        "Yes.",
        "994 (human reviews) + 1006 (NMT-Fake reviews) = 2000",
        "unanswerable \n\nHowever, the article does mention that they reproduce the results from See et al. See2017 to then apply LRP on it.",
        "unanswerable \n\nHowever the article does mention that they reproduce the results from See et al. See2017 and that they used a baseline model from See et al. See2017, but it does not mention any other baselines they compared.",
        "The model uses a single attention mechanism in the decoder.",
        "It depends on the case, as the article shows that saliency maps are truthful to the network's computation in some cases but not in others.",
        "The three measures of bias which are reduced in experiments are geometric bias, neighborhood bias, and RIPA metric.",
        "The nearest neighbor bias mitigation observations.",
        "In a low-resourced setting, high quality data turns out to be more important than high volume data.",
        "High quality data turns out to be more important, especially in a low-resourced setting.",
        "The model is improved more by quality than by quantity, as shown by the experiment results where high-quality data helps to significantly improve the quality of the embeddings, while massive but low-quality data does not.",
        "Character Word Embedding (CWE) and BERT.",
        "The paper specifically targets the Portuguese language, which is assumed to be European Portuguese given the context of Brazil already having widely diffused the use of machine learning techniques.",
        "The paper targets the Portuguese language in general, but since it mentions a \"large corpus representative of the Portuguese language\" and a public word2vec model pre-trained for the Portuguese language, it likely targets European Portuguese, but it does not exclude the possibility of Brazilian Portuguese.",
        "A large Portuguese corpus from several sources, including a multi-genre corpus representative of the Portuguese language.",
        "Portuguese word embeddings.",
        "Yes.",
        "unanswerable \n\nThe text mentions that the dataset is the result of processing the CzCDC 1.0 dataset, but it does not mention any experiments being performed on the final dataset.",
        "The quality of the citation is measured through the F1 measure in the initial recognition of the text spans and their classification.",
        "The dataset contains 237,723 court decisions issued between 1st January 1993 and 30th September 2018.",
        "unanswerable \n\nAlthough the article mentions that they use Twitter data, which is typically in English, it does not explicitly state that they only use English datasets.",
        "unanswerable",
        "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to Dryhootch manual, with four intensity categories: No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD.",
        "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants, with four categories: Non-existent, light, moderate, and high PTSD.",
        "The LIWC algorithm is used to calculate s-scores, which are probabilities that a word or string of characters was generated by the same underlying process as the training data, and the proportions of tweets scored positively by each LIWC category are used as a feature vector in a loglinear regression model.",
        "LIWC is incorporated into this system through the use of LIWC language models ($clm^{+}$, $ulm^{+}$, $clm^{-}$, and $ulm^{-}$) to calculate $s$-scores for each test tweet, and later through the exact similar method of LIWC to extract $\\alpha$-scores for each dimension and categories using the generated PTSD Linguistic Dictionary.",
        "210 users are surveyed using the clinically validated survey.",
        "The Domain-Specific Risk-Taking (DOSPERT) Scale, The Berlin Social Support Scales (BSSS), and the Values In Action Scale (VIAS).",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "They list 75 fine-grained named entity types.",
        "yes",
        "Yes.",
        "The different senses are annotated/labeled manually in some cases, and automatically induced from clusters in other cases.",
        "yes \n\nThe extrinsic evaluation was carried out through the evaluation of the relatedness scores on benchmark datasets and the performance of the sense vectors on the Task 13 of SemEval-2013 — Word Sense Induction.",
        "Yes, the model uses both spectrogram images and raw waveforms as features.",
        "unanswerable \n\nThe article does not mention comparing the performance against a baseline model.",
        "The performance of the proposed models is compared against various existing models in the field of spoken language identification.",
        "up to 96.0% on four languages (English, German, French, and Spanish) and 95.4% on six languages.",
        "Previous state-of-the-art vision-based methods, including CNN-mean and CNN-avgmax, as well as the show-tell model.",
        "The Adam algorithm is applied to optimize the model, and the caption model is trained with two baseline vision-based methods proposed in BIBREF6, BIBREF7, which are CNN-mean and CNN-avgmax.",
        "English, German, French, and Japanese.",
        "The languages used in the multi-lingual caption model are German, English, French, and Japanese.",
        "Yes.",
        "They do not explicitly compare to previous research due to the existing works using smaller data, outdated methods, or lacking standard benchmarks.",
        "Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, IDAT@FIRE2019 shared-task, MADAR shared task 1 corpus, and task 1 corpus.",
        "The datasets used in training include Arab-Tweet, UBC Twitter Gender Dataset, the MADAR shared task corpus, the LAMA-DINA dataset, and the IDAT@FIRE2019 shared-task dataset.",
        "They use the Wasserstein GAN (WGAN) and the energy-based GAN, and also propose two models, weGAN and deGAN, which are extensions of the GAN model.",
        "They use the Wasserstein GAN (Arjovsky et al., 2017) and also their own models weGAN and deGAN.",
        "unanswerable.",
        "The CNN data set and the 20 Newsgroups data set.",
        "The answer is yes.",
        "The authors do not explicitly define or exemplify 'incorrect words', but they mention that the Twitter Sentiment Classification dataset contains \"many mistakes, as specified in Table TABREF11\", and that the TTS-STT combinations used to generate incomplete sentences with STT error result in varying rates of missing and incorrect words.",
        "The article does not specify the exact number of vanilla transformers used after applying an embedding layer.",
        "yes.",
        "No, the authors do not test their approach on a dataset without incomplete data; they test it on datasets with incomplete data, specifically with missing or incorrectly transcribed words.",
        "No.\n\nThe authors state that \"in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language.\" This implies that their approach should be applied in general, not just when dealing with incomplete data.",
        "No.\n\nThe authors suggest that their approach should be applied generally to improve the robustness and efficiency of BERT when applied to incomplete data, not just when dealing with incomplete data.",
        "They outperform other models by 6% to 8% in the sentiment classification task and by 0.94% to 1.89% in the intent classification task with STT error.",
        "34,432 user conversations.",
        "unanswerable",
        "The metrics to measure user engagement are overall rating, mean number of turns, and mean word count.",
        "The metrics to measure user engagement are overall rating, mean number of turns, user's per-utterance word count, and the number of backstory questions asked.",
        "The system designs introduced are multi-step language understanding modules, a novel dialog act scheme, and an extensive persona database.",
        "unanswerable.",
        "yes",
        "They found a strong positive correlation between the number of backstory questions asked and user satisfaction.",
        "no \n\nThey have several baselines, including ROUGE, WordNCE, WPNCE, WordPPL, WPPPL, and word-overlap metrics like ROUGE-L-mult and ROUGE-L-single.",
        "No.",
        "They use neural language models (LMs), specifically recurrent neural networks (RNNs) and LSTM LMs.",
        "human judges are asked to rate the fluency of system-generated compressions on an ordinal scale from 1 (disfluent) through 3 (fluent).",
        "The model still takes into account and overrides the information brought by the text-based annotations when the attention loses track of the objects in the picture.",
        "The model still takes into account and overrides the information brought by the text-based annotations if the attention loses track of the objects in the picture and \"gets lost\".",
        "The baseline used is the model of Calixto et al. (2017) and also the multimodal baseline of Caglayan et al. (2016).",
        "Soft attention, hard stochastic attention, and local attention (which is a generalization of soft attention).",
        "unanswerable",
        "unanswerable",
        "unanswerable \n\nHowever, based on the article, we can infer that their system outperforms the lexicon-based models significantly, as it outperforms TF-IDF in all relevance metrics.",
        "unanswerable \n\nHowever, according to the text, our proposed model outperforms the lexicon-based model TF-IDF, which is a popular unsupervised baseline, in both retrieval and generative evaluations.",
        "TF-IDF and NVDM.",
        "4.5 million human comments in the candidate set.",
        "198,112 news articles.",
        "The Tencent News dataset with millions of real comments was used.",
        "up to four percentage points in accuracy.",
        "the 2019 GermEval shared task on hierarchical text classification and the Blurb Genre Collection (BGC).",
        "The dataset used is the 2019 GermEval shared task on hierarchical text classification.",
        "They combine text representations with the knowledge graph embeddings by concatenating them and passing them into a multilayer perceptron (MLP) with two layers.",
        "unanswerable.",
        "The traditional methods to identifying important attributes are simple co-occurrences and ranking process used to identify relatively more precise attributes from all attribute candidates.",
        "FastText.",
        "unanswerable (the article does not specify the exact type of user generated text data used)",
        "No, they proposed three characteristic metrics: diversity, density, and homogeneity.",
        "SST-2 (Stanford Sentiment Treebank, version 2) and Snips dataset.",
        "The SST-2 (Stanford Sentiment Treebank, version 2) dataset and the Snips dataset were used.",
        "unanswerable",
        "Wealth, population size, democracy, and official development assistance (ODA) are the country-specific drivers of international development rhetoric, with varying effects on the discussion of sustainable development (Topic 7) and economic development (Topic 2).",
        "unanswerable \n\nThe article does not provide information about the language(s) of the dataset.",
        "unanswerable \n\nThe article does not provide information on the language(s) used in the dataset.",
        "The main international development topics that states raise are identified through the application of structural topic models (STMs) to the UN General Debate Corpus (UNGDC) dataset.",
        "The authors evaluate their system's performance on datasets across various domains and conduct experiments with active learning to validate its effectiveness.",
        "The QnAMaker WebApp acts as a layer between the Bot, Management APIs, and Azure Search Index, handling ranking on top of retrieved results and feedback management for active learning.",
        "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot.",
        "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot.",
        "unanswerable \n\nHowever, it can be inferred that they test the robustness of the Plackett-Luce model through evaluation on the test data, comparing its performance to MERT and MIRA in two tasks: iterative training and N-best list reranking.",
        "unanswerable",
        "unanswerable \n\nHowever, the article suggests that the new method is able to alleviate over-fitting in some degree when k is not smaller than 5, which may indicate that it is more robust than MIRAs in certain cases.",
        "The experiments performed with large-scale features include full training of machine translation models and N-best list reranking.",
        "unanswerable",
        "unanswerable",
        "The work is evaluated over an anonymised user utterances dataset of approximately 8.7M annotated utterances across 23 domains.",
        "An anonymized user utterances corpus derived from requests across 23 domains, with approximately 8.7 million annotated utterances and a test set of around 300,000 utterances.",
        "topological properties.",
        "Class Membership Tests, Class Distinction Test, and Word Equivalence Test.",
        "The intrinsic evaluation metrics used are Class Membership Tests, Class Distinction Test, and Word Equivalence Test, which involve computing the ratio of pairs correctly classified as distinct, belonging to the same class, or equivalent based on cosine similarity.",
        "The validation loss consistently increases after about 15 epochs when using less than 50% of the data available, indicating overfitting.",
        "The multimodality available in the dataset is text and images.",
        "The answer is multimodal comprehension of cooking recipes.",
        "Hasty Student and Impatient Reader are previously reported models.",
        "The new model achieves state-of-the-art results compared to other neural models.",
        "The scoring model works by computing a score for a segmented sentence as the average of word scores and link scores, where word scores are calculated based on the possibility of a segment being a legal word and link scores are calculated based on the rationality of the link between words.",
        "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then using a gated neural network and LSTM to evaluate the likelihood of each candidate word being a legal word and its link to previous segmentation history.",
        "The active learning model works by employing a CRF-based segmenter as the learning engine and a scoring model based on information entropy and neural network as the selection engine, which iteratively selects and labels samples from unlabeled data to improve the accuracy of the classifier.",
        "A gated neural network, a long short-term memory (LSTM) model, and a gated recurrent neural network (GRNN) are employed.",
        "text understanding, expectation, coreference resolution, common-sense knowledge inference, and the embedding of sentences into context.",
        "The inter-annotator agreement was sufficiently high, and a qualified majority vote resulted in 90.5% agreement between annotators for coreference chain annotation.",
        "The inter-annotator agreement was found to be sufficiently high, and the percentage of pairs annotated by at least 3 annotators was 90.5%.",
        "unanswerable. \n\nHowever, according to the article, \"All annotations were done by undergraduate students of computational linguistics\" and \"The stories from each scenario were distributed among four different annotators.\" This implies that there are four subjects (annotators), but it does not mention how many students were involved in the project.",
        "WN18 and YAGO3-10.",
        "The datasets used to evaluate this approach are WN18 and YAGO3-10.",
        "By identifying the fact that, when removed, results in the least change in the model's prediction, indicating the error triple.",
        "unanswerable",
        "The proposed model has several advantages, including the ability to jointly model words in documents as arising from a mixture of topics, to learn from multiple annotators and crowds by accounting for their biases and different levels of expertise, and to scale to large datasets using an efficient stochastic variational inference algorithm.",
        "State-of-the-art approaches for predicting target variables associated with complex high-dimensional data, such as documents or images, are supervised topic models.",
        "The datasets used were 20-Newsgroups, Reuters-21578, and LabelMe.",
        "The 20-Newsgroups dataset, Reuters-21578, LabelMe, and we8there dataset were used.",
        "The dataset was collected through a human-to-human dialogue setting, where two paired workers, a user and a system (wizard), conversed synchronously in a chatroom to accomplish a goal generated by a multi-domain goal generator based on a database of travel information in Beijing.",
        "The dataset was collected through a human-to-human dialogue setting, where workers were paired to converse according to given goals, and their conversations were then annotated and recorded.",
        "The benchmark models provided are for different components of a pipelined task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG), implemented using ConvLab-2.",
        "The corpus was annotated automatically using some rules, and then manually checked by three experts for a subset of 50 dialogues.",
        "Attentive Mimicking.",
        "An absolute improvement of up to 24% over a BERT baseline is reported.",
        "MNLI, AG's News, and DBPedia.",
        "MNLI, AG's News, and DBPedia.",
        "WNLaMPro dataset.",
        "The model achieves the state-of-the-art results compared to baselines.",
        "unanswerable \n\nHowever, based on the article, it can be inferred that the method achieves the \"state-of-the-art results\" and \"best overall performance\" compared to the baseline methods, indicating a significant performance difference.",
        "The datasets used for evaluation include AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI.",
        "AIDA-B, AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI, and CWEB.",
        "lexical and statistical features such as the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc.",
        "The author's work ranked among the top submissions on the challenge, as their reinforcement learning approaches received good human evaluation results and were the best of their runs in two batches.",
        "Classification and regression approaches using deep learning models.",
        "Classification and regression approaches, and deep learning models, have been tried without reinforcement learning.",
        "Labeling sentences as \"summary\" based on their ROUGE score above a threshold, or the top m input sentences with the highest ROUGE score.",
        "Yes.",
        "unanswerable \n\nHowever, the article mentions that some languages have a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian, which could be a source of recall errors.",
        "They do not explicitly state looking for inconsistencies between different languages' annotations in UniMorph, but mention \"teasing out the annotation discrepancies within and across projects.\"",
        "unanswerable \n\nHowever, the article mentions that the UD datasets have been used in the CoNLL shared tasks and that each dataset is an annotated treebank, making it a resource of token-level annotations. But it does not mention looking for inconsistencies between different UD treebanks.",
        "The 31 languages with both UD and UniMorph data.",
        "31 languages with both UD and UniMorph data.",
        "The paper mentions that the personalization of the recognition algorithm (e.g., mean and standard deviation normalization) could help to adapt the classification for specific speakers and thus to reduce the bias towards negative emotional states, but does not explicitly evaluate this adjustment.",
        "Using confusion matrices and macro-averaged F1 scores.",
        "A neural network with an embedding layer, a bidirectional LSTM, and two dense layers followed by a soft max output layer, inspired by previous work.",
        "Off-the-shelf emotion recognition tools are used for facial expressions (manufacturer cannot be disclosed) and audio signals (specific manufacturer not mentioned).",
        "Off-the-shelf tools for emotion recognition from facial expressions and audio signals are used.",
        "30000 merge operations for BPE subword segmentation.",
        "30k and 90k subwords, with 30000 merge operations used for BPE subword segmentation.",
        "Our optimized NMT system outperforms the PBSMT system across all data settings, achieving a BLEU score of 9.4 in the ultra-low data condition and 32.8 in the full IWSLT 14 training corpus.",
        "They compare with Phrase-Based Statistical Machine Translation (PBSMT) systems.",
        "The paper mentions the following pitfalls: using hyperparameters typical for high-resource settings in low-resource settings, not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions, relying on large amounts of auxiliary data, and using vocabulary sizes that are not optimal for low-resource settings.",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "The technique used for text analysis and mining is a combination of natural language processing (NLP), argumentation mining, and the use of a semantic frame extractor, specifically the Penelope semantic frame extractor.",
        "Causal mapping methods employed include Axelrod's causal mapping method, cognitive mapping, and the Penelope semantic frame extractor.",
        "Ternary Trans-CNN model, which uses a Convolution 1D architecture with 3 layers and 2 dense fully connected layers, achieving an accuracy of 83.90%.",
        "The dataset used is HEOT obtained from a past study done by Mathur et al., which is a labelled dataset for Hinglish tweets, and a labelled dataset for English tweets obtained from a study conducted by Davidson et al.",
        "The dataset used is HEOT, a labelled dataset of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent, which was obtained from a past study by Mathur et al.",
        "3189 rows of text messages with a range of 1-1295 words.",
        "The dataset is 3189 rows of text messages of average length 116 words.",
        "The dataset, HEOT, is obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from Twitter for the conversations happening in the Indian subcontinent.",
        "unanswerable\n\nThe article mentions that the text augmentation techniques (Synonym Replacement, Random Insertion, Random Swap, and Random Deletion) were used together, but it does not mention whether they were experimented individually.",
        "Previous work uses Transfer Learning based approaches, Ternary Trans-CNN models, Hybrid multi-channel CNN and LSTM models, and other architectures such as Fully Connected dense networks and Convolution based architectures.",
        "unanswerable \n\nHowever, based on the information provided, it can be inferred that the dataset is obtained from Twitter.",
        "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.",
        "They demonstrate that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
        "They demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
        "The article suggests that a sentence representation in mBERT is composed of a language-specific component and a language-neutral component, implying that they are not necessarily disjunctive.",
        "They assume that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way, and they try to remove the language-specific information from the representations by centering the representations of sentences in each language.",
        "The representation similarity does not directly show language-neutrality, i.e., to what extent are similar phenomena represented similarly across languages.",
        "They propose two approaches: across models and tasks, and across input space, to develop a more nuanced methodology for defining and evaluating faithfulness.",
        "The authors propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful.",
        "The three assumptions are the Model Assumption, the Prediction Assumption, and the Linearity Assumption.",
        "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption.",
        "Be explicit in what you evaluate, faithfulness evaluation should not involve human-judgement on the quality of interpretation, faithfulness evaluation should not involve human-provided gold labels, do not trust “inherent interpretability” claims, and faithfulness evaluation of IUI systems should not rely on user performance.",
        "yes",
        "unanswerable \n\nThe article does not explicitly mention the performance of their model, but rather focuses on interpreting the behavior of the model's intermediate layers.",
        "unanswerable \n\nThe article does not explicitly discuss the performance of their model, but rather focuses on interpreting the behavior of deep learning models in natural language inference tasks.",
        "Three main parts are mentioned: input encoding, attention, and inference.",
        "unanswerable",
        "The answer is Multiple Perspective Context Matching.",
        "unanswerable \n\nThe article does not provide a clear measure of improvement for the adaptation model, but it does mention that when the number of hidden question types (K) is set to 100, the performance improves to 69.10%/78.38% on the development set.",
        "The baseline model is composed of typical components including word embedding, input encoder, alignment, aggregation, and prediction.",
        "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components.",
        "Our model achieves a 68.73% EM score and 77.39% F1 score on the SQuAD test set.",
        "The three fine-tuned BERT versions clearly outperform all other methods, achieving high correlations with human scores for most linguistic qualities rated, on three different datasets.",
        "They use datasets from the NIST DUC-05, DUC-06, and DUC-07 shared tasks.",
        "BiGRU with attention and language model (LM).",
        "BiGRU with attention and ROUGE.",
        "The five linguistic quality criteria addressed are given in Figure FIGREF2, and include: $\\mathcal {Q}1$ (Grammaticality), $\\mathcal {Q}2$ (Relevance), $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus), and $\\mathcal {Q}5$ (Structure & Coherence).",
        "WN18RR, FB15k-237, and YAGO3-10.",
        "WN18RR, FB15k-237, and YAGO3-10.",
        "TransE, DistMult, ComplEx, ConvE, and RotatE.",
        "HAKE significantly outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task.",
        "Entities are mapped onto the polar coordinate system by combining the modulus part and the phase part, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information.",
        "phrase-based word embedding and Abstract Syntax Tree (AST) can be incorporated for improved vocabulary mapping and more accurate target code generation.",
        "unanswerable",
        "A text-code parallel corpus with 18805 aligned data, where the source data is in English and the target data is in Python programming language.",
        "They use a text-code parallel corpus with 18805 aligned data in it.",
        "unanswerable \n\nThere is no information in the article about comparisons to other models.",
        "The architecture of the system is a neural network based machine translation model using a Long Short-Term Memory (LSTM) model with an encoder layer, a decoder layer, and an output layer.",
        "unanswerable \n\nHowever, the article does mention that programmers express a for-loop in a very few specific ways and variable declaration and value assignment expressions are also limited in nature.",
        "Phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficial to further improve accuracy.",
        "Python.",
        "The validation data from the source corpus with 500 lines, which is 3% of the training data, is used to measure accuracy.",
        "The text-to-image synthesis was initially carried out through a search and supervised learning combined process.",
        "The text-to-image synthesis was mainly carried out through a search and supervised learning combined process in the early stages of research.",
        "Although the article does not explicitly state the remaining challenges, it mentions that actual AI systems are far from achieving text-to-image synthesis, implying that there are still unresolved challenges in this area.",
        "unanswerable",
        "unanswerable \n\nHowever, the article does mention \"a learned vector gate (also referred to as feature-wise sigmoidal gate)\" but it does not specify where it is employed.",
        "BiLSTM with max pooling.",
        "They evaluate on 11 downstream transfer tasks, including sentiment classification, subjectivity classification, question-type classification, recognizing textual entailment, estimating semantic relatedness, and measuring textual semantic similarity.",
        "They use MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RareWords (RW) datasets for word-level similarity evaluation.",
        "The authors use the following word-similarity datasets: MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RW.",
        "Yes, the New York Times (NYT) datasets are used for experiments, with NYT24 having 24 relations and NYT29 having 29 relations, and both datasets have significantly larger numbers of instances of multiple relation tuples with overlapping entities.",
        "The WordDecoding (WDec) model performed better in experiments, achieving higher F1 scores than the PtrNetDecoding (PNDec) model on the NYT29 and NYT24 datasets.",
        "Previous work authors refer to researchers BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF9, BIBREF11, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, and BIBREF36.",
        "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively.",
        "Our WordDecoding (WDec) model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding (PNDec) model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively.",
        "The baselines were: (1) rank by the number of times a citation is mentioned in the document, (2) rank by the number of times the citation is cited in the literature (citation impact), (3) rank using Google Scholar Related Articles, (4) rank by the TF*IDF weighted cosine similarity, and (5) rank using a learning-to-rank model trained on text similarity rankings.",
        "The baselines included ranking by the number of times a citation is mentioned in the document, ranking by the number of times the citation is cited in the literature (citation impact), ranking using Google Scholar Related Articles, ranking by the TF*IDF weighted cosine similarity, and ranking using a learning-to-rank model trained on text similarity rankings.",
        "unanswerable \n\nHowever, based on the text, it appears that the authors tested three different supervised models, but they do not explicitly state that they developed a new supervised model.",
        "unanswerable",
        "Amazon Mechanical Turk (AMT) and Google Adwords (GA) are mentioned as crowdsourcing platforms, but the article states that the ideal annotators for the task are the authors themselves, and that they used unpaid authors as annotators.",
        "The CNN model performed the best with an achieved accuracy of 82.6%.",
        "The CNN model performed better with the highest achieved accuracy of 82.6%.",
        "unanswerable",
        "Their performance on the dataset achieved an accuracy of 82.6% with CNNs, outperforming autoencoders and MLP.",
        "The dataset is not explicitly mentioned as having a specific size, but it is mentioned that a subset of the dataset was used for training (60% of the dataset), and the rest was used for testing and validation (40% of the dataset).",
        "yes \n\nThey used Amazon Mechanical Turk (MTurk) to collect the Talk The Walk dataset.",
        "Yes.",
        "The dataset was collected by manually capturing 360-views of several neighborhoods in New York City and crowd-sourcing the collection of the dataset on Amazon Mechanical Turk.",
        "natural language",
        "The authors looked at F1 scores, precision, recall, and accuracy as evaluation metrics.",
        "They used a dataset collected from Amazon Mechanical Turk (MTurk) called Talk The Walk, which consists of over 10k successful dialogues.",
        "unanswerable",
        "unanswerable \n\nThe article mentions the use of the Quora duplicate question dataset, but does not specify the language of this dataset.",
        "The accuracy of the system is measured using a formula that rewards clustering which matches the article clustering and the total number of claims clustered.",
        "automatically retrieved by comparing the incoming claim to existing factchecks in the database.",
        "The Quora duplicate question dataset and the Full Fact database of news articles.",
        "The components in the factchecking algorithm include claim detection, claim clustering, and a graph-based method using Louvain Community Detection for community formation.",
        "The baseline is a simple, transparent, and interpretable model created to highlight the challenges and nature of RC-QED.",
        "The WikiHop dataset was used in the experiment.",
        "Yes.",
        "yes",
        "The dataset was annotated through a crowdsourcing framework where workers were asked to judge whether a statement could be derived from a given article, and if so, to provide justification explanations in the form of natural language summaries.",
        "WikiHop, an entity-based multi-hop QA dataset.",
        "Two labels are considered in the multi-label task, being sentiment polarity and the candidate/category in consideration.",
        "unanswerable",
        "The Washington Post political pundits.",
        "Washington Post political pundits.",
        "The crowd in these experiments refers to the users of social media, specifically those who post and share tweets on Twitter.",
        "The ground truth of who won a debate is established by the results given by the experts in Washington Post.",
        "Significantly better, as shown by the experimental results.",
        "The learned embeddings are further analyzed using t-SNE tool to visualize the learned embedding, and efficiency evaluation is also done to show the test accuracy with increasing epoch and different embedding sizes.",
        "Three typical knowledge graph embedding methods (TransE, TransR, and TransH), two path-based models (PTransE and ALL-PATHS), and three attribute-incorporated methods (R-GCN and KR-EAR) along with four variants of KANE are used for comparison.",
        "The three datasets used to measure performance are DBP24K, Game30K, and FB24K.",
        "DBP24K, Game30K, and FB24K.",
        "KANE captures both high-order structural and attribute information of KGs through recursive embedding propagation based on relation triples and multi-head attention-based aggregation of attribute triples.",
        "TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR.",
        "Yes",
        "The authors mention a few confounds such as \"incorrect hashtag supervision\" and \"pragmatic difficulty\" in their study.",
        "The baseline model used is SVM (Support Vector Machine) classifier.",
        "Spelling errors, capitalisation, POS ratios, length, and sentiment ratios.",
        "Spelling errors, POS ratios, length, sentiment ratios, capitalisation, and repeated characters.",
        "Yes. \n\nThe article states that the authors use hashtag-based supervision, but also mention that the hashtags are removed so that they provide labels, but do not act as features. This suggests that the labels provided by the hashtags are verified by the authors themselves.",
        "unanswerable",
        "The authors equate drunk tweeting with drunk texting, as they refer to both as \"drunk-texting\" throughout the paper.",
        "The text corpora (S) from aristo2016:combining.",
        "The text corpora (S) from BIBREF6 aristo2016:combining.",
        "The proposed technique, TupleInf, achieves a statistically significant improvement of 11.8% over the state-of-the-art system, TableILP, on a broad set of over 1,300 science questions.",
        "yes \n\nThis is based on the information that the article uses Open IE v4 to extract tuples from text corpora, which involves entity linking.",
        "no \n\nThe article states that \"We define a tuple as (subject; predicate; objects) with zero or more objects.\"",
        "ElasticSearch query against a text corpora (S), followed by running Open IE v4.",
        "The answer is unanswerable, as the article discusses the method's ability to answer complex questions that require multiple facts, but it does not explicitly state whether it can answer multi-hop questions. However, it does mention the possibility of adding inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates, which could be related to multi-hop reasoning.",
        "The textual source to which OpenIE was applied is the text corpora (S) from aristo2016:combining.",
        "Open IE v4.",
        "The answer is yes.",
        "The primary reason for the dense mode outperforming the sparse one is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.",
        "The evaluation is conducted using the SemEval 2010 Task 14 evaluation methodology, specifically the adjusted Rand index (ARI) as the quality measure.",
        "RuThes, a large-scale lexical ontology for Russian, and Wiktionary referred as Joint Exp SWN.",
        "Cosine similarity.",
        "The Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 were used for Quasar-T, while for Quasar-S, Lucene retrieval was used.",
        "The word embeddings used were pre-trained word embeddings for Vietnamese created by Kyubyong Park and Edouard Grave, specifically vectors of 100 dimensions and 300 dimensions, respectively, generated from Wikipedia database backup dumps.",
        "The BLSTM-CNN-CRF system produced five types of errors: No extraction, No annotation, Wrong range, Wrong tag, and Wrong range and tag.",
        "unanswerable \n\nHowever, it can be inferred that the BLSTM-CNN-CRF performed better than the BLSTM-CRF as it was used in the analysis experiments and achieved a 91.62 F1 score in comparison to another model that used BLSTM-CRF which achieved a 90.94 F1 score.",
        "Task A is used as a main task, and tasks B and C are used as auxiliary tasks for multitask learning.",
        "unanswerable",
        "The article does not explicitly mention the performance gap between their approach and the strong handcrafted method.",
        "unanswerable \n\nHowever, based on the context, it seems that the article is comparing different systems, and the feature-rich based system is mentioned as a strong system that performs well on tasks A and C.",
        "Yes, the authors performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task.",
        "Yes. \n\nThe article states that the Multi-Head Attention (MHA) can learn n_head different scores in parallel child spaces and is very powerful for alignments.",
        "AEN-GloVe has a relatively small model size, ranking second after MemNet, and is significantly smaller than most other models, including AEN-GloVe-BiLSTM.",
        "The proposed model is different from BERT as it employs attentional encoder layers and a label smoothing regularization to enhance performance on targeted sentiment classification tasks.",
        "unanswerable",
        "Nine emotions (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, and Neutral).",
        "Bag-of-words-based benchmarks using methods such as Classification with TF-IDF + Linear SVM, Classification with Depeche++ Emotion lexicons + Linear SVM, Classification with NRC Emotion lexicons + Linear SVM, and Combination of TF-IDF and NRC Emotion lexicons + Linear SVM.",
        "The dataset contains a total of 9710 passages.",
        "There were 3 unique annotators per passage, with in-house annotators also being used for consolidating labels when necessary.",
        "unanswerable",
        "Our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them.",
        "Fine-grained syntactic analysis is performed, focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements.",
        "It is demonstrated through a fine-grained syntactic analysis of the translations generated using the proposed method, as well as through quantitative results showing an improvement in BLEU scores when the correct information is provided.",
        "Google's machine translation system (GMT) accessed through its Cloud API.",
        "The black-box context injection system consists of a pre-defined textual hint and a wrapper method that injects the desired information into the input sentence to influence the output of a trained NMT system.",
        "Cepstral mean normalization (CMN) and variance with mean normalization (CMVN) are mentioned.",
        "They experiment with 40 mel-scaled log filterbank energies (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector), and also spectrogram.",
        "A 6-layer bidirectional LSTM with 1024 hidden units.",
        "Conversational speech.",
        "Previous models, such as MTMSN, used approaches like training a categorical variable to predict the number of spans to extract and generalizing the single-span head method using non-maximum suppression (NMS) algorithm.",
        "They use sequence tagging by individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually.",
        "The proposed model does not hinder performance on other question types, actually improving results on these types compared to the NABERT+ baseline.",
        "Our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.",
        "MTMSN.",
        "unanswerable \n\nHowever, it is implied that the XR model has access to a large collection of un-annotated text from the same distribution, whereas the fully supervised model does not.",
        "unanswerable",
        "The aspect based sentiment classifier trained only using the XR loss achieves a F1 score of 53.81 with a classifier trained on 100 labeled sentences.",
        "The expectation regularization loss is defined as the KL-divergence between the model's posterior distribution over a set and the expected label distribution, given by D(P || Q) = ∑[p(x)log(p(x)/q(x))], where p(x) is the expected label distribution and q(x) is the model's posterior distribution.",
        "The Lemming model, a log-linear model that performs joint morphological tagging and lemmatization, was used as a non-neural baseline for Task 2.",
        "The NLU-Benchmark (NLU-BM) dataset, which contains 25,716 utterances, and the ROMULUS dataset, which contains 1,431 sentences, are both publicly available and used in the research.",
        "Intent F1 and Exact Match (EM) accuracy of the entire sequence of labels.",
        "No, they do not provide decision sequences as supervision while training models, but rather use reinforcement learning to train the models.",
        "SQuAD and NewsQA, which are repurposed for the interactive MRC task.",
        "They use a combination of reinforcement learning (Rainbow algorithm) and supervised learning to train the models.",
        "Ctrl+F (search for token) and stop.",
        "The proposed multimodal models are the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
        "unanswerable \n\nThe article does not mention the language of the tweets.",
        "The dataset, called MMHS150K, is formed by 150,000 tweets.",
        "The multimodal models do not outperform the unimodal textual models.",
        "The author believes it is due to noisy data, complexity and diversity of multimodal relations, and a small set of multimodal examples.",
        "The F-score, the Area Under the ROC Curve (AUC), and the mean accuracy (ACC) are used to benchmark the results.",
        "The data is collected using the Twitter API.",
        "Correct, 150,000.",
        "The unimodal detection models used were a CNN for image features extraction and a single layer LSTM for tweet text classification.",
        "The different models for multimodal detection proposed were the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
        "hate or not.",
        "The evaluation metrics used were accuracy (ACC) and the normalized mutual information metric (NMI).",
        "Their performance results are reported in Tables TABREF43, TABREF44, TABREF46, and TABREF47, showing significant improvements over baseline methods.",
        "unanswerable \n\nThe article does not provide a direct comparison of the performance improvement of the proposed methods over the other methods, but rather discusses the performance enhancements of the proposed methods compared to other baseline methods.",
        "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector based clustering methods, as well as four baseline clustering methods based on unsupervised dimensionality reduction methods such as Laplacian Eigenmaps (LE), Locality Preserving Indexing (LPI), and Autoencoder (AE), and Latent Semantic Analysis (LSA).",
        "They used three public short text datasets: SearchSnippets, StackOverflow, and Biomedical.",
        "no.",
        "Three configurations of Transformer networks are explored: a small to medium-sized network with 4 Transformer layers, a small network with 2 Transformer layers, and a minimal network with 1 Transformer layer.",
        "unanswerable",
        "Sparse categorical cross-entropy is used for identifying classes from a feature set, and evaluation is the metric of performance, which assumes a large target classification set.",
        "The authors use four individual datasets: AI2, CC, IL, and MAWPS.",
        "The evaluation metrics used include AUC-ROC, accuracy, user interruption rate, reuse of VVA skills, number of active dialogs, top-1 accuracy, and relative change with respect to the popularity model.",
        "unanswerable",
        "Five-minute reuse and one-day return, which label an instance as positive if it is followed by a new joke request within a specified time interval.",
        "Vector space representation with TF-IDF weights.",
        "A dataset of 14 TDs, with subjects on aliens, stories, law, and news.",
        "They compare the similarity values produced by each of the similarity measures CS, SRCC, and PCC.",
        "The Masked Language Modeling (MLM) task on the French OSCAR corpus.",
        "unanswerable",
        "CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches.",
        "The results of CamemBERT outperformed previous results by a large margin for POS tagging and dependency parsing, with error reductions of up to 2.96% and 3.33% respectively, and showed a 2.36 point increase in the F1 score for named entity recognition.",
        "The article does not directly compare CamemBERT against multilingual BERT, but it mentions that CamemBERT outperforms multilingual language models such as mBERT on certain tasks.",
        "CamemBERT was pre-trained for 100k steps during 17 hours on 256 Nvidia V100 GPUs.",
        "The unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.",
        "The state of the art measures are structure-based controversy measures developed by Garimella et al.",
        "The article does not explicitly list the specific controversial topics experimented with, but it mentions that they include topics such as elections, corruption cases, and justice decisions, as well as specific examples like the Brazilian presidential elections in 2018, the Ukraine conflict, and the Kavanaugh confirmation hearings.",
        "They used thirty different discussions that took place between March 2015 and June 2019, in four languages and five regions, and included both controversial and non-controversial discussions.",
        "Twitter.",
        "Four languages: English, Portuguese, Spanish, and French.",
        "unanswerable \n\nThe article does not provide information about the current state-of-the-art (SOTA) for sentiment analysis on Twitter at the time of writing.",
        "The language of Twitter proved to be quite challenging due to its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.",
        "unanswerable \n\nHowever, based on the given text, it is mentioned that \"quantification is not a mere byproduct of classification, since a good classifier is not necessarily a good quantifier, and vice versa\" which implies that specific evaluation measures are required for quantification, but the article does not specify the exact metrics.",
        "unanswerable \n\nHowever, we can find the number of unique sentences with multiple transformations. The article states that out of 4262 distinct sentences, only 188 was recorded more than once, which means that for most sentences, there is only one transformation available.",
        "The dataset COSTRA 1.0 includes 293 annotations from 12 annotators, which include paraphrases, minimal change, nonsense, past, future, ban, and other transformations.",
        "They are represented as 15 different paraphrases of each sentence.",
        "unanswerable \n\nHowever, the article mentions that 15 modification types were selected for the second round of annotation, but it does not explicitly list them.",
        "yes, available at http://hdl.handle.net/11234/1-3123",
        "unanswerable",
        "Yes, they briefly describe the analysis of the LASER embeddings, noting that the minimal change operation and tense changes resulted in vectors very similar to the original sentence.",
        "They introduce language variation by wrapping COSTRA 1.0 into an API, such as SentEval, and extending the corpus in Czech and adding other language variants.",
        "Yes, they use external resources such as Korektor, a statistical spellchecker and grammar checker, to minimize the influence of typos on the performance of embedding methods.",
        "unanswerable \n\nHowever, the article mentions that the domain-specific embeddings are related to the domains of the documents, but it does not specify the size of the dataset.",
        "unanswerable",
        "unanswerable",
        "Our model outperformed the state-of-the-art models on all three datasets: STS Corpus, Sanders dataset, and HCR dataset.",
        "unanswerable",
        "They used three datasets: Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR).",
        "yes",
        "The Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset are used for experiments.",
        "The proposed semantic rules set includes five rules, displayed in Table TABREF15, which are used to capture important information and remove unessential parts in a tweet.",
        "They experiment with link prediction and triplet classification tasks.",
        "Yes, they evaluate LAN by comparing it with different scoring functions and analyzing its performance on sparse knowledge graphs and case studies on neighbors' weights.",
        "The results validate the superiority of LAN w.r.t. the three properties, but the article does not explicitly state that they evaluate existing methods in terms of these properties.",
        "unanswerable",
        "The dataset used to train the model is a combination of three clean-labeled datasets (movie sentence polarity dataset, laptop and restaurant datasets collected from SemEval-2016) and three noisy-labeled training datasets (2,000 reviews for each domain: movie, laptop, and restaurant).",
        "The proposed model NetAb outperforms a wide range of baselines markedly in sentence-level sentiment classification, especially in handling noisy labels.",
        "Yes.",
        "yes",
        "unanswerable \n\nThe article does not mention if STES (Semantic Text Exchange Score) has been previously used in the literature, it only introduces it as a new metric for evaluating overall performance of a model on STE (Semantic Text Exchange).",
        "NWN-STEM, GWN-STEM, and W2V-STEM.",
        "The performance of both the i-vector system and the x-vector system on CN-Celeb(E) was much worse than on SITW and SITW(S), indicating a significant difference between the two datasets.",
        "real-world settings including ambient noise, background babbling, music, cheers, and laugh, with strong and overlapped background speakers in some cases.",
        "The 11 genres covered are: entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement.",
        "unanswerable",
        "unanswerable \n\nThe article compares the performance of i-vector and x-vector systems on CN-Celeb, but it does not conclude which one works better overall.",
        "unanswerable, as the article does not provide a direct comparison of the two performances. However, it can be inferred that the performance on CN-Celeb is significantly inferior to VoxCeleb based on the experimental results.",
        "The new model, conditional BERT, is evaluated on six text classification datasets, including SST, Subj, MPQA, RT, TREC, and SST2.",
        "The authors measure performance using accuracy on six benchmark classification datasets.",
        "The conditional masked language model (C-MLM) objective performs better than the original masked language model (MLM) objective.",
        "No, the article only mentions BERT as the pre-trained language model evaluated for contextual augmentation.",
        "unanswerable \n\nHowever, the article does mention performance on tasks with data augmentation.",
        "unanswerable",
        "unanswerable \n\nHowever, based on the content of the article, it appears that the survey is not referencing a specific, newly-published paper, but rather providing a comprehensive overview of recent developments in the field of NQG.",
        "Yes.",
        "unanswerable.",
        "The learning paradigms covered in this survey include the change of learning paradigm, the broadening of the input spectrum, and the generation of deep questions.",
        "textual inputs, especially declarative sentences, knowledge bases, and images.",
        "No, they mainly focus on neural methods for question generation.",
        "Their model is a hypersphere-based named entity recognition model that uses Euclidean distance and a hypersphere representation to classify entities into different types.",
        "yes \n\nThe article states that they evaluate the hypersphere model on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0.",
        "The baseline method, Distant Supervision, and a model proposed by Ayushi Dalmia.",
        "The Effective Word Score (EFWS) of a word x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x.",
        "The tweet subjectivity is measured using TextBlob, which provides a subjectivity level between 0 and 1, or Opinion Finder Tool, which identifies subjective and objective parts of the text.",
        "Because the original DMN requires supporting facts to be labeled during training.",
        "Providing labeled facts relevant for answering a question during training.",
        "They proposed a new input module using a two-level encoder with a sentence reader and input fusion layer to allow for information flow between sentences.",
        "They proposed and implemented several improvements to the DMN, including an input fusion layer to allow interactions between input facts, a novel attention-based GRU for logical reasoning, and an untied model with a linear layer and ReLU activation for memory update.",
        "The model learns to select the important facts from a larger set without requiring labeled supporting facts during training.",
        "yes",
        "yes",
        "They perform manual evaluation by having three English Literature PhD candidates score the fluency of outputs on a 0-5 scale, and also have the annotators classify transferred sentences by style.",
        "BLEU scores and perplexity.",
        "They use part-of-speech (POS) tags to identify content words.",
        "They model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions, by extracting and counting surface-level features, often counts of function words.",
        "unanswerable \n\nThe article does not explicitly state that they only report results on English data, but it does mention that the data was collected from Twitter and other platforms, which are predominantly English. However, it does not exclude the possibility of other languages being present in the data.",
        "The article provides insights into the relationship between demographics and mental health, specifically that women are more likely to be diagnosed with depression, while men have higher suicide rates, and depression triggers vary across different age groups.",
        "unanswerable",
        "The framework facilitates demographic inference from social media by integrating heterogeneous features from different modalities, including aesthetic features, language features, sociability, and user engagement, to detect likely depressed individuals and infer demographic information such as age and gender.",
        "The features used from each data type are as follows: \n- Aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) \n- Choice of profile picture (for gender, age, and facial expression) \n- Screen name \n- Language features from both textual content and profile's description (n-gram, emotion, sentiment) \n- Sociability from ego-network \n- User engagement \n- General image features (normalized red, green, blue, mean of original colors, brightness, contrast, luminance, hue-saturation-value color space, mean and variance for saturation and hue) \n- Facial presence features \n- Facial expression features (Ekman's model of six emotions: anger, disgust, fear, joy, sadness, and surprise) \n- Qualitative language analysis features (analytical thinking, clout, authenticity, emotional tone, descriptors categories, informal language markers, and linguistic aspects)",
        "The data is annotated using self-reported depressed users, a lexicon of depression symptoms, and human judges who verified a subset of users.",
        "Self-disclosure in Twitter profile descriptions, such as age and gender explicitly mentioned in the text, and also from user-provided information like \"16 years old suicidal girl\".",
        "unanswerable",
        "Twitter.",
        "Twitter.",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "A sememe is a basic unit of meaning in a word, a fundamental concept or idea that contributes to the overall meaning of a word.",
        "They pre-trained their model on the Dutch section of the OSCAR corpus.",
        "RobBERT, a Dutch RoBERTa-based pre-trained language model, outperforms previous models and other BERT models on several tasks, including sentiment analysis and die/dat disambiguation.",
        "They experimented on sentiment analysis, die/dat disambiguation, and possibly other Dutch language tasks.",
        "Results of the experiments suggest that natural language based agents are more robust to task-nuisances and allow for better transfer.",
        "The natural language based agents outperform the other methods, including vision-based and feature-based representations, achieving better performance, faster convergence, and more robust solutions.",
        "unanswerable \n\nThis is because the article does not provide information about the convergence rate of the natural language agents compared to other agents. It only mentions that GloVe pretrained embedding vectors were used, which speeds up the training process.",
        "The authors perform experiments on several tasks in the Doom environment to compare the performance of natural language representation against visual-based and feature representations.",
        "The state is represented via natural language by converting labels and features of the environment into descriptive sentences that summarize the state, using techniques such as patching and parsing.",
        "Our TL-TranSum method outperforms the Maximal Relevance Minimal Redundancy (MRMR) baseline by 7% of ROUGE-SU4 score.",
        "No, the paper does not discuss previous models which have been applied to the same task, but it does mention existing research on detecting moral sentiment in online text and bias in NLP.",
        "The datasets used in the paper include the Google N-grams corpus, COHA, WordNet, and the valence ratings for the nearly 14,000 English nouns collected by BIBREF28.",
        "The parameter-free model works by summarizing each set of seed words by its expected vector in embedding space, and classifying concepts into the class of closest expected embedding in Euclidean distance following a softmax rule.",
        "They quantify moral relevance by using a set of seed words approximating moral irrelevance based on valence ratings, and then using a Centroid model or Naïve Bayes model to classify concepts into morally relevant and morally irrelevant categories.",
        "They showcase the fine-grained moral dimensions of \"Care / Harm\", \"Fairness / Cheating\", \"Loyalty / Betrayal\", \"Authority / Subversion\", and \"Sanctity / Degradation\".",
        "They use the Google N-grams corpus and the Moral Foundations Dictionary, as well as data from the Pew Research Center's 2013 Global Attitudes survey, and WordNet, and BIBREF30 (unspecified source) for their analysis.",
        "The system performed well, particularly in the fairy-tale genre, where the neural approach was deemed as coherent as the human-made game.",
        "The information is extracted using two methods: a neural question-answering technique and a rule-based information extraction technique, specifically OpenIE5.",
        "unanswerable",
        "unanswerable \n\nThe article does not provide a direct comparison of the proposed model's perplexity and BLEU score with those of typical UMT models.",
        "A corpus of poems and a corpus of vernacular literature from online resources.",
        "BLEU and Micro Entity F1.",
        "The baseline systems included Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, and DSR.",
        "InCar dataset and CamRest dataset.",
        "unanswerable",
        "equi-distant segments of the video.",
        "They initialize their word embeddings using 300-dimensional GloVe and then fine-tune them during training.",
        "yes \n\nThe article mentions that the authors also add an auxiliary decoding module, not concatenate the question, answer pairs before encoding them, and use the Time-Extended FiLM module for feature extraction, and running hyperparameter optimization over the validation set to select hyperparameters, giving them the observed performance boost.",
        "unanswerable",
        "unanswerable \n\nThe article mentions that the authors have an Android application ready for their system, but it does not provide information on whether it is publicly available.",
        "A multi-class Naive Bayes classifier is used for emergency categorization.",
        "Support Vector Machines (SVM) is used for the first stage of emergency detection, while Naive Bayes (NB) is used for the second stage classification model for categorizing tweets into specific emergency types.",
        "unanswerable \n\nThe article does not provide information about the source of the tweets, but mentions that the authors collected data using the Twitter API for saved data available for public use.",
        "unanswerable \n\nHowever, based on the article, it can be inferred that there are at least 4 categories: fire, earthquake, accident, and robbery, as well as a category for drunk driving and drunk driving accidents.",
        "unanswerable \n\nHowever, it can be inferred that the baseline was likely the conventional methods of emergency response, such as calling emergency numbers like 100 or 911, but the article does not explicitly mention this.",
        "Yes, the tweets are specific to a region, as the system is currently collecting tweets with a location filter for the city of \"Mumbai\" and displays its map location on the interface.",
        "unanswerable \n\nHowever, the article does mention the creation of a dataset called MED, but it doesn't explicitly state whether it is released or not.",
        "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.",
        "Upward reasoning allows an inference from a more general concept to a more specific one, while downward reasoning allows an inference from a more specific concept to a more general one.",
        "The ability to capture the interaction between lexical and syntactic structures, and to determine the direction of inference based on the presence of monotonicity operators and their arguments.",
        "The analysis found a key relationship between dialogue acts and emotional states, including Accept/Agree with Joy, Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc.",
        "The ensemble annotator extracts the final label by giving priority to the labels that are perfectly matching in all the neural annotators, then to the context-based models, and finally by ranking the labels with their confidence values.",
        "The dialogue act labels were defined based on the DAMSL (Dialogue Act Markup in Several Layers) tag set, which includes forward-looking functions (such as Statement, Info-request, Thanking) and backwards-looking functions (such as Accept, Reject, Answer).",
        "Five neural models were trained and used for dialogue act labeling.",
        "They do not directly compare their model against other models, but they mention SyncNet, a previous neural network model that achieves 99% accuracy on lip video synchronisation, and argue that their data is more challenging.",
        "They annotate their own dataset using self-supervision, generating and labelling pairs of ultrasound and audio segments as positive (correctly synchronised) or negative (randomly desynchronised).",
        "unanswerable \n\nHowever, the model predicts the synchronisation offset by calculating the average distance for each candidate offset across utterance segments and selecting the one with the minimum average distance.",
        "A two-stream neural network architecture.",
        "Aspects are identified as feature related to an opinion target.",
        "The MSM 2013 corpus, the UMBC Twitter corpus, and the Ritter NER corpus are used for the analysis.",
        "WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them.",
        "The encoder network has five convolutional layers, and the context network has seven convolutional layers.",
        "They explore that more data for pre-training improves performance, but a specific threshold or amount of data needed for a certain magnitude of improvement is not explicitly stated.",
        "The individual embeddings for a given character are shared among the languages jointly trained on.",
        "The Universal Dependencies (UD) treebanks datasets.",
        "Yes.",
        "The datasets are of varying sizes, with the in-house dataset containing 6 languages, 56 entity types, and 53 relation types, and the ACE05 dataset containing 3 languages, 7 entity types, and 6 relation types.",
        "They experiment on 7 target languages, including German, Spanish, Portuguese, Chinese, Italian, Japanese, and Arabic.",
        "The in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset are used.",
        "unanswerable \n\nThe article does not provide a direct comparison of auto-completion performance between using both language and vision and only language.",
        "The dataset provided by this research is large, consisting of 16k images and 740k region descriptions from the Visual Genome dataset, and 9k images and 54k referring expressions from the ReferIt dataset.",
        "They complete a user query prefix conditioned upon an image by using a modified FactorCell LSTM language model that incorporates image features from a pre-trained CNN.",
        "unanswerable \n\nThe article does not mention whether the collection process used a WoZ method.",
        "Their model outperformed the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.",
        "A standard sequence-to-sequence model augmented with an attention mechanism, a two-step baseline approach that divides the task into path generation and path verification, and an ablation model that only takes natural language instructions as input.",
        "The proposed model, \"Ours with Mask and Ordered Triplets\", outperformed the Baseline and Ablation models on all metrics in previously seen environments.",
        "The evaluation metrics used are EM (Exact Match), F1 (harmonic average of precision and recall), ED (minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence), and GM (Goal Match).",
        "The authors used Mechanical Turk for collecting their dataset.",
        "The navigation instructions were collected through Mechanical Turk using 100 simulated environments with a corresponding topological map.",
        "unanswerable\n\nHowever, based on the context, it can be inferred that the experiment is likely done in English.",
        "paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, plus several others, including clinical narratives, EEG biomarkers, brain anatomy, social functioning assessments, personality assessment, and symptom scales.",
        "Their initial results show a Fleiss's Kappa of 0.575, indicating moderate to substantial agreement among annotators.",
        "The authors used two datasets: a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, and a corpus of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms from the Research Patient Data Registry (RPDR).",
        "unanswerable",
        "The morphology knowledge is implemented by segmenting the words into morpheme units and then applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
        "The word segmentation method works by segmenting complex words into simple morpheme units, either by morpheme segmentation or by applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
        "unanswerable \n\nThe article does not mention if the word segmentation method is independently evaluated or not.",
        "unanswerable",
        "24 layers, with an additional feature re-presentation layer.",
        "No, they use the total objective function which is a combination of the auxiliary loss and the main loss, scaled by a coefficient lambda.",
        "unanswerable \n\nHowever, it can be inferred that the proposed model might require more computational resources due to the additional feature re-presentation layer and the iterated use of the objective function at intermediate layers, which could potentially increase training time.",
        "Yes, agglutinative languages are used in the prediction of both prefixing and suffixing languages, as indicated by the results showing that models pretrained on HUN (agglutinative) perform well for both English (suffixing) and Zulu (prefixing).",
        "Navajo, an Athabaskan language.",
        "The performance on the task is evaluated using test accuracy, as shown in Tables TABREF18 and TABREF19, and by manually annotating and categorizing errors in the validation set.",
        "English, Spanish, and Zulu.",
        "The baselines are residual-connected models, such as BASE-4L for IWSLT14 German-English and Turkish-English, and the best performance model provided in BIBREF2 for WMT14 English-German.",
        "yes",
        "The article explores the following language pairs: German-English, Turkish-English, and English-German.",
        "The article mentions the following datasets were used:\n\n1. IWSLT14 German-English dataset\n2. IWSLT14 Turkish-English dataset\n3. SETimes corpus\n4. WMT14 English-German dataset\n5. dev2010\n6. tst2010\n7. tst2011\n8. tst2012\n9. tst2013\n10. tst2014",
        "The movement of binomials is tracked by the difference in ordinality between the earliest and latest years in the dataset.",
        "News publications, wine reviews, Reddit, and sports communities.",
        "Yes, trinomials show more structure than binomials, with 36% being completely frozen, compared to 21% of binomials, and the last word often keeping the same position in different orderings.",
        "The new model proposed is a directed graph where an edge from $i$ to $j$ means that $i$ tends to come before $j$ in binomials.",
        "The most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general.",
        "The previously proposed rules for predicting binomial ordering mentioned in the article include phonological rules (such as rhythm and Panini's Law), semantic rules (such as the \"Me First\" principle and categorical tendencies), and frequency-based rules, among others.",
        "News publications, wine reviews, and Reddit are used to test binomial lists.",
        "They model a city description using embeddings by computing the TF-IDF weighted element-wise mean of the token-level representations from pretrained language models, such as ELMo, BERT, and GloVe, and then reducing the dimensionality of the city embeddings to 40 using PCA.",
        "They collect human judgements through an \"odd-one-out\" task, where crowd workers are asked to identify the intruder as the city with the maximum Euclidean distance from the other two in a triplet of cities.",
        "They devise a simple clustering algorithm that iteratively proposes random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence.",
        "The article does not explicitly state that the approach performs better in the multi-domain setting, but it does mention that the model achieves a joint goal accuracy of 45.72% on the multi-domain dataset MultiWoZ, which is significant better than most of the previous models, whereas on the single-domain dataset WoZ2.0, it maintains performance at the level of the state-of-the-art with a marginal drop of 0.3% compared with previous work.",
        "The metric of joint goal accuracy is used to compare the model to previous work, and cross-entropy loss is used during training.",
        "The WoZ2.0 dataset and the MultiWoZ dataset are used to evaluate performance.",
        "unanswerable",
        "unanswerable \n\nThe article does not address why the model cannot learn first-order logic on natural language, but rather discusses its ability to learn on an artificial language and its compositional generalization capabilities.",
        "unanswerable",
        "STORIES.",
        "unanswerable \n\nAlthough they mention training on various text corpora and using ensemble methods, they do not explicitly mention fine-tuning their model on the end task.",
        "Because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement.",
        "They measure grammaticality by computing the log ratio of the grammatical phrase over the ungrammatical phrase.",
        "The inflection model of D18-1473 is used to convert between masculine-inflected and feminine-inflected lemmata.",
        "unanswerable \n\nThe paper discusses the experiments and results of several models, but does not provide a single performance metric that can be used to describe the performance achieved by the model.",
        "unanswerable \n\nHowever, the article does mention that supervision afforded by the RNNG and ActionLSTM models did not translate into more robust or humanlike learning outcomes.",
        "The sizes of the datasets employed vary, including a relatively small corpus, the Penn Treebank, the French Treebank, the Billion Word benchmark, English Wikipedia, and a random subset of the frWaC dataset (approximately 4 million sentences, 138 million word tokens).",
        "The baseline models are the Recurrent Neural Network (RNN) models, specifically the two-layer recurrent neural language models with long short-term memory architecture (LSTM).",
        "BLEU and ROUGE (1, 2, L) scores with multiple references.",
        "The E2E NLG challenge dataset, which is a crowd-sourced dataset of 50k instances in the restaurant domain.",
        "The model's better performance is attributed to its deeper representations, enriched by different levels of text representation, including subword-level embeddings, pre-trained ELMo embeddings, and bi-attention modules.",
        "unanswerable \n\nHowever, we can say that recall is \"great\" for the proposed model but \"not good\" for the other models, as mentioned in the article.",
        "The article does not explicitly mention the f1 score and recall values for the model. However, it mentions that precision is not a priority in their case, and they stopped training when it crossed 70% precision, 90% recall on training and testing sets. \n\nIn the results section, it is mentioned that recall is great for their model but precision is not good, as the model is also trying to detect new potential topics which are not there even in reference Wikipedia-Titles and NER sets. \n\nTherefore, I will not be able to provide a specific answer to the question.",
        "unanswerable",
        "A dump of 15 million English news articles published in the past 4 years.",
        "Around 3 million news articles and 10 million Wikipedia titles.",
        "The BIBREF5 sieve system (RULE), BIBREF6 system (STAT), and the BIBREF11 deep reinforcement system (NEURAL) are tested.",
        "The proposed model achieves new state-of-the-art (SOTA) performances on both datasets, with significant improvements compared to the previous SOTA without BERT on SParC, improving Ques. Match and Int. Match by 10.6 and 5.4 points, respectively.",
        "SParC and CoSQL.",
        "Concat, Gate, Turn, Tree Copy, SQL Attn, Action Copy, and a combination of these methods.",
        "SParC and CoSQL.",
        "The improvement over the state-of-the-art results varies, with a 2.4% and 1.6% improvement in the “DS” part of the two datasets respectively, and a very large margin of +11.4% and +4.9% in the “HDS” part.",
        "Yes.",
        "Yes",
        "unanswerable",
        "Kappa value.",
        "High, with Kappa values ranging from 0.83 to 0.99 for different aspects of annotation.",
        "Adapted to take account of properties of Chinese described in Section 3.",
        "The results show significant improvement over baseline models, with some achieving better performance by up to $55.98\\%$ in F1 score.",
        "The majority baseline, SVM with RBF kernel, FastText, and BiLSTM with Attention.",
        "The pragmatic and discourse context is added to the dataset by structuring the argument path, which only consists of claims and corresponds to a particular line of reasoning for the given controversial topic.",
        "The dataset includes impact votes and the corresponding context of the argument, where users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact, and very high impact.",
        "The training set contains data up to December 31st, 2013 (2,557 days for France and 2,922 for the UK), the validation set contains data from 2014 and 2015 (730 days), and the test set contains data from January 1st, 2016 (974 days for France and 1,096 for the UK).",
        "Yes, the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$, which is an example of geometric property visible for context similarity between words.",
        "Geometric properties pertaining to the behavior of the time series, as well as grouping of words by topic, such as winter, summer, or day of the week clusters.",
        "Achieving less than 5% of MAPE was deemed impressive by expert electricity forecasters.",
        "unanswerable",
        "The proposed system improves the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.",
        "unanswerable \n\nThe article does not provide a clear definition of the state-of-the-art systems, but it mentions that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis.",
        "The proposed system performs multi-tasking by using a two-layered multi-task attention based neural network, where emotion analysis is utilized as an auxiliary task to improve the efficiency of sentiment analysis.",
        "The benchmark dataset of SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC).",
        "unanswerable \n\nHowever, the article does provide some details about the model's architecture, including the number of hidden state vectors and the dimensions of the context vector.",
        "unanswerable \n\nThe article does not specify the previous state-of-the-art model.",
        "unanswerable \n\nThe article does not provide information on the previous state-of-the-art performance.",
        "The classifier can significantly speed up the annotation process, making large-scale content analysis more feasible.",
        "Category I errors need further investigation, and category II and III errors could be improved by applying reasoning or irony detection methods.",
        "The classifiers use category I, II, and III errors, which are further categorized into direct inference, event-specific knowledge, and interpretation of the speaker's intention, respectively.",
        "A convolutional neural network (CNN).",
        "The hashtag-based baseline uses hashtags identified by the pointwise mutual information (pmi) between a hashtag and a class in the annotated dataset.",
        "unanswerable \n\nHowever, the dataset includes English tweets, as mentioned in the section \"Dataset\".",
        "The MH17 Twitter dataset introduced by BIBREF4, a dataset collected to study the flow of (dis)information about the MH17 plane crash on Twitter.",
        "Previous studies analysed information based on manually labeled content, such as television transcripts or tweets, and also used distant annotations, for example, classifying tweets based on the domain of a URL or a hashtag.",
        "The HMM models use spaces, indentation, special characters, and state transitions as features, and those features are somewhat interpretable due to the visualization of the HMM states and LSTM components.",
        "The HMMs learn about the state transitions and emission parameters, which are related to spaces, indentation, and special characters in the data.",
        "The authors use visualizations, such as Figures 3, 3, and 2, to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
        "The gap in performance between the HMMs and the LSTMs is relatively small, as the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension on all text data sets.",
        "Yes, the article does not mention any non-English data.",
        "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset are used.",
        "The embedding algorithm used is Glove Common Crawl Embeddings (840B Token) with a dimension size of 300.",
        "840B Token Common Crawl Embeddings.",
        "They found that their model required 100k parameters, while BIBREF8's model required 250k parameters, indicating a 150k parameter difference.",
        "excluding the word embeddings, our model requires 100k parameters.",
        "The datasets used were the Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset.",
        "The system's F1 performance outperformed previous results by as much as 12 F1 points, achieving a new state of the art for classifying hate speech.",
        "The baseline was a logistic regression model using character n-grams and word unigrams with TF*IDF weighting.",
        "The English-to-Japanese language pair datasets used were KFTT and BTEC, as well as the larger Japanese-English ASPEC dataset.",
        "English-to-Japanese, specifically on the KFTT and BTEC datasets.",
        "25% more roles.",
        "The coverage was measured by evaluating against expertly annotated data and comparing with PropBank.",
        "Quality was measured through F1 scores, specifically using Unlabeled Argument Detection (UA) and Labeled Argument Detection (LA) metrics, as well as Inter-Annotator Agreement (IAA) for dataset consistency.",
        "The corpus was obtained through crowdsourcing with 133K verbs annotated by 2 trained workers independently and a third consolidator.",
        "Through several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback, and 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations.",
        "Systematic screening of workers, concise guidelines, and a short training procedure.",
        "The previous dataset was annotated by trained annotators, but later resorted to crowdsourcing with a single QA-generating worker and validation by two others.",
        "unanswerable",
        "unanswerable",
        "unanswerable \n\nHowever, the article does mention that the dataset contains 36 million En → Fr sentence pairs, which was used to train a multilingual NMT model, but it does not provide any information about the relationship between the training data size and the performance of the multilingual encoder.",
        "unanswerable",
        "Between 2014-2016.",
        "The task success rate achieved is 97.6% when using the object's color to uniquely identify it and 96.0% when using the object's shape.",
        "The authors perform a simulated binning task in which a robot is tasked to place a cube into a bowl as outlined by the verbal command.",
        "unanswerable \n\nThe article does not explicitly state whether the proposed approach is reinforcement or supervised learning, but it is based on imitation learning which is typically a supervised learning methodology.",
        "yes \n\nHowever, it's only exploited for tackling unknown words of Japanese texts, not for Vietnamese texts due to the unavailability of Vietnamese WordNet.",
        "unanswerable",
        "unanswerable \n\nThe article does not explicitly mention the dataset used in the paper. However, it mentions various sources such as financial reports, press releases, earnings call transcripts, credit agreements, news articles, customer interaction logs, and social data that are used in the study.",
        "unanswerable \n\nThe paper discusses various methods and models for anomaly detection from text in the financial domain, but it does not provide specific information about their performance.",
        "unanswerable",
        "unanswerable \n\nThe paper does mention a study that improved the performance of standard methods for fake-news detection, but it does not provide a baseline for the task.",
        "Non-contextual properties of a word, derived directly from the word, and capture the general tendency of a word being echoed in explanations.",
        "The random model with a 15% echo rate.",
        "The proposed features are: non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
        "Our models outperform the random baseline by a wide margin, with the best F1 score more than doubling that of the random baseline (0.286 vs. 0.116).",
        "The F1 score is used as the evaluation metric, especially since the problem is imbalanced.",
        "The article provides explanations for intriguing patterns of word being echoed.",
        "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
        "The SQUAD dataset denoted as $\\mathcal {S}$, and the SQUAD dataset split into ${\\mathcal {S}}^{tr}$, ${\\mathcal {S}}^{val}$, and ${\\mathcal {S}}^{te}$ for training, validation, and testing respectively.",
        "The performance of this system is measured using the BLEU metric, human evaluation, and usability study.",
        "5 questions per image.",
        "Yes, the VQG model uses a similar Encoder-Decoder architecture with attention, similar to image captioning systems.",
        "A total of 15,000 images with 75,000 questions from MS COCO, Bing, and Flickr datasets are used for training the VQG model, while the chatbot model is trained on 162,064 utterances over 10,907 dialogues from Persona-chat dataset and 304,713 utterances over 220,579 conversational exchanges from Cornell-movie corpus.",
        "They obtain word lattices from words by treating all possible substrings of a sentence as vertices, connecting neighbor words by directed edges according to their positions in the original sentence, and using a lookup vocabulary to decide which substrings can be considered as words.",
        "P@1 (Precision@1), MAP (Mean Average Precision), and MRR (Mean Reciprocal Rank) are used for DBQA, while P@1 and MRR are used for KBRE.",
        "They evaluate on two Chinese question answering datasets, DBQA (document based question answering) and KBRE (knowledge based relation extraction).",
        "unanswerable",
        "The article does not specify the language of the data, but it mentions that the dataset (UXTD) contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, who are speaking English. \n\nHowever, the data selection for this investigation defines a simplified phonetic segment classification task using English phonemes, such as /p, b, v, f, /, and /r/. \n\nSo, the answer to your question is 'yes', but with the caveat that the language of the data is only specified as English in the context of the phonetic segment classification task.",
        "Yes, they suggest using recurrent architectures, evaluating the complementarity of audio and ultrasound signals, and extending the classification system to more fine-grained place of articulation.",
        "The dataset, UXTD, contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, with 31 female and 27 male participants.",
        "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification.",
        "They differ from previous work in focusing on child speech with a larger number of speakers (58 children) and using cross-validation to evaluate speaker-independent systems across all speakers.",
        "10700 training examples with roughly 2000 to 3000 examples per class.",
        "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used, with DNNs consisting of 3 hidden layers and CNNs with 2 convolutional and max pooling layers.",
        "58 speakers, specifically 31 female and 27 male children.",
        "Significant improvements in perplexity (with p-value < 0.05) were obtained by the proposed method compared to the baselines.",
        "The multi-turn dialog system learns by directly from the data, without human-defined heuristic rules, and with the help of an external text analysis program called the Linguistic Inquiry and Word Count (LIWC) to capture the emotion information carried in the context.",
        "Human evaluation is performed by recruiting English-speaking students to rate the responses generated by the models based on three criteria: grammatical correctness, contextual coherence, and emotional appropriateness.",
        "Yes.",
        "The vanilla sequence-to-sequence model (S2S) and HRAN are used as baseline models.",
        "unanswerable \n\nAlthough the article mentions that deep neural networks can be used for \"a range of collective natural language tasks, including chunking and extraction of named entities and relationships\", it does not specifically mention relation extraction as a task being evaluated.",
        "The neural-parser-based system.",
        "The main reason is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, providing crucial information for SRL.",
        "Two senior students majoring in Applied Linguistics.",
        "36.4% in rank-correlation with respect to human attention in the VQA-HAT dataset.",
        "They measure the correlation using rank-correlation, as in BIBREF4, BIBREF24.",
        "They obtain region descriptions and object annotations from the Visual Genome dataset.",
        "SNLI + MultiNLI, however, the article specifically states that even training on this combined dataset, accuracy drops significantly when tested on the SICK dataset.",
        "BERT.",
        "six high-performing models covering sentence encoding models, cross-sentence attention models, and fine-tuned pre-trained language models, specifically: BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT.",
        "The datasets used were SNLI, MultiNLI, and SICK.",
        "unanswerable \n\nHowever, we can make an inference that the baseline was not sufficient as the article states \"However, their effort is not enough since such efforts are primarily based on manual moderation to identify and delete offensive materials. The process is labour intensive, time consuming, and not sustainable or scalable in reality\"",
        "Yes.",
        "They use a Stacking method as their ensemble technique.",
        "unanswerable",
        "unanswerable",
        "VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD, and text crawled from Facebook.",
        "unanswerable \n\nAlthough the article discusses Twitter health-related data, there is no mention of the language of the data.",
        "The article does not explicitly state the other interesting correlations observed, but it mentions the discovery of correlations such as \"Yoga-Veganism\" and \"Women-Yoga\" in Topic 2.",
        "The baselines were a basic RNN text comprehension model well-trained on a large dataset and models pretrained on large data.",
        "The state of the art for ranking MC Test answers is the averaging-based model, which can beat even the best previously reported model (HABCNN-TE) on the MC-500 dataset.",
        "The final size of the dataset is outlined in Fig. FIGREF8.",
        "The Argus dataset, AI2-8grade/CK12 dataset, and MCTest dataset were used.",
        "The BLEU metric is adopted to evaluate the model performance during evaluation.",
        "The WMT 2014 English-French and English-German translation datasets, the IWSLT 2014 German-English translation dataset, the IWSLT 2015 English-Vietnamese translation dataset, and two small IWSLT datasets.",
        "WMT14 En-Fr and En-De datasets, IWSLT De-En dataset, and IWSLT 2015 En-Vi dataset.",
        "MUSE achieves 2.2 BLEU gains in En-Fr translation compared to Evolved Transformer.",
        "The opinion summary is determined via a bi-linear term to calculate the association score between the opinion representation and each aspect, and then the improved opinion summary is obtained via the weighted sum of the opinion representations.",
        "Yes.",
        "They use the SemEval ABSA challenge datasets from 2014 to 2016, as well as MPQA, to train the model.",
        "The proposed framework achieves 5.0%, 1.6%, 1.4%, and 1.3% absolute gains on the four datasets compared to the winning systems of SemEval ABSA.",
        "about 23.",
        "Several baseline models, including state-of-the-art neural seq2seq architectures such as n-gram, convolution, LSTM, and Transformer models, are offered.",
        "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations.",
        "unanswerable",
        "The traditional co-occurrence networks fail to establish links between similar words whenever they appear distant in the text because they only connect adjacent words, overlooking long-range syntactical links and semantically similar words not sharing the same lemma.",
        "They complement and add to the previous features of the model, rather than replacing them.",
        "The previous co-occurrence networks are based on simple models such as word adjacency networks and traditional word co-occurrence networks.",
        "The model explanation output is evaluated using BLEU score, but it is mentioned that BLEU score is not an appropriate measure for the quality of explanations, and human evaluation is suggested instead.",
        "unanswerable \n\nThe article does not mention the number of annotators used to write natural language explanations to SNLI-VE-2.0.",
        "unanswerable \n\nHowever, the article does mention that the authors randomly selected 100 image-sentence pairs in the validation set of SNLI-VE and their corresponding explanations in e-SNLI and examined how relevant these explanations are for the VTE task.",
        "The performance difference of the existing model between the original and corrected corpus is not significant, with a slight decrease of 0.50% in the corrected corpus.",
        "The neutral class in SNLI-VE has a substantial labelling error rate of approximately 31%.",
        "A dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
        "Yes.",
        "yes",
        "The dataset used to train Word2Vec for the Italian Language is composed of 2.6 GB of raw text, including 421,829,960 words divided into 17,305,401 sentences.",
        "Different parameter settings, such as the number of epochs, window size, and negative sampling value, have a significant impact on the performance and semantic capacity of the resulting model, leading to oscillatory trends and varying levels of accuracy in the syntactic and semantic macro-areas.",
        "unanswerable \n\nThe article does not compare the findings for Italian language with the English language version.",
        "The dataset used for training Word2Vec in Italian language was obtained from a dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
        "The auxiliary signals from the morphology table are incorporated in the decoder through an attention module that assigns different weights to the affixes in the table.",
        "Affixes of the target language.",
        "unanswerable \n\nThe article does not explicitly mention the language of the Twitter data used, but it does mention using the \"twint\" scraping tool and \"GloVe word embeddings\" which are typically used for English text.",
        "Yes, the authors mention several confounds, including the skew in the model due to the use of \"influential\" Twitter users who rarely make spelling errors or use sarcasm, which leads to a high rate of false negatives, and the potential bias in the study due to the influx of climate change deniers who tweet about hurricanes only after the event.",
        "The machine learning models used include neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel).",
        "The methodology used to compensate for limited labelled data is the \"influential tweet\" labeling technique, where a set of influential Twitter users who are certain to have a classifiable sentiment regarding the topic are used to label tweets in bulk.",
        "The East Coast Bomb Cyclone, the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, and the California Camp Fires.",
        "Twitter.",
        "They used a refined collection of tweets gathered from Twitter, specifically a dataset labeled for named entity recognition task containing 8,257 tweets with 12,784 entities in total.",
        "unanswerable",
        "Strong correlations were discovered between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s).",
        "Yes.",
        "They are experimenting with booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.",
        "unanswerable",
        "They select answer candidates by searching for triples in ConceptNet that share the same concept as the masked concept and relation.",
        "They use natural language processing and computational linguistics tools, including text mining procedures, Granger causality, and Latent Dirichlet Allocation (LDA) topic modeling.",
        "A random selection of tweets from Twitter, matched temporally to the causal corpus.",
        "The selection criteria for \"causal statements\" are that they are with high certainty causal statements, without attempting to study the factual correctness of these statements or offer any degree of verification.",
        "They use a combination of crowdsourcing, automatic methods, and external resources, including the Stanford CoreNLP toolkit and the labMT sentiment scores, which were crowdsourced.",
        "They collect a comparable corpus by gathering the same number of statements selected at random, but controlling for time of year.",
        "They collected the control corpus by selecting random documents that did not contain causal words or bidirectional words, and matched them temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013.",
        "unanswerable \n\n(Note: The article does not mention the languages used in the experiments.)",
        "MemN2N and Attentive and Impatient Readers.",
        "The type and correctness of all the question answer pairs were verified by at least two annotators.",
        "unanswerable",
        "unanswerable \n\nHowever, based on the context it seems they are not hand-crafted, as it is mentioned that \"we determine $\\pi _j$ for each mention $m_j$ in the following way\".",
        "Resolution mode variables, denoted as $\\Pi = \\lbrace \\pi _1, \\ldots, \\pi _n\\rbrace $, are introduced to distinguish mentions resolved by different categories of information, specifically indicating in which mode the mention should be resolved, such as string-matching (str), precise-construct (prec), or attribute-matching (attr).",
        "unanswerable \n\nHowever, the paper does mention that the model outperforms the baseline systems and comes close to the state-of-the-art supervised systems, but it does not explicitly claim to be the state-of-the-art.",
        "unanswerable \n\nHowever, based on the article, one of the problems found with the evaluation scheme for open domain chit-chat systems is the difficulty in constructing a gold standard (a reference set) to evaluate a response generated by such a system due to the infinite number of possible responses.",
        "unanswerable \n\nHowever, it can be inferred that the data is annotated by the voice resource department of the iFLYTEK Corporation for task 2, and possibly by Lingzhi Li, Yangzi Zhang, Jiaqi Zhu, and Xiaoming Shi for task 1, as mentioned in the acknowledgements section.",
        "The collection steps for task 1 include releasing data for training and developing, allowing participants to collect external data for training and developing, and considering two sub tasks, one for a closed evaluation using only the released data and the other for an open evaluation allowing external data collection.",
        "unanswerable",
        "unanswerable \n\nThe article does not explicitly state the result of the highest performing system.",
        "F1-score is used for task 1, and for task 2, the metrics are: task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, and guidance ability for out of scope input.",
        "They measure the quality of summaries using Rouge-L, which is a metric for evaluating the similarity between generated and reference summaries.",
        "The answer is yes.",
        "They refer to the different formats in which answers can be generated, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context.",
        "no \n\nThe article mentions that the model \"Masque\" can generate answers in multiple styles, and that it is capable of answering one question in multiple styles, such as concise phrases and well-formed sentences.",
        "MCAN, extractive approaches, and a model trained with the WFA set consisting of the single style.",
        "unanswerable",
        "The answer styles refer to different ways of answering a question, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question.",
        "LipCH-Net was the previous state of the art model for this task, but it only performed word classification for Chinese Mandarin lip reading, not at the complete sentence level.",
        "sequence-to-sequence learning problem.",
        "The visible movements of the neck, head, and mouth.",
        "unanswerable \n\nSince the provided text does not mention any information about the language of the data being used, we cannot determine if the results are reported only on English data.",
        "They demonstrate the robustness of their results by simulating the effects of an overly context-sensitive classifier in a cross-document setting and comparing the performances of different model types.",
        "unanswerable.",
        "unanswerable",
        "They are defined as units that bear great similarity with elementary discourse units (EDUs) in RST.",
        "Twitter corpus acquired from 2015/07/21 till 2015/08/04.",
        "The proposed word embeddings outperform Sindhi fastText word representations in intrinsic evaluation matrices and have a larger vocabulary.",
        "yes, they are used for various NLP tasks such as parts-of-speech tagging, named entity recognition, and text classification.",
        "unanswerable \n\nHowever, the article mentions that the corpus contains a \"large corpus of more than 61 million words\" and \"unique tokens\" but does not specify the exact number of unique words.",
        "The data was collected from multiple web resources, including news columns of daily Sindhi newspapers, Wikipedia dumps, short stories, sports news from social blogs, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board, and tweets regarding news and sports from Twitter.",
        "The trends found in musical preferences are that audiences want more contemporary, intense, and novel music, but less mellow and unpretentious.",
        "The decades looked at were the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, and 1990s.",
        "77 genres.",
        "Yes.",
        "The CRF model achieves an F1 score of 89.60 on the development set, 87.82 on the test set, and 71.49 on the supplemental test set.",
        "The paper motivates the use of CRF as the baseline model because it was the most popular model in both Shared Tasks on Language Identification for Code-Switched Data.",
        "The handcrafted features used are: Bias feature, Token feature, Uppercase feature, Titlecase feature, Character trigram feature, Quotation feature, Word suffix feature, POS tag, Word shape, and Word embedding.",
        "unanswerable \n\nThe article does not provide a clear definition of the current state of the art method.",
        "They outperform state of the art in terms of BLEU between output and human-written reformulations.",
        "The three new proposed architectures are: \n\n1. A model with a special dedicated discriminator added to control that the latent representation does not contain stylistic information.\n2. A shifted autoencoder (SAE) that feeds the \"soft\" generated sentence into an encoder and checks how close is the representation to the original representation in terms of the cosine distance.\n3. A combination of both the above approaches.",
        "Accuracy can change up to 5 percentage points, while BLEU can vary up to 8 points.",
        "The standard parametrized attention and the non-attention baseline are used as baseline methods.",
        "unanswerable, as the article does not provide a single BLEU score but rather presents various results in tables and figures.",
        "The article does not explicitly list all the datasets used in experiments, but it mentions that four large machine translation datasets of WMT'17 are used, specifically English-Czech, English-German, English-Finish, and English-Turkish. Additionally, the \"Toy Copy task\" and a \"toy data\" are also mentioned, but the specific dataset used for this task is not specified.",
        "A cycle consistency loss.",
        "The proposed model achieves the best results on a standard benchmark.",
        "Our model significantly outperforms competitive baselines, obtaining the best published results.",
        "unanswerable \n\nThe article does not provide information on the size of the data used in the experiments.",
        "unanswerable \n\nHowever, based on the information provided, the following language pairs are experimented on:\n\n- English $\\leftrightarrow $ Italian\n- English $\\leftrightarrow $ Malay (MS)\n- English $\\leftrightarrow $ English-Esperanto (EO)\n- English $\\leftrightarrow $ German (DE)\n- English $\\leftrightarrow $ Finnish (FI)\n- English $\\leftrightarrow $ Spanish (ES)",
        "unanswerable \n\nThis is because the article does not provide information about current state-of-the-art methods that consider the two tasks independently. It does mention that current state-of-the-art methods are often adversarial and do not consider symmetry, and that the proposed method is a simple yet effective approach that outperforms these methods.",
        "unanswerable",
        "The FEVER baseline system and the Enhanced Sequential Inference Model (ESIM) are compared to the Decomposable Attention baseline.",
        "A specific transformer network released by OpenAI, pre-trained for language modeling on the BookCorpus dataset.",
        "The FEVER task is a large-scale challenge that tests a combination of retrieval and textual entailment capabilities.",
        "unanswerable",
        "unanswerable",
        "Yes.",
        "The English$\\rightarrow $Italian/German portions of the MuST-C corpus, along with additional public and proprietary data.",
        "They enrich the positional embedding with length information by computing the distance from every position to the end of the sentence.",
        "They condition the output to a given target-source class by prepending a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$).",
        "English, Italian, and German.",
        "The MuST-C corpus, along with additional data for English-Italian and English-German.",
        "Yes.",
        "The proposed approach is compared against several state-of-the-art models, including the Collective Matrix Factorization model, the Maximum Entropy (MaxEnt) model, and a deep neural network (DNN) architecture, as well as four baseline trackers provided by the DSTC organisers.",
        "unanswerable",
        "The library is framework agnostic, but specifically integrates with PyTorch and TensorFlow.",
        "The self-critical baseline.",
        "Parallel Scan Inference, Vectorized Parsing, and Semiring Matrix Operations.",
        "The Sent baseline, which assumes all targets in each sentence to be of the same polarity.",
        "unanswerable",
        "The input representation of OpenIE tuples into the model is the concatenation of word embedding and another embedding indicating whether this word is a predicate: $\\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]$.",
        "The VIST dataset consists of 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
        "The FHVAE-based disentangled speech representation learning is more prominent than speaker adversarial training in improving the robustness of unsupervised feature learning towards speaker variation.",
        "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect.",
        "Intra-sentential and intra-word code-mixed puns.",
        "The dialogue is guided through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world, and the history.",
        "The semiguided dialogue refers to a type of dialogue where the Wizard is provided with a list of valid and relevant dialogue task actions, but still has some freedom to make choices and interact with the Operator. \n\nIn this type of dialogue, the Wizard interface is guided, as it provides possible actions on the right-hand side of the browser window, but the Wizard still has the option to type a message freely or select from predefined messages.",
        "The article mentions that the CRWIZ framework has been used for a data collection and the results are compared to a similar dataset collected in a more controlled lab setting with a single Wizard.",
        "The CRWIZ Intelligent Wizard Interface ensures compliance with procedures by providing a restricted list of valid and relevant dialogue task actions that changes dynamically based on the context.",
        "They combine the models using two strategies: Max Score Ensemble and Average Score Ensemble.",
        "The logistic regression model with character-level n-gram features.",
        "The context used includes user screen names, comments in the same thread, and the news article the comment is written for.",
        "The language that explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.",
        "The neural network model mainly consists of three parallel LSTM layers with three different inputs.",
        "The model consumes human interaction through the Storyline Planner and Story Writer modules, which process user input, edit and re-generate text, and guide generation by providing a topic and tweaking decoding parameters to control novelty.",
        "They evaluate generated stories through self-reported engagement, satisfaction, and perception of story quality from Mechanical Turk workers, as well as independent ratings from a separate set of Turkers for overall quality and specific improvement areas.",
        "unanswerable",
        "The turn-taking baseline that mimics the interaction of previous work, and the Title-to-Story system, which generates directly from topic.",
        "The spelling features described in BIBREF12.",
        "The contextual features used are the frequency of the token in the whole corpus, the frequency of contextual keywords from the windowed portions of the texts centering on the token in the whole corpus, and the size of the window.",
        "crawled online from 35 cybersecurity blogs published from 2001 to 2018.",
        "human-crafted features that heavily rely on specific language knowledge such as dependency structure.",
        "Yes.",
        "The data in RAFAEL is labelled manually by assigning information to each question.",
        "They take only the first word sense (usually the most common) into account.",
        "It is measured using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor.",
        "unanswerable \n\nThe article does not provide a direct comparison of various transfer learning techniques to determine which one yields the best performance. However, it does mention that mere fine-tuning reaches state-of-the-art results for factoid questions.",
        "The article does not explicitly describe the architecture of the model, but it mentions the use of a sequence-to-sequence Spanish-English ST model and Nonnegative Matrix Factorization (NMF) for topic modeling.",
        "Spanish.",
        "unanswerable \n\nHowever, based on the article, it can be inferred that the vocabulary is derived from the training corpus, as the article mentions that the word frequency is ranked from greatest to least in the training corpus to determine the segmentation point for the hybrid word-characters level.",
        "Character level.",
        "They used the LDC corpus for training data, and the NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as test sets for Chinese-to-English translation, and the NIST 2003(MT03) dataset as the validation set, and the NIST 2008(MT08) as test set for English-to-Chinese translation.",
        "The classic information retrieval metrics of precision (P), recall (R), and F1 are used to measure the correctness of the answers given by participants in the DQA experiments and the QALD system.",
        "Yes.",
        "gAnswer, QAKiS, Platypus, and WDAqua.",
        "unanswerable.",
        "Yes.",
        "The Europarl dataset consists of 70,057 dat-labeled and 33,814 die-labeled sentences, while the SoNaR dataset has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled sentences.",
        "To account for multiple-testing, Bonferroni correction was applied, so the researchers drew a random sample of 503 songs.",
        "unanswerable",
        "Topics typically ascribed to metal lyrics include sadness, death, freedom, nature, occultism, unpleasant/disgusting objects, harsh, gloomy, dystopian, or satanic themes.",
        "SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics with a big margin.",
        "ROUGE, ROUGE-1, ROUGE-2, ROUGE-L, Critical Information Completeness (CIC), relevance, conciseness, and readability are used for automatic and human evaluation metrics to compare SPNet to its counterparts.",
        "unanswerable \n\nThe article does not provide information on whether the proposed abstractive dialog summarization dataset is open source.",
        "The answer is partially, as speaker role scaffold is natural, but semantic slot scaffold is seldom explicitly annotated.",
        "SPNet utilizes additional speaker role, semantic slot, and dialog domain annotations through separate encoding for different speakers, delexicalization and slot filling, and multi-task learning with domain classification.",
        "Pointer-Generator and Transformer, which achieved high-quality summaries of news documents.",
        "The new evaluation metric, CIC (Critical Information Completeness), considers critical informative entities by counting the recall of semantic slot information between a candidate summary and a reference summary.",
        "yes",
        "The annotators were educated to rank results by their coverage, fluency, and plausibility in daily life.",
        "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore are used for this task.",
        "no \n\nThe article mentions that rationales are provided as an optional resource, and the models are not required to generate them.",
        "no \n\nThe article states that the annotators are required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes, implying that the rationales are generated simultaneously with the sentences.",
        "No, the crowd-sourced sentences were created by annotators who were given the concept-sets as the only signal, not shown the associated images or videos.",
        "They are sampled from several large corpora of image/video captions.",
        "at least 16% in absolute accuracy for all the MCQA datasets (except the SemEval dataset).",
        "MultiNLI and SNLI datasets.",
        "Previous SOTA models, as well as RoBERTa-Large models.",
        "The four representative MCQA datasets used for benchmark are DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.",
        "unanswerable",
        "Ambiguity in labelled PIO data, ambiguous sentence labels, and the assignment of a single label limiting the quality of predictions.",
        "They measure the diversity of inferences by using the distinct n-gram score, which is normalized to [0, 1] by dividing the total number of generated tokens.",
        "unanswerable\n\nHowever, the article does state that they \"outperform baseline methods in both the accuracy and diversity of inferences\" but it does not provide a specific percentage or numerical value of the improvement.",
        "unanswerable \n\nHowever, based on the context it can be inferred that the baseline models used are RNN-based encoder-decoder models, but the specific models used as baselines on the Atomic dataset are not explicitly mentioned in the article.",
        "By training on an auxiliary dataset with rich event background knowledge in the pretrain stage.",
        "The Atomic dataset is not explicitly stated in the article, but it is mentioned that it \"scales up the size of dataset and expands the scope to nine types of inference dimensions\" compared to the Event2Mind dataset, which contains 25K base events and 300K targets.",
        "a vendor-based transcription pipeline used at Microsoft for production data.",
        "Significant improvement.",
        "They used 2-layer LSTMs for both the source and target sides in the NMT system.",
        "They appended the domain tag \"<2domain>\" to the source sentences of the respective corpora to prime the NMT decoder to generate sentences for the specific domain.",
        "The potential for ongoing mixed-initiative multi-turn dialogues that do not follow a particular plan or pursue a particular fixed information need.",
        "reaction delay, which is defined as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred.",
        "Their model combines a text sequence sub-network with a vector representation sub-network, which are concatenated through a \"late fusion\" approach, consisting of an embedding layer, two 1-dimensional convolution layers, a max-pooling layer, and several dense layers.",
        "answer, elaboration, and question, etc., as well as agreement, disagreement, appreciation, humor, and \"other\".",
        "39-dimensional MFCCs.",
        "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061.",
        "The Switchboard conversational English corpus.",
        "Sizable improvements in average precision.",
        "Frege's sentence holism is analogous to the general distributional hypothesis, which states that the meaning of a word is given by the contexts in which it occurs.",
        "Meaning is defined as truth-value potential.",
        "yes",
        "unanswerable",
        "The latent variables are refined nonterminals that provide contextual information about each node in a given derivation.",
        "The baselines used are GraphParser without paraphrases and a monolingual machine translation based model for paraphrase generation.",
        "yes \n\nThey used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries.",
        "The strongest correlation among the topics was determined to be between exercise and obesity, but there was also a notable correlation between exercise and diabetes.",
        "Using a topic modeling approach with Latent Dirichlet Allocation (LDA) and fuzzy clustering of semantically related words.",
        "The Swissmetro dataset is used for evaluation, and other datasets are also used but not specified.",
        "They train their embeddings using a neural network model with a softmax output layer, and the embeddings are learned simultaneously with the choice model, allowing the model to accommodate each other's effects.",
        "They model travel behavior using a novel method called Travel Behavior embeddings, which is inspired by natural language processing (NLP) concepts and aims to preserve semantic distance relative to a certain choice problem.",
        "They interpret the coefficients by projecting them back into the original dummy variable space and following the rules of normal random variables to preserve statistical significance information.",
        "unanswerable \n\nHowever, the text does provide that the authors' proposed model outperforms previous state-of-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN), in terms of weighted binary classification accuracy (Wt. Acc.).",
        "Yes.",
        "The lexicon of trafficking flags is expanded through a data-driven approach by analyzing word embeddings learned from raw text data, particularly using a skip-gram model and t-SNE to visualize the vectors for emojis, and re-training the model periodically on new escort ads.",
        "yes",
        "unanswerable",
        "A comment that seems inoffensive on the surface, such as \"your comments fit well in Stormfront\", but may have a malicious intention due to the author's background.",
        "The potential solutions suggested include using additional knowledge from anthologies, modifying NELL, employing deeper semantics, jointly learning classifiers, and using the temporal sequence of response comments.",
        "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens.",
        "unanswerable",
        "Strong.",
        "The novel method (DBCA) achieves higher compound divergence at a similar atom divergence compared to other approaches.",
        "The authors justify that the question answering dataset presented is realistic by stating that the questions are generated automatically using a rule-based system, and the generation rules are designed to have few and meaningful atoms, which yields a large variety of compounds, making the examples come from composing them, and by filtering out unnatural questions using semantic and structural rules.",
        "The three machine architectures analyzed are not explicitly mentioned in the provided snippet.",
        "239,357 English question-answer pairs.",
        "unanswerable (the article does not provide a comprehensive list of other approaches to creating compositional generalization benchmarks).",
        "Low training data imbalanced classification task in Continuous Speech Keyword Spotting.",
        "Honk and DeepSpeech-finetune.",
        "English and Hindi.",
        "unanswerable \n\nHowever, the article mentions that the model has \"almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear\" which implies that the model's training time might be different from state-of-the-art models.",
        "The proposed method achieves competitive performance with slight degradation compared to state-of-the-art contemporaneous work on both WMT2014 and WMT2016 corpora.",
        "unanswerable \n\nHowever, the article does mention that the performance of a simple non-autoregressive model falls behind autoregressive models, and it compares FlowSeq with other non-autoregressive models, but it does not specify what those models are.",
        "WMT2014 DE-EN, WMT2016 RO-EN, and IWSLT2014 DE-EN.",
        "The visual attention is very sparse, with just one source encoding being attended to.",
        "The results of their Principal Component Analysis (PCA) show that the visual features in the How2 dataset are much more dominated by \"common\" dimensions, which makes them less discriminative among individual sentences and further aggravates the problem at the token level.",
        "The two models' architectures in the proposed solution are the Ranker and the Reasoner, both implemented with the MatchLSTM architecture.",
        "The two models, Ranker and Reasoner, cooperate in a cooperative game, where the Ranker selects passages and the Reasoner predicts the linking entity, with each model providing rewards to the other to improve the selection process.",
        "unanswerable.",
        "HotpotQA and MedHop datasets.",
        "They reference various empirical investigations, including BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF26, BIBREF27, BIBREF28, and BIBREF29, among others.",
        "Chinese to English and English to Czech, but mainly Chinese to English."
    ],
    "gpt_pred": [
        "a small list of 30 Japanese emotion predicates—15 positive and 15 negative (e.g., 喜ぶ, 嬉しい, 楽しい… and 怒る, 悲しい, 寂しい…)—used as seed words",
        "a list of 15 positive and 15 negative Japanese emotion predicates used as seeds (e.g., 喜ぶ/嬉しい … and 怒る/悲しい …)",
        "BiGRU trained with AL+CA+CO achieved the highest accuracy; discourse-based label propagation outperformed random, approached supervised performance, BERT was competitive but noise-sensitive, and the method was especially effective when labeled data were small.",
        "By exploiting discourse relations: for Cause pairs, the former event inherits the latter’s polarity; for Concession pairs, the former takes the opposite polarity.",
        "By using discourse relations between event pairs: for Cause, the former event takes the same polarity as the latter; for Concession, it takes the opposite polarity.",
        "about 100 million sentences",
        "about 100 million sentences",
        "positive and negative (event polarity) labels",
        "unanswerable",
        "By propagating sentiment labels through discourse-linked event pairs (Cause = same polarity, Concession = reversed) using a small seed lexicon, turning raw text with these relations into training signals.",
        "30 words",
        "about 100 million sentences",
        "no",
        "no",
        "By bootstrap resampling two groups of five annotators per stanza and comparing their labels using Cohen’s kappa across varying selection thresholds.",
        "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia",
        "yes",
        "yes",
        "They vary systematically by community identity: specialized, dynamic communities show higher retention but larger newcomer–insider (acculturation) gaps, with veterans engaging more in community‑specific and volatile content, whereas in generic communities outsiders engage more with volatile content and retention is lower.",
        "User engagement is highest in dynamic (constantly updating) communities, with distinctiveness boosting short‑term retention but not long‑term; niche, dynamic groups retain users better yet show larger newcomer–veteran linguistic gaps, while in generic communities outsiders engage more with volatile content than established members.",
        "They included all 2013–2014 subreddits with ≥500 vocabulary words in at least 4 months and manually excluded non‑English ones, yielding about 300 (283) communities.",
        "By including all subreddits (2013–2014) with enough English data—at least 500 vocabulary words in at least four months—and removing foreign-language communities, yielding about 300 (283) subreddits.",
        "By a language volatility metric: PMI comparing a word’s frequency in a given month of a community to its frequency over that community’s entire history, aggregated to the community level.",
        "by the average specificity of its language, computed via PMI comparing word frequencies in the community to those in all communities",
        "the Chinese general corpus (Google-pretrained)",
        "Chinese general corpus",
        "QANet and BERT-Base",
        "QANet and BERT-Base.",
        "Extracting or generating a key–value pair from a paragraph, where the key is a query term and the value is the corresponding result derived from the text.",
        "Extracting or generating a key–value pair from the paragraph text, where the key is a query term and the value is its result derived from the text.",
        "tumor size, proximal resection margin, and distal resection margin",
        "tumor size extraction, proximal resection margin extraction, and distal resection margin extraction",
        "There are unrelated sentences between questions.",
        "2,714",
        "Tumor size, proximal resection margin, and distal resection margin.",
        "unanswerable",
        "By integrating clinical named‑entity information (one‑hot CNER tag sequences for text and query) into BERT’s contextual representations, e.g., via concatenation.",
        "2,714 question–answer pairs.",
        "17,833 sentences",
        "QANet and BERT-Base.",
        "perplexity, R@3 (next-word prediction), latency, and energy usage",
        "the modified Kneser–Ney smoothed n-gram model (specifically a 5-gram, KN-5)",
        "perplexity",
        "perplexity",
        "the Yelp Challenge dataset",
        "the Yelp Challenge dataset",
        "no",
        "no",
        "By conditioning generation on an explicit context (rating, restaurant name, city/state, and cuisine tags) so the NMT model produces text aligned with that context.",
        "AdaBoost (an ensemble of shallow decision trees)",
        "yes",
        "2000",
        "unanswerable",
        "unanswerable",
        "one",
        "no",
        "WEAT, RIPA, and the neighborhood bias metric.",
        "That word embeddings inherently model conditional co‑occurrence probabilities (P(w|c)) and that fairness can be expressed as probabilistic parity (e.g., enforcing p(doctor|man) ≈ p(doctor|woman)).",
        "high-quality data",
        "high quality data",
        "Massive data: little to negative gain (e.g., Yorùbá 0.14→0.07); Quality data: large gains—up to 0.39 (Yorùbá) and 0.44 (Twi), i.e., >170% improvement.",
        "CBOW and skip-gram",
        "unanswerable",
        "unanswerable",
        "a large multi‑genre Portuguese corpus collected from several sources",
        "Portuguese word2vec embeddings",
        "yes",
        "yes",
        "F1-measure (precision and recall).",
        "237,723 court decisions",
        "yes",
        "no",
        "By how many of the three survey tools (DOSPERT, BSSS, VIAS) exceed their thresholds in a week: 0 = none (No PTSD), 1 = low, 2 = moderate, 3 = high.",
        "by how many of the three survey tools (DOSPERT, BSSS, VIAS) exceed their thresholds—0=none (no PTSD), 1=low, 2=moderate, 3=high",
        "By using LIWC-style dictionary categories to compute per-user category proportions and α-scores from tweets, then applying a modified LIWC algorithm to derive survey-based s-scores.",
        "By using LIWC’s dictionary framework and algorithm to compute category “alpha” scores from tweets (filtered to first‑person use) and then deriving “s-scores” from these to estimate survey-based PTSD measures.",
        "210",
        "DOSPERT, BSSS, and VIAS.",
        "yes",
        "29,500 documents",
        "29,500 documents",
        "no",
        "yes",
        "yes",
        "With a keyword per cluster—the word in that cluster having the highest anti-edge count (most anti-edges) is used to label the sense.",
        "no",
        "yes",
        "unanswerable",
        "unanswerable",
        "96.0%",
        "the CNN-mean and CNN-avgmax vision-based baselines",
        "CNN-mean and CNN-avgmax.",
        "English, German, French, and Japanese",
        "English, German, French, and Japanese.",
        "no",
        "They didn’t compare to previous research—only to their own baselines (e.g., a small-GRU vs. fine-tuned BERT).",
        "Arab-Tweet (Arap-Tweet), UBC Twitter Gender Dataset, MADAR Task 2 tweets plus the MADAR Task 1 corpus, LAMA-DINA and LAMA-DIST (combined as LAMA-D2), and the IDAT@FIRE2019 Arabic irony dataset.",
        "Arab-Tweet; UBC Twitter Gender; MADAR task‑2 tweets plus the MADAR task‑1 corpus; LAMA‑DINA and LAMA‑DIST (combined as LAMA‑D2); and IDAT@FIRE2019 irony tweets.",
        "weGAN and deGAN",
        "weGAN and deGAN",
        "no",
        "CNN news (politics, world, US) and 20 Newsgroups (grouped into religion, computer, cars, sport, science, and politics).",
        "yes",
        "misspelled or wrongly transcribed words (e.g., from noisy tweets or ASR/STT outputs)",
        "12",
        "yes",
        "yes",
        "unanswerable",
        "no",
        "About 6–8% on sentiment classification, and by 0.94% (gtts–wit.ai) to 1.89% (macsay–wit.ai) on intent classification with STT error.",
        "34,432",
        "unanswerable",
        "overall rating and mean number of turns",
        "overall rating and mean number of turns",
        "a multi-step NLU with a novel dialog act scheme, a fact/opinion interleaving strategy, and an extensive persona backstory database",
        "no",
        "yes",
        "By a linear regression of overall (log) rating on the (log) number of backstory questions asked.",
        "no",
        "no",
        "neural LSTM language models (both word-based and WordPiece-based)",
        "content and fluency ratings",
        "Attention sometimes “gets lost” on the image, overriding text-based cues and misleading the translation.",
        "The attention sometimes “gets lost” on the image, overriding text cues and misleading the translation.",
        "the CalixtoLC17b multimodal baseline",
        "soft attention; hard stochastic attention; local (predictive alignment, “local-m”) attention",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "TF-IDF and NVDM",
        "4.5 million",
        "198,112",
        "the Tencent News dataset",
        "up to four percentage points in accuracy",
        "the GermEval 2019 shared task dataset (Blurb Genre Collection, BGC)",
        "the GermEval 2019 Blurb Genre Collection (BGC) dataset",
        "By concatenating the BERT text embedding with the Wikidata author embeddings (and metadata) and feeding the concatenation into an MLP.",
        "Pattern-based extraction with Bayes/frequency scoring and simple co‑occurrence (popularity) counts.",
        "Pattern-based attribute extraction with statistical scoring (e.g., Bayes), POS-tag patterns with frequency metrics, and simple co-occurrence/popularity counting.",
        "FastText",
        "product enquiries",
        "no",
        "SST-2 (Stanford Sentiment Treebank v2) and Snips.",
        "SST-2 (Stanford Sentiment Treebank v2) and the Snips dataset.",
        "By visualizing the embeddings (PCA followed by t-SNE) so humans could inspect the clusters.",
        "wealth (GDP per capita), population size, aid/ODA received, regime type/democracy, conflict exposure, and regional location",
        "unanswerable",
        "unanswerable",
        "By applying structural topic modeling to UN General Debate speeches (UNGDC).",
        "Cross-domain evaluations with managed human judgments of query–QA relevance, comparing KBs with and without chit-chat (and chit-chat-only), reporting F1/AUC.",
        "By retrieving candidate Q&As from an Azure Search index and re-ranking them in the QnAMaker WebApp—using preprocessing plus WordNet, CDSSM, TF‑IDF, and contextual features—to return the best answer.",
        "QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and the Bot.",
        "QnAMaker Portal; QnAMaker Management APIs; Azure Search Index; QnAMaker WebApp; and the Bot.",
        "by BLEU score on held-out test sets (e.g., NIST MT04/05)",
        "unanswerable",
        "no",
        "N-best list reranking using a hierarchical phrase-based system with about 300 unique hypotheses and 7,491 features.",
        "unanswerable",
        "Majority Vote, Sort by Score, and Rerank (Oracle)",
        "an anonymized internal dataset of ~8.7M user utterances across 23 domains",
        "~8.7M annotated anonymized user utterances across 23 domains (proprietary data)",
        "Topological, rank-based metrics—e.g., checking that for each word all same-class words are closer than any words from other classes, independent of absolute cosine values.",
        "Class Membership, Class Distinction, and Word Equivalence tests based on cosine-similarity thresholds.",
        "cosine-similarity–based success rates for three tests: Class Membership, Class Distinction, and Word Equivalence",
        "a consistent rise in validation loss after about 15 epochs when trained with less than 50% of the data (e.g., at 25% and 50%)",
        "images and text",
        "text and images",
        "Hasty Student and Impatient Reader",
        "unanswerable",
        "Map each segmented sentence to candidate word embeddings, compute a word score (GCNN + information entropy for word legality) and a link score (LSTM for contextual linkage), then average these scores over the sentence to get a sequence score for sample selection.",
        "Map a segmented sentence to candidate word embeddings, compute a word score (legality) via a GCNN with information entropy and a link score (coherence with history) via LSTM, then average word and link scores over the sentence to get the sequence score for sampling.",
        "Train a CRF-based segmenter, auto-label unlabeled data, use an entropy+neural scoring model (word, link, and sequence scores) to select the most informative sentences for human relabeling, update the datasets, retrain, and iterate until convergence.",
        "a gated combination neural network (GCNN) and an LSTM",
        "Guiding reader expectations, supporting coreference resolution and commonsense inference, and embedding the current sentence into a larger context.",
        "yes — 90.5% agreement (qualified-majority on coreference pairs)",
        "yes — about 90.5% agreement (for coreference pairs)",
        "four",
        "WN18 and YAGO3-10",
        "WN18 and YAGO3-10",
        "By finding the neighbor triple with the least influence—i.e., the one whose removal causes the smallest change in the target score—and flagging it as the error.",
        "no",
        "A fully Bayesian, multi-annotator supervised topic model that jointly learns document topics and latent true labels while estimating per-annotator reliabilities/biases, uses scalable stochastic variational inference, and achieves superior predictive performance (classification and regression) on large, complex datasets.",
        "Supervised topic models.",
        "20-Newsgroups, Reuters-21578, and LabelMe.",
        "20-Newsgroups, Reuters-21578, and LabelMe",
        "via human-to-human, synchronous dialogues between paired trained workers (user/tourist and wizard/system) based on generated multi-domain goals",
        "Via synchronous human-to-human chats: pairs of trained workers (user and wizard) conversed in a chatroom following generated multi-domain goals, with states and acts annotated.",
        "BERTNLU (NLU); RuleDST and TRADE (DST); a supervised-learning SL policy (dialogue policy); TemplateNLG and SC-LSTM (NLG); plus a rule-based user simulator.",
        "Using rule-based automatic labeling (with keyword matching) from user/system states and dialogue history—workers annotated user and system states, and experts manually checked 50 dialogues for quality.",
        "Attentive Mimicking",
        "About 50% over standalone BERT and 31% over Attentive Mimicking on medium-frequency words (rare words also improve, but no exact figure given).",
        "MNLI, AG’s News, and DBPedia",
        "MNLI, AG’s News, and DBPedia",
        "WNLaMPro",
        "unanswerable",
        "unanswerable",
        "AIDA-B (AIDA-CoNLL), ACE2004, MSNBC, AQUAINT, WNED-CWEB (CWEB), WNED-WIKI (WIKI), and OURSELF-WIKI.",
        "AIDA-B (AIDA-CoNLL), ACE2004, MSNBC, AQUAINT, WNED-CWEB (CWEB), WNED-WIKI (WIKI), and OURSELF-WIKI.",
        "unanswerable",
        "unanswerable",
        "classification and regression approaches",
        "classification and regression approaches (including SVM-based and neural models)",
        "Labeling-based classifiers: (1) mark sentences as “summary” if their ROUGE-SU4 F1 exceeds a threshold t; and (2) mark the top m sentences with highest ROUGE-SU4 F1 as “summary.”",
        "yes",
        "missing values, language-specific attributes, and multiword expressions",
        "yes",
        "yes",
        "unanswerable",
        "the 31 languages with both UD and UniMorph data",
        "no",
        "Using confusion matrices and per-class metrics, including micro and macro averages.",
        "a neural network with frozen pretrained embeddings, a bidirectional LSTM, and two dense layers with a softmax output",
        "Off‑the‑shelf proprietary emotion recognition software (manufacturer undisclosed) for both face and audio.",
        "Off‑the‑shelf proprietary emotion recognition software (manufacturer undisclosed) for both face and audio.",
        "100k words and 3.2M words",
        "100k words and 3.2M words",
        "10.37 BLEU",
        "the NMT baseline, the NMT with mainstream improvements, and the fully optimized low‑resource NMT setup (configs 1, 2, and 8)",
        "training NMT with high‑resource hyperparameters without low-resource tuning (e.g., large BPE vocabularies, large batches, weak dropout), overreliance on auxiliary data, and domain/morphology mismatches that impair unsupervised methods",
        "no",
        "no",
        "unanswerable",
        "semantic frame extraction using a FrameNet-based causation frame detector (Penelope)",
        "automated extraction of causation frames using the Penelope semantic frame extractor (FrameNet Causation frame triggers)",
        "the Ternary Trans-CNN model (three-layer 1D CNN followed by dense layers)",
        "HEOT",
        "HEOT dataset",
        "3189 messages",
        "3189 messages",
        "Annotated, cleaned tweets obtained from Twitter (Indian subcontinent) from the HEOT dataset by Mathur et al.",
        "unanswerable",
        "a Ternary Trans-CNN and a hybrid multi‑channel CNN–LSTM model",
        "no",
        "HEOT",
        "By aligning words using cosine distances between mBERT token embeddings (minimum-weight edge cover) and showing these alignments outperform FastAlign on gold datasets—while being invariant to language-centroid shifts—demonstrating robust, language-neutral word-level semantics.",
        "By computing alignments using cosine distances between mBERT token embeddings and showing these alignments outperform FastAlign (higher F1), demonstrating that the centering-invariant, language-neutral component captures word-level semantics well enough for high-accuracy alignment.",
        "unanswerable",
        "By estimating and subtracting a per‑language centroid from mBERT sentence vectors (centering), they showed language identity acts like a near-constant shift—centering lowers language-ID accuracy and improves cross-lingual retrieval—indicating separable language-specific and language-neutral components.",
        "Removing language identity while preserving fine‑grained, cross‑lingual semantic equivalence—i.e., going beyond a simple constant-shift correction to capture subtle meaning needed for tasks like QE.",
        "Evaluating faithfulness in a graded way via two approaches: (1) across models and tasks, and (2) across the input space (subspaces/neighborhoods or specific instances).",
        "a graded criterion measuring the extent and likelihood that an interpretation is faithful in practice",
        "The Model Assumption, the Prediction Assumption, and the Linearity Assumption.",
        "the Model assumption, the Prediction assumption, and the Linearity assumption",
        "Be explicit about evaluating faithfulness (not plausibility); avoid human judgment of explanation quality; avoid human-provided gold labels; don’t assume “inherent interpretability”—evaluate it; and don’t use user performance as a proxy for faithfulness.",
        "yes",
        "unanswerable",
        "unanswerable",
        "two",
        "unanswerable",
        "unanswerable",
        "About +1.1 EM and +1.0 F1 on SQuAD (from 68.00/77.36 to 69.10/78.38 on the dev set).",
        "word embedding (char-CNN + GloVe), BiGRU input encoders, soft alignment/attention, aggregation with stacked residual BiGRUs, and a pointer network for span prediction",
        "a pipeline of word embeddings (char-CNN + GloVe), BiGRU input encoders, soft attention/alignment, multi-layer BiGRU aggregation, and a pointer-network span predictor",
        "68.73% Exact Match (EM) on SQuAD.",
        "Spearman’s ρ, Kendall’s τ, and Pearson’s r.",
        "the NIST DUC-05, DUC-06, and DUC-07 summarization datasets",
        "BiGRU-with-attention baselines, ROUGE, LM-perplexity baselines (GPT‑2 and BERT masked‑LM), and BERT next‑sentence‑prediction.",
        "BiGRU-with-attention baselines, ROUGE, language-model perplexity (GPT‑2 and BERT masked‑LM), and BERT next-sentence prediction.",
        "Grammaticality, Non-redundancy, Referential Clarity, Focus, and Structure & Coherence.",
        "WN18RR, FB15k-237, and YAGO3-10",
        "WN18RR, FB15k-237, and YAGO3-10",
        "TransE, DistMult, ComplEx, ConvE, and RotatE",
        "It significantly and consistently outperforms state-of-the-art methods (e.g., +0.021 MRR on WN18RR and +0.050 MRR on YAGO3-10 vs. RotatE).",
        "by using modulus as the radial coordinate and phase as the angular coordinate",
        "phrase-based word embedding",
        "phrase-based word embedding",
        "a text–code parallel corpus of English descriptions and Python code (18,805 aligned pairs)",
        "a text–code parallel corpus of 18,805 aligned English descriptions and Python code pairs",
        "no",
        "an encoder–decoder LSTM (seq2seq) with attention, implemented in OpenNMT/PyTorch",
        "one line long",
        "Phrase-based word embeddings and incorporating Abstract Syntax Trees (coding syntax tree models).",
        "Python",
        "validation data from the source corpus",
        "unanswerable",
        "supervised (using paired text–image data)",
        "Generating high-resolution, photo-realistic images from text descriptions.",
        "Slightly better than other approaches (e.g., GAN-INT-CLS and StackGAN) on the Oxford-102 Flowers dataset.",
        "in the vector gate (vg) used to combine character- and word-level representations",
        "a BiLSTM with max pooling",
        "CR, MPQA, MR, SST2, SST5, SUBJ, TREC, SICK E, SICK R, STS16, and STSB.",
        "MEN; MTurk287; MTurk771; RG; RW; SimLex999; SimVerb3500; WS353; WS353R; WS353S",
        "MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S",
        "Yes—NYT24 (24 relations) and NYT29 (29 relations) are used, but the article does not provide dataset sizes.",
        "WordDecoding (WDec) performed better.",
        "the previously published baselines: CopyR (BIBREF6), HRL (BIBREF11), and GraphR (BIBREF14)",
        "3.9% and 4.1% higher for WDec, and 3.0% and 1.3% higher for PNDec (vs. HRL).",
        "About 3–4% higher (3.9–4.1% for single models; up to 4.2% with ensembles).",
        "- Rank by number of times the citation is mentioned in the document\n- Rank by citation impact (times cited in the literature)\n- Google Scholar “Related Articles” ranking\n- TF*IDF-weighted cosine similarity ranking\n- A learning-to-rank model trained on text similarity rankings",
        "- Rank by number of times a citation is mentioned in the document\n- Rank by citation impact (times cited in the literature)\n- Google Scholar “Related Articles” ranking\n- TF*IDF-weighted cosine similarity ranking\n- A learning-to-rank model trained on text-similarity rankings",
        "SVMRank",
        "450 annotated citations",
        "unanswerable",
        "CNNs",
        "CNNs",
        "unanswerable",
        "83.13% accuracy",
        "unanswerable",
        "yes",
        "yes",
        "Crowdsourced on Amazon Mechanical Turk using ParlAI’s interface, pairing Turkers as tourist and guide to interact in a 360° NYC street-view grid environment.",
        "natural language",
        "localization accuracy and task success rate, plus F1 (with precision/recall) for landmark classification",
        "The Talk The Walk dataset crowd‑sourced on Amazon Mechanical Turk.",
        "yes",
        "unanswerable",
        "By a custom scoring formula that rewards claim clusters aligning with article (story) clusters and the proportion/number of claims included in clusters.",
        "By embedding the claim (USE Large), measuring its distance to all stored claims, linking those within a threshold, and returning the ones in the same Louvain-derived cluster.",
        "Quora duplicate question dataset",
        "claim detection and claim clustering",
        "a path ranking–based knowledge graph completion (PRKGC) model",
        "WikiHop",
        "yes",
        "yes",
        "Crowdsourced on Amazon Mechanical Turk: three workers per instance judged statements (True/Likely/Unsure), selected supporting sentences, and wrote summarized natural language derivations (NLDs).",
        "WikiHop",
        "two",
        "unanswerable",
        "the political pundits of the Washington Post",
        "the political pundits of the Washington Post",
        "Twitter users (the public posting on social media microblogs)",
        "the Washington Post experts’ declared winners",
        "unanswerable",
        "t-SNE visualization of the learned embeddings",
        "TransE, TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR.",
        "DBP24K, FB24K, and Game30K.",
        "DBP24K, Game30K, and FB24K.",
        "By recursively propagating embeddings along relation triples to capture high-order structure and using multi‑head attention to aggregate attribute triples within a GCN framework.",
        "TransH and TransR",
        "unanswerable",
        "yes—noisy hashtag-based supervision and the challenge of selecting true “sober” negatives (e.g., mislabeled/retrospective tweets)",
        "Support Vector Machine (SVM)",
        "capitalization patterns, spelling errors, POS tag ratios, named entity counts, discourse connectors, repeated characters (e.g., “happpy”), emoticon counts, sentiment ratio (positive/negative words), and tweet length",
        "capitalization patterns, spelling errors, POS tag ratios, named-entity mentions, discourse connectors, repeated characters (elongations), emoticon counts/presence, sentiment ratio (positive/negative word proportion), and tweet length",
        "yes",
        "unanswerable",
        "yes",
        "the Sentence corpus (S) from Aristo (BIBREF6)",
        "the Sentence corpus (S) used in BIBREF6 (Aristo)",
        "61.4%",
        "no",
        "no",
        "Open IE v4",
        "no",
        "the sentence corpus S (domain-targeted sentences and web text from Aristo)",
        "Open IE v4",
        "no",
        "Yes—because averaging dense word vectors implicitly handles semantically similar words, providing smoothing that reduces sparsity.",
        "An evaluation using the SemEval 2010 Task 14 (Word Sense Induction & Disambiguation) methodology, measured by adjusted Rand index (ARI).",
        "Watlink, RuThes, and RuWordNet.",
        "cosine similarity",
        "Apache Lucene",
        "pre-trained Vietnamese embeddings by Kyubyong Park (word2vec/fastText, 100-d) and by Edouard Grave et al. (fastText, 300-d)",
        "No extraction; No annotation; Wrong range; Wrong tag; Wrong range and tag.",
        "unanswerable",
        "tasks A and B (question–comment relevance and question–question relevance)",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "yes—preliminary experiments on the Arabic portion of SemEval-2016",
        "yes",
        "unanswerable",
        "A lightweight, task‑specific attentional encoder that models target–context interactions (intra/inter multi‑head attention with point‑wise conv and label smoothing) rather than relying on BERT’s generic encoder.",
        "Depeche++ emotion lexicons + Linear SVM",
        "9",
        "TF‑IDF + SVM; Depeche++ emotion lexicons + SVM; NRC emotion lexicons + SVM; TF‑IDF + NRC + SVM; Doc2Vec + SVM; Hierarchical BiLSTM; BiRNN with self‑attention; ELMo + BiRNN; and fine‑tuned BERT.",
        "9710 passages",
        "3",
        "unanswerable",
        "That the method effectively controls the morphological realization of first‑ and second‑person pronouns and their associated verbs/adjectives in translation.",
        "dependency parsing (using a Hebrew dependency parser)",
        "By adding gender/number-marked prefixes (e.g., “She said to them:”) and verifying—via a Hebrew dependency parse of the outputs—that verbs and pronouns show the expected feminine/plural morphology (also reflected in higher BLEU).",
        "Google’s machine translation system (Google Translate)",
        "pre-processing to add gender/number-indicating prefixes to the input and post-processing to strip the translated prefix from the output",
        "- Cepstral mean normalization (CMN)\n- Cepstral mean and variance normalization (CMVN)\n- Batch normalization",
        "mel filterbank energies (with deltas/delta-deltas) and spectrograms, under various normalizations (CMN/CMVN)",
        "6-layer bidirectional LSTM (bLSTM) with 1024 hidden units.",
        "conversational speech",
        "Predict the number of spans and then generalize single-span extraction using non‑maximum suppression to select multiple non‑overlapping spans.",
        "By tagging each token with BIO labels over the concatenated question–passage, then decoding the BIO sequence (via beam search/greedy) to yield the set of answer spans.",
        "similar results",
        "Best overall on DROP, slightly eclipsing the prior state of the art.",
        "MTMSN",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "KL(qS || pS) over each set S, i.e., LXR(S) = −∑k qkS log pkS with pkS = (1/|S|)∑x∈S pθ(y=k|x).",
        "The Lemming model (a log-linear joint tagger–lemmatizer with a second‑order CRF and edit‑tree pruning).",
        "NLU-Benchmark (NLU-BM)",
        "intent F1 and the combined intent-and-entity F1",
        "no",
        "SQuAD and NewsQA.",
        "With a hybrid scheme: the action generator is trained via Rainbow DQN with experience replay (sample 64 from a 500k buffer every 5 steps), and the question answerer is trained supervised with NLL on ground-truth spans from a separate replay buffer (sampled every 5 steps), reporting test results from the checkpoint with best validation performance.",
        "Ctrl+F (search), next, previous, and stop.",
        "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), and Textual Kernels Model (TKM).",
        "unanswerable",
        "150,000 tweets",
        "They do not outperform unimodal (text-only) models, yielding similar performance.",
        "Noisy, subjective annotations plus the complex, background-knowledge–heavy visual–text relations and too few truly multimodal hate examples lead models to rely mostly on text, so they don’t beat text-only methods.",
        "F-score, area under the ROC curve (AUC), and mean accuracy (ACC).",
        "Twitter API",
        "150,000 tweets",
        "a CNN (Inception v3) for images and a single-layer LSTM for text (including OCR’d image text)",
        "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), and Textual Kernels Model (TKM).",
        "Each tweet has a hate vs. not-hate label (and subcategory labels: racist, sexist, homophobic, religion-based, or other).",
        "accuracy (ACC) and normalized mutual information (NMI)",
        "They performed equally well (no clear winner), both showing large improvements over LPI.",
        "unanswerable",
        "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector–based clustering methods.",
        "SearchSnippets, StackOverflow (question titles), and Biomedical (BioASQ) paper titles.",
        "no",
        "Three Transformer setups: 4 layers (8 heads, d=512, FF=1024), 2 layers (8 heads, d=256, FF=1024), and 1 layer (8 heads, d=256, FF=512).",
        "unanswerable",
        "By BLEU-2 translation scores and exact-match accuracy (correct vs. incorrect) of the generated equation/answer.",
        "AI2, Common Core (CC), Illinois (IL), and MAWPS.",
        "AUC-ROC, accuracy, and top‑1 accuracy (offline); plus, in A/B tests, user interruption rate, reuse of this and other skills, and number of active dialogs.",
        "unanswerable",
        "five-minute reuse and one-day return",
        "TF‑IDF–weighted vectors in a vector space model.",
        "a dataset of 14 textual documents on aliens, stories, law, and news",
        "By comparing SRCC against cosine similarity and Pearson correlation on a 14‑document corpus (via similarity-value analyses and clustering/classification experiments).",
        "the French OSCAR Common Crawl corpus",
        "none",
        "large pretrained language models (e.g., BERT/RoBERTa)",
        "unanswerable",
        "yes",
        "17 hours",
        "the French OSCAR corpus extracted from Common Crawl (Nov 2018), unshuffled, ~138GB of text",
        "structure-based controversy measures",
        "netanyahu; ukraine; Kavanaugh (Oct 3); Bolsonaro (Oct 27); @mauriciomacri (Jan 1–11 and Mar 11–18)",
        "thirty different Twitter discussions (15 controversial and 15 non‑controversial) collected between March 2015 and June 2019",
        "Twitter",
        "four",
        "unanswerable",
        "Informal, noisy, and very short texts—e.g., creative spelling/punctuation, misspellings, slang, new words, URLs, hashtags/abbreviations—plus phenomena like sarcasm, multiple topics per tweet, and tricky negation handling.",
        "unanswerable",
        "approximately 28.4",
        "Labels for 15 sentence-transformation types (e.g., paraphrase and other modifications) for each transformed sentence.",
        "as distinct sentences generated by annotators applying specified transformation types to seed sentences",
        "unanswerable",
        "yes",
        "no",
        "yes",
        "By sourcing seed sentences from both Global Voices and OpenSubtitles to cover journalistic and colloquial (spoken) styles.",
        "no",
        "unanswerable",
        "unanswerable",
        "They aren’t better—SemSentSum matches or outperforms prior state-of-the-art.",
        "unanswerable",
        "unanswerable",
        "Stanford Twitter Sentiment (STS), Sanders Twitter Sentiment, and Health Care Reform (HCR) datasets.",
        "yes",
        "Stanford Twitter Sentiment (STS) Corpus, Sanders Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset.",
        "unanswerable",
        "link prediction and triplet classification",
        "yes",
        "yes",
        "Unlike GANs’ adversarial generator–discriminator setup, NetAb uses two cooperative CNNs—one predicts clean labels and the other estimates a noise transition matrix to handle label noise, trained with cross-entropy rather than adversarial loss.",
        "the movie sentence polarity dataset and the SemEval‑2016 Laptop and Restaurant datasets",
        "It outperforms the state of the art, achieving the best accuracy and F1 on all datasets (except negative-class F1 on Laptop).",
        "yes",
        "unanswerable",
        "no",
        "NWN-STEM, GWN-STEM, and W2V-STEM.",
        "unanswerable",
        "unconstrained conditions",
        "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement",
        "no",
        "unanswerable",
        "unanswerable",
        "SST2, SST5, Subj, MPQA, RT, and TREC.",
        "classification accuracy on benchmark text classification datasets (using CNN and LSTM-RNN classifiers)",
        "yes",
        "no",
        "no",
        "yes",
        "unanswerable",
        "yes",
        "unanswerable",
        "The traditional reductionist pipeline (separate content selection and question construction) and the neural end-to-end Seq2Seq paradigm, including multi-task learning.",
        "text, knowledge bases, and images",
        "no",
        "a hypersphere-based NE model that represents each entity type as a hypersphere in embedding space (center + radius)",
        "yes—on CoNLL-2003 and ONTONOTES 5.0",
        "the Distant Supervision baseline of Go et al. (Naive Bayes/SVM/MaxEnt with unigram/bigram/POS features)",
        "EFWS(x) = N(+x) − N(−x), where N(x) is the number of words in the tweet with polarity score x.",
        "Using TextBlob’s subjectivity score (a 0–1 value).",
        "Because the original DMN learns its attention over facts using labeled supporting facts; without that supervision it struggles to select the relevant facts from the story.",
        "labels indicating which facts/sentences are relevant to answer each question during training",
        "A new two‑level input encoder: a sentence reader plus an input fusion layer (bi-directional GRU) to allow information flow between sentences.",
        "A new two‑level input module with an input‑fusion layer, an attention‑augmented GRU for the memory (with untied passes and a ReLU memory update), and an image input module.",
        "By learning to select the important facts from the larger set using attention.",
        "yes",
        "yes",
        "Annotators rated fluency on a 0–5 scale (0=incoherent, 5=well‑written) and performed style classification (which-of-3 and which-of-2) to judge whether outputs matched the target style.",
        "BLEU, perplexity, classifier accuracy, and the control-realization rates (Exact, Direction, Atomic).",
        "By POS tagging and heuristics—treating nouns, verbs, and adjectives (the words left after removing predefined function-word/control lists) as content.",
        "As a heuristic vector of count-based control features (e.g., counts of function words and syntactic markers like SBAR) embedded as decoder feature embeddings.",
        "unanswerable",
        "Depression risk varies by demographics: females show higher depressive behavior (and higher prevalence) while men have higher suicide rates; age shapes both triggers and expression—youth are less analytically oriented with lower clout (confidence) and higher authenticity, whereas analytic thinking and clout rise with age and authenticity declines.",
        "unanswerable",
        "By integrating multimodal signals—visual features from images/profile photos (including CNN-based face age/gender), textual and profile-description cues (lexicon-based language features and self-disclosures), screen names, and sociability/engagement metrics—to infer demographics like age and gender.",
        "- Images: computational aesthetics (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) and face-based cues (facial presence/expression, age/gender from profile photos; OCR text sentiment).\n- Text/profile text: n-grams and psycholinguistic emotion/sentiment features (e.g., LIWC), plus screen name.\n- Social/behavior: ego-network sociability and user engagement metrics.",
        "Using self-disclosure cues (e.g., profile text) with labels verified by two human judges.",
        "users’ self-disclosed Twitter profile descriptions",
        "Twitter",
        "Twitter",
        "Twitter (users’ profile and posted images)",
        "unanswerable",
        "unanswerable",
        "yes",
        "an atomic unit of meaning used in HowNet to annotate concepts",
        "the Dutch section of the OSCAR corpus (from Common Crawl)",
        "BERT-based transformer models",
        "sentiment analysis and Dutch “die/dat” pronoun disambiguation",
        "They still outperformed vision and feature representations when the environment was flooded with task-nuisances (long, 250+ word descriptions), showing high robustness to added noise.",
        "They outperform visual and feature-based agents, with faster convergence and greater robustness and transfer across scenarios.",
        "unanswerable",
        "They compared natural language state representations to raw image, feature-vector, and semantic-segmentation inputs on multiple VizDoom scenarios (basic, health gathering, take cover, defend the center/line, and a mixed “super” task) using DQN and PPO agents.",
        "By parsing each frame’s semantic segmentation into sentences that describe counts, distances, and directions of objects relative to the agent—built by dividing the screen into patches and concatenating their descriptions.",
        "unanswerable",
        "no",
        "- Google N-grams (Google Books) and COHA corpora (for diachronic embeddings)\n- Moral Foundations Dictionary (MFD)\n- Human valence (and concreteness) ratings for ~14,000 English words\n- WordNet (for noun validation)\n- Historical word-frequency lists for 1800–1999 (top 10,000 nouns)\n- Word frequency and word-length norms",
        "By averaging the seed-word embeddings per class to form centroids and assigning a query to the class with the nearest centroid in embedding space (via a softmax over Euclidean distances).",
        "As the model’s posterior probability p(c1 | q) that a concept belongs to the “morally relevant” class, computed from word embeddings using relevant vs. irrelevant seed words.",
        "Care/Harm; Fairness/Cheating; Loyalty/Betrayal; Authority/Subversion; Sanctity/Degradation",
        "the Google Books N-grams diachronic corpus",
        "Generally better than the rule-based and random baselines—especially in fairy tales—and as coherent as the human-made game, though the human-made version was best overall and the rules-based game was judged more interesting in mystery.",
        "By querying a QA model (ALBERT) with targeted questions to iteratively extract entities/relations (AskBERT), or via a rule‑based OpenIE5 triple extractor with NER/POS postprocessing.",
        "unanswerable",
        "unanswerable",
        "An unaligned corpus collected online: 163K Tang/Song quatrain poems and 337K vernacular paragraphs from 281 books.",
        "BLEU and Micro Entity F1 (and human ratings on correctness, fluency, and human-likeness).",
        "Attn seq2seq; Ptr-UNK; KV Net; Mem2Seq; DSR",
        "InCar and CamRest datasets.",
        "unanswerable",
        "from multiple equal-duration (equi-distant) segments of the video",
        "yes",
        "no",
        "unanswerable",
        "unanswerable",
        "Naive Bayes (multi-class)",
        "Support Vector Machine (SVM)",
        "yes",
        "unanswerable",
        "unanswerable",
        "yes",
        "unanswerable",
        "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.",
        "Upward reasoning: entailment holds when a phrase is replaced by a more general one; downward reasoning: entailment holds when a phrase is replaced by a more specific one.",
        "A form of logical inference that tests entailment by replacing phrases with more general or specific ones according to the upward/downward polarity set by monotonicity operators and syntax.",
        "A strong correlation between MELD’s emotion and sentiment labels.",
        "By first taking labels that match across all models, then preferring consensus among the context models; if none, ranking labels by softmax confidence and choosing the highest agreed label; otherwise assigning “xx” (unknown).",
        "According to the DAMSL tag set (as in the Switchboard DA corpus), with forward- and backward-looking functions.",
        "five",
        "no",
        "an existing dataset (UltraSuite)",
        "yes",
        "a two‑stream (Siamese-style) neural network architecture",
        "By combining a CNN-based sequence tagger (using word embeddings and POS/context) with linguistic pattern rules and taking tokens tagged B–A/I–A as aspect terms.",
        "Ritter (Twitter), MSM 2013 (Twitter), and the UMBC Twitter corpus.",
        "Unlabeled audio from WSJ and LibriSpeech (the 80‑hour clean subset or the full 960‑hour set), or their combination.",
        "12",
        "yes",
        "By sharing character embeddings in a common vector space learned jointly across languages.",
        "Universal Dependencies (UD) treebanks",
        "yes",
        "unanswerable",
        "German, Spanish, Italian, Portuguese, Chinese, Japanese, and Arabic.",
        "An in-house multilingual RE dataset (EN, DE, ES, IT, JA, PT) and the ACE 2005 multilingual dataset (EN, AR, ZH).",
        "unanswerable",
        "approximately 1.73 million samples",
        "By using a modified FactorCell LSTM that conditions on CNN-extracted image features (fed via the adaptation matrix) to generate character-level completions with beam search.",
        "unanswerable",
        "35%",
        "A two-step seq2seq+attention baseline with DFS-based path verification, plus two ablations of their model: (1) no graph input (no attention/FC, no masking) and (2) the same ablation but with the output-layer masking.",
        "It achieved the best performance on the dataset, outperforming all baselines.",
        "Exact Match (EM), F1 score, Edit Distance (ED), and Goal Match (GM).",
        "yes",
        "Via Amazon Mechanical Turk, using 100 simulated environments with corresponding topological maps.",
        "unanswerable",
        "Clinical sentiment features and structured biomarkers/assessments, including EEG, MRI anatomy (gray matter volume, cortical thickness, surface area), social/role functioning, NEO-FFI, and symptom scales (PANSS, MADRS, YMRS).",
        "Strongest topic coherence in themes of substance use, suicidality, and anxiety disorders.",
        "EHRs from McLean Hospital’s OnTrack program (Meditech; 240k paragraphs from 220 patients) and psychiatric EHRs from the Partners HealthCare Research Patient Data Registry (RPDR; ~30,000 patients).",
        "unanswerable",
        "By segmenting words into stems and suffixes via morphological analysis and then applying BPE only to the stem (marking stems with “##”, suffixes with “$$”, and BPE continuations with “@@”).",
        "It splits each word into a stem and suffix morphemes, applies BPE to the stem only, and marks boundaries (“##” for intact stems, “@@” for non-final stem subwords, “$$” for suffixes) to preserve morphology for NMT.",
        "unanswerable",
        "unanswerable",
        "36 layers",
        "Yes—via a weighted sum (main loss plus λ=0.3 times the auxiliary losses across selected layers).",
        "unanswerable",
        "yes",
        "Zulu",
        "by accuracy on held-out sets (exact-match inflection accuracy)",
        "English, Spanish, and Zulu",
        "residual-connected encoder–decoder models (e.g., BASE-4L and the WMT14 baseline from BIBREF2)",
        "yes",
        "German–English, Turkish–English, and English–German",
        "IWSLT14 German–English, Turkish–English (IWSLT14 + SETimes), and WMT14 English–German",
        "By computing yearly ordinality o_{x,y,t} and defining movement as the range over time: max_t o_{x,y,t} − min_t o_{x,y,t}.",
        "news publications, wine reviews, and Reddit posts",
        "yes — trinomials show more structure: many are completely frozen, and when not, the last word typically remains fixed, unlike binomials",
        "a null model in which binomial orderings are randomly assigned according to their global asymmetry values",
        "Largely ineffective—previously proposed rules performed poorly at scale.",
        "phonological, semantic, and metadata (frequency) rules",
        "Reddit, news articles, and wine reviews.",
        "By taking the TF‑IDF‑weighted element‑wise mean of token embeddings (ELMo/BERT/GloVe) for each city and then reducing to 40 dimensions with PCA.",
        "by crowdsourcing an odd-one-out task where workers pick the intruder among triplets of city descriptions",
        "a custom exchange-based clustering algorithm that iteratively swaps memberships to maximize “cluster strength” (intra- vs inter-group distance) under equal-size constraints",
        "single-domain setting",
        "- Joint goal accuracy\n- Inference Time Multiplier (ITM)\n- Joint domain accuracy (ablation)\n- Joint domain-slot accuracy (ablation)",
        "WoZ2.0 and MultiWoZ",
        "By applying a standard first‑order logic theorem prover to the FOL translations of the sentences using a fixed background ontology/axioms.",
        "unanswerable",
        "unanswerable",
        "STORIES (the customized CommonCrawl subset)",
        "no",
        "Because many languages have rich gender agreement, not like English; simply swapping gendered words doesn’t update all agreeing forms, making sentences ungrammatical.",
        "the LM log-likelihood ratio of a grammatical phrase to its ungrammatical counterpart",
        "a Markov random field (for morpho-syntactic agreement)",
        "unanswerable",
        "unanswerable",
        "about 4 million sentences (≈138 million word tokens)",
        "two-layer LSTM language models (LSTM (PTB) and LSTM (FTB))",
        "BLEU and ROUGE (ROUGE‑1, ROUGE‑2, ROUGE‑L) scores.",
        "the E2E NLG Challenge dataset (restaurant domain)",
        "Because it augments word embeddings (subword + ELMo) and uses a deep residual bi-attention encoder, yielding richer, better sentence-pair representations.",
        "unanswerable",
        "F1: unanswerable; recall: about 90%",
        "one",
        "unanswerable",
        "about 3 million news articles (≈150 million sentences)",
        "the RULE sieve system, the STAT statistical system, and the NEURAL deep reinforcement system",
        "10.6 and 5.4 percentage points (Ques.Match and Int.Match on SParC, vs prior SOTA).",
        "SParC and CoSQL",
        "Concat; Turn; Gate; SQL Attention; Tree Copy; Action Copy",
        "SParC and CoSQL",
        "unanswerable",
        "yes",
        "no",
        "no",
        "Cohen’s kappa (kappa value)",
        "≥ 0.9 (Cohen’s kappa)",
        "By modifying the PDTB-3 scheme for Chinese—e.g., removing AltLexC, adding a Progression sense, and adjusting argument scope and connective treatment.",
        "Significantly better than the baselines (p<0.001).",
        "FastText and a BiLSTM with attention (both using only the claim text).",
        "By including each claim’s argument-path context—its predecessor claims from the Kialo argument tree (from the thesis down to the claim).",
        "impact votes for each claim and the corresponding argument context (its position in the argument tree)",
        "Training: 2,557 days (France) and 2,922 days (UK); Testing: 974 days (France) and 1,096 days (UK).",
        "Yes—e.g., the vector king − man + woman is close to queen.",
        "They encode linguistic similarity as vector-space geometry (e.g., king − man + woman ≈ queen).",
        "less than 5% relative error (MAPE)",
        "an improvement of about 5 F-score points over the prior state-of-the-art on SSEC",
        "unanswerable",
        "Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN.",
        "By jointly training sentiment and emotion analysis with a shared BiLSTM word encoder and task‑specific two-level attentions, feeding each to separate output layers.",
        "SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC)",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "By pre-filtering tweets to surface likely polarized (pro-Russian/pro-Ukrainian) cases, increasing the hit rate and speeding manual annotation.",
        "Applying reasoning and irony-detection methods and better handling Twitter-specific/unknown words (e.g., via more data and improved representations).",
        "Category I, II, and III errors.",
        "a convolutional neural network (CNN)",
        "class-indicative hashtags selected via PMI from the annotated dataset",
        "English and Russian",
        "the MH17 Twitter dataset",
        "Domains of shared URLs and hashtags.",
        "Either raw characters (discrete emissions) or LSTM hidden-state vectors (continuous emissions); the resulting HMM states are quite interpretable, often aligning with human-readable cues like spaces, indentation, punctuation, and comment symbols.",
        "spaces, indentation, and punctuation/comment symbols (e.g., code comments)",
        "By visualization: color‑coding the text with HMM state posteriors and with k‑means clusters of LSTM hidden states (supplemented by decision‑tree probes of individual LSTM units).",
        "unanswerable",
        "unanswerable",
        "Waseem and Hovy’s Sexist/Racist (SR), Davidson et al.’s Hate (HATE), and Golbeck et al.’s Harassment (HAR) Twitter datasets.",
        "GloVe (Common Crawl), 300-dimensional",
        "Common Crawl (GloVe, 840B tokens)",
        "about 150,000 fewer parameters",
        "100k parameters (excluding embedding weights)",
        "the Sexist/Racist (SR), HATE, and HAR (Harassment) Twitter datasets",
        "unanswerable",
        "a logistic regression using TF–IDF-weighted character n-grams and word unigrams",
        "KFTT, BTEC, and ASPEC",
        "English–Japanese (and Japanese–English for ASPEC)",
        "25% more coverage.",
        "by evaluating recall against an expertly annotated sample and via comparison with PropBank",
        "an additional validation step in which workers judged the validity of the questions (but not the answers)",
        "by crowdsourcing (on Amazon Mechanical Turk)",
        "Through several short training rounds (up to 15 predicates each) with extensive personal feedback, selecting workers who perform well against expert annotations.",
        "Screened, trained workers annotate each verb twice independently, and a third “consolidator” merges/validates their QAs under refined guidelines (e.g., handling modality/negation and single-entity answers).",
        "By non-expert crowd workers after a brief training procedure.",
        "133K verbs",
        "unanswerable",
        "unanswerable",
        "English–French parallel translation data.",
        "2014–2016",
        "97.6%",
        "a simulated binning task where a robot places a cube into a specified bowl based on language and vision",
        "supervised (imitation) learning from demonstrations",
        "yes",
        "no",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "Features derived directly from the word itself—e.g., its frequency, length (#characters), and WordNet depth.",
        "a random model that predicts “echoed” with 15% probability",
        "non-contextual word properties; word usage features in the OP and in the PC; features capturing OP–PC connection/differences; and general OP/PC conversation properties",
        "Random baseline: predict the positive label with 0.15 probability (F1 ≈ 0.116 on content words).",
        "F1 score",
        "no",
        "- Non-contextual word features\n- Word usage features in the OP and in the PC\n- Features capturing OP–PC differences (how a word connects OP and PC)\n- General OP/PC conversation properties",
        "SQuAD (Stanford Question Answering Dataset).",
        "By BLEU on the VQG model’s validation set and human evaluation/usability surveys for the chatbot’s dialogue quality.",
        "5",
        "yes",
        "15,000 images with 75,000 questions",
        "By taking all substrings that appear in a lookup vocabulary as nodes and connecting neighboring words with directed edges according to their positions in the sentence.",
        "Precision@1 (P@1), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR)",
        "DBQA and KBRE (two Chinese QA datasets from NLPCC-2016)",
        "two competing languages (language 1 and language 2)",
        "yes",
        "Adding the speaker’s mean ultrasound frame as an extra input (e.g., a second channel in CNNs) to aid speaker normalization.",
        "UltraSuite UXTD: synchronized acoustic–ultrasound from 58 typically developing children (5–12 years; 31F/27M), phone-aligned B-mode midsagittal tongue images recorded on a SonixRP at 121 fps with 135° FOV, each frame 63×412 samples, using UXTD type A and B utterances.",
        "feedforward neural networks and convolutional neural networks",
        "yes",
        "10,700",
        "feedforward (DNN) and convolutional (CNN) neural networks",
        "58",
        "unanswerable",
        "In an end-to-end, data-driven way, learning emotional interactions directly from multi-turn dialog data without hand-crafted rules.",
        "By four English-speaking students who rated each model’s generated response (given the first three turns of 100 four-turn dialogs) on grammatical correctness, contextual coherence, and emotional appropriateness using a 0–2 scale.",
        "yes",
        "the vanilla sequence-to-sequence model (S2S) and HRAN (Hierarchical Recurrent Attention Network)",
        "no",
        "the neural-parser-based baseline",
        "yes — because parsers can recover correct structure for grammatical fragments in L2 texts, providing crucial cues that improve SRL.",
        "two senior students majoring in Applied Linguistics",
        "unanswerable",
        "by computing rank-correlation between human and model attention maps",
        "from the Visual Genome dataset",
        "MultiNLI",
        "BERT",
        "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT.",
        "SNLI, MultiNLI, and SICK",
        "unanswerable",
        "yes",
        "a stacking classifier using a simple fully connected neural network (dense model) as the meta-model",
        "unanswerable",
        "unanswerable",
        "the VLSP 2019 Hate Speech Detection (HSD) dataset",
        "unanswerable",
        "Yoga–Veganism and Women–Yoga.",
        "Evidence Averaging, and Evidence Averaging augmented with a BM25 word-overlap feature.",
        "an ensemble of hand-crafted syntactic and frame-semantic features",
        "unanswerable",
        "Argus (their question–evidence dataset), AI2-8grade/CK12 science QA, and MCTest (MC-160 and MC-500)",
        "BLEU",
        "WMT14 En–Fr, WMT14 En–De, IWSLT 2014 De–En, and IWSLT 2015 En–Vi.",
        "WMT14 English–French, WMT14 English–German, and IWSLT2014 German–English.",
        "2.2 BLEU points (on En–Fr).",
        "By applying bi-linear attention to aspect-transformed opinion representations and taking the weighted sum of those representations.",
        "yes",
        "the four SemEval ABSA ATE datasets: SemEval 2014 Laptop and the Restaurant sets from SemEval 2014, 2015, and 2016",
        "5.0%, 1.6%, 1.4%, and 1.3% absolute gains (across the four datasets).",
        "about 23",
        "n‑gram (3- and 4-gram), convolutional seq2seq (fconv), LSTM (with and without attention), Transformer, GPT‑2; and for argument prediction, a seq2seq Transformer and a copy-augmented Transformer.",
        "ordering pizza; creating auto repair appointments; setting up ride service; ordering movie tickets; ordering coffee drinks; making restaurant reservations",
        "other text classification tasks",
        "Because word adjacency networks connect only adjacent words, they ignore long‑range relationships, so distant semantically similar words are not linked.",
        "They augment the co-occurrence network with embedding-based virtual edges; they don’t replace the existing features.",
        "word adjacency networks (window-based co-occurrence graphs linking adjacent words)",
        "Yes—via manual human evaluation of explanation relevance (percentage of explanations judged relevant).",
        "three",
        "one per training instance and three per validation/test instance",
        "0.16 percentage points",
        "neutral",
        "A 2.6‑GB Italian corpus from the 2019‑04‑01 Italian Wikipedia dump, Italian Google News categories, and anonymized Laila customer‑care chats.",
        "no",
        "yes",
        "2.6 GB of raw text",
        "Performance depends strongly on tuning: more epochs generally improve results; high negative sampling—especially with small windows—hurts accuracy and causes oscillations; the best K depends on window size; Skip‑gram outperforms CBOW; and semantics are captured more easily than syntax (3COSMUL best overall, 3COSADD slightly better in limited-learning/semantic cases).",
        "unanswerable",
        "a combined corpus of Italian Wikipedia (2019-04-01 dump), Italian Google News articles, and anonymized Laila customer-care chat logs",
        "By an attention mechanism over the morphology table, whose weighted affix embeddings are concatenated to the decoder’s last layer to guide prediction.",
        "target-language affixes (affix embeddings)",
        "unanswerable",
        "yes",
        "RNNs (including an LSTM), CNNs, Naive Bayes with Laplace smoothing, k‑clustering, and linear‑kernel SVMs.",
        "Using distant supervision: label tweets by “influential” accounts with known pro/anti climate stances, assigning binary labels to their tweets to generate training data.",
        "East Coast Bomb Cyclone; Mendocino (California) wildfires; Hurricane Florence; Hurricane Michael; California Camp Fires.",
        "Twitter",
        "the BIBREF8 Twitter NER dataset (8,257 labeled tweets)",
        "unanswerable",
        "Harassment was more frequent at night; conductors/drivers and then friends/relatives were leading identified harassers; young perpetrators tended to act in groups, mostly on streets, while adults more often acted alone, especially on public transportation; and there were strong correlations between perpetrator age and location, group size and location, and age and group size.",
        "yes",
        "Booking flights (United), renting homes (Airbnb), buying bus tickets (Greyhound), and making restaurant reservations (OpenTable).",
        "unanswerable",
        "By using the masked concept as the correct answer and choosing four distractors from ConceptNet triples that share the same relation and the other concept—i.e., from (∗, r, c2) or (c1, r, ∗).",
        "By keyword filtering—selecting tweets that explicitly use causal verbs like “caused,” “causes,” or “causing,” then analyzing surrounding n-grams.",
        "Random, time-matched English tweets from the 2013 Twitter Gardenhose that exclude “caused/causing/causes” and bidirectional words.",
        "Tweets containing explicit causal verbs—e.g., “caused,” “causes,” or “causing”—to ensure high‑certainty causal statements.",
        "crowdsourcing",
        "By sampling an equal number of random tweets from the same time periods (controlling for time of year) as the causal set.",
        "Randomly selected English tweets that exclude “caused/causes/causing” and bidirectional words, time-matched to the causal set to have the same count in each 15‑minute interval.",
        "Chinese",
        "MemN2N, Attentive Reader, and Impatient Reader.",
        "unanswerable",
        "unanswerable",
        "yes",
        "Variables πj that indicate, for each mention, which resolution mode to use—string-matching (str), precise-construct (prec), or attribute-matching (attr).",
        "no",
        "unanswerable",
        "unanswerable",
        "unanswerable",
        "31",
        "unanswerable",
        "Task 1: F1-score; Task 2: task completion ratio, user satisfaction, response fluency, number of dialogue turns, and guidance ability for out-of-scope input.",
        "ROUGE‑L.",
        "yes",
        "Different formats of the answer—e.g., concise phrases (Q&A) versus well‑formed sentences (NLG) that are understandable without the question’s context.",
        "no",
        "unanswerable",
        "unanswerable",
        "the desired format of the generated answer, e.g., a concise phrase vs. a well‑formed standalone sentence",
        "LipCH-Net",
        "a sequence-to-sequence architecture (with dual attention)",
        "visible movements of the neck, head, and mouth",
        "yes",
        "By showing their models significantly outperform the majority-class baseline (macro F1) across all prediction settings and type variables.",
        "unanswerable",
        "With tree-based sentiment scores encoded as a 5-class one-hot vector, assigned to the highest parse node covering the span.",
        "unanswerable",
        "the Russian Twitter stream (15 days, 2015/07/21–2015/08/04)",
        "They outperform SdfastText, achieving higher intrinsic similarity scores with a larger, cleaner vocabulary (SG best), while SdfastText has limited vocabulary and lower performance.",
        "yes",
        "unanswerable",
        "Using a web-scrapy-based crawler, from daily Kawish and Awami Awaz (Sindhi) newspapers, Wikipedia dumps, Wichaar social blog, Focus WordPress blog, Sindh Salamat literary sites, Sindhi Adabi Board (novels/history/religion), and Twitter (news/sports).",
        "Audiences increasingly prefer music that is contemporary, intense, and somewhat novel/sophisticated, and less mellow and unpretentious.",
        "the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, 1990s, and 2000s",
        "77",
        "yes",
        "F1 ≈ 89.6 on the dev set, 87.8 on the test set, and 71.5 on the supplemental test set.",
        "yes",
        "Bias, token, uppercase (y/n), titlecase (y/n), character trigrams, quotation (y/n), word suffix (last 3 chars), POS tag, word shape, and word embedding.",
        "using an external pre-trained style classifier to measure transfer accuracy",
        "unanswerable",
        "a model with a style-removal discriminator (Dz), a shifted autoencoder (SAE), and the combination of both",
        "up to 5 percentage points",
        "the standard (parametrized) attention-based model and a non-attention baseline",
        "unanswerable",
        "- Synthetic Toy Copy datasets (sequence lengths L ∈ {10, 50, 100, 200})\n- WMT’17 MT datasets: English–Czech, English–German, English–Finnish, English–Turkish (train), with newstest2015 for validation and newstest2016 for testing",
        "Cycle consistency losses (using average cosine similarity) in both directions.",
        "unanswerable",
        "significantly better, achieving the best published results compared to competitive baselines",
        "unanswerable",
        "English–Italian, English–German, English–Finnish, English–Spanish, English–Esperanto, and English–Malay.",
        "Conneau et al. (2018)’s adversarial MUSE model (Adv-C)",
        "unanswerable",
        "the FEVER baseline using Decomposable Attention (AllenNLP) on five concatenated sentences (FEVER Five/Oracle)",
        "OpenAI’s pre-trained GPT (12-layer) language model trained on BookCorpus.",
        "Verifying textual claims against Wikipedia by retrieving evidence sentences and classifying each claim as supported, refuted, or not enough info.",
        "unanswerable",
        "unanswerable",
        "yes",
        "the English→Italian/German portions of the MuST-C corpus (TED Talks)",
        "By adding to each decoder timestep an extra positional vector that encodes distance to the target’s end—using len−pos (absolute) or a quantized relative proportion—and summing it with the target embedding.",
        "By prepending a special length token to the source (e.g., <short>, <normal>, <long>) indicating the desired target/source length-ratio class.",
        "English–Italian and English–German (English, Italian, and German)",
        "the English→Italian/German portions of the MuST-C corpus (TED Talks)",
        "yes",
        "Baseline, Focus, HWU, HWU+, MaxEnt, and a DNN model.",
        "unanswerable",
        "a PyTorch library (not framework-agnostic)",
        "self-critical baseline",
        "parallel scan inference, vectorized parsing, and efficient semiring matrix operations (via custom CUDA kernels)",
        "the Sent baseline—assuming all targets in a sentence have the same polarity",
        "unanswerable",
        "A BIO tagging scheme: O for non-span words, Bp/Ip for the predicate span, and Ba_i/Ia_i for each argument span.",
        "40,071 training, 4,988 validation, and 5,050 testing stories.",
        "FHVAE-based disentangled representation learning outperforms adversarial training—both help, but FHVAE yields larger (more prominent) ABX error reductions, with only marginal gains from combining them.",
        "Wordplay jokes where a word or phrase suggests multiple meanings by exploiting polysemy, homonymy, or phonological similarity for humorous or rhetorical effect.",
        "intra-sentential and intra-word",
        "By using an FSM-driven Wizard interface that restricts the Wizard to predefined, context-appropriate actions/messages.",
        "A dialogue where only the Wizard’s side is constrained—guided by an FSM with predefined message/action options—while free text and the Operator’s turns remain unguided.",
        "Yes—CRWIZ has been used on AMT to collect emergency-response dialogues and generate a dataset compared against a prior lab-set dataset; it demonstrates feasible, scalable collection, but no specific quantitative results are reported.",
        "By constraining the Wizard to context‑dependent actions in an FSM—offering only a restricted set of predefined, valid dialogue options.",
        "By ensembling their prediction scores—taking either the maximum score (Max Score Ensemble) or the average score (Average Score Ensemble).",
        "logistic regression with character-level n-gram features",
        "the news article title and the user’s screen name",
        "language that explicitly or implicitly threatens or demeans a person or group based on an identity facet (e.g., gender, ethnicity, sexual orientation)",
        "Three parallel bi-directional LSTMs (attention on the comment branch), whose outputs are concatenated and fed to a sigmoid layer.",
        "User inputs are lowercased and tokenized (via spaCy), with OOVs mapped via WordNet hypernym/hyponym search, then passed as conditioning to the Storyline Planner and Story Writer.",
        "By crowdworker (MTurk) ratings on a five-point scale for overall quality and for creativity, relevance, and causal/temporal coherence (two ratings per story, discarding pairs differing by >2).",
        "no",
        "the Title-to-Story model and a turn-taking interaction baseline modeled on prior work",
        "Judging tokens as IOCs using only spelling features (a spelling-rule baseline).",
        "Frequencies of automatically extracted contextual keywords co-occurring within a fixed window around each token (a co‑occurrence vector over those keywords).",
        "687 English APT reports and 5,427 Chinese articles crawled from 35 cybersecurity blogs.",
        "Human-crafted linguistic features, e.g., dependency-structure-based features.",
        "yes",
        "manually",
        "They use only the first (most common) WordNet sense for polysemous words.",
        "by surrounding uniformity (SU): |w + Σ a_i| divided by (|w| + Σ |a_i|) for the N nearest neighbors",
        "fine-tuning with forgetting cost regularization",
        "unanswerable",
        "Spanish",
        "from the training corpus (LDC data)",
        "word-level granularity",
        "LDC corpus",
        "by precision, recall, and F1-score",
        "yes",
        "WDAqua, QAKiS, gAnswer, and Platypus (askplatyp.us).",
        "unanswerable",
        "no",
        "103,871 sentences (Europarl) and 1,269,091 sentences (SoNaR).",
        "unanswerable",
        "unanswerable",
        "sadness, death, freedom, nature, occultism, and unpleasant/disgusting objects",
        "unanswerable",
        "ROUGE-1/ROUGE-2/ROUGE-L and CIC (automatic); human ratings of relevance, conciseness, and readability, plus pairwise ranking.",
        "unanswerable",
        "No—speaker role and domain are often available, but semantic slot annotations are rarely provided.",
        "By using them as semantic scaffolds: separate encoders for user/system roles, delexicalized template generation with copy/pointing to fill slot values, and an auxiliary multi-label domain classification task jointly trained with summarization.",
        "sequence-to-sequence neural models with attention (e.g., pointer-generator)",
        "By using CIC, which measures the recall of semantic slot values (delexicalized critical entities) matched between the candidate and reference summaries.",
        "no",
        "coverage, fluency, and plausibility in daily life",
        "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore.",
        "no",
        "unanswerable",
        "Yes—for the dev/test sets, sentences were written by humans shown only the concept-sets.",
        "from large corpora of image/video captions (e.g., VATEX, LSMDC, ActivityNet, SNLI)",
        "At least 16% absolute accuracy (e.g., 16.9% on DREAM), except SemEval.",
        "MultiNLI and SNLI",
        "unanswerable",
        "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.",
        "the existing ebm-nlp leaderboard models and SCIBERT results",
        "Ambiguous, single-label sentence annotations—often noisy or wrong—because sentences can contain multiple PICO elements, which limits prediction quality.",
        "by normalized distinct n-gram counts (e.g., distinct-1/2)",
        "unanswerable",
        "unanswerable",
        "By pretraining on narrative corpora with a context-conditioned latent variable z_c and minimizing the distance between z_c and z_{c′} to transfer the background knowledge into z_{c′}.",
        "unanswerable",
        "Microsoft’s vendor-based two-pass transcription pipeline (first-pass transcription with second-pass QC).",
        "unanswerable",
        "2-layer LSTM encoder–decoder with attention (NMT).",
        "By appending a special token (e.g., <2domain>) to each source sentence to indicate its domain during training.",
        "because they’re open-ended and lack a fixed plan or information need, forcing the system to maintain coherence and choose among many possible actions while integrating diverse content in real time",
        "as reaction delay—the time between the original post/tweet and the reaction comment/tweet",
        "a linguistically infused late‑fusion neural net: a CNN text branch (200‑d GloVe → two 1D convs → max‑pool → dense) combined with a LIWC feature branch (two dense layers), concatenating features from the reaction and its parent",
        "agreement; answer; appreciation; disagreement; elaboration; humor; negative reaction; question; other",
        "39‑dimensional MFCCs with delta and delta‑delta coefficients (per-frame).",
        "1061",
        "Switchboard conversational English corpus",
        "unanswerable",
        "It aligns with the distributional hypothesis, grounding meaning holistically in sentential/contextual use rather than isolated words.",
        "Meaning is something words have only in the context of sentences, and sentences have within a language.",
        "no",
        "10n",
        "syntactic and semantic/topical latent states",
        "GraphParser without paraphrases and a monolingual MT-based paraphrase generator (Moses).",
        "yes",
        "unanswerable",
        "By applying LDA topic modeling (Mallet) to DDEO-related tweets, then filtering topics with LIWC’s health dictionary and labeling those containing DDEO terms.",
        "the Swissmetro dataset",
        "By jointly learning them in a supervised softmax model that predicts the target choice, training all embeddings simultaneously with regularization (including a reconstruction softmax regularizer) using train/dev/test splits.",
        "Using a multinomial logit choice model augmented with learned “travel behavior” embeddings for categorical variables.",
        "By projecting the embeddings/PCA back into the original dummy-variable space and interpreting the category coefficients there.",
        "unanswerable",
        "yes",
        "By training Word2Vec on escort ads and using t-SNE to find emojis (and terms) used in similar contexts, then adding those similar items to the existing flags.",
        "yes",
        "no",
        "“your comments fit well in Stormfront”",
        "- Incorporate external knowledge (e.g., modify NELL) and sentiment/polarity cues\n- Use deeper semantic representations (logical forms) to infer intent\n- Jointly learn interpretation and response strategy\n- Exploit conversation temporality by using earlier responses/interpretations as features",
        "1000 conversations",
        "unanswerable",
        "A strong negative correlation (R² ≈ 0.81–0.88).",
        "They yield splits with significantly higher compound divergence at similar atom divergence than prior length/pattern splits, while still correlating with those aspects, and test accuracy strongly tracks compound divergence across models.",
        "Because all questions are natural-language queries grounded in Freebase—each paired with an executable SPARQL query and answerable using real Freebase data—after filtering to keep them natural.",
        "LSTM seq2seq with attention; Transformer; Universal Transformer",
        "239,357 question–answer pairs",
        "Query-pattern splits for SQL datasets; the SCAN splits; CLEVR splits by factors like object counts or program depth; compositional VQA datasets with functional programs; greedy splits that keep question-type/answer pairs disjoint while maximizing concept coverage; and SQOOP’s object-pair generalization setup.",
        "a different audio task",
        "Honk and DeepSpeech-finetune.",
        "English and Hindi",
        "unanswerable",
        "slightly worse than state-of-the-art (competitive with only slight degradation)",
        "unanswerable",
        "WMT2014 DE–EN, WMT2016 RO–EN, and IWSLT2014 DE–EN.",
        "Visual attention was extremely sparse—peaked on a single source token (max ≈0.99, std ≈0.015)—showing little variation and limited utility.",
        "Most of the variance is captured by a few principal components—visual features lie in a low-dimensional subspace, with How2 especially dominated by “common” directions, indicating low discriminativeness.",
        "Both the Ranker and the Reasoner are MatchLSTM-based models (with 2‑layer bi-GRU encoders).",
        "By playing a cooperative game: the Ranker selects passage chains, the Reasoner predicts the linking entities between adjacent passages, and when the Reasoner’s predictions are consistent, it passes extra rewards to the Ranker to guide selection toward confident chains.",
        "unanswerable",
        "HotpotQA and DocRED.",
        "the choice of raters, the use of linguistic context, and the creation/quality of reference translations",
        "Chinese to English"
    ]
}