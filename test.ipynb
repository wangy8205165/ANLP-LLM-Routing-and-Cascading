{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_json(\"dataset/coqa.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "coqa    7849\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 10\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.sample(10)\n",
    "print(f\"Number of inputs: {len(inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "coqa    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_template import dataset_prompts_and_instructions\n",
    "# ===========================\n",
    "# Dataset name mapping\n",
    "# ===========================\n",
    "\n",
    "def normalize_dataset_name(name: str) -> str:\n",
    "    name_lower = name.lower()\n",
    "    mapping = {\n",
    "        \"cnli\": \"cnli\",\n",
    "        \"coqa\": \"coqa\",\n",
    "        \"narrativeqa\": \"narrative_qa\",\n",
    "        \"narrative_qa\": \"narrative_qa\",\n",
    "        \"qasper\": \"qasper\",\n",
    "        \"quality\": \"quality\",\n",
    "    }\n",
    "    # 去掉中间的非字母字符再匹配一次（以防万一）\n",
    "    key = ''.join(ch for ch in name_lower if ch.isalpha() or ch == '_')\n",
    "    return mapping.get(key, key)  # 如果正好已经是 key，就直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(row) -> str:\n",
    "    ds_key = normalize_dataset_name(row[\"dataset\"])\n",
    "    cfg = dataset_prompts_and_instructions[ds_key]\n",
    "\n",
    "    full_prompt = cfg[\"prompt\"].format(\n",
    "              context=row[\"base_ctx\"],\n",
    "        instruction=cfg[\"instruction\"],\n",
    "        question=row[\"question\"],\n",
    "    )\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "MAX_NEW_TOKENS = 300\n",
    "api_key = \"your openai api token\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def generate_with_gpt(prompt: str, model_name: str = \"gpt-5\",temperature: float = 0.0,max_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    response = client.responses.create(\n",
    "        model=model_name,\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        input=prompt,\n",
    "    )\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solver_job(df, engine_func, max_workers: int = 4):\n",
    "    prompts = [build_prompt(row) for _, row in df.iterrows()]\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for res in tqdm(executor.map(engine_func, prompts), total=len(prompts)):\n",
    "            results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ChatGPT (large model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = {}\n",
    "print(\"Running ChatGPT (large model)...\")\n",
    "outputs[\"gpt_pred\"] = run_solver_job(inputs,partial(generate_with_gpt, model_name=\"gpt-5\"),max_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "Dataset: coqa\n",
      "GPT predicts: The answer is: Yes—coal and cotton.\n",
      "standard answer: Yes.\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2\n",
      "Dataset: coqa\n",
      "GPT predicts: the rest of the day\n",
      "standard answer: the rest of the day\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 3\n",
      "Dataset: coqa\n",
      "GPT predicts: by catching her in her arms and covering her face with kisses\n",
      "standard answer: considerable earnestness.\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 4\n",
      "Dataset: coqa\n",
      "GPT predicts: Tom Rover\n",
      "standard answer: Tom\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 5\n",
      "Dataset: coqa\n",
      "GPT predicts: No.\n",
      "standard answer: no\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 6\n",
      "Dataset: coqa\n",
      "GPT predicts: The Meadows\n",
      "standard answer: \"The Meadows\"\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 7\n",
      "Dataset: coqa\n",
      "GPT predicts: climatic and biological\n",
      "standard answer: Physical and climatic\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 8\n",
      "Dataset: coqa\n",
      "GPT predicts: Rand Paul\n",
      "standard answer: his ally, Sen. Mike Lee\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 9\n",
      "Dataset: coqa\n",
      "GPT predicts: Rafael Cardenas Vela\n",
      "standard answer: His uncle\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 10\n",
      "Dataset: coqa\n",
      "GPT predicts: The answer is: No—it's the largest.\n",
      "standard answer: No\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, pred in enumerate(outputs[\"gpt_pred\"]):\n",
    "    gold = inputs[\"output\"].iloc[idx]\n",
    "    dataset = inputs[\"dataset\"].iloc[idx]\n",
    "    print(f\"Sample {idx+1}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"GPT predicts: {pred}\")\n",
    "    print(f\"standard answer: {gold}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6165                         Yes.\n",
      "5564          the rest of the day\n",
      "5158    considerable earnestness.\n",
      "5320                          Tom\n",
      "371                            no\n",
      "7837                \"The Meadows\"\n",
      "1260        Physical and climatic\n",
      "4433      his ally, Sen. Mike Lee\n",
      "7548                    His uncle\n",
      "7328                           No\n",
      "Name: output, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_output_coqa.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43moutputs\u001b[49m, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_path = \"baseline_output_coqa.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputs, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
