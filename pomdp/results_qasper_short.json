[
  {
    "id": "753990d0b621d390ed58f20c4d9e4f065f0dc672",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a vocabulary of positive and negative predicates that helps determine the polarity score of an event",
    "final_answer": "A seed lexicon consisting of 15 positive words and 15 negative words."
  },
  {
    "id": "753990d0b621d390ed58f20c4d9e4f065f0dc672",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "seed lexicon consists of positive and negative predicates",
    "final_answer": "A seed lexicon consisting of 15 positive words and 15 negative words."
  },
  {
    "id": "9d578ddccc27dd849244d632dd0f6bf27348ad81",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",
    "final_answer": "The proposed method performed well, even with a minimal amount of supervision, and achieved high accuracy, with the BiGRU encoder trained with the AL+CA+CO dataset obtaining the highest score."
  },
  {
    "id": "02e4bf719b1a504e385c35c6186742e720bcb281",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",
    "final_answer": "The answer is through the exploitation of discourse relations, specifically Cause and Concession."
  },
  {
    "id": "02e4bf719b1a504e385c35c6186742e720bcb281",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity",
    "final_answer": "The relations are used to propagate polarity through label propagation, where the polarity of an event is determined based on its discourse relation with another event."
  },
  {
    "id": "44c4bd6decc86f1091b5fc0728873d9324cdde4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",
    "final_answer": "The Japanese corpus used in the experiment contained about 100 million sentences."
  },
  {
    "id": "44c4bd6decc86f1091b5fc0728873d9324cdde4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The ACP corpus has around 700k events split into positive and negative polarity ",
    "final_answer": "The Japanese data consists of a corpus of about 100 million sentences."
  },
  {
    "id": "86abeff85f3db79cf87a8c993e5e5aa61226dc98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "negative, positive",
    "final_answer": "The labels available in the dataset for supervision are positive and negative labels, as shown in Table TABREF19."
  },
  {
    "id": "c029deb7f99756d2669abad0a349d917428e9c12",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "3%",
    "final_answer": "The results show a significant gain over a purely supervised baseline when labeled data were small."
  },
  {
    "id": "39f8db10d949c6b477fa4b51e7c184016505884f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity",
    "final_answer": "The model learns using mostly raw data by exploiting discourse relation-based label propagation from a small seed lexicon and a large raw corpus."
  },
  {
    "id": "d0bc782961567dc1dd7e074b621a6d6be44bb5b4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "30 words",
    "final_answer": "15 positive words and 15 negative words."
  },
  {
    "id": "a592498ba2fac994cd6fad7372836f0adb37e22a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "100 million sentences",
    "final_answer": "about 100 million sentences."
  },
  {
    "id": "3a9d391d25cde8af3334ac62d478b36b30079d74",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "3a9d391d25cde8af3334ac62d478b36b30079d74",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "8d8300d88283c73424c8f301ad9fdd733845eb47",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "confusion matrices of labels between annotators",
    "final_answer": "The annotation experiment is evaluated through agreement statistics, including heatmaps, Cohen's kappa scores, and the use of bootstrap resampling to compare the agreement of different annotator groups."
  },
  {
    "id": "48b12eb53e2d507343f19b8a667696a39b719807",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”",
    "final_answer": "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia."
  },
  {
    "id": "003f884d3893532f8c302431c9f70be6f64d9be8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, implying that their results are based on English data."
  },
  {
    "id": "003f884d3893532f8c302431c9f70be6f64d9be8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, resulting in 283 English-speaking communities for their analysis."
  },
  {
    "id": "bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n",
    "final_answer": "They manifest in systematically varying patterns across different types of communities."
  },
  {
    "id": "eea089baedc0ce80731c8fdcb064b82f584f483a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ",
    "final_answer": "They observe that user engagement varies systematically with a community's distinctiveness and dynamicity, with dynamic communities exhibiting higher rates of user retention and engagement, and distinctive communities fostering strong, focused interest from users at one particular point in time."
  },
  {
    "id": "edb2d24d6d10af13931b3a47a6543bd469752f0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.",
    "final_answer": "They were selected from a dataset of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate their measures, in at least 4 months of the subreddit's history."
  },
  {
    "id": "edb2d24d6d10af13931b3a47a6543bd469752f0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They collect subreddits from January 2013 to December 2014,2 for which there are at\nleast 500 words in the vocabulary used to estimate the measures,\nin at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.",
    "final_answer": "They selected the 300 Reddit communities for comparison from all subreddits on Reddit from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of activity."
  },
  {
    "id": "938cf30c4f1d14fa182e82919e16072fdbcf2a82",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "the average volatility of all utterances",
    "final_answer": "The authors measure a community's temporal dynamicity by computing the pointwise mutual information (PMI) of words in a community relative to its entire history, indicating how frequently a word occurs more in a small window of time than in the entire history, which they call \"volatility.\""
  },
  {
    "id": "93f4ad6568207c9bd10d712a52f8de25b3ebadd4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " the average specificity of all utterances",
    "final_answer": "The authors measure how distinctive a community is by calculating the average specificity of all utterances in the community, using a framework based on pointwise mutual information (PMI)."
  },
  {
    "id": "71a7153e12879defa186bfb6dbafe79c74265e10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Chinese general corpus",
    "final_answer": "Chinese general corpus."
  },
  {
    "id": "71a7153e12879defa186bfb6dbafe79c74265e10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "Chinese general corpus."
  },
  {
    "id": "85d1831c28d3c19c84472589a252e28e9884500f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BERT-Base, QANet",
    "final_answer": "BERT-Base and QANet."
  },
  {
    "id": "85d1831c28d3c19c84472589a252e28e9884500f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "QANet BIBREF39, BERT-Base BIBREF26",
    "final_answer": "BERT-Base and QANet."
  },
  {
    "id": "1959e0ebc21fafdf1dd20c6ea054161ba7446f61",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ",
    "final_answer": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is a result of the query term according to the paragraph text."
  },
  {
    "id": "1959e0ebc21fafdf1dd20c6ea054161ba7446f61",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.",
    "final_answer": "Clinical text structuring is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text."
  },
  {
    "id": "77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " three types of questions, namely tumor size, proximal resection margin and distal resection margin",
    "final_answer": "Clinical text structuring tasks."
  },
  {
    "id": "77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The specific tasks being unified are different clinical text structuring tasks."
  },
  {
    "id": "06095a4dee77e9a570837b35fc38e77228664f91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ",
    "final_answer": "There are unrelated sentences in between questions."
  },
  {
    "id": "19c9cfbc4f29104200393e848b7b9be41913a7ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "2,714 ",
    "final_answer": "2,714 question-answer pairs."
  },
  {
    "id": "6743c1dd7764fc652cfe2ea29097ea09b5544bc3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The tasks evaluated are clinical text structuring (CTS) and question answering based clinical text structuring (QA-CTS)."
  },
  {
    "id": "14323046220b2aea8f15fba86819cbccc389ed8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable\n\nHowever, the article mentions that labeling necessary amount of data for training neural network requires expensive labor cost, implying that there may be concerns about data privacy and security, but it does not explicitly address the issue."
  },
  {
    "id": "08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "integrate clinical named entity information into pre-trained language model",
    "final_answer": "They introduce clinical named entity information into the model."
  },
  {
    "id": "975a4ac9773a4af551142c324b64a0858670d06e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs",
    "final_answer": "The QA-CTS task dataset contains 17,833 sentences, 826,987 characters, and 2,714 question-answer pairs."
  },
  {
    "id": "326e08a0f5753b90622902bd4a9c94849a24b773",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs",
    "final_answer": "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs."
  },
  {
    "id": "bd78483a746fda4805a7678286f82d9621bc45cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26",
    "final_answer": "QANet and BERT-Base."
  },
  {
    "id": "dd155f01f6f4a14f9d25afc97504aefdc6d29c13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",
    "final_answer": "Word-level perplexity, R@3 in next-word prediction, latency, and energy usage."
  },
  {
    "id": "a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Kneser–Ney smoothing",
    "final_answer": "Kneser-Ney smoothing and Skip-LSTM are mentioned as classic language models."
  },
  {
    "id": "e07df8f613dbd567a35318cd6f6f4cb959f5c82d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "perplexity",
    "final_answer": "Perplexity and log perplexity are commonly used evaluation metrics for language models, with log perplexity being more human-understandable in terms of recall."
  },
  {
    "id": "e07df8f613dbd567a35318cd6f6f4cb959f5c82d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "perplexity",
    "final_answer": "Perplexity is a commonly used evaluation metric for language models, but the article also mentions that log perplexity provides a more human-understandable indicator of R@3."
  },
  {
    "id": "1a43df221a567869964ad3b275de30af2ac35598",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the Yelp Challenge dataset",
    "final_answer": "The Yelp Challenge dataset."
  },
  {
    "id": "1a43df221a567869964ad3b275de30af2ac35598",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yelp Challenge dataset BIBREF2",
    "final_answer": "The Yelp Challenge dataset."
  },
  {
    "id": "98b11f70239ef0e22511a3ecf6e413ecb726f954",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention using a pretrained NMT model, but it mentions using the Yelp Challenge dataset to train the NMT model."
  },
  {
    "id": "98b11f70239ef0e22511a3ecf6e413ecb726f954",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention the use of a pretrained NMT model for review generation."
  },
  {
    "id": "d4d771bcb59bab4f3eb9026cda7d182eb582027d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The context is a clear-text sequence of: the review rating, restaurant name, city, state, and food tags."
  },
  {
    "id": "12f1919a3e8ca460b931c6cacc268a926399dff4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AdaBoost-based classifier",
    "final_answer": "AdaBoost with 200 shallow decision trees."
  },
  {
    "id": "cd1034c183edf630018f47ff70b48d74d2bb1649",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "bd9930a613dd36646e2fc016b6eb21ab34c77621",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "1,006 fake reviews and 994 real reviews",
    "final_answer": "994 (human reviews) + 1006 (NMT-Fake reviews) = 2000"
  },
  {
    "id": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
    "final_answer": "unanswerable \n\nHowever, the article does mention that they reproduce the results from See et al. See2017 to then apply LRP on it."
  },
  {
    "id": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
    "final_answer": "unanswerable \n\nHowever the article does mention that they reproduce the results from See et al. See2017 and that they used a baseline model from See et al. See2017, but it does not mention any other baselines they compared."
  },
  {
    "id": "aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "one",
    "final_answer": "The model uses a single attention mechanism in the decoder."
  },
  {
    "id": "710c1f8d4c137c8dad9972f5ceacdbf8004db208",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "It depends on the case, as the article shows that saliency maps are truthful to the network's computation in some cases but not in others."
  },
  {
    "id": "8958465d1eaf81c8b781ba4d764a4f5329f026aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "RIPA, Neighborhood Metric, WEAT",
    "final_answer": "The three measures of bias which are reduced in experiments are geometric bias, neighborhood bias, and RIPA metric."
  },
  {
    "id": "31b6544346e9a31d656e197ad01756813ee89422",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The nearest neighbor bias mitigation observations."
  },
  {
    "id": "347e86893e8002024c2d10f618ca98e14689675f",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "only high-quality data helps",
    "final_answer": "high-quality data"
  },
  {
    "id": "347e86893e8002024c2d10f618ca98e14689675f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "high-quality",
    "final_answer": "High quality data turns out to be more important, especially in a low-resourced setting."
  },
  {
    "id": "10091275f777e0c2890c3ac0fd0a7d8e266b57cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The model is improved more by quality than by quantity, as shown by the experiment results where high-quality data helps to significantly improve the quality of the embeddings, while massive but low-quality data does not."
  },
  {
    "id": "cbf1137912a47262314c94d36ced3232d5fa1926",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "fastText, CWE-LP",
    "final_answer": "Character Word Embedding (CWE) and BERT."
  },
  {
    "id": "519db0922376ce1e87fcdedaa626d665d9f3e8ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The paper specifically targets the Portuguese language, which is assumed to be European Portuguese given the context of Brazil already having widely diffused the use of machine learning techniques."
  },
  {
    "id": "519db0922376ce1e87fcdedaa626d665d9f3e8ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The paper targets the Portuguese language in general, but since it mentions a \"large corpus representative of the Portuguese language\" and a public word2vec model pre-trained for the Portuguese language, it likely targets European Portuguese, but it does not exclude the possibility of Brazilian Portuguese."
  },
  {
    "id": "99a10823623f78dbff9ccecb210f187105a196e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "large Portuguese corpus",
    "final_answer": "A large Portuguese corpus from several sources, including a multi-genre corpus representative of the Portuguese language."
  },
  {
    "id": "09f0dce416a1e40cc6a24a8b42a802747d2c9363",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Continuous Bag-of-Words (CBOW)",
    "final_answer": "Portuguese word embeddings."
  },
  {
    "id": "ac706631f2b3fa39bf173cd62480072601e44f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes."
  },
  {
    "id": "ac706631f2b3fa39bf173cd62480072601e44f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nThe text mentions that the dataset is the result of processing the CzCDC 1.0 dataset, but it does not mention any experiments being performed on the final dataset."
  },
  {
    "id": "8b71ede8170162883f785040e8628a97fc6b5bcb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification.",
    "final_answer": "The quality of the citation is measured through the F1 measure in the initial recognition of the text spans and their classification."
  },
  {
    "id": "fa2a384a23f5d0fe114ef6a39dced139bddac20e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "903019 references",
    "final_answer": "The dataset contains 237,723 court decisions issued between 1st January 1993 and 30th September 2018."
  },
  {
    "id": "53712f0ce764633dbb034e550bb6604f15c0cacd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nAlthough the article mentions that they use Twitter data, which is typically in English, it does not explicitly state that they only use English datasets."
  },
  {
    "id": "0bffc3d82d02910d4816c16b390125e5df55fd01",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "bdd8368debcb1bdad14c454aaf96695ac5186b09",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",
    "final_answer": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to Dryhootch manual, with four intensity categories: No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD."
  },
  {
    "id": "bdd8368debcb1bdad14c454aaf96695ac5186b09",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "defined into four categories from high risk, moderate risk, to low risk",
    "final_answer": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants, with four categories: Non-existent, light, moderate, and high PTSD."
  },
  {
    "id": "3334f50fe1796ce0df9dd58540e9c08be5856c23",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " For each user, we calculate the proportion of tweets scored positively by each LIWC category.",
    "final_answer": "The LIWC algorithm is used to calculate s-scores, which are probabilities that a word or string of characters was generated by the same underlying process as the training data, and the proportions of tweets scored positively by each LIWC category are used as a feature vector in a loglinear regression model."
  },
  {
    "id": "3334f50fe1796ce0df9dd58540e9c08be5856c23",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "to calculate the possible scores of each survey question using PTSD Linguistic Dictionary ",
    "final_answer": "LIWC is incorporated into this system through the use of LIWC language models ($clm^{+}$, $ulm^{+}$, $clm^{-}$, and $ulm^{-}$) to calculate $s$-scores for each test tweet, and later through the exact similar method of LIWC to extract $\\alpha$-scores for each dimension and categories using the generated PTSD Linguistic Dictionary."
  },
  {
    "id": "7081b6909cb87b58a7b85017a2278275be58bf60",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "210",
    "final_answer": "210 users are surveyed using the clinically validated survey."
  },
  {
    "id": "1870f871a5bcea418c44f81f352897a2f53d0971",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "DOSPERT, BSSS and VIAS",
    "final_answer": "The Domain-Specific Risk-Taking (DOSPERT) Scale, The Berlin Social Support Scales (BSSS), and the Values In Action Scale (VIAS)."
  },
  {
    "id": "ce6201435cc1196ad72b742db92abd709e0f9e8d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "928828544e38fe26c53d81d1b9c70a9fb1cc3feb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "29,500 documents",
    "final_answer": "unanswerable"
  },
  {
    "id": "928828544e38fe26c53d81d1b9c70a9fb1cc3feb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "29,500 documents in the CORD-19 corpus (2020-03-13)",
    "final_answer": "unanswerable"
  },
  {
    "id": "4f243056e63a74d1349488983dc1238228ca76a7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "They list 75 fine-grained named entity types."
  },
  {
    "id": "d94ac550dfdb9e4bbe04392156065c072b9d75e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "d94ac550dfdb9e4bbe04392156065c072b9d75e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "eeb6e0caa4cf5fdd887e1930e22c816b99306473",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The contexts are manually labelled with WordNet senses of the target words",
    "final_answer": "The different senses are annotated/labeled manually in some cases, and automatically induced from clusters in other cases."
  },
  {
    "id": "3c0eaa2e24c1442d988814318de5f25729696ef5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThe extrinsic evaluation was carried out through the evaluation of the relatedness scores on benchmark datasets and the performance of the sense vectors on the Task 13 of SemEval-2013 — Word Sense Induction."
  },
  {
    "id": "dc1fe3359faa2d7daa891c1df33df85558bc461b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes, the model uses both spectrogram images and raw waveforms as features."
  },
  {
    "id": "922f1b740f8b13fdc8371e2a275269a44c86195e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nThe article does not mention comparing the performance against a baseline model."
  },
  {
    "id": "922f1b740f8b13fdc8371e2a275269a44c86195e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "The performance of the proposed models is compared against various existing models in the field of spoken language identification."
  },
  {
    "id": "b39f2249a1489a2cef74155496511cc5d1b2a73d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Answer with content missing: (Table 1)\nPrevious state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",
    "final_answer": "up to 96.0% on four languages (English, German, French, and Spanish) and 95.4% on six languages."
  },
  {
    "id": "591231d75ff492160958f8aa1e6bfcbbcd85a776",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CNN-mean, CNN-avgmax",
    "final_answer": "Previous state-of-the-art vision-based methods, including CNN-mean and CNN-avgmax, as well as the show-tell model."
  },
  {
    "id": "9e805020132d950b54531b1a2620f61552f06114",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CNN-mean, CNN-avgmax",
    "final_answer": "The Adam algorithm is applied to optimize the model, and the caption model is trained with two baseline vision-based methods proposed in BIBREF6, BIBREF7, which are CNN-mean and CNN-avgmax."
  },
  {
    "id": "95abda842c4df95b4c5e84ac7d04942f1250b571",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "German-English, French-English, and Japanese-English",
    "final_answer": "English, German, French, and Japanese."
  },
  {
    "id": "95abda842c4df95b4c5e84ac7d04942f1250b571",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "multiple language pairs including German-English, French-English, and Japanese-English.",
    "final_answer": "The languages used in the multi-lingual caption model are German, English, French, and Japanese."
  },
  {
    "id": "2419b38624201d678c530eba877c0c016cccd49f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "b99d100d17e2a121c3c8ff789971ce66d1d40a4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)",
    "final_answer": "They do not explicitly compare to previous research due to the existing works using smaller data, outdated methods, or lacking standard benchmarks."
  },
  {
    "id": "578d0b23cb983b445b1a256a34f969b34d332075",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34",
    "final_answer": "Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, IDAT@FIRE2019 shared-task, MADAR shared task 1 corpus, and task 1 corpus."
  },
  {
    "id": "578d0b23cb983b445b1a256a34f969b34d332075",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects",
    "final_answer": "The datasets used in training include Arab-Tweet, UBC Twitter Gender Dataset, the MADAR shared task corpus, the LAMA-DINA dataset, and the IDAT@FIRE2019 shared-task dataset."
  },
  {
    "id": "6548db45fc28e8a8b51f114635bad14a13eaec5b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . ",
    "final_answer": "They use the Wasserstein GAN (WGAN) and the energy-based GAN, and also propose two models, weGAN and deGAN, which are extensions of the GAN model."
  },
  {
    "id": "6548db45fc28e8a8b51f114635bad14a13eaec5b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "weGAN, deGAN",
    "final_answer": "They use the Wasserstein GAN (Arjovsky et al., 2017) and also their own models weGAN and deGAN."
  },
  {
    "id": "4c4f76837d1329835df88b0921f4fe8bda26606f",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "No",
    "final_answer": "no"
  },
  {
    "id": "819d2e97f54afcc7cdb3d894a072bcadfba9b747",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CNN, TIME, 20 Newsgroups, and Reuters-21578",
    "final_answer": "The CNN data set and the 20 Newsgroups data set."
  },
  {
    "id": "637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The answer is yes."
  },
  {
    "id": "4b8257cdd9a60087fa901da1f4250e7d910896df",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "typos in spellings or ungrammatical words",
    "final_answer": "The authors do not explicitly define or exemplify 'incorrect words', but they mention that the Twitter Sentiment Classification dataset contains \"many mistakes, as specified in Table TABREF11\", and that the TTS-STT combinations used to generate incomplete sentences with STT error result in varying rates of missing and incorrect words."
  },
  {
    "id": "7e161d9facd100544fa339b06f656eb2fc64ed28",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The article does not specify the exact number of vanilla transformers used after applying an embedding layer."
  },
  {
    "id": "abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes."
  },
  {
    "id": "abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No, the authors do not test their approach on a dataset without incomplete data; they test it on datasets with incomplete data, specifically with missing or incorrectly transcribed words."
  },
  {
    "id": "4debd7926941f1a02266b1a7be2df8ba6e79311a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No.\n\nThe authors state that \"in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language.\" This implies that their approach should be applied in general, not just when dealing with incomplete data."
  },
  {
    "id": "4debd7926941f1a02266b1a7be2df8ba6e79311a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No.\n\nThe authors suggest that their approach should be applied generally to improve the robustness and efficiency of BERT when applied to incomplete data, not just when dealing with incomplete data."
  },
  {
    "id": "3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average",
    "final_answer": "They outperform other models by 6% to 8% in the sentiment classification task and by 0.94% to 1.89% in the intent classification task with STT error."
  },
  {
    "id": "44c7c1fbac80eaea736622913d65fe6453d72828",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "34,432 user conversations",
    "final_answer": "34,432 user conversations."
  },
  {
    "id": "44c7c1fbac80eaea736622913d65fe6453d72828",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "34,432 ",
    "final_answer": "unanswerable"
  },
  {
    "id": "3e0c9469821cb01a75e1818f2acb668d071fcf40",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "overall rating, mean number of turns",
    "final_answer": "The metrics to measure user engagement are overall rating, mean number of turns, and mean word count."
  },
  {
    "id": "3e0c9469821cb01a75e1818f2acb668d071fcf40",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "overall rating, mean number of turns",
    "final_answer": "The metrics to measure user engagement are overall rating, mean number of turns, user's per-utterance word count, and the number of backstory questions asked."
  },
  {
    "id": "a725246bac4625e6fe99ea236a96ccb21b5f30c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)",
    "final_answer": "The system designs introduced are multi-step language understanding modules, a novel dialog act scheme, and an extensive persona database."
  },
  {
    "id": "516626825e51ca1e8a3e0ac896c538c9d8a747c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable."
  },
  {
    "id": "77af93200138f46bb178c02f710944a01ed86481",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "71538776757a32eee930d297f6667cd0ec2e9231",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions",
    "final_answer": "They found a strong positive correlation between the number of backstory questions asked and user satisfaction."
  },
  {
    "id": "7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "no \n\nThey have several baselines, including ROUGE, WordNCE, WPNCE, WordPPL, WPPPL, and word-overlap metrics like ROUGE-L-mult and ROUGE-L-single."
  },
  {
    "id": "7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",
    "final_answer": "No."
  },
  {
    "id": "3ac30bd7476d759ea5d9a5abf696d4dfc480175b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "LSTM LMs",
    "final_answer": "They use neural language models (LMs), specifically recurrent neural networks (RNNs) and LSTM LMs."
  },
  {
    "id": "0e57a0983b4731eba9470ba964d131045c8c7ea7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "human judges are asked to rate the fluency of system-generated compressions on an ordinal scale from 1 (disfluent) through 3 (fluent)."
  },
  {
    "id": "f0317e48dafe117829e88e54ed2edab24b86edb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
    "final_answer": "The model still takes into account and overrides the information brought by the text-based annotations when the attention loses track of the objects in the picture."
  },
  {
    "id": "f0317e48dafe117829e88e54ed2edab24b86edb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
    "final_answer": "The model still takes into account and overrides the information brought by the text-based annotations if the attention loses track of the objects in the picture and \"gets lost\"."
  },
  {
    "id": "ec91b87c3f45df050e4e16018d2bf5b62e4ca298",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The baseline used is the model of Calixto et al. (2017) and also the multimodal baseline of Caglayan et al. (2016)."
  },
  {
    "id": "f129c97a81d81d32633c94111018880a7ffe16d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Soft attention, Hard Stochastic attention, Local Attention",
    "final_answer": "Soft attention, hard stochastic attention, and local attention (which is a generalization of soft attention)."
  },
  {
    "id": "100cf8b72d46da39fedfe77ec939fb44f25de77f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1",
    "final_answer": "unanswerable"
  },
  {
    "id": "100cf8b72d46da39fedfe77ec939fb44f25de77f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Chinese dataset BIBREF0",
    "final_answer": "unanswerable"
  },
  {
    "id": "8cc56fc44136498471754186cfa04056017b4e54",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029",
    "final_answer": "unanswerable"
  },
  {
    "id": "8cc56fc44136498471754186cfa04056017b4e54",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.",
    "final_answer": "unanswerable \n\nHowever, according to the text, our proposed model outperforms the lexicon-based model TF-IDF, which is a popular unsupervised baseline, in both retrieval and generative evaluations."
  },
  {
    "id": "5fa431b14732b3c47ab6eec373f51f2bca04f614",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "TF-IDF, NVDM",
    "final_answer": "TF-IDF and NVDM."
  },
  {
    "id": "33ccbc401b224a48fba4b167e86019ffad1787fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "from 50K to 4.8M",
    "final_answer": "4.5 million human comments in the candidate set."
  },
  {
    "id": "cca74448ab0c518edd5fc53454affd67ac1a201c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "198,112",
    "final_answer": "198,112 news articles."
  },
  {
    "id": "b69ffec1c607bfe5aa4d39254e0770a3433a191b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Chinese dataset BIBREF0",
    "final_answer": "The Tencent News dataset with millions of real comments was used."
  },
  {
    "id": "f5cf8738e8d211095bb89350ed05ee7f9997eb19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "up to four percentage points in accuracy",
    "final_answer": "up to four percentage points in accuracy."
  },
  {
    "id": "bed527bcb0dd5424e69563fba4ae7e6ea1fca26a",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "2019 GermEval shared task on hierarchical text classification",
    "final_answer": "the GermEval 2019 shared task dataset (Blurb Genre Collection, BGC)"
  },
  {
    "id": "bed527bcb0dd5424e69563fba4ae7e6ea1fca26a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "GermEval 2019 shared task",
    "final_answer": "The dataset used is the 2019 GermEval shared task on hierarchical text classification."
  },
  {
    "id": "aeab5797b541850e692f11e79167928db80de1ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "all three representations are concatenated and passed into a MLP",
    "final_answer": "They combine text representations with the knowledge graph embeddings by concatenating them and passing them into a multilayer perceptron (MLP) with two layers."
  },
  {
    "id": "cda4612b4bda3538d19f4b43dde7bc30c1eda4e5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm",
    "final_answer": "unanswerable."
  },
  {
    "id": "cda4612b4bda3538d19f4b43dde7bc30c1eda4e5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "TextRank, Word2vec BIBREF19, GloVe BIBREF20",
    "final_answer": "The traditional methods to identifying important attributes are simple co-occurrences and ranking process used to identify relatively more precise attributes from all attribute candidates."
  },
  {
    "id": "e12674f0466f8c0da109b6076d9939b30952c7da",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "FastText",
    "final_answer": "FastText."
  },
  {
    "id": "9fe6339c7027a1a0caffa613adabe8b5bb6a7d4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable (the article does not specify the exact type of user generated text data used)"
  },
  {
    "id": "b5c3787ab3784214fc35f230ac4926fe184d86ba",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "No, they proposed three characteristic metrics: diversity, density, and homogeneity."
  },
  {
    "id": "9174aded45bc36915f2e2adb6f352f3c7d9ada8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SST-2 (Stanford Sentiment Treebank, version 2), Snips",
    "final_answer": "SST-2 (Stanford Sentiment Treebank, version 2) and Snips dataset."
  },
  {
    "id": "9174aded45bc36915f2e2adb6f352f3c7d9ada8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SST-2, Snips",
    "final_answer": "The SST-2 (Stanford Sentiment Treebank, version 2) dataset and the Snips dataset were used."
  },
  {
    "id": "a8f1029f6766bffee38a627477f61457b2d6ed5c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "a2103e7fe613549a9db5e65008f33cf2ee0403bd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "wealth , democracy , population, levels of ODA, conflict ",
    "final_answer": "Wealth, population size, democracy, and official development assistance (ODA) are the country-specific drivers of international development rhetoric, with varying effects on the discussion of sustainable development (Topic 7) and economic development (Topic 2)."
  },
  {
    "id": "13b36644357870008d70e5601f394ec3c6c07048",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not provide information about the language(s) of the dataset."
  },
  {
    "id": "13b36644357870008d70e5601f394ec3c6c07048",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not provide information on the language(s) used in the dataset."
  },
  {
    "id": "e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",
    "final_answer": "The main international development topics that states raise are identified through the application of structural topic models (STMs) to the UN General Debate Corpus (UNGDC) dataset."
  },
  {
    "id": "fd0ef5a7b6f62d07776bf672579a99c67e61a568",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs",
    "final_answer": "The authors evaluate their system's performance on datasets across various domains and conduct experiments with active learning to validate its effectiveness."
  },
  {
    "id": "071bcb4b054215054f17db64bfd21f17fd9e1a80",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The QnAMaker WebApp acts as a layer between the Bot, Management APIs, and Azure Search Index, handling ranking on top of retrieved results and feedback management for active learning."
  },
  {
    "id": "f399d5a8dbeec777a858f81dc4dd33a83ba341a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot",
    "final_answer": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot."
  },
  {
    "id": "f399d5a8dbeec777a858f81dc4dd33a83ba341a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot",
    "final_answer": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot."
  },
  {
    "id": "d28260b5565d9246831e8dbe594d4f6211b60237",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "We empirically provide a formula to measure the richness in the scenario of machine translation.",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that they test the robustness of the Plackett-Luce model through evaluation on the test data, comparing its performance to MERT and MIRA in two tasks: iterative training and N-best list reranking."
  },
  {
    "id": "d28260b5565d9246831e8dbe594d4f6211b60237",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$",
    "final_answer": "unanswerable"
  },
  {
    "id": "8670989ca39214eda6c1d1d272457a3f3a92818b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article suggests that the new method is able to alleviate over-fitting in some degree when k is not smaller than 5, which may indicate that it is more robust than MIRAs in certain cases."
  },
  {
    "id": "923b12c0a50b0ee22237929559fad0903a098b7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Plackett-Luce Model for SMT Reranking",
    "final_answer": "The experiments performed with large-scale features include full training of machine translation models and N-best list reranking."
  },
  {
    "id": "67131c15aceeb51ae1d3b2b8241c8750a19cca8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Oracle ",
    "final_answer": "unanswerable"
  },
  {
    "id": "579a0603ec56fc2b4aa8566810041dbb0cd7b5e7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "perform experiments to utilize ASR $n$-best hypotheses during evaluation",
    "final_answer": "unanswerable"
  },
  {
    "id": "c9c85eee41556c6993f40e428fa607af4abe80a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "$\\sim $ 8.7M annotated anonymised user utterances",
    "final_answer": "The work is evaluated over an anonymised user utterances dataset of approximately 8.7M annotated utterances across 23 domains."
  },
  {
    "id": "c9c85eee41556c6993f40e428fa607af4abe80a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "on $\\sim $ 8.7M annotated anonymised user utterances",
    "final_answer": "An anonymized user utterances corpus derived from requests across 23 domains, with approximately 8.7 million annotated utterances and a test set of around 300,000 utterances."
  },
  {
    "id": "982979cb3c71770d8d7d2d1be8f92b66223dec85",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine",
    "final_answer": "topological properties."
  },
  {
    "id": "5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Class Membership Tests, Class Distinction Test, Word Equivalence Test",
    "final_answer": "Class Membership Tests, Class Distinction Test, and Word Equivalence Test."
  },
  {
    "id": "5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)",
    "final_answer": "The intrinsic evaluation metrics used are Class Membership Tests, Class Distinction Test, and Word Equivalence Test, which involve computing the ratio of pairs correctly classified as distinct, belonging to the same class, or equivalent based on cosine similarity."
  },
  {
    "id": "7ce7edd06925a943e32b59f3e7b5159ccb7acaf6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "consistent increase in the validation loss after about 15 epochs",
    "final_answer": "The validation loss consistently increases after about 15 epochs when using less than 50% of the data available, indicating overfitting."
  },
  {
    "id": "a883bb41449794e0a63b716d9766faea034eb359",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "context is a procedural text, the question and the multiple choice answers are composed of images",
    "final_answer": "The multimodality available in the dataset is text and images."
  },
  {
    "id": "a883bb41449794e0a63b716d9766faea034eb359",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "images and text",
    "final_answer": "The answer is multimodal comprehension of cooking recipes."
  },
  {
    "id": "5d83b073635f5fd8cd1bdb1895d3f13406583fbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory",
    "final_answer": "Hasty Student and Impatient Reader are previously reported models."
  },
  {
    "id": "171ebfdc9b3a98e4cdee8f8715003285caeb2f39",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59",
    "final_answer": "The new model achieves state-of-the-art results compared to other neural models."
  },
  {
    "id": "3c3cb51093b5fd163e87a773a857496a4ae71f03",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word",
    "final_answer": "The scoring model works by computing a score for a segmented sentence as the average of word scores and link scores, where word scores are calculated based on the possibility of a segment being a legal word and link scores are calculated based on the rationality of the link between words."
  },
  {
    "id": "3c3cb51093b5fd163e87a773a857496a4ae71f03",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history",
    "final_answer": "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then using a gated neural network and LSTM to evaluate the likelihood of each candidate word being a legal word and its link to previous segmentation history."
  },
  {
    "id": "53a0763eff99a8148585ac642705637874be69d4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",
    "final_answer": "The active learning model works by employing a CRF-based segmenter as the learning engine and a scoring model based on information entropy and neural network as the selection engine, which iteratively selects and labels samples from unlabeled data to improve the accuracy of the classifier."
  },
  {
    "id": "0bfed6f9cfe93617c5195c848583e3945f2002ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "gated neural network ",
    "final_answer": "A gated neural network, a long short-term memory (LSTM) model, and a gated recurrent neural network (GRNN) are employed."
  },
  {
    "id": "352c081c93800df9654315e13a880d6387b91919",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "text understanding, expectation, coreference resolution, common-sense knowledge inference, and the embedding of sentences into context."
  },
  {
    "id": "18fbf9c08075e3b696237d22473c463237d153f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.",
    "final_answer": "The inter-annotator agreement was sufficiently high, and a qualified majority vote resulted in 90.5% agreement between annotators for coreference chain annotation."
  },
  {
    "id": "18fbf9c08075e3b696237d22473c463237d153f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Moderate agreement of 0.64-0.68 Fleiss’ Kappa over event type labels, 0.77 Fleiss’ Kappa over participant labels, and good agreement of 90.5% over coreference information.",
    "final_answer": "The inter-annotator agreement was found to be sufficiently high, and the percentage of pairs annotated by at least 3 annotators was 90.5%."
  },
  {
    "id": "a37ef83ab6bcc6faff3c70a481f26174ccd40489",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " four different annotators",
    "final_answer": "unanswerable. \n\nHowever, according to the article, \"All annotations were done by undergraduate students of computational linguistics\" and \"The stories from each scenario were distributed among four different annotators.\" This implies that there are four subjects (annotators), but it does not mention how many students were involved in the project."
  },
  {
    "id": "bc9c31b3ce8126d1d148b1025c66f270581fde10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",
    "final_answer": "WN18 and YAGO3-10."
  },
  {
    "id": "bc9c31b3ce8126d1d148b1025c66f270581fde10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WN18 and YAGO3-10",
    "final_answer": "The datasets used to evaluate this approach are WN18 and YAGO3-10."
  },
  {
    "id": "185841e979373808d99dccdade5272af02b98774",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. ",
    "final_answer": "By identifying the fact that, when removed, results in the least change in the model's prediction, indicating the error triple."
  },
  {
    "id": "d427e3d41c4c9391192e249493be23926fc5d2e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "330f2cdeab689670b68583fc4125f5c0b26615a8",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.",
    "final_answer": "The proposed model has several advantages, including the ability to jointly model words in documents as arising from a mixture of topics, to learn from multiple annotators and crowds by accounting for their biases and different levels of expertise, and to scale to large datasets using an efficient stochastic variational inference algorithm."
  },
  {
    "id": "c87b2dd5c439d5e68841a705dd81323ec0d64c97",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)",
    "final_answer": "State-of-the-art approaches for predicting target variables associated with complex high-dimensional data, such as documents or images, are supervised topic models."
  },
  {
    "id": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ",
    "final_answer": "The datasets used were 20-Newsgroups, Reuters-21578, and LabelMe."
  },
  {
    "id": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe",
    "final_answer": "The 20-Newsgroups dataset, Reuters-21578, LabelMe, and we8there dataset were used."
  },
  {
    "id": "2376c170c343e2305dac08ba5f5bda47c370357f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ",
    "final_answer": "The dataset was collected through a human-to-human dialogue setting, where two paired workers, a user and a system (wizard), conversed synchronously in a chatroom to accomplish a goal generated by a multi-domain goal generator based on a database of travel information in Beijing."
  },
  {
    "id": "2376c170c343e2305dac08ba5f5bda47c370357f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",
    "final_answer": "The dataset was collected through a human-to-human dialogue setting, where workers were paired to converse according to given goals, and their conversations were then annotated and recorded."
  },
  {
    "id": "0137ecebd84a03b224eb5ca51d189283abb5f6d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)",
    "final_answer": "The benchmark models provided are for different components of a pipelined task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG), implemented using ConvLab-2."
  },
  {
    "id": "5f6fbd57cce47f20a0fda27d954543c00c4344c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories",
    "final_answer": "The corpus was annotated automatically using some rules, and then manually checked by three experts for a subset of 50 dialogues."
  },
  {
    "id": "d6e2b276390bdc957dfa7e878de80cee1f41fbca",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Only Bert base and Bert large are compared to proposed approach.",
    "final_answer": "Attentive Mimicking."
  },
  {
    "id": "32537fdf0d4f76f641086944b413b2f756097e5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking",
    "final_answer": "An absolute improvement of up to 24% over a BERT baseline is reported."
  },
  {
    "id": "ef081d78be17ef2af792e7e919d15a235b8d7275",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23",
    "final_answer": "MNLI, AG's News, and DBPedia."
  },
  {
    "id": "ef081d78be17ef2af792e7e919d15a235b8d7275",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MNLI, AG's News, DBPedia",
    "final_answer": "MNLI, AG's News, and DBPedia."
  },
  {
    "id": "537b2d7799124d633892a1ef1a485b3b071b303d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WNLaMPro dataset",
    "final_answer": "WNLaMPro dataset."
  },
  {
    "id": "9aca4b89e18ce659c905eccc78eda76af9f0072a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The model achieves the state-of-the-art results compared to baselines."
  },
  {
    "id": "b0376a7f67f1568a7926eff8ff557a93f434a253",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",
    "final_answer": "unanswerable \n\nHowever, based on the article, it can be inferred that the method achieves the \"state-of-the-art results\" and \"best overall performance\" compared to the baseline methods, indicating a significant performance difference."
  },
  {
    "id": "dad8cc543a87534751f9f9e308787e1af06f0627",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI",
    "final_answer": "The datasets used for evaluation include AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI."
  },
  {
    "id": "dad8cc543a87534751f9f9e308787e1af06f0627",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI",
    "final_answer": "AIDA-B, AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI, and CWEB."
  },
  {
    "id": "0481a8edf795768d062c156875d20b8fb656432c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7",
    "final_answer": "lexical and statistical features such as the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc."
  },
  {
    "id": "b6a4ab009e6f213f011320155a7ce96e713c11cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The author's work ranked among the top submissions on the challenge, as their reinforcement learning approaches received good human evaluation results and were the best of their runs in two batches."
  },
  {
    "id": "cfffc94518d64cb3c8789395707e4336676e0345",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "classification, regression, neural methods",
    "final_answer": "Classification and regression approaches using deep learning models."
  },
  {
    "id": "cfffc94518d64cb3c8789395707e4336676e0345",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models",
    "final_answer": "Classification and regression approaches, and deep learning models, have been tried without reinforcement learning."
  },
  {
    "id": "f60629c01f99de3f68365833ee115b95a3388699",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)",
    "final_answer": "Labeling sentences as \"summary\" based on their ROUGE score above a threshold, or the top m input sentences with the highest ROUGE score."
  },
  {
    "id": "a7cb4f8e29fd2f3d1787df64cd981a6318b65896",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "642c4704a71fd01b922a0ef003f234dcc7b223cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources",
    "final_answer": "unanswerable \n\nHowever, the article mentions that some languages have a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian, which could be a source of recall errors."
  },
  {
    "id": "e477e494fe15a978ff9c0a5f1c88712cdaec0c5c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "They do not explicitly state looking for inconsistencies between different languages' annotations in UniMorph, but mention \"teasing out the annotation discrepancies within and across projects.\""
  },
  {
    "id": "04495845251b387335bf2e77e2c423130f43c7d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the UD datasets have been used in the CoNLL shared tasks and that each dataset is an annotated treebank, making it a resource of token-level annotations. But it does not mention looking for inconsistencies between different UD treebanks."
  },
  {
    "id": "564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",
    "final_answer": "The 31 languages with both UD and UniMorph data."
  },
  {
    "id": "564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish",
    "final_answer": "31 languages with both UD and UniMorph data."
  },
  {
    "id": "f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "The paper mentions that the personalization of the recognition algorithm (e.g., mean and standard deviation normalization) could help to adapt the classification for specific speakers and thus to reduce the bias towards negative emotional states, but does not explicitly evaluate this adjustment."
  },
  {
    "id": "9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "confusion matrices, $\\text{F}_1$ score",
    "final_answer": "Using confusion matrices and macro-averaged F1 scores."
  },
  {
    "id": "00bcdffff7e055f99aaf1b05cf41c98e2748e948",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For the emotion recognition from text they use described neural network as baseline.\nFor audio and face there is no baseline.",
    "final_answer": "A neural network with an embedding layer, a bidirectional LSTM, and two dense layers followed by a soft max output layer, inspired by previous work."
  },
  {
    "id": "f92ee3c5fce819db540bded3cfcc191e21799cb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)",
    "final_answer": "Off-the-shelf emotion recognition tools are used for facial expressions (manufacturer cannot be disclosed) and audio signals (specific manufacturer not mentioned)."
  },
  {
    "id": "f92ee3c5fce819db540bded3cfcc191e21799cb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "cannot be disclosed due to licensing restrictions",
    "final_answer": "Off-the-shelf tools for emotion recognition from facial expressions and audio signals are used."
  },
  {
    "id": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",
    "final_answer": "30000 merge operations for BPE subword segmentation."
  },
  {
    "id": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)",
    "final_answer": "30k and 90k subwords, with 30000 merge operations used for BPE subword segmentation."
  },
  {
    "id": "07d7652ad4a0ec92e6b44847a17c378b0d9f57f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "10.37 BLEU",
    "final_answer": "Our optimized NMT system outperforms the PBSMT system across all data settings, achieving a BLEU score of 9.4 in the ultra-low data condition and 32.8 in the full IWSLT 14 training corpus."
  },
  {
    "id": "9f3444c9fb2e144465d63abf58520cddd4165a01",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "gu-EtAl:2018:EMNLP1",
    "final_answer": "They compare with Phrase-Based Statistical Machine Translation (PBSMT) systems."
  },
  {
    "id": "2348d68e065443f701d8052018c18daa4ecc120e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "highly data-inefficient, underperform phrase-based statistical machine translation",
    "final_answer": "The paper mentions the following pitfalls: using hyperparameters typical for high-resource settings in low-resource settings, not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions, relying on large amounts of auxiliary data, and using vocabulary sizes that are not optimal for low-resource settings."
  },
  {
    "id": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "a939a53cabb4893b2fd82996f3dbe8688fdb7bbb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "8b99767620fd4efe51428b68841cc3ec06699280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The technique used for text analysis and mining is a combination of natural language processing (NLP), argumentation mining, and the use of a semantic frame extractor, specifically the Penelope semantic frame extractor."
  },
  {
    "id": "312417675b3dc431eb7e7b16a917b7fed98d4376",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Axelrod's causal mapping method",
    "final_answer": "Causal mapping methods employed include Axelrod's causal mapping method, cognitive mapping, and the Penelope semantic frame extractor."
  },
  {
    "id": "792d7b579cbf7bfad8fe125b0d66c2059a174cf9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Ternary Trans-CNN",
    "final_answer": "Ternary Trans-CNN model, which uses a Convolution 1D architecture with 3 layers and 2 dense fully connected layers, achieving an accuracy of 83.90%."
  },
  {
    "id": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "HEOT , A labelled dataset for a corresponding english tweets",
    "final_answer": "The dataset used is HEOT obtained from a past study done by Mathur et al., which is a labelled dataset for Hinglish tweets, and a labelled dataset for English tweets obtained from a study conducted by Davidson et al."
  },
  {
    "id": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "HEOT",
    "final_answer": "The dataset used is HEOT, a labelled dataset of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent, which was obtained from a past study by Mathur et al."
  },
  {
    "id": "5908d7fb6c48f975c5dfc5b19bb0765581df2b25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "3189 rows of text messages",
    "final_answer": "3189 rows of text messages with a range of 1-1295 words."
  },
  {
    "id": "5908d7fb6c48f975c5dfc5b19bb0765581df2b25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Resulting dataset was 7934 messages for train and 700 messages for test.",
    "final_answer": "The dataset is 3189 rows of text messages of average length 116 words."
  },
  {
    "id": "cca3301f20db16f82b5d65a102436bebc88a2026",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al",
    "final_answer": "The dataset, HEOT, is obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from Twitter for the conversations happening in the Indian subcontinent."
  },
  {
    "id": "cfd67b9eeb10e5ad028097d192475d21d0b6845b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable\n\nThe article mentions that the text augmentation techniques (Synonym Replacement, Random Insertion, Random Swap, and Random Deletion) were used together, but it does not mention whether they were experimented individually."
  },
  {
    "id": "e1c681280b5667671c7f78b1579d0069cba72b0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM",
    "final_answer": "Previous work uses Transfer Learning based approaches, Ternary Trans-CNN models, Hybrid multi-channel CNN and LSTM models, and other architectures such as Fully Connected dense networks and Convolution based architectures."
  },
  {
    "id": "58d50567df71fa6c3792a0964160af390556757d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nHowever, based on the information provided, it can be inferred that the dataset is obtained from Twitter."
  },
  {
    "id": "07c79edd4c29635dbc1c2c32b8df68193b7701c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "HEOT , A labelled dataset for a corresponding english tweets ",
    "final_answer": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al."
  },
  {
    "id": "66125cfdf11d3bf8e59728428e02021177142c3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.",
    "final_answer": "They demonstrate that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool."
  },
  {
    "id": "66125cfdf11d3bf8e59728428e02021177142c3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "explicit projection had a negligible effect on the performance",
    "final_answer": "They demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool."
  },
  {
    "id": "222b2469eede9a0448e0226c6c742e8c91522af3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "The article suggests that a sentence representation in mBERT is composed of a language-specific component and a language-neutral component, implying that they are not necessarily disjunctive."
  },
  {
    "id": "6f8386ad64dce3a20bc75165c5c7591df8f419cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.",
    "final_answer": "They assume that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way, and they try to remove the language-specific information from the representations by centering the representations of sentences in each language."
  },
  {
    "id": "81dc39ee6cdacf90d5f0f62134bf390a29146c65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks",
    "final_answer": "The representation similarity does not directly show language-neutrality, i.e., to what extent are similar phenomena represented similarly across languages."
  },
  {
    "id": "eeaceee98ef1f6c971dac7b0b8930ee8060d71c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.",
    "final_answer": "They propose two approaches: across models and tasks, and across input space, to develop a more nuanced methodology for defining and evaluating faithfulness."
  },
  {
    "id": "3371d586a3a81de1552d90459709c57c0b1a2594",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.",
    "final_answer": "The authors propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful."
  },
  {
    "id": "d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.",
    "final_answer": "The three assumptions are the Model Assumption, the Prediction Assumption, and the Linearity Assumption."
  },
  {
    "id": "d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.",
    "final_answer": "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption."
  },
  {
    "id": "2a859e80d8647923181cb2d8f9a2c67b1c3f4608",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust “inherent interpretability” claims., Faithfulness evaluation of IUI systems should not rely on user performance.",
    "final_answer": "Be explicit in what you evaluate, faithfulness evaluation should not involve human-judgement on the quality of interpretation, faithfulness evaluation should not involve human-provided gold labels, do not trust “inherent interpretability” claims, and faithfulness evaluation of IUI systems should not rely on user performance."
  },
  {
    "id": "aceac4ad16ffe1af0f01b465919b1d4422941a6b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we provide an extensive analysis of the state-of-the-art model",
    "final_answer": "yes"
  },
  {
    "id": "f7070b2e258beac9b09514be2bfcc5a528cc3a0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention the performance of their model, but rather focuses on interpreting the behavior of the model's intermediate layers."
  },
  {
    "id": "f7070b2e258beac9b09514be2bfcc5a528cc3a0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly discuss the performance of their model, but rather focuses on interpreting the behavior of deep learning models in natural language inference tasks."
  },
  {
    "id": "2efdcebebeb970021233553104553205ce5d6567",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "two LSTM layers",
    "final_answer": "Three main parts are mentioned: input encoding, attention, and inference."
  },
  {
    "id": "4fa851d91388f0803e33f6cfae519548598cd37c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "a891039441e008f1fd0a227dbed003f76c140737",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "machine comprehension",
    "final_answer": "The answer is Multiple Perspective Context Matching."
  },
  {
    "id": "73738e42d488b32c9db89ac8adefc75403fa2653",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " 69.10%/78.38%",
    "final_answer": "unanswerable \n\nThe article does not provide a clear measure of improvement for the adaptation model, but it does mention that when the number of hidden question types (K) is set to 100, the performance improves to 69.10%/78.38% on the development set."
  },
  {
    "id": "6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "word embedding, input encoder, alignment, aggregation, and prediction.",
    "final_answer": "The baseline model is composed of typical components including word embedding, input encoder, alignment, aggregation, and prediction."
  },
  {
    "id": "6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction.",
    "final_answer": "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components."
  },
  {
    "id": "fa218b297d9cdcae238cef71096752ce27ca8f4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Our model achieves a 68.73% EM score and 77.39% F1 score",
    "final_answer": "Our model achieves a 68.73% EM score and 77.39% F1 score on the SQuAD test set."
  },
  {
    "id": "ff28d34d1aaa57e7ad553dba09fc924dc21dd728",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "High correlation results range from 0.472 to 0.936",
    "final_answer": "The three fine-tuned BERT versions clearly outperform all other methods, achieving high correlations with human scores for most linguistic qualities rated, on three different datasets."
  },
  {
    "id": "ae8354e67978b7c333094c36bf9d561ca0c2d286",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks",
    "final_answer": "They use datasets from the NIST DUC-05, DUC-06, and DUC-07 shared tasks."
  },
  {
    "id": "02348ab62957cb82067c589769c14d798b1ceec7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction",
    "final_answer": "BiGRU with attention and language model (LM)."
  },
  {
    "id": "02348ab62957cb82067c589769c14d798b1ceec7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",
    "final_answer": "BiGRU with attention and ROUGE."
  },
  {
    "id": "3748787379b3a7d222c3a6254def3f5bfb93a60e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",
    "final_answer": "The five linguistic quality criteria addressed are given in Figure FIGREF2, and include: $\\mathcal {Q}1$ (Grammaticality), $\\mathcal {Q}2$ (Relevance), $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus), and $\\mathcal {Q}5$ (Structure & Coherence)."
  },
  {
    "id": "6852217163ea678f2009d4726cb6bd03cf6a8f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WN18RR, FB15k-237, YAGO3-10",
    "final_answer": "WN18RR, FB15k-237, and YAGO3-10."
  },
  {
    "id": "6852217163ea678f2009d4726cb6bd03cf6a8f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27",
    "final_answer": "WN18RR, FB15k-237, and YAGO3-10."
  },
  {
    "id": "cd1ad7e18d8eef8f67224ce47f3feec02718ea1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "TransE, DistMult, ComplEx, ConvE, RotatE",
    "final_answer": "TransE, DistMult, ComplEx, ConvE, and RotatE."
  },
  {
    "id": "9c9e90ceaba33242342a5ae7568e89fe660270d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively",
    "final_answer": "HAKE significantly outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task."
  },
  {
    "id": "2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively",
    "final_answer": "Entities are mapped onto the polar coordinate system by combining the modulus part and the phase part, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information."
  },
  {
    "id": "db9021ddd4593f6fadf172710468e2fdcea99674",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "phrase-based word embedding and Abstract Syntax Tree (AST) can be incorporated for improved vocabulary mapping and more accurate target code generation."
  },
  {
    "id": "db9021ddd4593f6fadf172710468e2fdcea99674",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "incorporating coding syntax tree model",
    "final_answer": "unanswerable"
  },
  {
    "id": "8ea4bd4c1d8a466da386d16e4844ea932c44a412",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "A parallel corpus where the source is an English expression of code and the target is Python code.",
    "final_answer": "A text-code parallel corpus with 18805 aligned data, where the source data is in English and the target data is in Python programming language."
  },
  {
    "id": "8ea4bd4c1d8a466da386d16e4844ea932c44a412",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " text-code parallel corpus",
    "final_answer": "They use a text-code parallel corpus with 18805 aligned data in it."
  },
  {
    "id": "92240eeab107a4f636705b88f00cefc4f0782846",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThere is no information in the article about comparisons to other models."
  },
  {
    "id": "4196d329061f5a9d147e1e77aeed6a6bd9b35d18",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "seq2seq translation",
    "final_answer": "The architecture of the system is a neural network based machine translation model using a Long Short-Term Memory (LSTM) model with an encoder layer, a decoder layer, and an output layer."
  },
  {
    "id": "a37e4a21ba98b0259c36deca0d298194fa611d2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article does mention that programmers express a for-loop in a very few specific ways and variable declaration and value assignment expressions are also limited in nature."
  },
  {
    "id": "321429282557e79061fe2fe02a9467f3d0118cdd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "phrase-based word embedding, Abstract Syntax Tree(AST)",
    "final_answer": "Phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficial to further improve accuracy."
  },
  {
    "id": "891cab2e41d6ba962778bda297592c916b432226",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Python",
    "final_answer": "Python."
  },
  {
    "id": "1eeabfde99594b8d9c6a007f50b97f7f527b0a17",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "validation data",
    "final_answer": "The validation data from the source corpus with 500 lines, which is 3% of the training data, is used to measure accuracy."
  },
  {
    "id": "e96adf8466e67bd19f345578d5a6dc68fd0279a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "unsupervised ",
    "final_answer": "The text-to-image synthesis was initially carried out through a search and supervised learning combined process."
  },
  {
    "id": "e96adf8466e67bd19f345578d5a6dc68fd0279a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis",
    "final_answer": "The text-to-image synthesis was mainly carried out through a search and supervised learning combined process in the early stages of research."
  },
  {
    "id": "c1477a6c86bd1670dd17407590948000c9a6b7c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images",
    "final_answer": "Although the article does not explicitly state the remaining challenges, it mentions that actual AI systems are far from achieving text-to-image synthesis, implying that there are still unresolved challenges in this area."
  },
  {
    "id": "e020677261d739c35c6f075cde6937d0098ace7f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception",
    "final_answer": "unanswerable"
  },
  {
    "id": "7fe48939ce341212c1d801095517dc552b98e7b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "gating mechanism acts upon each dimension of the word and character-level vectors",
    "final_answer": "unanswerable \n\nHowever, the article does mention \"a learned vector gate (also referred to as feature-wise sigmoidal gate)\" but it does not specify where it is employed."
  },
  {
    "id": "65ad17f614b7345f0077424c04c94971c831585b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiLSTM with max pooling",
    "final_answer": "BiLSTM with max pooling."
  },
  {
    "id": "323e100a6c92d3fe503f7a93b96d821408f92109",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BIBREF13 , BIBREF18",
    "final_answer": "They evaluate on 11 downstream transfer tasks, including sentiment classification, subjectivity classification, question-type classification, recognizing textual entailment, estimating semantic relatedness, and measuring textual semantic similarity."
  },
  {
    "id": "9f89bff89cea722debc991363f0826de945bc582",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S",
    "final_answer": "They use MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RareWords (RW) datasets for word-level similarity evaluation."
  },
  {
    "id": "9f89bff89cea722debc991363f0826de945bc582",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WS353S, SimLex999, SimVerb3500",
    "final_answer": "The authors use the following word-similarity datasets: MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RW."
  },
  {
    "id": "735f58e28d84ee92024a36bc348cfac2ee114409",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, the New York Times (NYT) datasets are used for experiments, with NYT24 having 24 relations and NYT29 having 29 relations, and both datasets have significantly larger numbers of instances of multiple relation tuples with overlapping entities."
  },
  {
    "id": "710fa8b3e74ee63d2acc20af19f95f7702b7ce5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WordDecoding (WDec) model",
    "final_answer": "The WordDecoding (WDec) model performed better in experiments, achieving higher F1 scores than the PtrNetDecoding (PNDec) model on the NYT29 and NYT24 datasets."
  },
  {
    "id": "56123dd42cf5c77fc9a88fc311ed2e1eb672126e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention",
    "final_answer": "Previous work authors refer to researchers BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF9, BIBREF11, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, and BIBREF36."
  },
  {
    "id": "1898f999626f9a6da637bd8b4857e5eddf2fc729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively",
    "final_answer": "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively."
  },
  {
    "id": "1898f999626f9a6da637bd8b4857e5eddf2fc729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores",
    "final_answer": "Our WordDecoding (WDec) model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding (PNDec) model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively."
  },
  {
    "id": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings",
    "final_answer": "The baselines were: (1) rank by the number of times a citation is mentioned in the document, (2) rank by the number of times the citation is cited in the literature (citation impact), (3) rank using Google Scholar Related Articles, (4) rank by the TF*IDF weighted cosine similarity, and (5) rank using a learning-to-rank model trained on text similarity rankings."
  },
  {
    "id": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.",
    "final_answer": "The baselines included ranking by the number of times a citation is mentioned in the document, ranking by the number of times the citation is cited in the literature (citation impact), ranking using Google Scholar Related Articles, ranking by the TF*IDF weighted cosine similarity, and ranking using a learning-to-rank model trained on text similarity rankings."
  },
  {
    "id": "c10f38ee97ed80484c1a70b8ebba9b1fb149bc91",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "SVMRank",
    "final_answer": "unanswerable \n\nHowever, based on the text, it appears that the authors tested three different supervised models, but they do not explicitly state that they developed a new supervised model."
  },
  {
    "id": "340501f23ddc0abe344a239193abbaaab938cc3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations",
    "final_answer": "unanswerable"
  },
  {
    "id": "fbb85cbd41de6d2818e77e8f8d4b91e431931faa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "asked the authors to rank by closeness five citations we selected from their paper",
    "final_answer": "Amazon Mechanical Turk (AMT) and Google Adwords (GA) are mentioned as crowdsourcing platforms, but the article states that the ideal annotators for the task are the authors themselves, and that they used unpaid authors as annotators."
  },
  {
    "id": "1951cde612751410355610074c3c69cec94824c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "autoencoders",
    "final_answer": "The CNN model performed the best with an achieved accuracy of 82.6%."
  },
  {
    "id": "1951cde612751410355610074c3c69cec94824c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CNN",
    "final_answer": "The CNN model performed better with the highest achieved accuracy of 82.6%."
  },
  {
    "id": "4140d8b5a78aea985546aa1e323de12f63d24add",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "61272b1d0338ed7708cf9ed9c63060a6a53e97a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "accuracy of 82.6%",
    "final_answer": "Their performance on the dataset achieved an accuracy of 82.6% with CNNs, outperforming autoencoders and MLP."
  },
  {
    "id": "53b02095ba7625d85721692fce578654f66bbdf0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The dataset is not explicitly mentioned as having a specific size, but it is mentioned that a subset of the dataset was used for training (60% of the dataset), and the rest was used for testing and validation (40% of the dataset)."
  },
  {
    "id": "0cd0755ac458c3bafbc70e4268c1e37b87b9721b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThey used Amazon Mechanical Turk (MTurk) to collect the Talk The Walk dataset."
  },
  {
    "id": "0cd0755ac458c3bafbc70e4268c1e37b87b9721b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "c1ce652085ef9a7f02cb5c363ce2b8757adbe213",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)",
    "final_answer": "The dataset was collected by manually capturing 360-views of several neighborhoods in New York City and crowd-sourcing the collection of the dataset on Amazon Mechanical Turk."
  },
  {
    "id": "96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English",
    "final_answer": "natural language"
  },
  {
    "id": "b85ab5f862221fac819cf2fef239bcb08b9cafc6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "localization accuracy",
    "final_answer": "The authors looked at F1 scores, precision, recall, and accuracy as evaluation metrics."
  },
  {
    "id": "7e34501255b89d64b9598b409d73f96489aafe45",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " dataset on Mechanical Turk involving human perception, action and communication",
    "final_answer": "They used a dataset collected from Amazon Mechanical Turk (MTurk) called Talk The Walk, which consists of over 10k successful dialogues."
  },
  {
    "id": "e854edcc5e9111922e6e120ae17d062427c27ec1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "e854edcc5e9111922e6e120ae17d062427c27ec1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article mentions the use of the Quora duplicate question dataset, but does not specify the language of this dataset."
  },
  {
    "id": "bd6cec2ab620e67b3e0e7946fc045230e6906020",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems",
    "final_answer": "The accuracy of the system is measured using a formula that rewards clustering which matches the article clustering and the total number of claims clustered."
  },
  {
    "id": "4b0ba460ae3ba7a813f204abd16cf631b871baca",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "text clustering on the embeddings of texts",
    "final_answer": "automatically retrieved by comparing the incoming claim to existing factchecks in the database."
  },
  {
    "id": "63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Quora duplicate question dataset BIBREF22",
    "final_answer": "The Quora duplicate question dataset and the Full Fact database of news articles."
  },
  {
    "id": "d27f23bcd80b12f6df8e03e65f9b150444925ecf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The components in the factchecking algorithm include claim detection, claim clustering, and a graph-based method using Louvain Community Detection for community formation."
  },
  {
    "id": "b11ee27f3de7dd4a76a1f158dc13c2331af37d9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " path ranking-based KGC (PRKGC)",
    "final_answer": "The baseline is a simple, transparent, and interpretable model created to highlight the challenges and nature of RC-QED."
  },
  {
    "id": "7aba5e4483293f5847caad144ee0791c77164917",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WikiHop",
    "final_answer": "The WikiHop dataset was used in the experiment."
  },
  {
    "id": "565d668947ffa6d52dad019af79289420505889b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "565d668947ffa6d52dad019af79289420505889b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "d83304c70fe66ae72e78aa1d183e9f18b7484cd6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (“Not stated in the article” or “Other”), The “summary” text boxes",
    "final_answer": "The dataset was annotated through a crowdsourcing framework where workers were asked to judge whether a statement could be derived from a given article, and if so, to provide justification explanations in the form of natural language summaries."
  },
  {
    "id": "e90ac9ee085dc2a9b6fe132245302bbce5f3f5ab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "WikiHop, an entity-based multi-hop QA dataset."
  },
  {
    "id": "5b029ad0d20b516ec11967baaf7d2006e8d7199f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " two labels ",
    "final_answer": "Two labels are considered in the multi-label task, being sentiment polarity and the candidate/category in consideration."
  },
  {
    "id": "79bd2ad4cb5c630ce69d5a859ed118132cae62d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "d3a1a53521f252f869fdae944db986931d9ffe48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "political pundits of the Washington Post",
    "final_answer": "The Washington Post political pundits."
  },
  {
    "id": "d3a1a53521f252f869fdae944db986931d9ffe48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the experts in the field",
    "final_answer": "Washington Post political pundits."
  },
  {
    "id": "38e11663b03ac585863742044fd15a0e875ae9ab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " peoples' sentiments expressed over social media",
    "final_answer": "The crowd in these experiments refers to the users of social media, specifically those who post and share tweets on Twitter."
  },
  {
    "id": "14421b7ae4459b647033b3ccba635d4ba7bb114b",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "experts in Washington Post",
    "final_answer": "the Washington Post experts’ declared winners"
  },
  {
    "id": "52f7e42fe8f27d800d1189251dfec7446f0e1d3b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",
    "final_answer": "Significantly better, as shown by the experimental results."
  },
  {
    "id": "00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we use t-SNE tool BIBREF27 to visualize the learned embedding",
    "final_answer": "The learned embeddings are further analyzed using t-SNE tool to visualize the learned embedding, and efficiency evaluation is also done to show the test accuracy with increasing epoch and different embedding sizes."
  },
  {
    "id": "aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26",
    "final_answer": "Three typical knowledge graph embedding methods (TransE, TransR, and TransH), two path-based models (PTransE and ALL-PATHS), and three attribute-incorporated methods (R-GCN and KR-EAR) along with four variants of KANE are used for comparison."
  },
  {
    "id": "cf0085c1d7bd9bc9932424e4aba4e6812d27f727",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "FB24K, DBP24K, Game30K",
    "final_answer": "The three datasets used to measure performance are DBP24K, Game30K, and FB24K."
  },
  {
    "id": "cf0085c1d7bd9bc9932424e4aba4e6812d27f727",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph",
    "final_answer": "DBP24K, Game30K, and FB24K."
  },
  {
    "id": "586b7470be91efe246c3507b05e30651ea6b9832",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.",
    "final_answer": "KANE captures both high-order structural and attribute information of KGs through recursive embedding propagation based on relation triples and multi-head attention-based aggregation of attribute triples."
  },
  {
    "id": "31b20a4bab09450267dfa42884227103743e3426",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24",
    "final_answer": "TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR."
  },
  {
    "id": "45306b26447ea4b120655d6bb2e3636079d3d6e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes"
  },
  {
    "id": "0c08af6e4feaf801185f2ec97c4da04c8b767ad6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "The authors mention a few confounds such as \"incorrect hashtag supervision\" and \"pragmatic difficulty\" in their study."
  },
  {
    "id": "6412e97373e8e9ae3aa20aa17abef8326dc05450",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Human evaluators",
    "final_answer": "The baseline model used is SVM (Support Vector Machine) classifier."
  },
  {
    "id": "957bda6b421ef7d2839c3cec083404ac77721f14",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio",
    "final_answer": "Spelling errors, capitalisation, POS ratios, length, and sentiment ratios."
  },
  {
    "id": "957bda6b421ef7d2839c3cec083404ac77721f14",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.",
    "final_answer": "Spelling errors, POS ratios, length, sentiment ratios, capitalisation, and repeated characters."
  },
  {
    "id": "368317b4fd049511e00b441c2e9550ded6607c37",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes. \n\nThe article states that the authors use hashtag-based supervision, but also mention that the hashtags are removed so that they provide labels, but do not act as features. This suggests that the labels provided by the hashtags are verified by the authors themselves."
  },
  {
    "id": "b3ec918827cd22b16212265fcdd5b3eadee654ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "387970ebc7ef99f302f318d047f708274c0e8f21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The authors equate drunk tweeting with drunk texting, as they refer to both as \"drunk-texting\" throughout the paper."
  },
  {
    "id": "2fffff59e57b8dbcaefb437a6b3434fc137f813b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining",
    "final_answer": "The text corpora (S) from aristo2016:combining."
  },
  {
    "id": "2fffff59e57b8dbcaefb437a6b3434fc137f813b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The text corpora (S) from BIBREF6 aristo2016:combining."
  },
  {
    "id": "eb95af36347ed0e0808e19963fe4d058e2ce3c9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge",
    "final_answer": "The proposed technique, TupleInf, achieves a statistically significant improvement of 11.8% over the state-of-the-art system, TableILP, on a broad set of over 1,300 science questions."
  },
  {
    "id": "cd1792929b9fa5dd5b1df0ae06fc6aece4c97424",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes \n\nThis is based on the information that the article uses Open IE v4 to extract tuples from text corpora, which involves entity linking."
  },
  {
    "id": "65d34041ffa4564385361979a08706b10b92ebc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "no \n\nThe article states that \"We define a tuple as (subject; predicate; objects) with zero or more objects.\""
  },
  {
    "id": "e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$",
    "final_answer": "ElasticSearch query against a text corpora (S), followed by running Open IE v4."
  },
  {
    "id": "a8545f145d5ea2202cb321c8f93e75ad26fcf4aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The answer is unanswerable, as the article discusses the method's ability to answer complex questions that require multiple facts, but it does not explicitly state whether it can answer multi-hop questions. However, it does mention the possibility of adding inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates, which could be related to multi-hop reasoning."
  },
  {
    "id": "417dabd43d6266044d38ed88dbcb5fdd7a426b22",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining",
    "final_answer": "The textual source to which OpenIE was applied is the text corpora (S) from aristo2016:combining."
  },
  {
    "id": "fed230cef7c130f6040fb04304a33bbc17ca3a36",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$",
    "final_answer": "Open IE v4."
  },
  {
    "id": "7917d44e952b58ea066dc0b485d605c9a1fe3dda",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The answer is yes."
  },
  {
    "id": "7d5ba230522df1890619dedcfb310160958223c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The primary reason for the dense mode outperforming the sparse one is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words."
  },
  {
    "id": "a48cc6d3d322a7b159ff40ec162a541bf74321eb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Word Sense Induction & Disambiguation",
    "final_answer": "The evaluation is conducted using the SemEval 2010 Task 14 evaluation methodology, specifically the adjusted Rand index (ARI) as the quality measure."
  },
  {
    "id": "2bc0bb7d3688fdd2267c582ca593e2ce72718a91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Wiktionary",
    "final_answer": "RuThes, a large-scale lexical ontology for Russian, and Wiktionary referred as Joint Exp SWN."
  },
  {
    "id": "8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "cosine similarity",
    "final_answer": "Cosine similarity."
  },
  {
    "id": "dcb18516369c3cf9838e83168357aed6643ae1b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.",
    "final_answer": "The Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 were used for Quasar-T, while for Quasar-S, Lucene retrieval was used."
  },
  {
    "id": "f46a907360d75ad566620e7f6bf7746497b6e4a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Kyubyong Park, Edouard Grave et al BIBREF11",
    "final_answer": "The word embeddings used were pre-trained word embeddings for Vietnamese created by Kyubyong Park and Edouard Grave, specifically vectors of 100 dimensions and 300 dimensions, respectively, generated from Wikipedia database backup dumps."
  },
  {
    "id": "79d999bdf8a343ce5b2739db3833661a1deab742",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag",
    "final_answer": "The BLSTM-CNN-CRF system produced five types of errors: No extraction, No annotation, Wrong range, Wrong tag, and Wrong range and tag."
  },
  {
    "id": "71d59c36225b5ee80af11d3568bdad7425f17b0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the BLSTM-CNN-CRF performed better than the BLSTM-CRF as it was used in the analysis experiments and achieved a 91.62 F1 score in comparison to another model that used BLSTM-CRF which achieved a 90.94 F1 score."
  },
  {
    "id": "efc65e5032588da4a134d121fe50d49fe8fe5e8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",
    "final_answer": "Task A is used as a main task, and tasks B and C are used as auxiliary tasks for multitask learning."
  },
  {
    "id": "a30958c7123d1ad4723dcfd19d8346ccedb136d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "08333e4dd1da7d6b5e9b645d40ec9d502823f5d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",
    "final_answer": "The article does not explicitly mention the performance gap between their approach and the strong handcrafted method."
  },
  {
    "id": "bc1bc92920a757d5ec38007a27d0f49cb2dde0d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, based on the context, it seems that the article is comparing different systems, and the feature-rich based system is mentioned as a strong system that performs well on tasks A and C."
  },
  {
    "id": "942eb1f7b243cdcfd47f176bcc71de2ef48a17c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, the authors performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task."
  },
  {
    "id": "9bffc9a9c527e938b2a95ba60c483a916dbd1f6b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes. \n\nThe article states that the Multi-Head Attention (MHA) can learn n_head different scores in parallel child spaces and is very powerful for alignments."
  },
  {
    "id": "8434974090491a3c00eed4f22a878f0b70970713",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Proposed model has 1.16 million parameters and 11.04 MB.",
    "final_answer": "AEN-GloVe has a relatively small model size, ranking second after MemNet, and is significantly smaller than most other models, including AEN-GloVe-BiLSTM."
  },
  {
    "id": "b67420da975689e47d3ea1c12b601851018c4071",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.",
    "final_answer": "The proposed model is different from BERT as it employs attentional encoder layers and a label smoothing regularization to enhance performance on targeted sentiment classification tasks."
  },
  {
    "id": "a4e66e842be1438e5cd8d7cb2a2c589f494aee27",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Depeche + SVM",
    "final_answer": "unanswerable"
  },
  {
    "id": "cb78e280e3340b786e81636431834b75824568c3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "9",
    "final_answer": "Nine emotions (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, and Neutral)."
  },
  {
    "id": "2941874356e98eb2832ba22eae9cb08ec8ce0308",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT",
    "final_answer": "Bag-of-words-based benchmarks using methods such as Classification with TF-IDF + Linear SVM, Classification with Depeche++ Emotion lexicons + Linear SVM, Classification with NRC Emotion lexicons + Linear SVM, and Combination of TF-IDF and NRC Emotion lexicons + Linear SVM."
  },
  {
    "id": "4e50e9965059899d15d3c3a0c0a2d73e0c5802a0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words",
    "final_answer": "The dataset contains a total of 9710 passages."
  },
  {
    "id": "67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "3 ",
    "final_answer": "There were 3 unique annotators per passage, with in-house annotators also being used for consolidating labels when necessary."
  },
  {
    "id": "aecb485ea7d501094e50ad022ade4f0c93088d80",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "2fea3c955ff78220b2c31a8ad1322bc77f6706f8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them",
    "final_answer": "Our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them."
  },
  {
    "id": "faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Speaker's Gender Effects, Interlocutors' Gender and Number Effects",
    "final_answer": "Fine-grained syntactic analysis is performed, focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements."
  },
  {
    "id": "da068b20988883bc324e55c073fb9c1a5c39be33",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference",
    "final_answer": "It is demonstrated through a fine-grained syntactic analysis of the translations generated using the proposed method, as well as through quantitative results showing an improvement in BLEU scores when the correct information is provided."
  },
  {
    "id": "0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Google's machine translation system (GMT)",
    "final_answer": "Google's machine translation system (GMT) accessed through its Cloud API."
  },
  {
    "id": "edcde2b675cf8a362a63940b2bbdf02c150fe01f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences",
    "final_answer": "The black-box context injection system consists of a pre-defined textual hint and a wrapper method that injects the desired information into the input sentence to influence the output of a trained NMT system."
  },
  {
    "id": "d20d6c8ecd7cb0126479305d27deb0c8b642b09f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)",
    "final_answer": "Cepstral mean normalization (CMN) and variance with mean normalization (CMVN) are mentioned."
  },
  {
    "id": "11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram",
    "final_answer": "They experiment with 40 mel-scaled log filterbank energies (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector), and also spectrogram."
  },
  {
    "id": "2677b88c2def3ed94e25a776599555a788d197f2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "6-layer bLSTM with 1024 hidden units",
    "final_answer": "A 6-layer bidirectional LSTM with 1024 hidden units."
  },
  {
    "id": "8ca31caa34cc5b65dc1d01d0d1f36bf8c4928805",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "Conversational speech."
  },
  {
    "id": "9ab43f941c11a4b09a0e4aea61b4a5b4612e7933",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span",
    "final_answer": "Previous models, such as MTMSN, used approaches like training a categorical variable to predict the number of spans to extract and generalizing the single-span head method using non-maximum suppression (NMS) algorithm."
  },
  {
    "id": "5a02a3dd26485a4e4a77411b50b902d2bda3731b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span",
    "final_answer": "They use sequence tagging by individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually."
  },
  {
    "id": "579941de2838502027716bae88e33e79e69997a6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",
    "final_answer": "The proposed model does not hinder performance on other question types, actually improving results on these types compared to the NABERT+ baseline."
  },
  {
    "id": "9a65cfff4d99e4f9546c72dece2520cae6231810",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",
    "final_answer": "Our model slightly eclipses the current state-of-the-art results on the entire DROP dataset."
  },
  {
    "id": "a9def7958eac7b9a780403d4f136927f756bab83",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MTMSN BIBREF4",
    "final_answer": "MTMSN."
  },
  {
    "id": "547be35cff38028648d199ad39fb48236cfb99ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "e42fbf6c183abf1c6c2321957359c7683122b48e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n",
    "final_answer": "The aspect based sentiment classifier trained only using the XR loss achieves a F1 score of 53.81 with a classifier trained on 100 labeled sentences."
  },
  {
    "id": "e574f0f733fb98ecef3c64044004aa7a320439be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "DISPLAYFORM0",
    "final_answer": "The expectation regularization loss is defined as the KL-divergence between the model's posterior distribution over a set and the expected label distribution, given by D(P || Q) = ∑[p(x)log(p(x)/q(x))], where p(x) is the expected label distribution and q(x) is the model's posterior distribution."
  },
  {
    "id": "b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "The Lemming model in BIBREF17",
    "final_answer": "The Lemming model, a log-linear model that performs joint morphological tagging and lemmatization, was used as a non-neural baseline for Task 2."
  },
  {
    "id": "bd3ccb63fd8ce5575338d7332e96def7a3fabad6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ROMULUS dataset, NLU-Benchmark dataset",
    "final_answer": "The NLU-Benchmark (NLU-BM) dataset, which contains 25,716 utterances, and the ROMULUS dataset, which contains 1,431 sentences, are both publicly available and used in the research."
  },
  {
    "id": "7c794fa0b2818d354ca666969107818a2ffdda0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities",
    "final_answer": "Intent F1 and Exact Match (EM) accuracy of the entire sequence of labels."
  },
  {
    "id": "1ef5fc4473105f1c72b4d35cf93d312736833d3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No, they do not provide decision sequences as supervision while training models, but rather use reinforcement learning to train the models."
  },
  {
    "id": "5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)",
    "final_answer": "SQuAD and NewsQA, which are repurposed for the interactive MRC task."
  },
  {
    "id": "b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).",
    "final_answer": "They use a combination of reinforcement learning (Rainbow algorithm) and supervised learning to train the models."
  },
  {
    "id": "e9cf1b91f06baec79eb6ddfd91fc5d434889f652",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "previous, next, Ctrl+F $<$query$>$, stop",
    "final_answer": "Ctrl+F (search for token) and stop."
  },
  {
    "id": "6976296126e4a5c518e6b57de70f8dc8d8fde292",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)",
    "final_answer": "The proposed multimodal models are the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM)."
  },
  {
    "id": "53640834d68cf3b86cf735ca31f1c70aa0006b72",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not mention the language of the tweets."
  },
  {
    "id": "b2b0321b0aaf58c3aa9050906ade6ef35874c5c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " $150,000$ tweets",
    "final_answer": "The dataset, called MMHS150K, is formed by 150,000 tweets."
  },
  {
    "id": "4e9684fd68a242cb354fa6961b0e3b5c35aae4b6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ",
    "final_answer": "The multimodal models do not outperform the unimodal textual models."
  },
  {
    "id": "2e632eb5ad611bbd16174824de0ae5efe4892daf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples",
    "final_answer": "The author believes it is due to noisy data, complexity and diversity of multimodal relations, and a small set of multimodal examples."
  },
  {
    "id": "d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)",
    "final_answer": "The F-score, the Area Under the ROC Curve (AUC), and the mean accuracy (ACC) are used to benchmark the results."
  },
  {
    "id": "24c0f3d6170623385283dfda7f2b6ca2c7169238",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Twitter API",
    "final_answer": "The data is collected using the Twitter API."
  },
  {
    "id": "21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "$150,000$ tweets",
    "final_answer": "Correct, 150,000."
  },
  {
    "id": "a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification",
    "final_answer": "The unimodal detection models used were a CNN for image features extraction and a single layer LSTM for tweet text classification."
  },
  {
    "id": "b0799e26152197aeb3aa3b11687a6cc9f6c31011",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)",
    "final_answer": "The different models for multimodal detection proposed were the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM)."
  },
  {
    "id": "4ce4db7f277a06595014db181342f8cb5cb94626",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities",
    "final_answer": "hate or not."
  },
  {
    "id": "62a6382157d5f9c1dce6e6c24ac5994442053002",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "accuracy, normalized mutual information",
    "final_answer": "The evaluation metrics used were accuracy (ACC) and the normalized mutual information metric (NMI)."
  },
  {
    "id": "9e04730907ad728d62049f49ac828acb4e0a1a2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",
    "final_answer": "Their performance results are reported in Tables TABREF43, TABREF44, TABREF46, and TABREF47, showing significant improvements over baseline methods."
  },
  {
    "id": "5a0841cc0628e872fe473874694f4ab9411a1d10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of the performance improvement of the proposed methods over the other methods, but rather discusses the performance enhancements of the proposed methods compared to other baseline methods."
  },
  {
    "id": "a5dd569e6d641efa86d2c2b2e970ce5871e0963f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods",
    "final_answer": "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector based clustering methods, as well as four baseline clustering methods based on unsupervised dimensionality reduction methods such as Laplacian Eigenmaps (LE), Locality Preserving Indexing (LPI), and Autoencoder (AE), and Latent Semantic Analysis (LSA)."
  },
  {
    "id": "785c054f6ea04701f4ab260d064af7d124260ccc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SearchSnippets, StackOverflow, Biomedical",
    "final_answer": "They used three public short text datasets: SearchSnippets, StackOverflow, and Biomedical."
  },
  {
    "id": "3f6610d1d68c62eddc2150c460bf1b48a064e5e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "no."
  },
  {
    "id": "4c854d33a832f3f729ce73b206ff90677e131e48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3",
    "final_answer": "Three configurations of Transformer networks are explored: a small to medium-sized network with 4 Transformer layers, a small network with 2 Transformer layers, and a minimal network with 1 Transformer layer."
  },
  {
    "id": "163c15da1aa0ba370a00c5a09294cd2ccdb4b96d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "90dd5c0f5084a045fd6346469bc853c33622908f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets",
    "final_answer": "Sparse categorical cross-entropy is used for identifying classes from a feature set, and evaluation is the metric of performance, which assumes a large target classification set."
  },
  {
    "id": "095888f6e10080a958d9cd3f779a339498f3a109",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20",
    "final_answer": "The authors use four individual datasets: AI2, CC, IL, and MAWPS."
  },
  {
    "id": "57e783f00f594e08e43a31939aedb235c9d5a102",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AUC-ROC",
    "final_answer": "The evaluation metrics used include AUC-ROC, accuracy, user interruption rate, reuse of VVA skills, number of active dialogs, top-1 accuracy, and relative change with respect to the popularity model."
  },
  {
    "id": "9646fa1abbe3102a0364f84e0a55d107d45c97f0",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": " jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)",
    "final_answer": "unanswerable"
  },
  {
    "id": "29983f4bc8a5513a198755e474361deee93d4ab6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "five-minute reuse and one-day return",
    "final_answer": "Five-minute reuse and one-day return, which label an instance as positive if it is followed by a new joke request within a specified time interval."
  },
  {
    "id": "6c0f97807cd83a94a4d26040286c6f89c4a0f8e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "finite sequence of terms",
    "final_answer": "Vector space representation with TF-IDF weights."
  },
  {
    "id": "13ca4bf76565564c8ec3238c0cbfacb0b41e14d2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "14 TDs, BIBREF15",
    "final_answer": "A dataset of 14 TDs, with subjects on aliens, stories, law, and news."
  },
  {
    "id": "70797f66d96aa163a3bee2be30a328ba61c40a18",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SRCC",
    "final_answer": "They compare the similarity values produced by each of the similarity measures CS, SRCC, and PCC."
  },
  {
    "id": "71f2b368228a748fd348f1abf540236568a61b07",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "unshuffled version of the French OSCAR corpus",
    "final_answer": "The Masked Language Modeling (MLM) task on the French OSCAR corpus."
  },
  {
    "id": "d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa",
    "final_answer": "unanswerable"
  },
  {
    "id": "63723c6b398100bba5dc21754451f503cb91c9b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)",
    "final_answer": "CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches."
  },
  {
    "id": "5471766ca7c995dd7f0f449407902b32ac9db269",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT",
    "final_answer": "The results of CamemBERT outperformed previous results by a large margin for POS tagging and dependency parsing, with error reductions of up to 2.96% and 3.33% respectively, and showed a 2.36 point increase in the F1 score for named entity recognition."
  },
  {
    "id": "dc49746fc98647445599da9d17bc004bafdc4579",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The article does not directly compare CamemBERT against multilingual BERT, but it mentions that CamemBERT outperforms multilingual language models such as mBERT on certain tasks."
  },
  {
    "id": "8720c096c8b990c7b19f956ee4930d5f2c019e2b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "CamemBERT was pre-trained for 100k steps during 17 hours on 256 Nvidia V100 GPUs."
  },
  {
    "id": "b573b36936ffdf1d70e66f9b5567511c989b46b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "unshuffled version of the French OSCAR corpus",
    "final_answer": "The unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens."
  },
  {
    "id": "bf25a202ac713a34e09bf599b3601058d9cace46",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Randomwalk, Walktrap, Louvain clustering",
    "final_answer": "The state of the art measures are structure-based controversy measures developed by Garimella et al."
  },
  {
    "id": "abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "political events such as elections, corruption cases or justice decisions",
    "final_answer": "The article does not explicitly list the specific controversial topics experimented with, but it mentions that they include topics such as elections, corruption cases, and justice decisions, as well as specific examples like the Brazilian presidential elections in 2018, the Ukraine conflict, and the Kavanaugh confirmation hearings."
  },
  {
    "id": "2df910c9806f0c379d7bb1bc2be2610438e487dc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. ",
    "final_answer": "They used thirty different discussions that took place between March 2015 and June 2019, in four languages and five regions, and included both controversial and non-controversial discussions."
  },
  {
    "id": "a2a3af59f3f18a28eb2ca7055e1613948f395052",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Twitter",
    "final_answer": "Twitter."
  },
  {
    "id": "d92f1c15537b33b32bfc436e6d017ae7d9d6c29a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "four different languages: English, Portuguese, Spanish and French",
    "final_answer": "Four languages: English, Portuguese, Spanish, and French."
  },
  {
    "id": "fa3663567c48c27703e09c42930e51bacfa54905",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "deep convolutional networks BIBREF53 , BIBREF54",
    "final_answer": "unanswerable \n\nThe article does not provide information about the current state-of-the-art (SOTA) for sentiment analysis on Twitter at the time of writing."
  },
  {
    "id": "7997b9971f864a504014110a708f215c84815941",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",
    "final_answer": "The language of Twitter proved to be quite challenging due to its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations."
  },
  {
    "id": "0d1408744651c3847469c4a005e4a9dccbd89cf1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, based on the given text, it is mentioned that \"quantification is not a mere byproduct of classification, since a good classifier is not necessarily a good quantifier, and vice versa\" which implies that specific evaluation measures are required for quantification, but the article does not specify the exact metrics."
  },
  {
    "id": "a3d83c2a1b98060d609e7ff63e00112d36ce2607",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "27.41 transformation on average of single seed sentence is available in dataset.",
    "final_answer": "unanswerable \n\nHowever, we can find the number of unique sentences with multiple transformations. The article states that out of 4262 distinct sentences, only 188 was recorded more than once, which means that for most sentences, there is only one transformation available."
  },
  {
    "id": "aeda22ae760de7f5c0212dad048e4984cd613162",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",
    "final_answer": "The dataset COSTRA 1.0 includes 293 annotations from 12 annotators, which include paraphrases, minimal change, nonsense, past, future, ban, and other transformations."
  },
  {
    "id": "d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes, as new sentences.",
    "final_answer": "They are represented as 15 different paraphrases of each sentence."
  },
  {
    "id": "2d536961c6e1aec9f8491e41e383dc0aac700e0a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past",
    "final_answer": "unanswerable \n\nHowever, the article mentions that 15 modification types were selected for the second round of annotation, but it does not explicitly list them."
  },
  {
    "id": "18482658e0756d69e39a77f8fcb5912545a72b9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes, available at http://hdl.handle.net/11234/1-3123"
  },
  {
    "id": "9d336c4c725e390b6eba8bb8fe148997135ee981",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "016b59daa84269a93ce821070f4f5c1a71752a8a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, they briefly describe the analysis of the LASER embeddings, noting that the minimal change operation and tense changes resulted in vectors very similar to the original sentence."
  },
  {
    "id": "771b373d09e6eb50a74fffbf72d059ad44e73ab0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " we were looking for original and uncommon sentence change suggestions",
    "final_answer": "They introduce language variation by wrapping COSTRA 1.0 into an API, such as SentEval, and extending the corpus in Czech and adding other language variants."
  },
  {
    "id": "efb52bda7366d2b96545cf927f38de27de3b5b77",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes, they use external resources such as Korektor, a statistical spellchecker and grammar checker, to minimize the influence of typos on the performance of embedding methods."
  },
  {
    "id": "1a7d28c25bb7e7202230e1b70a885a46dac8a384",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the domain-specific embeddings are related to the domains of the documents, but it does not specify the size of the dataset."
  },
  {
    "id": "6bc45d4f908672945192390642da5a2760971c40",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "48cc41c372d44b69a477998be449f8b81384786b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score",
    "final_answer": "unanswerable"
  },
  {
    "id": "efb3a87845460655c53bd7365bcb8393c99358ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",
    "final_answer": "Our model outperformed the state-of-the-art models on all three datasets: STS Corpus, Sanders dataset, and HCR dataset."
  },
  {
    "id": "0619fc797730a3e59ac146a5a4575c81517cc618",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. ",
    "final_answer": "unanswerable"
  },
  {
    "id": "846a1992d66d955fa1747bca9a139141c19908e8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)",
    "final_answer": "They used three datasets: Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR)."
  },
  {
    "id": "1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "12d77ac09c659d2e04b5e3955a283101c3ad1058",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)",
    "final_answer": "The Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset are used for experiments."
  },
  {
    "id": "d60a3887a0d434abc0861637bbcd9ad0c596caf4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "rules that compute polarity of words after POS tagging or parsing steps",
    "final_answer": "The proposed semantic rules set includes five rules, displayed in Table TABREF15, which are used to capture important information and remove unessential parts in a tweet."
  },
  {
    "id": "69a7a6675c59a4c5fb70006523b9fe0f01ca415c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "link prediction , triplet classification",
    "final_answer": "They experiment with link prediction and triplet classification tasks."
  },
  {
    "id": "60cb756d382b3594d9e1f4a5e2366db407e378ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes, they evaluate LAN by comparing it with different scoring functions and analyzing its performance on sparse knowledge graphs and case studies on neighbors' weights."
  },
  {
    "id": "352a1bf734b2d7f0618e9e2b0dbed4a3f1787160",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The results validate the superiority of LAN w.r.t. the three properties, but the article does not explicitly state that they evaluate existing methods in terms of these properties."
  },
  {
    "id": "045dbdbda5d96a672e5c69442e30dbf21917a1ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "c20b012ad31da46642c553ce462bc0aad56912db",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source",
    "final_answer": "The dataset used to train the model is a combination of three clean-labeled datasets (movie sentence polarity dataset, laptop and restaurant datasets collected from SemEval-2016) and three noisy-labeled training datasets (2,000 reviews for each domain: movie, laptop, and restaurant)."
  },
  {
    "id": "13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",
    "final_answer": "The proposed model NetAb outperforms a wide range of baselines markedly in sentence-level sentiment classification, especially in handling noisy labels."
  },
  {
    "id": "89b9a2389166b992c42ca19939d750d88c5fa79b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "dccc3b182861fd19ccce5bd00ce9c3f40451ed6e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes"
  },
  {
    "id": "98ba7a7aae388b1a77dd6cab890977251d906359",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention if STES (Semantic Text Exchange Score) has been previously used in the literature, it only introduces it as a new metric for evaluating overall performance of a model on STE (Semantic Text Exchange)."
  },
  {
    "id": "3da9a861dfa25ed486cff0ef657d398fdebf8a93",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)",
    "final_answer": "NWN-STEM, GWN-STEM, and W2V-STEM."
  },
  {
    "id": "8c0a0747a970f6ea607ff9b18cfeb738502d9a95",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ERR of 19.05 with i-vectors and 15.52 with x-vectors",
    "final_answer": "The performance of both the i-vector system and the x-vector system on CN-Celeb(E) was much worse than on SITW and SITW(S), indicating a significant difference between the two datasets."
  },
  {
    "id": "529dabe7b4a8a01b20ee099701834b60fb0c43b0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",
    "final_answer": "real-world settings including ambient noise, background babbling, music, cheers, and laugh, with strong and overlapped background speakers in some cases."
  },
  {
    "id": "a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",
    "final_answer": "The 11 genres covered are: entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement."
  },
  {
    "id": "5699996a7a2bb62c68c1e62e730cabf1e3186eef",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "944d5dbe0cfc64bf41ea36c11b1d378c408d40b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "x-vector",
    "final_answer": "unanswerable \n\nThe article compares the performance of i-vector and x-vector systems on CN-Celeb, but it does not conclude which one works better overall."
  },
  {
    "id": "327e6c6609fbd4c6ae76284ca639951f03eb4a4c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",
    "final_answer": "unanswerable, as the article does not provide a direct comparison of the two performances. However, it can be inferred that the performance on CN-Celeb is significantly inferior to VoxCeleb based on the experimental results."
  },
  {
    "id": "df8cc1f395486a12db98df805248eb37c087458b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types",
    "final_answer": "The new model, conditional BERT, is evaluated on six text classification datasets, including SST, Subj, MPQA, RT, TREC, and SST2."
  },
  {
    "id": "6e97c06f998f09256be752fa75c24ba853b0db24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Accuracy across six datasets",
    "final_answer": "The authors measure performance using accuracy on six benchmark classification datasets."
  },
  {
    "id": "de2d33760dc05f9d28e9dabc13bab2b3264cadb7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The conditional masked language model (C-MLM) objective performs better than the original masked language model (MLM) objective."
  },
  {
    "id": "63bb39fd098786a510147f8ebc02408de350cb7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No, the article only mentions BERT as the pre-trained language model evaluated for contextual augmentation."
  },
  {
    "id": "6333845facb22f862ffc684293eccc03002a4830",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nHowever, the article does mention performance on tasks with data augmentation."
  },
  {
    "id": "a12a08099e8193ff2833f79ecf70acf132eda646",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "999b20dc14cb3d389d9e3ba5466bc3869d2d6190",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Kim et al. (2019)",
    "final_answer": "unanswerable \n\nHowever, based on the content of the article, it appears that the survey is not referencing a specific, newly-published paper, but rather providing a comprehensive overview of recent developments in the field of NQG."
  },
  {
    "id": "ca4b66ffa4581f9491442dcec78ca556253c8146",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "b3ff166bd480048e099d09ba4a96e2e32b42422b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable."
  },
  {
    "id": "3703433d434f1913307ceb6a8cfb9a07842667dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Considering \"What\" and \"How\" separately versus jointly optimizing for both.",
    "final_answer": "The learning paradigms covered in this survey include the change of learning paradigm, the broadening of the input spectrum, and the generation of deep questions."
  },
  {
    "id": "f7c34b128f8919e658ba4d5f1f3fc604fb7ff793",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Textual inputs, knowledge bases, and images.",
    "final_answer": "textual inputs, especially declarative sentences, knowledge bases, and images."
  },
  {
    "id": "d42031893fd4ba5721c7d37e1acb1c8d229ffc21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "No, they mainly focus on neural methods for question generation."
  },
  {
    "id": "a999761aa976458bbc7b4f330764796446d030ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "cross-lingual NE recognition",
    "final_answer": "Their model is a hypersphere-based named entity recognition model that uses Euclidean distance and a hypersphere representation to classify entities into different types."
  },
  {
    "id": "f229069bcb05c2e811e4786c89b0208af90d9a25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThe article states that they evaluate the hypersphere model on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0."
  },
  {
    "id": "6b55b558ed581759425ede5d3a6fcdf44b8082ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Naive Bayes, SVM, Maximum Entropy classifiers",
    "final_answer": "The baseline method, Distant Supervision, and a model proposed by Ayushi Dalmia."
  },
  {
    "id": "3e3f5254b729beb657310a5561950085fa690e83",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x.",
    "final_answer": "The Effective Word Score (EFWS) of a word x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x."
  },
  {
    "id": "5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The tweet subjectivity is measured using TextBlob, which provides a subjectivity level between 0 and 1, or Opinion Finder Tool, which identifies subjective and objective parts of the text."
  },
  {
    "id": "129c03acb0963ede3915415953317556a55f34ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.",
    "final_answer": "Because the original DMN requires supporting facts to be labeled during training."
  },
  {
    "id": "58b3b630a31fcb9bffb510390e1ec30efe87bfbf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " the facts that are relevant for answering a particular question) are labeled during training.",
    "final_answer": "Providing labeled facts relevant for answering a question during training."
  },
  {
    "id": "141dab98d19a070f1ce7e7dc384001d49125d545",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer",
    "final_answer": "They proposed a new input module using a two-level encoder with a sentence reader and input fusion layer to allow for information flow between sentences."
  },
  {
    "id": "afdad4c9bdebf88630262f1a9a86ac494f06c4c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images.",
    "final_answer": "They proposed and implemented several improvements to the DMN, including an input fusion layer to allow interactions between input facts, a novel attention-based GRU for logical reasoning, and an untied model with a linear layer and ReLU activation for memory update."
  },
  {
    "id": "bfd4fc82ffdc5b2b32c37f4222e878106421ce2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. ",
    "final_answer": "The model learns to select the important facts from a larger set without requiring labeled supporting facts during training."
  },
  {
    "id": "1ce26783f0ff38925bfc07bbbb65d206e52c2d21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "9213159f874b3bdd9b4de956a88c703aac988411",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "accuracy",
    "final_answer": "They perform manual evaluation by having three English Literature PhD candidates score the fluency of outputs on a 0-5 scale, and also have the annotators classify transferred sentences by style."
  },
  {
    "id": "a234bcbf2e41429422adda37d9e926b49ef66150",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "classification accuracy, BLEU scores, model perplexities of the reconstruction",
    "final_answer": "BLEU scores and perplexity."
  },
  {
    "id": "c383fa9170ae00a4a24a8e39358c38395c5f034b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " words found in the control word lists are then removed, The remaining words, which represent the content",
    "final_answer": "They use part-of-speech (POS) tags to identify content words."
  },
  {
    "id": "83251fd4a641cea8b180b49027e74920bca2699a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style",
    "final_answer": "They model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions, by extracting and counting surface-level features, often counts of function words."
  },
  {
    "id": "5d70c32137e82943526911ebdf78694899b3c28a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly state that they only report results on English data, but it does mention that the data was collected from Twitter and other platforms, which are predominantly English. However, it does not exclude the possibility of other languages being present in the data."
  },
  {
    "id": "97dac7092cf8082a6238aaa35f4b185343b914af",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression",
    "final_answer": "The article provides insights into the relationship between demographics and mental health, specifically that women are more likely to be diagnosed with depression, while men have higher suicide rates, and depression triggers vary across different age groups."
  },
  {
    "id": "195611926760d1ceec00bd043dfdc8eba2df5ad1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Random Forest classifier",
    "final_answer": "unanswerable"
  },
  {
    "id": "445e792ce7e699e960e2cb4fe217aeacdd88d392",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Demographic information is predicted using weighted lexicon of terms.",
    "final_answer": "The framework facilitates demographic inference from social media by integrating heterogeneous features from different modalities, including aesthetic features, language features, sociability, and user engagement, to detect likely depressed individuals and infer demographic information such as age and gender."
  },
  {
    "id": "a3b1520e3da29d64af2b6e22ff15d330026d0b36",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns",
    "final_answer": "- Images: computational aesthetics (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) and face-based cues (facial presence/expression, age/gender from profile photos; OCR text sentiment).\n- Text/profile text: n-grams and psycholinguistic emotion/sentiment features (e.g., LIWC), plus screen name.\n- Social/behavior: ego-network sociability and user engagement metrics."
  },
  {
    "id": "2cf8825639164a842c3172af039ff079a8448592",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The data are self-reported by Twitter users and then verified by two human experts.",
    "final_answer": "The data is annotated using self-reported depressed users, a lexicon of depression symptoms, and human judges who verified a subset of users."
  },
  {
    "id": "36b25021464a9574bf449e52ae50810c4ac7b642",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "From Twitter profile descriptions of the users.",
    "final_answer": "Self-disclosure in Twitter profile descriptions, such as age and gender explicitly mentioned in the text, and also from user-provided information like \"16 years old suicidal girl\"."
  },
  {
    "id": "98515bd97e4fae6bfce2d164659cd75e87a9fc89",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Sociability from ego-network on Twitter",
    "final_answer": "unanswerable"
  },
  {
    "id": "53bf6238baa29a10f4ff91656c470609c16320e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Users' tweets",
    "final_answer": "Twitter."
  },
  {
    "id": "b27f7993b1fe7804c5660d1a33655e424cea8d10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Profile pictures from the Twitter users' profiles.",
    "final_answer": "Twitter."
  },
  {
    "id": "e21a8581cc858483a31c6133e53dd0cfda76ae4c",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "9f6e877e3bde771595e8aee10c2656a0e7b9aeb2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "a3783e42c2bf616c8a07bd3b3d503886660e4344",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "0d0959dba3f7c15ee4f5cdee51682656c4abbd8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes",
    "final_answer": "A sememe is a basic unit of meaning in a word, a fundamental concept or idea that contributes to the overall meaning of a word."
  },
  {
    "id": "589be705a5cc73a23f30decba23ce58ec39d313b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the Dutch section of the OSCAR corpus",
    "final_answer": "They pre-trained their model on the Dutch section of the OSCAR corpus."
  },
  {
    "id": "6e962f1f23061f738f651177346b38fd440ff480",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT",
    "final_answer": "RobBERT, a Dutch RoBERTa-based pre-trained language model, outperforms previous models and other BERT models on several tasks, including sentiment analysis and die/dat disambiguation."
  },
  {
    "id": "594a6bf37eab64a16c6a05c365acc100e38fcff1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "sentiment analysis, the disambiguation of demonstrative pronouns,",
    "final_answer": "They experimented on sentiment analysis, die/dat disambiguation, and possibly other Dutch language tasks."
  },
  {
    "id": "d79d897f94e666d5a6fcda3b0c7e807c8fad109e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances",
    "final_answer": "Results of the experiments suggest that natural language based agents are more robust to task-nuisances and allow for better transfer."
  },
  {
    "id": "599d9ca21bbe2dbe95b08cf44dfc7537bde06f98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The natural language based agents outperform the other methods, including vision-based and feature-based representations, achieving better performance, faster convergence, and more robust solutions."
  },
  {
    "id": "827464c79f33e69959de619958ade2df6f65fdee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThis is because the article does not provide information about the convergence rate of the natural language agents compared to other agents. It only mentions that GloVe pretrained embedding vectors were used, which speeds up the training process."
  },
  {
    "id": "8e857e44e4233193c7b2d538e520d37be3ae1552",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios",
    "final_answer": "The authors perform experiments on several tasks in the Doom environment to compare the performance of natural language representation against visual-based and feature representations."
  },
  {
    "id": "084fb7c80a24b341093d4bf968120e3aff56f693",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " represent the state using natural language",
    "final_answer": "The state is represented via natural language by converting labels and features of the environment into descriptive sentences that summarize the state, using techniques such as patching and parsing."
  },
  {
    "id": "babe72f0491e65beff0e5889380e8e32d7a81f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )",
    "final_answer": "Our TL-TranSum method outperforms the Maximal Relevance Minimal Redundancy (MRMR) baseline by 7% of ROUGE-SU4 score."
  },
  {
    "id": "31ee92e521be110b6a5a8d08cc9e6f90a3a97aae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "No, the paper does not discuss previous models which have been applied to the same task, but it does mention existing research on detecting moral sentiment in online text and bias in NLP."
  },
  {
    "id": "737397f66751624bcf4ef891a10b29cfc46b0520",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n",
    "final_answer": "The datasets used in the paper include the Google N-grams corpus, COHA, WordNet, and the valence ratings for the nearly 14,000 English nouns collected by BIBREF28."
  },
  {
    "id": "87cb19e453cf7e248f24b5f7d1ff9f02d87fc261",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;",
    "final_answer": "The parameter-free model works by summarizing each set of seed words by its expected vector in embedding space, and classifying concepts into the class of closest expected embedding in Euclidean distance following a softmax rule."
  },
  {
    "id": "5fb6a21d10adf4e81482bb5c1ec1787dc9de260d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence",
    "final_answer": "They quantify moral relevance by using a set of seed words approximating moral irrelevance based on valence ratings, and then using a Centroid model or Naïve Bayes model to classify concepts into morally relevant and morally irrelevant categories."
  },
  {
    "id": "542a87f856cb2c934072bacaa495f3c2645f93be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation",
    "final_answer": "They showcase the fine-grained moral dimensions of \"Care / Harm\", \"Fairness / Cheating\", \"Loyalty / Betrayal\", \"Authority / Subversion\", and \"Sanctity / Degradation\"."
  },
  {
    "id": "4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "They use the Google N-grams corpus and the Moral Foundations Dictionary, as well as data from the Pew Research Center's 2013 Global Attitudes survey, and WordNet, and BIBREF30 (unspecified source) for their analysis."
  },
  {
    "id": "c180f44667505ec03214d44f4970c0db487a8bae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all",
    "final_answer": "The system performed well, particularly in the fairy-tale genre, where the neural approach was deemed as coherent as the human-made game."
  },
  {
    "id": "76d62e414a345fe955dc2d99562ef5772130bc7e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique",
    "final_answer": "The information is extracted using two methods: a neural question-answering technique and a rule-based information extraction technique, specifically OpenIE5."
  },
  {
    "id": "6b9310b577c6232e3614a1612cbbbb17067b3886",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs",
    "final_answer": "unanswerable"
  },
  {
    "id": "d484a71e23d128f146182dccc30001df35cdf93f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50.",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of the proposed model's perplexity and BLEU score with those of typical UMT models."
  },
  {
    "id": "5787ac3e80840fe4cf7bfae7e8983fa6644d6220",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We collected a corpus of poems and a corpus of vernacular literature from online resources",
    "final_answer": "A corpus of poems and a corpus of vernacular literature from online resources."
  },
  {
    "id": "ee31c8a94e07b3207ca28caef3fbaf9a38d94964",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5",
    "final_answer": "BLEU and Micro Entity F1."
  },
  {
    "id": "66d743b735ba75589486e6af073e955b6bb9d2a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR",
    "final_answer": "The baseline systems included Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, and DSR."
  },
  {
    "id": "b9f852256113ef468d60e95912800fab604966f6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Camrest, InCar Assistant",
    "final_answer": "InCar dataset and CamRest dataset."
  },
  {
    "id": "88f8ab2a417eae497338514142ac12c3cec20876",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "05e3b831e4c02bbd64a6e35f6c52f0922a41539a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "equi-distant segments of the video."
  },
  {
    "id": "bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "They initialize their word embeddings using 300-dimensional GloVe and then fine-tune them during training."
  },
  {
    "id": "6472f9d0a385be81e0970be91795b1b97aa5a9cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Answer with content missing: (list missing) \nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\n\nYes.",
    "final_answer": "yes \n\nThe article mentions that the authors also add an auxiliary decoding module, not concatenate the question, answer pairs before encoding them, and use the Time-Extended FiLM module for feature extraction, and running hyperparameter optimization over the validation set to select hyperparameters, giving them the observed performance boost."
  },
  {
    "id": "2173809eb117570d289cefada6971e946b902bd6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "293e9a0b30670f4f0a380e574a416665a8c55bc2",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "17de58c17580350c9da9c2f3612784b432154d11",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "multi-class Naive Bayes",
    "final_answer": "A multi-class Naive Bayes classifier is used for emergency categorization."
  },
  {
    "id": "ff27d6e6eb77e55b4d39d343870118d1a6debd5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SVM",
    "final_answer": "Support Vector Machines (SVM) is used for the first stage of emergency detection, while Naive Bayes (NB) is used for the second stage classification model for categorizing tweets into specific emergency types."
  },
  {
    "id": "29772ba04886bee2d26b7320e1c6d9b156078891",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nThe article does not provide information about the source of the tweets, but mentions that the authors collected data using the Twitter API for saved data available for public use."
  },
  {
    "id": "94dc437463f7a7d68b8f6b57f1e3606eacc4490a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, based on the article, it can be inferred that there are at least 4 categories: fire, earthquake, accident, and robbery, as well as a category for drunk driving and drunk driving accidents."
  },
  {
    "id": "9d9d84822a9c42eb0257feb7c18086d390dae3be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the baseline was likely the conventional methods of emergency response, such as calling emergency numbers like 100 or 911, but the article does not explicitly mention this."
  },
  {
    "id": "d27e3a099954e917b6491e81b2e907478d7f1233",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes, the tweets are specific to a region, as the system is currently collecting tweets with a location filter for the city of \"Mumbai\" and displays its map location on the interface."
  },
  {
    "id": "c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "dfc393ba10ec4af5a17e5957fcbafdffdb1a6443",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, BERT",
    "final_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT."
  },
  {
    "id": "311a7fa62721e82265f4e0689b4adc05f6b74215",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",
    "final_answer": "Upward reasoning allows an inference from a more general concept to a more specific one, while downward reasoning allows an inference from a more specific concept to a more general one."
  },
  {
    "id": "82bcacad668351c0f81bd841becb2dbf115f000e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures",
    "final_answer": "The ability to capture the interaction between lexical and syntactic structures, and to determine the direction of inference based on the presence of monotonicity operators and their arguments."
  },
  {
    "id": "5937ebbf04f62d41b48cbc6b5c38fc309e5c2328",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration', Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset, Acknowledgements (b) are mostly with positive or neutral, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP), Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral, No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny)., Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'",
    "final_answer": "The analysis found a key relationship between dialogue acts and emotional states, including Accept/Agree with Joy, Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc."
  },
  {
    "id": "dcd6f18922ac5c00c22cef33c53ff5ae08b42298",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.",
    "final_answer": "The ensemble annotator extracts the final label by giving priority to the labels that are perfectly matching in all the neural annotators, then to the context-based models, and finally by ranking the labels with their confidence values."
  },
  {
    "id": "2965c86467d12b79abc16e1457d848cb6ca88973",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Dialogue Act Markup in Several Layers (DAMSL) tag set",
    "final_answer": "The dialogue act labels were defined based on the DAMSL (Dialogue Act Markup in Several Layers) tag set, which includes forward-looking functions (such as Statement, Info-request, Thanking) and backwards-looking functions (such as Accept, Reject, Answer)."
  },
  {
    "id": "b99948ac4810a7fe3477f6591b8cf211d6398e67",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "five",
    "final_answer": "Five neural models were trained and used for dialogue act labeling."
  },
  {
    "id": "73d657d6faed0c11c65b1ab60e553db57f4971ca",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "They do not directly compare their model against other models, but they mention SyncNet, a previous neural network model that achieves 99% accuracy on lip video synchronisation, and argue that their data is more challenging."
  },
  {
    "id": "9ef182b61461d0d8b6feb1d6174796ccde290a15",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Use an existing one",
    "final_answer": "They annotate their own dataset using self-supervision, generating and labelling pairs of ultrasound and audio segments as positive (correctly synchronised) or negative (randomly desynchronised)."
  },
  {
    "id": "f6f8054f327a2c084a73faca16cf24a180c094ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nHowever, the model predicts the synchronisation offset by calculating the average distance for each candidate offset across utterance segments and selecting the one with the minimum average distance."
  },
  {
    "id": "b8f711179a468fec9a0d8a961fb0f51894af4b31",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "CNN",
    "final_answer": "A two-stream neural network architecture."
  },
  {
    "id": "3bf429633ecbbfec3d7ffbcfa61fa90440cc918b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "apply an ensemble of deep learning and linguistics t",
    "final_answer": "Aspects are identified as feature related to an opinion target."
  },
  {
    "id": "94e0cf44345800ef46a8c7d52902f074a1139e1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",
    "final_answer": "The MSM 2013 corpus, the UMBC Twitter corpus, and the Ritter NER corpus are used for the analysis."
  },
  {
    "id": "ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "1000 hours of WSJ audio data",
    "final_answer": "WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them."
  },
  {
    "id": "12eaaf3b6ebc51846448c6e1ad210dbef7d25a96",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "wav2vec has 12 convolutional layers",
    "final_answer": "The encoder network has five convolutional layers, and the context network has seven convolutional layers."
  },
  {
    "id": "828615a874512844ede9d7f7d92bdc48bb48b18d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "They explore that more data for pre-training improves performance, but a specific threshold or amount of data needed for a certain magnitude of improvement is not explicitly stated."
  },
  {
    "id": "a43c400ae37a8705ff2effb4828f4b0b177a74c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "shared character embeddings for taggers in both languages together through optimization of a joint loss function",
    "final_answer": "The individual embeddings for a given character are shared among the languages jointly trained on."
  },
  {
    "id": "4056ee2fd7a0a0f444275e627bb881134a1c2a10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . ",
    "final_answer": "The Universal Dependencies (UD) treebanks datasets."
  },
  {
    "id": "f6496b8d09911cdf3a9b72aec0b0be6232a6dba1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "5c90e1ed208911dbcae7e760a553e912f8c237a5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents",
    "final_answer": "The datasets are of varying sizes, with the in-house dataset containing 6 languages, 56 entity types, and 53 relation types, and the ACE05 dataset containing 3 languages, 7 entity types, and 6 relation types."
  },
  {
    "id": "3c3b4797e2b21e2c31cf117ad9e52f327721790f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese",
    "final_answer": "They experiment on 7 target languages, including German, Spanish, Portuguese, Chinese, Italian, Japanese, and Arabic."
  },
  {
    "id": "a7d72f308444616a0befc8db7ad388b3216e2143",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "in-house dataset, ACE05 dataset ",
    "final_answer": "The in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset are used."
  },
  {
    "id": "dfb0351e8fa62ceb51ce77b0f607885523d1b8e8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of auto-completion performance between using both language and vision and only language."
  },
  {
    "id": "a130aa735de3b65c71f27018f20d3c068bafb826",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "16k images and 740k corresponding region descriptions",
    "final_answer": "The dataset provided by this research is large, consisting of 16k images and 740k region descriptions from the Visual Genome dataset, and 9k images and 54k referring expressions from the ReferIt dataset."
  },
  {
    "id": "0c1663a7f7750b399f40ef7b4bf19d5c598890ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "we replace user embeddings with a low-dimensional image representation",
    "final_answer": "By using a modified FactorCell LSTM that conditions on CNN-extracted image features (fed via the adaptation matrix) to generate character-level completions with beam search."
  },
  {
    "id": "aa800b424db77e634e82680f804894bfa37f2a34",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention whether the collection process used a WoZ method."
  },
  {
    "id": "fbd47705262bfa0a2ba1440a2589152def64cbbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models",
    "final_answer": "Their model outperformed the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments."
  },
  {
    "id": "51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search",
    "final_answer": "A standard sequence-to-sequence model augmented with an attention mechanism, a two-step baseline approach that divides the task into path generation and path verification, and an ablation model that only takes natural language instructions as input."
  },
  {
    "id": "3aee5c856e0ee608a7664289ffdd11455d153234",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",
    "final_answer": "The proposed model, \"Ours with Mask and Ordered Triplets\", outperformed the Baseline and Ablation models on all metrics in previously seen environments."
  },
  {
    "id": "f42d470384ca63a8e106c7caf1cb59c7b92dbc27",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "exact match, f1 score, edit distance and goal match",
    "final_answer": "The evaluation metrics used are EM (Exact Match), F1 (harmonic average of precision and recall), ED (minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence), and GM (Goal Match)."
  },
  {
    "id": "29bdd1fb20d013b23b3962a065de3a564b14f0fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The authors used Mechanical Turk for collecting their dataset."
  },
  {
    "id": "25b2ae2d86b74ea69b09c140a41593c00c47a82b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "using Amazon Mechanical Turk using simulated environments with topological maps",
    "final_answer": "The navigation instructions were collected through Mechanical Turk using 100 simulated environments with a corresponding topological map."
  },
  {
    "id": "fd7f13b63f6ba674f5d5447b6114a201fe3137cb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "english language",
    "final_answer": "unanswerable\n\nHowever, based on the context, it can be inferred that the experiment is likely done in English."
  },
  {
    "id": "c82e945b43b2e61c8ea567727e239662309e9508",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort",
    "final_answer": "paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, plus several others, including clinical narratives, EEG biomarkers, brain anatomy, social functioning assessments, personality assessment, and symptom scales."
  },
  {
    "id": "fbee81a9d90ff23603ee4f5986f9e8c0eb035b52",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",
    "final_answer": "Their initial results show a Fleiss's Kappa of 0.575, indicating moderate to substantial agreement among annotators."
  },
  {
    "id": "39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)",
    "final_answer": "The authors used two datasets: a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, and a corpus of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms from the Research Patient Data Registry (RPDR)."
  },
  {
    "id": "1f6180bba0bc657c773bd3e4269f87540a520ead",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "57388bf2693d71eb966d42fa58ab66d7f595e55f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "A BPE model is applied to the stem after morpheme segmentation.",
    "final_answer": "The morphology knowledge is implemented by segmenting the words into morpheme units and then applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation."
  },
  {
    "id": "47796c7f0a7de76ccb97ccbd43dc851bb8a613d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12",
    "final_answer": "The word segmentation method works by segmenting complex words into simple morpheme units, either by morpheme segmentation or by applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation."
  },
  {
    "id": "9d5153a7553b7113716420a6ddceb59f877eb617",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention if the word segmentation method is independently evaluated or not."
  },
  {
    "id": "55c840a2f1f663ab2bff984ae71501b17429d0c0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "fa5357c56ea80a21a7ca88a80f21711c5431042c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "36",
    "final_answer": "24 layers, with an additional feature re-presentation layer."
  },
  {
    "id": "35915166ab2fd3d39c0297c427d4ac00e8083066",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "No, they use the total objective function which is a combination of the auxiliary loss and the main loss, scaled by a coefficient lambda."
  },
  {
    "id": "e6c872fea474ea96ca2553f7e9d5875df4ef55cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the proposed model might require more computational resources due to the additional feature re-presentation layer and the iterated use of the objective function at intermediate layers, which could potentially increase training time."
  },
  {
    "id": "fc29bb14f251f18862c100e0d3cd1396e8f2c3a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, agglutinative languages are used in the prediction of both prefixing and suffixing languages, as indicated by the results showing that models pretrained on HUN (agglutinative) perform well for both English (suffixing) and Zulu (prefixing)."
  },
  {
    "id": "f3e96c5487d87557a661a65395b0162033dc05b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Zulu",
    "final_answer": "Navajo, an Athabaskan language."
  },
  {
    "id": "74db8301d42c7e7936eb09b2171cd857744c52eb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors",
    "final_answer": "The performance on the task is evaluated using test accuracy, as shown in Tables TABREF18 and TABREF19, and by manually annotating and categorizing errors in the validation set."
  },
  {
    "id": "587885bc86543b8f8b134c20e2c62f6251195571",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English, Spanish and Zulu",
    "final_answer": "English, Spanish, and Zulu."
  },
  {
    "id": "26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256",
    "final_answer": "The baselines are residual-connected models, such as BASE-4L for IWSLT14 German-English and Turkish-English, and the best performance model provided in BIBREF2 for WMT14 English-German."
  },
  {
    "id": "8c0621016e96d86a7063cb0c9ec20c76a2dba678",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "f1214a05cc0e6d870c789aed24a8d4c768e1db2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "German-English, Turkish-English, English-German",
    "final_answer": "The article explores the following language pairs: German-English, Turkish-English, and English-German."
  },
  {
    "id": "41d3ab045ef8e52e4bbe5418096551a22c5e9c43",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",
    "final_answer": "The article mentions the following datasets were used:\n\n1. IWSLT14 German-English dataset\n2. IWSLT14 Turkish-English dataset\n3. SETimes corpus\n4. WMT14 English-German dataset\n5. dev2010\n6. tst2010\n7. tst2011\n8. tst2012\n9. tst2013\n10. tst2014"
  },
  {
    "id": "62736ad71c76a20aee8e003c462869bab4ab4b1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time",
    "final_answer": "The movement of binomials is tracked by the difference in ordinality between the earliest and latest years in the dataset."
  },
  {
    "id": "aaf50a6a9f449389ef212d25d0fae59c10b0df92",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "news publications, wine reviews, and Reddit",
    "final_answer": "News publications, wine reviews, Reddit, and sports communities."
  },
  {
    "id": "a1917232441890a89b9a268ad8f987184fa50f7a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Trinomials are likely to appear in exactly one order",
    "final_answer": "Yes, trinomials show more structure than binomials, with 36% being completely frozen, compared to 21% of binomials, and the last word often keeping the same position in different orderings."
  },
  {
    "id": "574f17134e4dd041c357ebb75a7ef590da294d22",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "null model ",
    "final_answer": "The new model proposed is a directed graph where an edge from $i$ to $j$ means that $i$ tends to come before $j$ in binomials."
  },
  {
    "id": "41fd359b8c1402b31b6f5efd660143d1414783a0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " close to random,",
    "final_answer": "The most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general."
  },
  {
    "id": "d216d715ec27ee2d4949f9e908895a18fb3238e2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "word length, number of phonemes, number of syllables, alphabetical order, and frequency",
    "final_answer": "The previously proposed rules for predicting binomial ordering mentioned in the article include phonological rules (such as rhythm and Panini's Law), semantic rules (such as the \"Me First\" principle and categorical tendencies), and frequency-based rules, among others."
  },
  {
    "id": "ba973b13f26cd5eb1da54663c0a72842681d5bf5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "news publications, wine reviews, and Reddit",
    "final_answer": "News publications, wine reviews, and Reddit are used to test binomial lists."
  },
  {
    "id": "508580af51483b5fb0df2630e8ea726ff08d537b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.",
    "final_answer": "They model a city description using embeddings by computing the TF-IDF weighted element-wise mean of the token-level representations from pretrained language models, such as ELMo, BERT, and GloVe, and then reducing the dimensionality of the city embeddings to 40 using PCA."
  },
  {
    "id": "89d1687270654979c53d0d0e6a845cdc89414c67",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Using crowdsourcing ",
    "final_answer": "They collect human judgements through an \"odd-one-out\" task, where crowd workers are asked to identify the intruder as the city with the maximum Euclidean distance from the other two in a triplet of cities."
  },
  {
    "id": "fc6cfac99636adda28654e1e19931c7394d76c7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. ",
    "final_answer": "They devise a simple clustering algorithm that iteratively proposes random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence."
  },
  {
    "id": "ed7a3e7fc1672f85a768613e7d1b419475950ab4",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "single-domain setting",
    "final_answer": "The article does not explicitly state that the approach performs better in the multi-domain setting, but it does mention that the model achieves a joint goal accuracy of 45.72% on the multi-domain dataset MultiWoZ, which is significant better than most of the previous models, whereas on the single-domain dataset WoZ2.0, it maintains performance at the level of the state-of-the-art with a marginal drop of 0.3% compared with previous work."
  },
  {
    "id": "72ceeb58e783e3981055c70a3483ea706511fac3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "joint goal accuracy",
    "final_answer": "The metric of joint goal accuracy is used to compare the model to previous work, and cross-entropy loss is used during training."
  },
  {
    "id": "9bfa46ad55136f2a365e090ce585fc012495393c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ",
    "final_answer": "The WoZ2.0 dataset and the MultiWoZ dataset are used to evaluate performance."
  },
  {
    "id": "42812113ec720b560eb9463ff5e74df8764d1bff",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "4f4892f753b1d9c5e5e74c7c94d8c9b6ef523e7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not address why the model cannot learn first-order logic on natural language, but rather discusses its ability to learn on an artificial language and its compositional generalization capabilities."
  },
  {
    "id": "f258ada8577bb71873581820a94695f4a2c223b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "70,000",
    "final_answer": "unanswerable"
  },
  {
    "id": "05bb75a1e1202850efa9191d6901de0a34744af0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "documents from the CommonCrawl dataset that has the most overlapping n-grams with the question",
    "final_answer": "STORIES."
  },
  {
    "id": "770aeff30846cd3d0d5963f527691f3685e8af02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nAlthough they mention training on various text corpora and using ensemble methods, they do not explicitly mention fine-tuning their model on the end task."
  },
  {
    "id": "f7817b949605fb04b1e4fec9dd9ca8804fb92ae9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Because, unlike other languages, English does not mark grammatical genders",
    "final_answer": "Because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement."
  },
  {
    "id": "8255f74cae1352e5acb2144fb857758dda69be02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "by calculating log ratio of grammatical phrase over ungrammatical phrase",
    "final_answer": "They measure grammaticality by computing the log ratio of the grammatical phrase over the ungrammatical phrase."
  },
  {
    "id": "db62d5d83ec187063b57425affe73fef8733dd28",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Markov random field with an optional neural parameterization",
    "final_answer": "The inflection model of D18-1473 is used to convert between masculine-inflected and feminine-inflected lemmata."
  },
  {
    "id": "946676f1a836ea2d6fe98cb4cfc26b9f4f81984d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe paper discusses the experiments and results of several models, but does not provide a single performance metric that can be used to describe the performance achieved by the model."
  },
  {
    "id": "3b090b416c4ad7d9b5b05df10c5e7770a4590f6a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article does mention that supervision afforded by the RNNG and ActionLSTM models did not translate into more robust or humanlike learning outcomes."
  },
  {
    "id": "a1e07c7563ad038ee2a7de5093ea08efdd6077d4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark",
    "final_answer": "The sizes of the datasets employed vary, including a relatively small corpus, the Penn Treebank, the French Treebank, the Billion Word benchmark, English Wikipedia, and a random subset of the frWaC dataset (approximately 4 million sentences, 138 million word tokens)."
  },
  {
    "id": "a1c4f9e8661d4d488b8684f055e0ee0e2275f767",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)",
    "final_answer": "The baseline models are the Recurrent Neural Network (RNN) models, specifically the two-layer recurrent neural language models with long short-term memory architecture (LSTM)."
  },
  {
    "id": "c5171daf82107fce0f285fa18f19e91fbd1215c5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the evaluation metrics include BLEU and ROUGE (1, 2, L) scores",
    "final_answer": "BLEU and ROUGE (1, 2, L) scores with multiple references."
  },
  {
    "id": "baeb6785077931e842079e9d0c9c9040947ffa4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The E2E NLG challenge dataset BIBREF21",
    "final_answer": "The E2E NLG challenge dataset, which is a crowd-sourced dataset of 50k instances in the restaurant domain."
  },
  {
    "id": "bb570d4a1b814f508a07e74baac735bf6ca0f040",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "better sentence pair representations",
    "final_answer": "The model's better performance is attributed to its deeper representations, enriched by different levels of text representation, including subword-level embeddings, pre-trained ELMo embeddings, and bi-attention modules."
  },
  {
    "id": "1771a55236823ed44d3ee537de2e85465bf03eaf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",
    "final_answer": "unanswerable \n\nHowever, we can say that recall is \"great\" for the proposed model but \"not good\" for the other models, as mentioned in the article."
  },
  {
    "id": "1d74fd1d38a5532d20ffae4abbadaeda225b6932",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",
    "final_answer": "The article does not explicitly mention the f1 score and recall values for the model. However, it mentions that precision is not a priority in their case, and they stopped training when it crossed 70% precision, 90% recall on training and testing sets. \n\nIn the results section, it is mentioned that recall is great for their model but precision is not good, as the model is also trying to detect new potential topics which are not there even in reference Wikipedia-Titles and NER sets. \n\nTherefore, I will not be able to provide a specific answer to the question."
  },
  {
    "id": "da8bda963f179f5517a864943dc0ee71249ee1ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "4 layers",
    "final_answer": "unanswerable"
  },
  {
    "id": "5c059a13d59947f30877bed7d0180cca20a83284",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "A dump of 15 million English news articles published in the past 4 years."
  },
  {
    "id": "a1885f807753cff7a59f69b5cf6d0fdef8484057",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "English wikipedia dataset has more than 18 million, a dump of 15 million English news articles ",
    "final_answer": "Around 3 million news articles and 10 million Wikipedia titles."
  },
  {
    "id": "c2553166463b7b5ae4d9786f0446eb06a90af458",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).",
    "final_answer": "The BIBREF5 sieve system (RULE), BIBREF6 system (STAT), and the BIBREF11 deep reinforcement system (NEURAL) are tested."
  },
  {
    "id": "cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.",
    "final_answer": "The proposed model achieves new state-of-the-art (SOTA) performances on both datasets, with significant improvements compared to the previous SOTA without BERT on SParC, improving Ques. Match and Int. Match by 10.6 and 5.4 points, respectively."
  },
  {
    "id": "69e678666d11731c9bfa99953e2cd5a5d11a4d4f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SParC BIBREF2 and CoSQL BIBREF6",
    "final_answer": "SParC and CoSQL."
  },
  {
    "id": "471d624498ab48549ce492ada9e6129da05debac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Concat\nTurn\nGate\nAction Copy\nTree Copy\nSQL Attn\nConcat + Action Copy\nConcat + Tree Copy\nConcat + SQL Attn\nTurn + Action Copy\nTurn + Tree Copy\nTurn + SQL Attn\nTurn + SQL Attn + Action Copy",
    "final_answer": "Concat, Gate, Turn, Tree Copy, SQL Attn, Action Copy, and a combination of these methods."
  },
  {
    "id": "f858031ebe57b6139af46ee0f25c10870bb00c3c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SParC BIBREF2 and CoSQL BIBREF6",
    "final_answer": "SParC and CoSQL."
  },
  {
    "id": "1763a029daca7cab10f18634aba02a6bd1b6faa7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain",
    "final_answer": "The improvement over the state-of-the-art results varies, with a 2.4% and 1.6% improvement in the “DS” part of the two datasets respectively, and a very large margin of +11.4% and +4.9% in the “HDS” part."
  },
  {
    "id": "f9de9ddea0c70630b360167354004ab8cbfff041",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "There were hierarchical and non-hierarchical baselines; BERT was one of those baselines",
    "final_answer": "Yes"
  },
  {
    "id": "58e65741184c81c9e7fe0ca15832df2d496beb6f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "269b05b74d5215b09c16e95a91ae50caedd9e2aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "agreement rates, Kappa value",
    "final_answer": "Kappa value."
  },
  {
    "id": "0d7f514f04150468b2d1de9174c12c28e52c5511",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "agreement of 0.85 and Kappa value of 0.83",
    "final_answer": "High, with Kappa values ranging from 0.83 to 0.99 for different aspects of annotation."
  },
  {
    "id": "4d223225dbf84a80e2235448a4d7ba67bfb12490",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "removing AltLexC and adding Progression into our sense hierarchy",
    "final_answer": "Adapted to take account of properties of Chinese described in Section 3."
  },
  {
    "id": "ca26cfcc755f9d0641db0e4d88b4109b903dbb26",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.",
    "final_answer": "The results show significant improvement over baseline models, with some achieving better performance by up to $55.98\\%$ in F1 score."
  },
  {
    "id": "6cdd61ebf84aa742155f4554456cc3233b6ae2bf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SVM with RBF kernel",
    "final_answer": "The majority baseline, SVM with RBF kernel, FastText, and BiLSTM with Attention."
  },
  {
    "id": "8e8097cada29d89ca07166641c725e0f8fed6676",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.",
    "final_answer": "The pragmatic and discourse context is added to the dataset by structuring the argument path, which only consists of claims and corresponds to a particular line of reasoning for the given controversial topic."
  },
  {
    "id": "951098f0b7169447695b47c142384f278f451a1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact",
    "final_answer": "The dataset includes impact votes and the corresponding context of the argument, where users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact, and very high impact."
  },
  {
    "id": "07c59824f5e7c5399d15491da3543905cfa5f751",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "4,261  days for France and 4,748 for the UK",
    "final_answer": "The training set contains data up to December 31st, 2013 (2,557 days for France and 2,922 for the UK), the validation set contains data from 2014 and 2015 (730 days), and the test set contains data from January 1st, 2016 (974 days for France and 1,096 for the UK)."
  },
  {
    "id": "77f04cd553df691e8f4ecbe19da89bc32c7ac734",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$, which is an example of geometric property visible for context similarity between words."
  },
  {
    "id": "728a55c0f628f2133306b6bd88af00eb54017b12",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.",
    "final_answer": "Geometric properties pertaining to the behavior of the time series, as well as grouping of words by topic, such as winter, summer, or day of the week clusters."
  },
  {
    "id": "d5498d16e8350c9785782b57b1e5a82212dbdaad",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Relative error is less than 5%",
    "final_answer": "Achieving less than 5% of MAPE was deemed impressive by expert electricity forecasters."
  },
  {
    "id": "3e839783d8a4f2fe50ece4a9b476546f0842b193",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "F1 score of 66.66%",
    "final_answer": "unanswerable"
  },
  {
    "id": "2869d19e54fb554fcf1d6888e526135803bb7d75",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "F1 score of 82.10%",
    "final_answer": "The proposed system improves the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis."
  },
  {
    "id": "894c086a2cbfe64aa094c1edabbb1932a3d7c38a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",
    "final_answer": "unanswerable \n\nThe article does not provide a clear definition of the state-of-the-art systems, but it mentions that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis."
  },
  {
    "id": "722e9b6f55971b4c48a60f7a9fe37372f5bf3742",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism",
    "final_answer": "The proposed system performs multi-tasking by using a two-layered multi-task attention based neural network, where emotion analysis is utilized as an auxiliary task to improve the efficiency of sentiment analysis."
  },
  {
    "id": "9c2f306044b3d1b3b7fdd05d1c046e887796dd7a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15",
    "final_answer": "The benchmark dataset of SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC)."
  },
  {
    "id": "3d99bc8ab2f36d4742e408f211bec154bc6696f7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article does provide some details about the model's architecture, including the number of hidden state vectors and the dimensions of the context vector."
  },
  {
    "id": "9219eef636ddb020b9d394868959325562410f83",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN",
    "final_answer": "unanswerable \n\nThe article does not specify the previous state-of-the-art model."
  },
  {
    "id": "ff83eea2df9976c1a01482818340871b17ad4f8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on the previous state-of-the-art performance."
  },
  {
    "id": "0ee20a3a343e1e251b74a804e9aa1393d17b46d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets",
    "final_answer": "The classifier can significantly speed up the annotation process, making large-scale content analysis more feasible."
  },
  {
    "id": "f0e8f045e2e33a2129e67fb32f356242db1dc280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "applying reasoning BIBREF36 or irony detection methods BIBREF37",
    "final_answer": "Category I errors need further investigation, and category II and III errors could be improved by applying reasoning or irony detection methods."
  },
  {
    "id": "b6c235d5986914b380c084d9535a7b01310c0278",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly",
    "final_answer": "The classifiers use category I, II, and III errors, which are further categorized into direct inference, event-specific knowledge, and interpretation of the speaker's intention, respectively."
  },
  {
    "id": "e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " convolutional neural network (CNN) BIBREF29",
    "final_answer": "A convolutional neural network (CNN)."
  },
  {
    "id": "1e4450e23ec81fdd59821055f998fd9db0398b16",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The hashtag-based baseline uses hashtags identified by the pointwise mutual information (pmi) between a hashtag and a class in the annotated dataset."
  },
  {
    "id": "02ce4c288df14a90a210cb39973c6ac0fb4cec59",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "English",
    "final_answer": "unanswerable \n\nHowever, the dataset includes English tweets, as mentioned in the section \"Dataset\"."
  },
  {
    "id": "60726d9792d301d5ff8e37fbb31d5104a520dea3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MH17 Twitter dataset",
    "final_answer": "The MH17 Twitter dataset introduced by BIBREF4, a dataset collected to study the flow of (dis)information about the MH17 plane crash on Twitter."
  },
  {
    "id": "e39d90b8d959697d9780eddce3a343e60543be65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content",
    "final_answer": "Previous studies analysed information based on manually labeled content, such as television transcripts or tweets, and also used distant annotations, for example, classifying tweets based on the domain of a URL or a hashtag."
  },
  {
    "id": "5f7850254b723adf891930c6faced1058b99bd57",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. ",
    "final_answer": "The HMM models use spaces, indentation, special characters, and state transitions as features, and those features are somewhat interpretable due to the visualization of the HMM states and LSTM components."
  },
  {
    "id": "4d05a264b2353cff310edb480a917d686353b007",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The HMM can identify punctuation or pick up on vowels.",
    "final_answer": "The HMMs learn about the state transitions and emission parameters, which are related to spaces, indentation, and special characters in the data."
  },
  {
    "id": "7cdce4222cea6955b656c1a3df1129bb8119e2d0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters",
    "final_answer": "The authors use visualizations, such as Figures 3, 3, and 2, to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data."
  },
  {
    "id": "6ea63327ffbab2fc734dd5c2414e59d3acc56ea5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.",
    "final_answer": "The gap in performance between the HMMs and the LSTMs is relatively small, as the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension on all text data sets."
  },
  {
    "id": "50690b72dc61748e0159739a9a0243814d37f360",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, the article does not mention any non-English data."
  },
  {
    "id": "8266642303fbc6a1138b4e23ee1d859a6f584fbb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BIBREF3, BIBREF4, BIBREF9",
    "final_answer": "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset are used."
  },
  {
    "id": "3685bf2409b23c47bfd681989fb4a763bcab6be2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "300 Dimensional Glove",
    "final_answer": "The embedding algorithm used is Glove Common Crawl Embeddings (840B Token) with a dimension size of 300."
  },
  {
    "id": "19225e460fff2ac3aebc7fe31fcb4648eda813fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Common Crawl ",
    "final_answer": "840B Token Common Crawl Embeddings."
  },
  {
    "id": "f37026f518ab56c859f6b80b646d7f19a7b684fa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "our model requires 100k parameters , while BIBREF8 requires 250k parameters",
    "final_answer": "They found that their model required 100k parameters, while BIBREF8's model required 250k parameters, indicating a 150k parameter difference."
  },
  {
    "id": "1231934db6adda87c1b15e571468b8e9d225d6fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Excluding the embedding weights, our model requires 100k parameters",
    "final_answer": "excluding the word embeddings, our model requires 100k parameters."
  },
  {
    "id": "81303f605da57ddd836b7c121490b0ebb47c60e7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Sexist/Racist (SR) data set, HATE dataset, HAR",
    "final_answer": "The datasets used were the Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset."
  },
  {
    "id": "a3f108f60143d13fe38d911b1cc3b17bdffde3bd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",
    "final_answer": "The system's F1 performance outperformed previous results by as much as 12 F1 points, achieving a new state of the art for classifying hate speech."
  },
  {
    "id": "118ff1d7000ea0d12289d46430154cc15601fd8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "logistic regression",
    "final_answer": "The baseline was a logistic regression model using character n-grams and word unigrams with TF*IDF weighting."
  },
  {
    "id": "102a0439739428aac80ac11795e73ce751b93ea1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "KFTT BIBREF12 and BTEC BIBREF13",
    "final_answer": "The English-to-Japanese language pair datasets used were KFTT and BTEC, as well as the larger Japanese-English ASPEC dataset."
  },
  {
    "id": "d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English-Japanese",
    "final_answer": "English-to-Japanese, specifically on the KFTT and BTEC datasets."
  },
  {
    "id": "04f72eddb1fc73dd11135a80ca1cf31e9db75578",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "278 more annotations",
    "final_answer": "25% more roles."
  },
  {
    "id": "f74eaee72cbd727a6dffa1600cdf1208672d713e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "QA pairs per predicate",
    "final_answer": "The coverage was measured by evaluating against expertly annotated data and comparing with PropBank."
  },
  {
    "id": "068dbcc117c93fa84c002d3424bafb071575f431",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",
    "final_answer": "Quality was measured through F1 scores, specifically using Unlabeled Argument Detection (UA) and Labeled Argument Detection (LA) metrics, as well as Inter-Annotator Agreement (IAA) for dataset consistency."
  },
  {
    "id": "96526a14820b7debfd6f7c5beeade0a854b93d1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " trained annotators BIBREF4, crowdsourcing BIBREF5 ",
    "final_answer": "The corpus was obtained through crowdsourcing with 133K verbs annotated by 2 trained workers independently and a third consolidator."
  },
  {
    "id": "32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "extensive personal feedback",
    "final_answer": "Through several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback, and 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations."
  },
  {
    "id": "78c010db6413202b4063dc3fb6e3cc59ec16e7e3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a trained worker consolidates existing annotations ",
    "final_answer": "Systematic screening of workers, concise guidelines, and a short training procedure."
  },
  {
    "id": "a69af5937cab861977989efd72ad1677484b5c8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the annotation machinery of BIBREF5",
    "final_answer": "The previous dataset was annotated by trained annotators, but later resorted to crowdsourcing with a single QA-generating worker and validation by two others."
  },
  {
    "id": "8847f2c676193189a0f9c0fe3b86b05b5657b76a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "1593 annotations",
    "final_answer": "unanswerable"
  },
  {
    "id": "05196588320dfb0b9d9be7d64864c43968d329bc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "e930f153c89dfe9cff75b7b15e45cd3d700f4c72",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article does mention that the dataset contains 36 million En → Fr sentence pairs, which was used to train a multilingual NMT model, but it does not provide any information about the relationship between the training data size and the performance of the multilingual encoder."
  },
  {
    "id": "545ff2f76913866304bfacdb4cc10d31dbbd2f37",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "WMT 2014 En-Fr parallel corpus",
    "final_answer": "unanswerable"
  },
  {
    "id": "cf93a209c8001ffb4ef505d306b6ced5936c6b63",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "late 2014",
    "final_answer": "Between 2014-2016."
  },
  {
    "id": "fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "96-97.6% using the objects color or shape and 79% using shape alone",
    "final_answer": "The task success rate achieved is 97.6% when using the object's color to uniquely identify it and 96.0% when using the object's shape."
  },
  {
    "id": "1e2ffa065b640e912d6ed299ff713a12195e12c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command",
    "final_answer": "The authors perform a simulated binning task in which a robot is tasked to place a cube into a bowl as outlined by the verbal command."
  },
  {
    "id": "28b2a20779a78a34fb228333dc4b93fd572fda15",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "supervised learning",
    "final_answer": "unanswerable \n\nThe article does not explicitly state whether the proposed approach is reinforcement or supervised learning, but it is based on imitation learning which is typically a supervised learning methodology."
  },
  {
    "id": "b367b823c5db4543ac421d0057b02f62ea16bf9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nHowever, it's only exploited for tackling unknown words of Japanese texts, not for Vietnamese texts due to the unavailability of Vietnamese WordNet."
  },
  {
    "id": "84737d871bde8058d8033e496179f7daec31c2d3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "7b3d207ed47ae58286029b62fd0c160a0145e73d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention the dataset used in the paper. However, it mentions various sources such as financial reports, press releases, earnings call transcripts, credit agreements, news articles, customer interaction logs, and social data that are used in the study."
  },
  {
    "id": "d58c264068d8ca04bb98038b4894560b571bab3e",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe paper discusses various methods and models for anomaly detection from text in the financial domain, but it does not provide specific information about their performance."
  },
  {
    "id": "f80d89fb905b3e7e17af1fe179b6f441405ad79b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "5f6fac08c97c85d5f4f4d56d8b0691292696f8e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe paper does mention a study that improved the performance of standard methods for fake-news detection, but it does not provide a baseline for the task."
  },
  {
    "id": "6adec34d86095643e6b89cda5c7cd94f64381acc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.",
    "final_answer": "Non-contextual properties of a word, derived directly from the word, and capture the general tendency of a word being echoed in explanations."
  },
  {
    "id": "62ba1fefc1eb826fe0cbac092d37a3e2098967e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "random method , LSTM ",
    "final_answer": "The random model with a 15% echo rate."
  },
  {
    "id": "93ac147765ee2573923f68aa47741d4bcbf88fa8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties",
    "final_answer": "The proposed features are: non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties."
  },
  {
    "id": "14c0328e8ec6360a913b8ecb3e50cb27650ff768",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)",
    "final_answer": "Our models outperform the random baseline by a wide margin, with the best F1 score more than doubling that of the random baseline (0.286 vs. 0.116)."
  },
  {
    "id": "6073fa9050da76eeecd8aa3ccc7ecb16a238d83f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "F1 score",
    "final_answer": "The F1 score is used as the evaluation metric, especially since the problem is imbalanced."
  },
  {
    "id": "eacd7e540cc34cb45770fcba463f4bf968681d59",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "The article provides explanations for intriguing patterns of word being echoed."
  },
  {
    "id": "1124804c3702499b78cf0678bab5867e81284b6c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties",
    "final_answer": "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties."
  },
  {
    "id": "2b78052314cb730824836ea69bc968df7964b4e4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SQUAD",
    "final_answer": "The SQUAD dataset denoted as $\\mathcal {S}$, and the SQUAD dataset split into ${\\mathcal {S}}^{tr}$, ${\\mathcal {S}}^{val}$, and ${\\mathcal {S}}^{te}$ for training, validation, and testing respectively."
  },
  {
    "id": "11d2f0d913d6e5f5695f8febe2b03c6c125b667c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "using the BLEU score as a quantitative metric and human evaluation for quality",
    "final_answer": "The performance of this system is measured using the BLEU metric, human evaluation, and usability study."
  },
  {
    "id": "1c85a25ec9d0c4f6622539f48346e23ff666cd5f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "5 questions per image",
    "final_answer": "5 questions per image."
  },
  {
    "id": "37d829cd42db9ae3d56ab30953a7cf9eda050841",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, the VQG model uses a similar Encoder-Decoder architecture with attention, similar to image captioning systems."
  },
  {
    "id": "4b41f399b193d259fd6e24f3c6e95dc5cae926dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",
    "final_answer": "A total of 15,000 images with 75,000 questions from MS COCO, Bing, and Flickr datasets are used for training the VQG model, while the chatbot model is trained on 162,064 utterances over 10,907 dialogues from Persona-chat dataset and 304,713 utterances over 220,579 conversational exchanges from Cornell-movie corpus."
  },
  {
    "id": "76377e5bb7d0a374b0aefc54697ac9cd89d2eba8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "By considering words as vertices and generating directed edges between neighboring words within a sentence",
    "final_answer": "They obtain word lattices from words by treating all possible substrings of a sentence as vertices, connecting neighbor words by directed edges according to their positions in the original sentence, and using a lookup vocabulary to decide which substrings can be considered as words."
  },
  {
    "id": "85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Precision@1, Mean Average Precision, Mean Reciprocal Rank",
    "final_answer": "P@1 (Precision@1), MAP (Mean Average Precision), and MRR (Mean Reciprocal Rank) are used for DBQA, while P@1 and MRR are used for KBRE."
  },
  {
    "id": "4b128f9e94d242a8e926bdcb240ece279d725729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "DBQA, KBRE",
    "final_answer": "They evaluate on two Chinese question answering datasets, DBQA (document based question answering) and KBRE (knowledge based relation extraction)."
  },
  {
    "id": "f8f13576115992b0abb897ced185a4f9d35c5de9",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "1fdcc650c65c11908f6bde67d5052087245f3dde",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The article does not specify the language of the data, but it mentions that the dataset (UXTD) contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, who are speaking English. \n\nHowever, the data selection for this investigation defines a simplified phonetic segment classification task using English phonemes, such as /p, b, v, f, /, and /r/. \n\nSo, the answer to your question is 'yes', but with the caveat that the language of the data is only specified as English in the context of the phonetic segment classification task."
  },
  {
    "id": "abad9beb7295d809d7e5e1407cbf673c9ffffd19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes, they suggest using recurrent architectures, evaluating the complementarity of audio and ultrasound signals, and extending the classification system to more fine-grained place of articulation."
  },
  {
    "id": "265c9b733e4dfffb76acfbade4c0c9b14d3ccde1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)",
    "final_answer": "The dataset, UXTD, contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, with 31 female and 27 male participants."
  },
  {
    "id": "0f928732f226185c76ad5960402e9342c0619310",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "feedforward neural networks (DNNs), convolutional neural networks (CNNs)",
    "final_answer": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification."
  },
  {
    "id": "11c5b12e675cfd8d1113724f019d8476275bd700",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "They differ from previous work in focusing on child speech with a larger number of speakers (58 children) and using cross-validation to evaluate speaker-independent systems across all speakers."
  },
  {
    "id": "d24acc567ebaec1efee52826b7eaadddc0a89e8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "10700",
    "final_answer": "10700 training examples with roughly 2000 to 3000 examples per class."
  },
  {
    "id": "2d62a75af409835e4c123a615b06235a352a67fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "feedforward neural networks, convolutional neural networks",
    "final_answer": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used, with DNNs consisting of 3 hidden layers and CNNs with 2 convolutional and max pooling layers."
  },
  {
    "id": "fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "58",
    "final_answer": "58 speakers, specifically 31 female and 27 male children."
  },
  {
    "id": "c034f38a570d40360c3551a6469486044585c63c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.",
    "final_answer": "Significant improvements in perplexity (with p-value < 0.05) were obtained by the proposed method compared to the baselines."
  },
  {
    "id": "9cbea686732b5b85f77868ca47d2f93cf34516ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution",
    "final_answer": "The multi-turn dialog system learns by directly from the data, without human-defined heuristic rules, and with the help of an external text analysis program called the Linguistic Inquiry and Word Count (LIWC) to capture the emotion information carried in the context."
  },
  {
    "id": "6aee16c4f319a190c2a451c1c099b66162299a28",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness",
    "final_answer": "Human evaluation is performed by recruiting English-speaking students to rate the responses generated by the models based on three criteria: grammatical correctness, contextual coherence, and emotional appropriateness."
  },
  {
    "id": "4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes."
  },
  {
    "id": "180047e1ccfc7c98f093b8d1e1d0479a4cca99cc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " sequence-to-sequence model (denoted as S2S), HRAN",
    "final_answer": "The vanilla sequence-to-sequence model (S2S) and HRAN are used as baseline models."
  },
  {
    "id": "fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nAlthough the article mentions that deep neural networks can be used for \"a range of collective natural language tasks, including chunking and extraction of named entities and relationships\", it does not specifically mention relation extraction as a task being evaluated."
  },
  {
    "id": "b5d6357d3a9e3d5fdf9b344ae96cddd11a407875",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6",
    "final_answer": "The neural-parser-based system."
  },
  {
    "id": "f33a21c6a9c75f0479ffdbb006c40e0739134716",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "syntax-based system may generate correct syntactic analyses for partial grammatical fragments",
    "final_answer": "The main reason is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, providing crucial information for SRL."
  },
  {
    "id": "8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Authors",
    "final_answer": "Two senior students majoring in Applied Linguistics."
  },
  {
    "id": "17f5f4a5d943c91d46552fb75940b67a72144697",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X",
    "final_answer": "36.4% in rank-correlation with respect to human attention in the VQA-HAT dataset."
  },
  {
    "id": "83f22814aaed9b5f882168e22a3eac8f5fda3882",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "rank-correlation BIBREF25",
    "final_answer": "They measure the correlation using rank-correlation, as in BIBREF4, BIBREF24."
  },
  {
    "id": "ed11b4ff7ca72dd80a792a6028e16ba20fccff66",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "they are available in the Visual Genome dataset",
    "final_answer": "They obtain region descriptions and object annotations from the Visual Genome dataset."
  },
  {
    "id": "a48c6d968707bd79469527493a72bfb4ef217007",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "MultiNLI",
    "final_answer": "SNLI + MultiNLI, however, the article specifically states that even training on this combined dataset, accuracy drops significantly when tested on the SICK dataset."
  },
  {
    "id": "b69897deb5fb80bf2adb44f9cbf6280d747271b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BERT",
    "final_answer": "BERT."
  },
  {
    "id": "ad1f230f10235413d1fe501e414358245b415476",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",
    "final_answer": "six high-performing models covering sentence encoding models, cross-sentence attention models, and fine-tuned pre-trained language models, specifically: BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT."
  },
  {
    "id": "0a521541b9e2b5c6d64fb08eb318778eba8ac9f7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SNLI, MultiNLI and SICK",
    "final_answer": "The datasets used were SNLI, MultiNLI, and SICK."
  },
  {
    "id": "11e376f98df42f487298ec747c32d485c845b5cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, we can make an inference that the baseline was not sufficient as the article states \"However, their effort is not enough since such efforts are primarily based on manual moderation to identify and delete offensive materials. The process is labour intensive, time consuming, and not sustainable or scalable in reality\""
  },
  {
    "id": "284ea817fd79bc10b7a82c88d353e8f8a9d7e93c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "c0122190119027dc3eb51f0d4b4483d2dbedc696",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN",
    "final_answer": "They use a Stacking method as their ensemble technique."
  },
  {
    "id": "1ed6acb88954f31b78d2821bb230b722374792ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).",
    "final_answer": "unanswerable"
  },
  {
    "id": "5a33ec23b4341584a8079db459d89a4e23420494",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",
    "final_answer": "unanswerable"
  },
  {
    "id": "1b9119813ea637974d21862a8ace83bc1acbab8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).",
    "final_answer": "VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD, and text crawled from Facebook."
  },
  {
    "id": "8abb96b2450ebccfcc5c98772cec3d86cd0f53e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nAlthough the article discusses Twitter health-related data, there is no mention of the language of the data."
  },
  {
    "id": "f52ec4d68de91dba66668f0affc198706949ff90",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Women-Yoga",
    "final_answer": "The article does not explicitly state the other interesting correlations observed, but it mentions the discovery of correlations such as \"Yoga-Veganism\" and \"Women-Yoga\" in Topic 2."
  },
  {
    "id": "225a567eeb2698a9d3f1024a8b270313a6d15f82",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix",
    "final_answer": "The baselines were a basic RNN text comprehension model well-trained on a large dataset and models pretrained on large data."
  },
  {
    "id": "35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ensemble of hand-crafted syntactic and frame-semantic features BIBREF16",
    "final_answer": "The state of the art for ranking MC Test answers is the averaging-based model, which can beat even the best previously reported model (HABCNN-TE) on the MC-500 dataset."
  },
  {
    "id": "f5eac66c08ebec507c582a2445e99317a83e9ebe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The final size of the dataset is outlined in Fig. FIGREF8."
  },
  {
    "id": "62613aca3d7c7d534c9f6d8cb91ff55626bb8695",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset",
    "final_answer": "The Argus dataset, AI2-8grade/CK12 dataset, and MCTest dataset were used."
  },
  {
    "id": "6e4505609a280acc45b0a821755afb1b3b518ffd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The BLEU metric ",
    "final_answer": "The BLEU metric is adopted to evaluate the model performance during evaluation."
  },
  {
    "id": "9bd938859a8b063903314a79f09409af8801c973",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets",
    "final_answer": "The WMT 2014 English-French and English-German translation datasets, the IWSLT 2014 German-English translation dataset, the IWSLT 2015 English-Vietnamese translation dataset, and two small IWSLT datasets."
  },
  {
    "id": "68ba5bf18f351e8c83fae7b444cc50bef7437f13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "De-En, En-Fr and En-Vi translation tasks",
    "final_answer": "WMT14 En-Fr and En-De datasets, IWSLT De-En dataset, and IWSLT 2015 En-Vi dataset."
  },
  {
    "id": "f6a1125c5621a2f32c9bcdd188dff14efa096083",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "2.2 BLEU gains",
    "final_answer": "MUSE achieves 2.2 BLEU gains in En-Fr translation compared to Evolved Transformer."
  },
  {
    "id": "282aa4e160abfa7569de7d99b8d45cabee486ba4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the weighted sum of the new opinion representations, according to their associations with the current aspect representation",
    "final_answer": "The opinion summary is determined via a bi-linear term to calculate the association score between the opinion representation and each aspect, and then the improved opinion summary is obtained via the weighted sum of the opinion representations."
  },
  {
    "id": "ecfb2e75eb9a8eba8f640a039484874fa0d2fceb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "a6950c22c7919f86b16384facc97f2cf66e5941d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.",
    "final_answer": "They use the SemEval ABSA challenge datasets from 2014 to 2016, as well as MPQA, to train the model."
  },
  {
    "id": "54be3541cfff6574dba067f1e581444537a417db",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.",
    "final_answer": "The proposed framework achieves 5.0%, 1.6%, 1.4%, and 1.3% absolute gains on the four datasets compared to the winning systems of SemEval ABSA."
  },
  {
    "id": "221e9189a9d2431902d8ea833f486a38a76cbd8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The average number of utterances per dialog is about 23 ",
    "final_answer": "about 23."
  },
  {
    "id": "a276d5931b989e0a33f2a0bc581456cca25658d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2",
    "final_answer": "Several baseline models, including state-of-the-art neural seq2seq architectures such as n-gram, convolution, LSTM, and Transformer models, are offered."
  },
  {
    "id": "c21d26130b521c9596a1edd7b9ef3fe80a499f1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations",
    "final_answer": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations."
  },
  {
    "id": "ec8043290356fcb871c2f5d752a9fe93a94c2f71",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques",
    "final_answer": "unanswerable"
  },
  {
    "id": "728c2fb445173fe117154a2a5482079caa42fe24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach",
    "final_answer": "The traditional co-occurrence networks fail to establish links between similar words whenever they appear distant in the text because they only connect adjacent words, overlooking long-range syntactical links and semantically similar words not sharing the same lemma."
  },
  {
    "id": "23d32666dfc29ed124f3aa4109e2527efa225fbc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "They use it as addition to previous model - they add new edge between words if word embeddings are similar.",
    "final_answer": "They complement and add to the previous features of the model, rather than replacing them."
  },
  {
    "id": "076928bebde4dffcb404be216846d9d680310622",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks",
    "final_answer": "The previous co-occurrence networks are based on simple models such as word adjacency networks and traditional word co-occurrence networks."
  },
  {
    "id": "f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "balanced accuracy, i.e., the average of the three accuracies on each class",
    "final_answer": "The model explanation output is evaluated using BLEU score, but it is mentioned that BLEU score is not an appropriate measure for the quality of explanations, and human evaluation is suggested instead."
  },
  {
    "id": "66bf0d61ffc321f15e7347aaed191223f4ce4b4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "2,060 workers",
    "final_answer": "unanswerable \n\nThe article does not mention the number of annotators used to write natural language explanations to SNLI-VE-2.0."
  },
  {
    "id": "5dfa59c116e0ceb428efd99bab19731aa3df4bbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Totally 6980 validation and test image-sentence pairs have been corrected.",
    "final_answer": "unanswerable \n\nHowever, the article does mention that the authors randomly selected 100 image-sentence pairs in the validation set of SNLI-VE and their corresponding explanations in e-SNLI and examined how relevant these explanations are for the VTE task."
  },
  {
    "id": "0c557b408183630d1c6c325b5fb9ff1573661290",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set",
    "final_answer": "The performance difference of the existing model between the original and corrected corpus is not significant, with a slight decrease of 0.50% in the corrected corpus."
  },
  {
    "id": "a08b5018943d4428f067c08077bfff1af3de9703",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "neutral class",
    "final_answer": "The neutral class in SNLI-VE has a substantial labelling error rate of approximately 31%."
  },
  {
    "id": "9447ec36e397853c04dcb8f67492ca9f944dbd4b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words",
    "final_answer": "A dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila)."
  },
  {
    "id": "12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "32c149574edf07b1a96d7f6bc49b95081de1abd2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "3de27c81af3030eb2d9de1df5ec9bfacdef281a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences",
    "final_answer": "The dataset used to train Word2Vec for the Italian Language is composed of 2.6 GB of raw text, including 421,829,960 words divided into 17,305,401 sentences."
  },
  {
    "id": "cc680cb8f45aeece10823a3f8778cf215ccc8af0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others",
    "final_answer": "Different parameter settings, such as the number of epochs, window size, and negative sampling value, have a significant impact on the performance and semantic capacity of the resulting model, leading to oscillatory trends and varying levels of accuracy in the syntactic and semantic macro-areas."
  },
  {
    "id": "fab4ec639a0ea1e07c547cdef1837c774ee1adb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not compare the findings for Italian language with the English language version."
  },
  {
    "id": "9190c56006ba84bf41246a32a3981d38adaf422c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)",
    "final_answer": "The dataset used for training Word2Vec in Italian language was obtained from a dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila)."
  },
  {
    "id": "7aab78e90ba1336950a2b0534cc0cb214b96b4fd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.",
    "final_answer": "The auxiliary signals from the morphology table are incorporated in the decoder through an attention module that assigns different weights to the affixes in the table."
  },
  {
    "id": "b7fe91e71da8f4dc11e799b3bd408d253230e8c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "target-side affixes",
    "final_answer": "Affixes of the target language."
  },
  {
    "id": "16fa6896cf4597154363a6c9a98deb49fffef15f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention the language of the Twitter data used, but it does mention using the \"twint\" scraping tool and \"GloVe word embeddings\" which are typically used for English text."
  },
  {
    "id": "0f60864503ecfd5b048258e21d548ab5e5e81772",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "Yes, the authors mention several confounds, including the skew in the model due to the use of \"influential\" Twitter users who rarely make spelling errors or use sarcasm, which leads to a high rate of false negatives, and the potential bias in the study due to the influx of climate change deniers who tweet about hurricanes only after the event."
  },
  {
    "id": "fe578842021ccfc295209a28cf2275ca18f8d155",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel",
    "final_answer": "The machine learning models used include neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel)."
  },
  {
    "id": "00ef9cc1d1d60f875969094bb246be529373cb1d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.",
    "final_answer": "The methodology used to compensate for limited labelled data is the \"influential tweet\" labeling technique, where a set of influential Twitter users who are certain to have a classifiable sentiment regarding the topic are used to label tweets in bulk."
  },
  {
    "id": "279b633b90fa2fd69e84726090fadb42ebdf4c02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires",
    "final_answer": "The East Coast Bomb Cyclone, the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, and the California Camp Fires."
  },
  {
    "id": "0106bd9d54e2f343cc5f30bb09a5dbdd171e964b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "twitter ",
    "final_answer": "Twitter."
  },
  {
    "id": "e015d033d4ee1c83fe6f192d3310fb820354a553",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BIBREF8 a refined collection of tweets gathered from twitter",
    "final_answer": "They used a refined collection of tweets gathered from Twitter, specifically a dataset labeled for named entity recognition task containing 8,257 tweets with 12,784 entities in total."
  },
  {
    "id": "8a871b136ccef78391922377f89491c923a77730",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",
    "final_answer": "unanswerable"
  },
  {
    "id": "af60462881b2d723adeb4acb5fbc07ea27b6bde2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.",
    "final_answer": "Strong correlations were discovered between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s)."
  },
  {
    "id": "3c378074111a6cc7319c0db0aced5752c30bfffb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system",
    "final_answer": "Yes."
  },
  {
    "id": "b464bc48f176a5945e54051e3ffaea9a6ad886d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant",
    "final_answer": "They are experimenting with booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant."
  },
  {
    "id": "3b40799f25dbd98bba5b526e0a1d0d0bb51173e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "3c16d4cf5dc23223980d9c0f924cb9e4e6943f13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "AMS method.",
    "final_answer": "They select answer candidates by searching for triples in ConceptNet that share the same concept as the masked concept and relation."
  },
  {
    "id": "4c822bbb06141433d04bbc472f08c48bc8378865",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",
    "final_answer": "They use natural language processing and computational linguistics tools, including text mining procedures, Granger causality, and Latent Dirichlet Allocation (LDA) topic modeling."
  },
  {
    "id": "1baf87437b70cc0375b8b7dc2cfc2830279bc8b5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Randomly selected from a Twitter dump, temporally matched to causal documents",
    "final_answer": "A random selection of tweets from Twitter, matched temporally to the causal corpus."
  },
  {
    "id": "0b31eb5bb111770a3aaf8a3931d8613e578e07a8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Presence of only the exact unigrams 'caused', 'causing', or 'causes'",
    "final_answer": "The selection criteria for \"causal statements\" are that they are with high certainty causal statements, without attempting to study the factual correctness of these statements or offer any degree of verification."
  },
  {
    "id": "7348e781b2c3755b33df33f4f0cab4b94fcbeb9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Only automatic methods",
    "final_answer": "They use a combination of crowdsourcing, automatic methods, and external resources, including the Stanford CoreNLP toolkit and the labMT sentiment scores, which were crowdsourced."
  },
  {
    "id": "f68bd65b5251f86e1ed89f0c858a8bb2a02b233a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Randomly from a Twitter dump",
    "final_answer": "They collect a comparable corpus by gathering the same number of statements selected at random, but controlling for time of year."
  },
  {
    "id": "e111925a82bad50f8e83da274988b9bea8b90005",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Randomly from Twitter",
    "final_answer": "They collected the control corpus by selecting random documents that did not contain causal words or bidirectional words, and matched them temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013."
  },
  {
    "id": "ba48c095c496d01c7717eaa271470c3406bf2d7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Chinese",
    "final_answer": "Chinese"
  },
  {
    "id": "42a61773aa494f7b12838f71a949034c12084de1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6",
    "final_answer": "MemN2N and Attentive and Impatient Readers."
  },
  {
    "id": "48c3e61b2ed7b3f97706e2a522172bf9b51ec467",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "correctness of all the question answer pairs are verified by at least two annotators",
    "final_answer": "The type and correctness of all the question answer pairs were verified by at least two annotators."
  },
  {
    "id": "61fba3ab10f7b6906e27b028fb1d42ec601c3fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "80de3baf97a55ea33e0fe0cafa6f6221ba347d0a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nHowever, based on the context it seems they are not hand-crafted, as it is mentioned that \"we determine $\\pi _j$ for each mention $m_j$ in the following way\"."
  },
  {
    "id": "f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",
    "final_answer": "Resolution mode variables, denoted as $\\Pi = \\lbrace \\pi _1, \\ldots, \\pi _n\\rbrace $, are introduced to distinguish mentions resolved by different categories of information, specifically indicating in which mode the mention should be resolved, such as string-matching (str), precise-construct (prec), or attribute-matching (attr)."
  },
  {
    "id": "e76139c63da0f861c097466983fbe0c94d1d9810",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No, supervised models perform better for this task.",
    "final_answer": "unanswerable \n\nHowever, the paper does mention that the model outperforms the baseline systems and comes close to the state-of-the-art supervised systems, but it does not explicitly claim to be the state-of-the-art."
  },
  {
    "id": "b8b588ca1e876b3094ae561a875dd949c8965b2e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue",
    "final_answer": "unanswerable \n\nHowever, based on the article, one of the problems found with the evaluation scheme for open domain chit-chat systems is the difficulty in constructing a gold standard (a reference set) to evaluate a response generated by such a system due to the infinite number of possible responses."
  },
  {
    "id": "2ec640e6b4f1ebc158d13ee6589778b4c08a04c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the data is annotated by the voice resource department of the iFLYTEK Corporation for task 2, and possibly by Lingzhi Li, Yangzi Zhang, Jiaqi Zhu, and Xiaoming Shi for task 1, as mentioned in the acknowledgements section."
  },
  {
    "id": "ab0bb4d0a9796416d3d7ceba0ba9ab50c964e9d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The collection steps for task 1 include releasing data for training and developing, allowing participants to collect external data for training and developing, and considering two sub tasks, one for a closed evaluation using only the released data and the other for an open evaluation allowing external data collection."
  },
  {
    "id": "0460019eb2186aef835f7852fc445b037bd43bb7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "two",
    "final_answer": "unanswerable"
  },
  {
    "id": "96c09ece36a992762860cde4c110f1653c110d96",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",
    "final_answer": "unanswerable \n\nThe article does not explicitly state the result of the highest performing system."
  },
  {
    "id": "a9cc4b17063711c8606b8fc1c5eaf057b317a0c9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input",
    "final_answer": "F1-score is used for task 1, and for task 2, the metrics are: task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, and guidance ability for out of scope input."
  },
  {
    "id": "6ead576ee5813164684a8cdda36e6a8c180455d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Rouge-L, Bleu-1",
    "final_answer": "They measure the quality of summaries using Rouge-L, which is a metric for evaluating the similarity between generated and reference summaries."
  },
  {
    "id": "0117aa1266a37b0d2ef429f1b0653b9dde3677fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "The answer is yes."
  },
  {
    "id": "5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "well-formed sentences vs concise answers",
    "final_answer": "They refer to the different formats in which answers can be generated, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context."
  },
  {
    "id": "6c80bc3ed6df228c8ca6e02c0a8a1c2889498688",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "no \n\nThe article mentions that the model \"Masque\" can generate answers in multiple styles, and that it is capable of answering one question in multiple styles, such as concise phrases and well-formed sentences."
  },
  {
    "id": "2d274c93901c193cf7ad227ab28b1436c5f410af",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",
    "final_answer": "MCAN, extractive approaches, and a model trained with the WFA set consisting of the single style."
  },
  {
    "id": "e63bde5c7b154fbe990c3185e2626d13a1bad171",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",
    "final_answer": "unanswerable"
  },
  {
    "id": "cb8a6f5c29715619a137e21b54b29e9dd48dad7d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "well-formed sentences vs concise answers",
    "final_answer": "The answer styles refer to different ways of answering a question, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question."
  },
  {
    "id": "8a7bd9579d2783bfa81e055a7a6ebc3935da9d20",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WAS, LipCH-Net-seq, CSSMCM-w/o video",
    "final_answer": "LipCH-Net was the previous state of the art model for this task, but it only performed word classification for Chinese Mandarin lip reading, not at the complete sentence level."
  },
  {
    "id": "27b01883ed947b457d3bab0c66de26c0736e4f90",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "syllables",
    "final_answer": "sequence-to-sequence learning problem."
  },
  {
    "id": "9714cb7203c18a0c53805f6c889f2e20b4cab5dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "video sequence is first fed into the VGG model BIBREF9 to extract visual feature",
    "final_answer": "The visible movements of the neck, head, and mouth."
  },
  {
    "id": "a22b900fcd76c3d36b5679691982dc6e9a3d34bf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nSince the provided text does not mention any information about the language of the data being used, we cannot determine if the results are reported only on English data."
  },
  {
    "id": "fb2593de1f5cc632724e39d92e4dd82477f06ea1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "performances of a purely content-based model naturally stays stable",
    "final_answer": "They demonstrate the robustness of their results by simulating the effects of an overly context-sensitive classifier in a cross-document setting and comparing the performances of different model types."
  },
  {
    "id": "476d0b5579deb9199423bb843e584e606d606bc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "BIBREF13, majority baseline",
    "final_answer": "unanswerable."
  },
  {
    "id": "eddabb24bc6de6451bcdaa7940f708e925010912",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.",
    "final_answer": "unanswerable"
  },
  {
    "id": "f0946fb9df9839977f4d16c43476e4c2724ff772",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "They are defined as units that bear great similarity with elementary discourse units (EDUs) in RST."
  },
  {
    "id": "e51d0c2c336f255e342b5f6c3cf2a13231789fed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They collected tweets in Russian language using a heuristic query specific to Russian",
    "final_answer": "Twitter corpus acquired from 2015/07/21 till 2015/08/04."
  },
  {
    "id": "5b6aec1b88c9832075cd343f59158078a91f3597",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",
    "final_answer": "The proposed word embeddings outperform Sindhi fastText word representations in intrinsic evaluation matrices and have a larger vocabulary."
  },
  {
    "id": "a6717e334c53ebbb87e5ef878a77ef46866e3aed",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes, they are used for various NLP tasks such as parts-of-speech tagging, named entity recognition, and text classification."
  },
  {
    "id": "a1064307a19cd7add32163a70b6623278a557946",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "908456 unique words are available in collected corpus.",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the corpus contains a \"large corpus of more than 61 million words\" and \"unique tokens\" but does not specify the exact number of unique words."
  },
  {
    "id": "8cb9006bcbd2f390aadc6b70d54ee98c674e45cc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter",
    "final_answer": "The data was collected from multiple web resources, including news columns of daily Sindhi newspapers, Wikipedia dumps, short stories, sports news from social blogs, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board, and tweets regarding news and sports from Twitter."
  },
  {
    "id": "75043c17a2cddfce6578c3c0e18d4b7cf2f18933",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious",
    "final_answer": "The trends found in musical preferences are that audiences want more contemporary, intense, and novel music, but less mellow and unpretentious."
  },
  {
    "id": "95bb3ea4ebc3f2174846e8d422abc076e1407d6a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "between 1900s and 2010s",
    "final_answer": "The decades looked at were the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, and 1990s."
  },
  {
    "id": "3ebdc15480250f130cf8f5ab82b0595e4d870e2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "77 genres",
    "final_answer": "77 genres."
  },
  {
    "id": "bbc58b193c08ccb2a1e8235a36273785a3b375fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "3c34187a248d179856b766e9534075da1aa5d1cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)",
    "final_answer": "The CRF model achieves an F1 score of 89.60 on the development set, 87.82 on the test set, and 71.49 on the supplemental test set."
  },
  {
    "id": "8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data",
    "final_answer": "The paper motivates the use of CRF as the baseline model because it was the most popular model in both Shared Tasks on Language Identification for Code-Switched Data."
  },
  {
    "id": "97757a69d9fc28b260e68284fd300726fbe358d0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)",
    "final_answer": "The handcrafted features used are: Bias feature, Token feature, Uppercase feature, Titlecase feature, Character trigram feature, Quotation feature, Word suffix feature, POS tag, Word shape, and Word embedding."
  },
  {
    "id": "41830ebb8369a24d490e504b7cdeeeaa9b09fd9c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide a clear definition of the current state of the art method."
  },
  {
    "id": "4904ef32a8f84cf2f53b1532ccf7aa77273b3d19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "They outperform state of the art in terms of BLEU between output and human-written reformulations."
  },
  {
    "id": "45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches",
    "final_answer": "The three new proposed architectures are: \n\n1. A model with a special dedicated discriminator added to control that the latent representation does not contain stylistic information.\n2. A shifted autoencoder (SAE) that feeds the \"soft\" generated sentence into an encoder and checks how close is the representation to the original representation in terms of the cosine distance.\n3. A combination of both the above approaches."
  },
  {
    "id": "d6a27c41c81f12028529e97e255789ec2ba39eaa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points",
    "final_answer": "Accuracy can change up to 5 percentage points, while BLEU can vary up to 8 points."
  },
  {
    "id": "2d3bf170c1647c5a95abae50ee3ef3b404230ce4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "standard parametrized attention and a non-attention baseline",
    "final_answer": "The standard parametrized attention and the non-attention baseline are used as baseline methods."
  },
  {
    "id": "6e8c587b6562fafb43a7823637b84cd01487059a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Ranges from 44.22 to 100.00 depending on K and the sequence length.",
    "final_answer": "unanswerable, as the article does not provide a single BLEU score but rather presents various results in tables and figures."
  },
  {
    "id": "ab9453fa2b927c97b60b06aeda944ac5c1bfef1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Sequence Copy Task and WMT'17",
    "final_answer": "The article does not explicitly list all the datasets used in experiments, but it mentions that four large machine translation datasets of WMT'17 are used, specifically English-Czech, English-German, English-Finish, and English-Turkish. Additionally, the \"Toy Copy task\" and a \"toy data\" are also mentioned, but the specific dataset used for this task is not specified."
  },
  {
    "id": "3a8d65eb8e1dbb995981a0e02d86ebf3feab107a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "an adversarial loss ($\\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\\ell _{cycle}$) on each side",
    "final_answer": "A cycle consistency loss."
  },
  {
    "id": "d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "New best results of accuracy (P@1) on Vecmap:\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",
    "final_answer": "The proposed model achieves the best results on a standard benchmark."
  },
  {
    "id": "54c7fc08598b8b91a8c0399f6ab018c45e259f79",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Proposed method vs best baseline result on Vecmap (Accuracy P@1):\nEN-IT: 50 vs 50\nIT-EN: 42.67 vs 42.67\nEN-DE: 51.6 vs 51.47\nDE-EN: 47.22 vs 46.96\nEN-FI: 35.88 vs 36.24\nFI-EN: 39.62 vs 39.57\nEN-ES: 39.47 vs 39.30\nES-EN: 36.43 vs 36.06",
    "final_answer": "Our model significantly outperforms competitive baselines, obtaining the best published results."
  },
  {
    "id": "5112bbf13c7cf644bf401daecb5e3265889a4bfc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on the size of the data used in the experiments."
  },
  {
    "id": "03ce42ff53aa3f1775bc57e50012f6eb1998c480",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI",
    "final_answer": "unanswerable \n\nHowever, based on the information provided, the following language pairs are experimented on:\n\n- English $\\leftrightarrow $ Italian\n- English $\\leftrightarrow $ Malay (MS)\n- English $\\leftrightarrow $ English-Esperanto (EO)\n- English $\\leftrightarrow $ German (DE)\n- English $\\leftrightarrow $ Finnish (FI)\n- English $\\leftrightarrow $ Spanish (ES)"
  },
  {
    "id": "ebeedbb8eecdf118d543fdb5224ae610eef212c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT",
    "final_answer": "unanswerable \n\nThis is because the article does not provide information about current state-of-the-art methods that consider the two tasks independently. It does mention that current state-of-the-art methods are often adversarial and do not consider symmetry, and that the proposed method is a simple yet effective approach that outperforms these methods."
  },
  {
    "id": "9efd025cfa69c6ff2777528bd158f79ead9353d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "559c1307610a15427caeb8aff4d2c01ae5c9de20",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .",
    "final_answer": "The FEVER baseline system and the Enhanced Sequential Inference Model (ESIM) are compared to the Decomposable Attention baseline."
  },
  {
    "id": "4ecb6674bcb4162bf71aea8d8b82759255875df3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BIBREF5",
    "final_answer": "A specific transformer network released by OpenAI, pre-trained for language modeling on the BookCorpus dataset."
  },
  {
    "id": "eacc1eb65daad055df934e0e878f417b73b2ecc1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem",
    "final_answer": "The FEVER task is a large-scale challenge that tests a combination of retrieval and textual entailment capabilities."
  },
  {
    "id": "d353a6bbdc66be9298494d0c853e0d8d752dec4b",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)",
    "final_answer": "unanswerable"
  },
  {
    "id": "e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "22c36082b00f677e054f0f0395ed685808965a02",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)",
    "final_answer": "The English$\\rightarrow $Italian/German portions of the MuST-C corpus, along with additional public and proprietary data."
  },
  {
    "id": "90bc60320584ebba11af980ed92a309f0c1b5507",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).",
    "final_answer": "They enrich the positional embedding with length information by computing the distance from every position to the end of the sentence."
  },
  {
    "id": "f52b2ca49d98a37a6949288ec5f281a3217e5ae8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",
    "final_answer": "They condition the output to a given target-source class by prepending a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$)."
  },
  {
    "id": "228425783a4830e576fb98696f76f4c7c0a1b906",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "two translation directions (En-It and En-De)",
    "final_answer": "English, Italian, and German."
  },
  {
    "id": "9d1135303212356f3420ed010dcbe58203cc7db4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)",
    "final_answer": "The MuST-C corpus, along with additional data for English-Italian and English-German."
  },
  {
    "id": "d8bf4a29c7af213a9a176eb1503ec97d01cc8f51",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "73abb173a3cc973ab229511cf53b426865a2738b",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model",
    "final_answer": "The proposed approach is compared against several state-of-the-art models, including the Collective Matrix Factorization model, the Maximum Entropy (MaxEnt) model, and a deep neural network (DNN) architecture, as well as four baseline trackers provided by the DSTC organisers."
  },
  {
    "id": "1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "unanswerable"
  },
  {
    "id": "093039f974805952636c19c12af3549aa422ec43",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "It uses deep learning framework (pytorch)",
    "final_answer": "The library is framework agnostic, but specifically integrates with PyTorch and TensorFlow."
  },
  {
    "id": "8df89988adff57279db10992846728ec4f500eaa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient",
    "final_answer": "The self-critical baseline."
  },
  {
    "id": "94edac71eea1e78add678fb5ed2d08526b51016b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations",
    "final_answer": "Parallel Scan Inference, Vectorized Parsing, and Semiring Matrix Operations."
  },
  {
    "id": "9c4ed8ca59ba6d240f031393b01f634a9dc3615d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "VecMap, Muse, Barista",
    "final_answer": "The Sent baseline, which assumes all targets in each sentence to be of the same polarity."
  },
  {
    "id": "ca7e71131219252d1fab69865804b8f89a2c0a8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.",
    "final_answer": "unanswerable"
  },
  {
    "id": "d77c9ede2727c28e0b5a240b2521fd49a19442e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "word embeddings",
    "final_answer": "The input representation of OpenIE tuples into the model is the concatenation of word embedding and another embedding indicating whether this word is a predicate: $\\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]$."
  },
  {
    "id": "a9610cbcca813f4376fbfbf21cc14689c7fbd677",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
    "final_answer": "The VIST dataset consists of 40,071 training, 4,988 validation, and 5,050 usable testing stories."
  },
  {
    "id": "64ab2b92e986e0b5058bf4f1758e849f6a41168b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The FHVAE-based disentangled speech representation learning is more prominent than speaker adversarial training in improving the robustness of unsupervised feature learning towards speaker variation."
  },
  {
    "id": "bcd6befa65cab3ffa6334c8ecedd065a4161028b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect",
    "final_answer": "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect."
  },
  {
    "id": "479fc9e6d6d80e69f425d9e82e618e6b7cd12764",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "intra-sequential and intra-word",
    "final_answer": "Intra-sentential and intra-word code-mixed puns."
  },
  {
    "id": "bc26eee4ef1c8eff2ab8114a319901695d044edb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction",
    "final_answer": "The dialogue is guided through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world, and the history."
  },
  {
    "id": "9c94ff8c99d3e51c256f2db78c34b2361f26b9c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard.",
    "final_answer": "The semiguided dialogue refers to a type of dialogue where the Wizard is provided with a list of valid and relevant dialogue task actions, but still has some freedom to make choices and interact with the Operator. \n\nIn this type of dialogue, the Wizard interface is guided, as it provides possible actions on the right-hand side of the browser window, but the Wizard still has the option to type a message freely or select from predefined messages."
  },
  {
    "id": "8e9de181fa7d96df9686d0eb2a5c43841e6400fa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",
    "final_answer": "The article mentions that the CRWIZ framework has been used for a data collection and the results are compared to a similar dataset collected in a more controlled lab setting with a single Wizard."
  },
  {
    "id": "ff1595a388769c6429423a75b6e1734ef88d3e46",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.",
    "final_answer": "The CRWIZ Intelligent Wizard Interface ensures compliance with procedures by providing a restricted list of valid and relevant dialogue task actions that changes dynamically based on the context."
  },
  {
    "id": "dd2046f5481f11b7639a230e8ca92904da75feed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "maximum of two scores assigned by the two separate models, average score",
    "final_answer": "They combine the models using two strategies: Max Score Ensemble and Average Score Ensemble."
  },
  {
    "id": "47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Logistic regression model with character-level n-gram features",
    "final_answer": "The logistic regression model with character-level n-gram features."
  },
  {
    "id": "569ad21441e99ae782d325d5f5e1ac19e08d5e76",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "title of the news article, screen name of the user",
    "final_answer": "The context used includes user screen names, comments in the same thread, and the news article the comment is written for."
  },
  {
    "id": "90741b227b25c42e0b81a08c279b94598a25119d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation",
    "final_answer": "The language that explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation."
  },
  {
    "id": "1d739bb8e5d887fdfd1f4b6e39c57695c042fa25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "three parallel LSTM BIBREF21 layers",
    "final_answer": "The neural network model mainly consists of three parallel LSTM layers with three different inputs."
  },
  {
    "id": "5c70fdd3d6b67031768d3e28336942e49bf9a500",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages",
    "final_answer": "The model consumes human interaction through the Storyline Planner and Story Writer modules, which process user input, edit and re-generate text, and guide generation by providing a topic and tweaking decoding parameters to control novelty."
  },
  {
    "id": "f27502c3ece9ade265389d5ace90ca9ca42b46f3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "separate set of Turkers to rate the stories for overall quality and the three improvement areas",
    "final_answer": "They evaluate generated stories through self-reported engagement, satisfaction, and perception of story quality from Mechanical Turk workers, as well as independent ratings from a separate set of Turkers for overall quality and specific improvement areas."
  },
  {
    "id": "ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "aa4b38f601cc87bf93849245d5f65124da3dc112",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Title-to-Story system",
    "final_answer": "The turn-taking baseline that mimics the interaction of previous work, and the Title-to-Story system, which generates directly from topic."
  },
  {
    "id": "08b87a90139968095433f27fc88f571d939cd433",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12",
    "final_answer": "The spelling features described in BIBREF12."
  },
  {
    "id": "ef872807cb0c9974d18bbb886a7836e793727c3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.",
    "final_answer": "The contextual features used are the frequency of the token in the whole corpus, the frequency of contextual keywords from the windowed portions of the texts centering on the token in the whole corpus, and the size of the window."
  },
  {
    "id": "4db3c2ca6ddc87209c31b20763b7a3c1c33387bc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018",
    "final_answer": "crawled online from 35 cybersecurity blogs published from 2001 to 2018."
  },
  {
    "id": "63337fd803f6fdd060ebd0f53f9de79d451810cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "human-crafted features that heavily rely on specific language knowledge such as dependency structure."
  },
  {
    "id": "63496705fff20c55d4b3d8cdf4786f93e742dd3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "7b44bee49b7cb39cb7d5eec79af5773178c27d4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "action": "keep",
    "gold_answer": "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",
    "final_answer": "The data in RAFAEL is labelled manually by assigning information to each question."
  },
  {
    "id": "6d54bad91b6ccd1108d1ddbff1d217c6806e0842",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "only the first word sense (usually the most common) is taken into account",
    "final_answer": "They take only the first word sense (usually the most common) into account."
  },
  {
    "id": "238ec3c1e1093ce2f5122ee60209b969f7669fae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.\n4) Computing the mean m and the sample variance σ for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",
    "final_answer": "It is measured using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor."
  },
  {
    "id": "f704d182c9e01a2002381b76bf21e4bb3c0d3efc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of various transfer learning techniques to determine which one yields the best performance. However, it does mention that mere fine-tuning reaches state-of-the-art results for factoid questions."
  },
  {
    "id": "da544015511e535503dee2eaf4912a5e36c806cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14",
    "final_answer": "The article does not explicitly describe the architecture of the model, but it mentions the use of a sequence-to-sequence Spanish-English ST model and Nonnegative Matrix Factorization (NMF) for topic modeling."
  },
  {
    "id": "7bc993b32484d6ae3c86d0b351a68e59fd2757a5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Spanish",
    "final_answer": "Spanish."
  },
  {
    "id": "da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "LDC corpus",
    "final_answer": "unanswerable \n\nHowever, based on the article, it can be inferred that the vocabulary is derived from the training corpus, as the article mentions that the word frequency is ranked from greatest to least in the training corpus to determine the segmentation point for the hybrid word-characters level."
  },
  {
    "id": "e44a5514d7464993997212341606c2c0f3a72eb4",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "Character level."
  },
  {
    "id": "310e61b9dd4d75bc1bebbcb1dae578f55807cd04",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)",
    "final_answer": "They used the LDC corpus for training data, and the NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as test sets for Chinese-to-English translation, and the NIST 2003(MT03) dataset as the validation set, and the NIST 2008(MT08) as test set for English-to-Chinese translation."
  },
  {
    "id": "bdc6664cec2b94b0b3769bc70a60914795f39574",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values",
    "final_answer": "The classic information retrieval metrics of precision (P), recall (R), and F1 are used to measure the correctness of the answers given by participants in the DQA experiments and the QALD system."
  },
  {
    "id": "e40df8c685a28b98006c47808f506def68f30e26",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "Yes."
  },
  {
    "id": "9653c89a93ac5c717a0a26cf80e9aa98a5ccf910",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8",
    "final_answer": "gAnswer, QAKiS, Platypus, and WDAqua."
  },
  {
    "id": "b921a1771ed0ba9dbeff9da000336ecf2bb38322",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable."
  },
  {
    "id": "412aff0b2113b7d61c914edf90b90f2994390088",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "010e3793eb1342225857d3f95e147d8f8467192a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains.",
    "final_answer": "The Europarl dataset consists of 70,057 dat-labeled and 33,814 die-labeled sentences, while the SoNaR dataset has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled sentences."
  },
  {
    "id": "c20bb0847ced490a793657fbaf6afb5ef54dad81",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "To account for multiple-testing, Bonferroni correction was applied, so the researchers drew a random sample of 503 songs."
  },
  {
    "id": "ff8557d93704120b65d9b597a4fab40b49d24b6d",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "action": "route",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "447eb98e602616c01187960c9c3011c62afd7c27",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Table TABREF10 displays the twenty resulting topics",
    "final_answer": "Topics typically ascribed to metal lyrics include sadness, death, freedom, nature, occultism, unpleasant/disgusting objects, harsh, gloomy, dystopian, or satanic themes."
  },
  {
    "id": "f398587b9a0008628278a5ea858e01d3f5559f65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "SPNet vs best baseline:\nROUGE-1: 90.97 vs 90.68\nCIC: 70.45 vs 70.25",
    "final_answer": "SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics with a big margin."
  },
  {
    "id": "d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair",
    "final_answer": "ROUGE, ROUGE-1, ROUGE-2, ROUGE-L, Critical Information Completeness (CIC), relevance, conciseness, and readability are used for automatic and human evaluation metrics to compare SPNet to its counterparts."
  },
  {
    "id": "58f3bfbd01ba9768172be45a819faaa0de2ddfa4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on whether the proposed abstractive dialog summarization dataset is open source."
  },
  {
    "id": "73633afbefa191b36cca594977204c6511f9dad4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Not at the moment, but summaries can be additionaly extended with this annotations.",
    "final_answer": "The answer is partially, as speaker role scaffold is natural, but semantic slot scaffold is seldom explicitly annotated."
  },
  {
    "id": "db39a71080e323ba2ddf958f93778e2b875dcd24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework.",
    "final_answer": "SPNet utilizes additional speaker role, semantic slot, and dialog domain annotations through separate encoding for different speakers, delexicalization and slot filling, and multi-task learning with domain classification."
  },
  {
    "id": "6da2cb3187d3f28b75ac0a61f6562a8adf716109",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Pointer-Generator, Transformer",
    "final_answer": "Pointer-Generator and Transformer, which achieved high-quality summaries of news documents."
  },
  {
    "id": "c47e87efab11f661993a14cf2d7506be641375e4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities",
    "final_answer": "The new evaluation metric, CIC (Critical Information Completeness), considers critical informative entities by counting the recall of semantic slot information between a candidate summary and a reference summary."
  },
  {
    "id": "14684ad200915ff1e3fc2a89cb614e472a1a2854",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes"
  },
  {
    "id": "8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself).",
    "final_answer": "The annotators were educated to rank results by their coverage, fluency, and plausibility in daily life."
  },
  {
    "id": "5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore",
    "final_answer": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore are used for this task."
  },
  {
    "id": "c34a15f1d113083da431e4157aceb11266e9a1b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "no \n\nThe article mentions that rationales are provided as an optional resource, and the models are not required to generate them."
  },
  {
    "id": "061682beb3dbd7c76cfa26f7ae650e548503d977",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "no \n\nThe article states that the annotators are required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes, implying that the rationales are generated simultaneously with the sentences."
  },
  {
    "id": "3518d8eb84f6228407cfabaf509fd63d60351203",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "No, the crowd-sourced sentences were created by annotators who were given the concept-sets as the only signal, not shown the associated images or videos."
  },
  {
    "id": "617c77a600be5529b3391ab0c21504cd288cc7c7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "These concept-sets are sampled from several large corpora of image/video captions",
    "final_answer": "They are sampled from several large corpora of image/video captions."
  },
  {
    "id": "53d6cbee3606dd106494e2e98aa93fdd95920375",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "test accuracy of 88.9%, which exceeds the previous best by 16.9%",
    "final_answer": "at least 16% in absolute accuracy for all the MCQA datasets (except the SemEval dataset)."
  },
  {
    "id": "9dc844f82f520daf986e83466de0c84d93953754",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "MultiNLI BIBREF15 and SNLI BIBREF16 ",
    "final_answer": "MultiNLI and SNLI datasets."
  },
  {
    "id": "9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "FTLM++, BERT-large, XLNet",
    "final_answer": "Previous SOTA models, as well as RoBERTa-Large models."
  },
  {
    "id": "36d892460eb863220cd0881d5823d73bbfda172c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11",
    "final_answer": "The four representative MCQA datasets used for benchmark are DREAM, MCTest, TOEFL, and SemEval-2018 Task 11."
  },
  {
    "id": "4cbc56d0d53c4c03e459ac43e3c374b75fd48efe",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "LSTM, SCIBERT",
    "final_answer": "unanswerable"
  },
  {
    "id": "e5a965e7a109ae17a42dd22eddbf167be47fca75",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Some sentences are associated to ambiguous dimensions in the hidden state output",
    "final_answer": "Ambiguity in labelled PIO data, ambiguous sentence labels, and the assignment of a single label limiting the quality of predictions."
  },
  {
    "id": "7d59374d9301a0c09ea5d023a22ceb6ce07fb490",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "by number of distinct n-grams",
    "final_answer": "They measure the diversity of inferences by using the distinct n-gram score, which is normalized to [0, 1] by dividing the total number of generated tokens."
  },
  {
    "id": "8e2b125426d1220691cceaeaf1875f76a6049cbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",
    "final_answer": "unanswerable\n\nHowever, the article does state that they \"outperform baseline methods in both the accuracy and diversity of inferences\" but it does not provide a specific percentage or numerical value of the improvement."
  },
  {
    "id": "42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained",
    "final_answer": "unanswerable \n\nHowever, based on the context it can be inferred that the baseline models used are RNN-based encoder-decoder models, but the specific models used as baselines on the Atomic dataset are not explicitly mentioned in the article."
  },
  {
    "id": "fb76e994e2e3fa129f1e94f1b043b274af8fb84c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",
    "final_answer": "By training on an auxiliary dataset with rich event background knowledge in the pretrain stage."
  },
  {
    "id": "99ef97336c0112d9f60df108f58c8b04b519a854",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "The Atomic dataset is not explicitly stated in the article, but it is mentioned that it \"scales up the size of dataset and expands the scope to nine types of inference dimensions\" compared to the Event2Mind dataset, which contains 25K base events and 300K targets."
  },
  {
    "id": "95d8368b1055d97250df38d1e8c4a2b283d2b57e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "pipeline that is used at Microsoft for production data",
    "final_answer": "a vendor-based transcription pipeline used at Microsoft for production data."
  },
  {
    "id": "a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",
    "final_answer": "Significant improvement."
  },
  {
    "id": "46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "LSTMs",
    "final_answer": "They used 2-layer LSTMs for both the source and target sides in the NMT system."
  },
  {
    "id": "4f12b41bd3bb2610abf7d7835291496aa69fb78c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Appending the domain tag “<2domain>\" to the source sentences of the respective corpora",
    "final_answer": "They appended the domain tag \"<2domain>\" to the source sentences of the respective corpora to prime the NMT decoder to generate sentences for the specific domain."
  },
  {
    "id": "65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency",
    "final_answer": "The potential for ongoing mixed-initiative multi-turn dialogues that do not follow a particular plan or pursue a particular fixed information need."
  },
  {
    "id": "b54fc86dc2cc6994e10c1819b6405de08c496c7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred",
    "final_answer": "reaction delay, which is defined as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred."
  },
  {
    "id": "b43a8a0f4b8496b23c89730f0070172cd5dca06a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.",
    "final_answer": "Their model combines a text sequence sub-network with a vector representation sub-network, which are concatenated through a \"late fusion\" approach, consisting of an embedding layer, two 1-dimensional convolution layers, a max-pooling layer, and several dense layers."
  },
  {
    "id": "b161febf86cdd58bd247a934120410068b24b7d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other",
    "final_answer": "answer, elaboration, and question, etc., as well as agreement, disagreement, appreciation, humor, and \"other\"."
  },
  {
    "id": "d40662236eed26f17dd2a3a9052a4cee1482d7d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "a vector of frame-level acoustic features",
    "final_answer": "39-dimensional MFCCs."
  },
  {
    "id": "1d791713d1aa77358f11501f05c108045f53c8aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "1061",
    "final_answer": "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061."
  },
  {
    "id": "6b6360fab2edc836901195c0aba973eae4891975",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Switchboard conversational English corpus",
    "final_answer": "The Switchboard conversational English corpus."
  },
  {
    "id": "b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Their best average precision tops previous best result by 0.202",
    "final_answer": "Sizable improvements in average precision."
  },
  {
    "id": "86a93a2d1c19cd0cd21ad1608f2a336240725700",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "interpretation of Frege's work are examples of holistic approaches to meaning",
    "final_answer": "Frege's sentence holism is analogous to the general distributional hypothesis, which states that the meaning of a word is given by the contexts in which it occurs."
  },
  {
    "id": "6090d3187c41829613abe785f0f3665d9ecd90d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Only in the context of a sentence does a word have a meaning.",
    "final_answer": "Meaning is defined as truth-value potential."
  },
  {
    "id": "117aa7811ed60e84d40cd8f9cb3ca78781935a98",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "yes"
  },
  {
    "id": "c359ab8ebef6f60c5a38f5244e8c18d85e92761d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",
    "final_answer": "unanswerable"
  },
  {
    "id": "ad362365656b0b218ba324ae60701eb25fe664c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "syntactic information, semantic and topical information",
    "final_answer": "The latent variables are refined nonterminals that provide contextual information about each node in a given derivation."
  },
  {
    "id": "423bb905e404e88a168e7e807950e24ca166306c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation",
    "final_answer": "The baselines used are GraphParser without paraphrases and a monolingual machine translation based model for paraphrase generation."
  },
  {
    "id": "e5ae8ac51946db7475bb20b96e0a22083b366a6d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThey used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries."
  },
  {
    "id": "18288c7b0f8bd7839ae92f9c293e7fb85c7e146a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "weak correlation with p-value of 0.08",
    "final_answer": "The strongest correlation among the topics was determined to be between exercise and obesity, but there was also a notable correlation between exercise and diabetes."
  },
  {
    "id": "b5e883b15e63029eb07d6ff42df703a64613a18a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "using topic modeling model Latent Dirichlet Allocation (LDA)",
    "final_answer": "Using a topic modeling approach with Latent Dirichlet Allocation (LDA) and fuzzy clustering of semantically related words."
  },
  {
    "id": "c45a160d31ca8eddbfea79907ec8e59f543aab86",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Swissmetro dataset",
    "final_answer": "The Swissmetro dataset is used for evaluation, and other datasets are also used but not specified."
  },
  {
    "id": "7358a1ce2eae380af423d4feeaa67d2bd23ae9dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The embeddings are learned several times using the training set, then the average is taken.",
    "final_answer": "They train their embeddings using a neural network model with a softmax output layer, and the embeddings are learned simultaneously with the choice model, allowing the model to accommodate each other's effects."
  },
  {
    "id": "1165fb0b400ec1c521c1aef7a4e590f76fee1279",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The data from collected travel surveys is used to model travel behavior.",
    "final_answer": "They model travel behavior using a novel method called Travel Behavior embeddings, which is inspired by natural language processing (NLP) concepts and aims to preserve semantic distance relative to a certain choice problem."
  },
  {
    "id": "f2c5da398e601e53f9f545947f61de5f40ede1ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The coefficients are projected back to the dummy variable space.",
    "final_answer": "They interpret the coefficients by projecting them back into the original dummy variable space and following the rules of normal random variables to preserve statistical significance information."
  },
  {
    "id": "2d4d0735c50749aa8087d1502ab7499faa2f0dd8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",
    "final_answer": "unanswerable \n\nHowever, the text does provide that the authors' proposed model outperforms previous state-of-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN), in terms of weighted binary classification accuracy (Wt. Acc.)."
  },
  {
    "id": "43761478c26ad65bec4f0fd511ec3181a100681c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "Yes."
  },
  {
    "id": "01866fe392d9196dda1d0b472290edbd48a99f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones",
    "final_answer": "The lexicon of trafficking flags is expanded through a data-driven approach by analyzing word embeddings learned from raw text data, particularly using a skip-gram model and t-SNE to visualize the vectors for emojis, and re-training the model periodically on new escort ads."
  },
  {
    "id": "394cf73c0aac8ccb45ce1b133f4e765e8e175403",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Yes",
    "final_answer": "yes"
  },
  {
    "id": "2c4003f25e8d95a3768204f52a7a5f5e17cb2102",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "No",
    "final_answer": "unanswerable"
  },
  {
    "id": "65e32f73357bb26a29a58596e1ac314f7e9c6c91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between “Frustrate” and “Neutralize”, distinction between the classes “Troll” and “Engage”",
    "final_answer": "A comment that seems inoffensive on the surface, such as \"your comments fit well in Stormfront\", but may have a malicious intention due to the author's background."
  },
  {
    "id": "46f175e1322d648ab2c0258a9609fe6f43d3b44e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": " inclusion of longer parts of the conversation",
    "final_answer": "The potential solutions suggested include using additional knowledge from anthologies, modifying NELL, employing deeper semantics, jointly learning classifiers, and using the temporal sequence of response comments."
  },
  {
    "id": "7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "1000 conversations composed of 6833 sentences and 88047 tokens",
    "final_answer": "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens."
  },
  {
    "id": "3fa638e6167e1c7a931c8ee5c0e2e397ec1b6cda",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable"
  },
  {
    "id": "d2b3f2178a177183b1aeb88784e48ff7e3e5070c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": " between 0.81 and 0.88",
    "final_answer": "Strong."
  },
  {
    "id": "d5ff8fc4d3996db2c96cb8af5a6d215484991e62",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments",
    "final_answer": "The novel method (DBCA) achieves higher compound divergence at a similar atom divergence compared to other approaches."
  },
  {
    "id": "d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets",
    "final_answer": "The authors justify that the question answering dataset presented is realistic by stating that the questions are generated automatically using a rule-based system, and the generation rules are designed to have few and meaningful atoms, which yields a large variety of compounds, making the examples come from composing them, and by filtering out unnatural questions using semantic and structural rules."
  },
  {
    "id": "0427ca83d6bf4ec113bc6fec484b2578714ae8ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "LSTM+attention, Transformer , Universal Transformer",
    "final_answer": "The three machine architectures analyzed are not explicitly mentioned in the provided snippet."
  },
  {
    "id": "f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "239,357 English question-answer pairs",
    "final_answer": "239,357 English question-answer pairs."
  },
  {
    "id": "8db45a8217f6be30c31f9b9a3146bf267de68389",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "random , Output length, Input length, Output pattern, Input pattern",
    "final_answer": "unanswerable (the article does not provide a comprehensive list of other approaches to creating compositional generalization benchmarks)."
  },
  {
    "id": "4e379d6d5f87554fabf6f7f7b6ed92d2025e7280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "CSKS task",
    "final_answer": "Low training data imbalanced classification task in Continuous Speech Keyword Spotting."
  },
  {
    "id": "518d0847b02b4f23a8f441faa38b935c9b892e1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Honk, DeepSpeech-finetune",
    "final_answer": "Honk and DeepSpeech-finetune."
  },
  {
    "id": "8112d18681e266426cf7432ac4928b87f5ce8311",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English, Hindi",
    "final_answer": "English and Hindi."
  },
  {
    "id": "b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the model has \"almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear\" which implies that the model's training time might be different from state-of-the-art models."
  },
  {
    "id": "ba6422e22297c7eb0baa381225a2f146b9621791",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Difference is around 1 BLEU score lower on average than state of the art methods.",
    "final_answer": "The proposed method achieves competitive performance with slight degradation compared to state-of-the-art contemporaneous work on both WMT2014 and WMT2016 corpora."
  },
  {
    "id": "65e72ad72a9cbfc379f126b10b0ce80cfe44579b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "action": "keep",
    "gold_answer": "NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM",
    "final_answer": "unanswerable \n\nHowever, the article does mention that the performance of a simple non-autoregressive model falls behind autoregressive models, and it compares FlowSeq with other non-autoregressive models, but it does not specify what those models are."
  },
  {
    "id": "cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "WMT2014, WMT2016 and IWSLT-2014",
    "final_answer": "WMT2014 DE-EN, WMT2016 RO-EN, and IWSLT2014 DE-EN."
  },
  {
    "id": "04aff4add28e6343634d342db92b3ac36aa8c255",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings",
    "final_answer": "The visual attention is very sparse, with just one source encoding being attended to."
  },
  {
    "id": "a8e4522ce2ce7336e731286654d6ad0931927a4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT",
    "final_answer": "The results of their Principal Component Analysis (PCA) show that the visual features in the How2 dataset are much more dominated by \"common\" dimensions, which makes them less discriminative among individual sentences and further aggravates the problem at the token level."
  },
  {
    "id": "bd7039f81a5417474efa36f703ebddcf51835254",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Reasoner model, also implemented with the MatchLSTM architecture, Ranker model",
    "final_answer": "The two models' architectures in the proposed solution are the Ranker and the Reasoner, both implemented with the MatchLSTM architecture."
  },
  {
    "id": "022e5c996a72aeab890401a7fdb925ecd0570529",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards",
    "final_answer": "The two models, Ranker and Reasoner, cooperate in a cooperative game, where the Ranker selects passages and the Reasoner predicts the linking entity, with each model providing rewards to the other to improve the selection process."
  },
  {
    "id": "2a950ede24b26a45613169348d5db9176fda4f82",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable."
  },
  {
    "id": "34af2c512ec38483754e94e1ea814aa76552d60a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples",
    "final_answer": "HotpotQA and MedHop datasets."
  },
  {
    "id": "c1429f7fed5a4dda11ac7d9643f97af87a83508b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation",
    "final_answer": "They reference various empirical investigations, including BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF26, BIBREF27, BIBREF28, and BIBREF29, among others."
  },
  {
    "id": "a93d4aa89ac3abbd08d725f3765c4f1bed35c889",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "action": "keep",
    "gold_answer": "English , Chinese ",
    "final_answer": "Chinese to English and English to Czech, but mainly Chinese to English."
  }
]