[
  {
    "id": "753990d0b621d390ed58f20c4d9e4f065f0dc672",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a vocabulary of positive and negative predicates that helps determine the polarity score of an event",
    "final_answer": "A seed lexicon consisting of 15 positive words and 15 negative words.",
    "slm_pred": "A seed lexicon consisting of 15 positive words and 15 negative words.",
    "llm_pred": "a small list of 30 Japanese emotion predicates—15 positive and 15 negative (e.g., 喜ぶ, 嬉しい, 楽しい… and 怒る, 悲しい, 寂しい…)—used as seed words",
    "perf_chosen": 0.3571428571,
    "perf_slm": 0.3571428571,
    "perf_llm": 0.25641025640000004,
    "cost": 5.0
  },
  {
    "id": "753990d0b621d390ed58f20c4d9e4f065f0dc672",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "seed lexicon consists of positive and negative predicates",
    "final_answer": "A seed lexicon consisting of 15 positive words and 15 negative words.",
    "slm_pred": "A seed lexicon consisting of 15 positive words and 15 negative words.",
    "llm_pred": "a list of 15 positive and 15 negative Japanese emotion predicates used as seeds (e.g., 喜ぶ/嬉しい … and 怒る/悲しい …)",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.3571428571,
    "cost": 5.0
  },
  {
    "id": "9d578ddccc27dd849244d632dd0f6bf27348ad81",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",
    "final_answer": "The proposed method performed well, even with a minimal amount of supervision, and achieved high accuracy, with the BiGRU encoder trained with the AL+CA+CO dataset obtaining the highest score.",
    "slm_pred": "The proposed method performed well, even with a minimal amount of supervision, and achieved high accuracy, with the BiGRU encoder trained with the AL+CA+CO dataset obtaining the highest score.",
    "llm_pred": "BiGRU trained with AL+CA+CO achieved the highest accuracy; discourse-based label propagation outperformed random, approached supervised performance, BERT was competitive but noise-sensitive, and the method was especially effective when labeled data were small.",
    "perf_chosen": 0.0900900901,
    "perf_slm": 0.0900900901,
    "perf_llm": 0.1052631579,
    "cost": 5.0
  },
  {
    "id": "02e4bf719b1a504e385c35c6186742e720bcb281",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",
    "final_answer": "The answer is through the exploitation of discourse relations, specifically Cause and Concession.",
    "slm_pred": "The answer is through the exploitation of discourse relations, specifically Cause and Concession.",
    "llm_pred": "By exploiting discourse relations: for Cause pairs, the former event inherits the latter’s polarity; for Concession pairs, the former takes the opposite polarity.",
    "perf_chosen": 0.1764705882,
    "perf_slm": 0.1764705882,
    "perf_llm": 0.3181818182,
    "cost": 5.0
  },
  {
    "id": "02e4bf719b1a504e385c35c6186742e720bcb281",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity",
    "final_answer": "The relations are used to propagate polarity through label propagation, where the polarity of an event is determined based on its discourse relation with another event.",
    "slm_pred": "The relations are used to propagate polarity through label propagation, where the polarity of an event is determined based on its discourse relation with another event.",
    "llm_pred": "By using discourse relations between event pairs: for Cause, the former event takes the same polarity as the latter; for Concession, it takes the opposite polarity.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.3555555556,
    "cost": 5.0
  },
  {
    "id": "44c4bd6decc86f1091b5fc0728873d9324cdde4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",
    "final_answer": "The Japanese corpus used in the experiment contained about 100 million sentences.",
    "slm_pred": "The Japanese corpus used in the experiment contained about 100 million sentences.",
    "llm_pred": "about 100 million sentences",
    "perf_chosen": 0.24242424240000002,
    "perf_slm": 0.24242424240000002,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "44c4bd6decc86f1091b5fc0728873d9324cdde4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The ACP corpus has around 700k events split into positive and negative polarity ",
    "final_answer": "The Japanese data consists of a corpus of about 100 million sentences.",
    "slm_pred": "The Japanese data consists of a corpus of about 100 million sentences.",
    "llm_pred": "about 100 million sentences",
    "perf_chosen": 0.16,
    "perf_slm": 0.16,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "86abeff85f3db79cf87a8c993e5e5aa61226dc98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "negative, positive",
    "final_answer": "The labels available in the dataset for supervision are positive and negative labels, as shown in Table TABREF19.",
    "slm_pred": "The labels available in the dataset for supervision are positive and negative labels, as shown in Table TABREF19.",
    "llm_pred": "positive and negative (event polarity) labels",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "c029deb7f99756d2669abad0a349d917428e9c12",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3%",
    "final_answer": "The results show a significant gain over a purely supervised baseline when labeled data were small.",
    "slm_pred": "The results show a significant gain over a purely supervised baseline when labeled data were small.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "39f8db10d949c6b477fa4b51e7c184016505884f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity",
    "final_answer": "The model learns using mostly raw data by exploiting discourse relation-based label propagation from a small seed lexicon and a large raw corpus.",
    "slm_pred": "The model learns using mostly raw data by exploiting discourse relation-based label propagation from a small seed lexicon and a large raw corpus.",
    "llm_pred": "By propagating sentiment labels through discourse-linked event pairs (Cause = same polarity, Concession = reversed) using a small seed lexicon, turning raw text with these relations into training signals.",
    "perf_chosen": 0.2702702703,
    "perf_slm": 0.2702702703,
    "perf_llm": 0.24390243900000003,
    "cost": 5.0
  },
  {
    "id": "d0bc782961567dc1dd7e074b621a6d6be44bb5b4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "30 words",
    "final_answer": "15 positive words and 15 negative words.",
    "slm_pred": "15 positive words and 15 negative words.",
    "llm_pred": "30 words",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a592498ba2fac994cd6fad7372836f0adb37e22a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "100 million sentences",
    "final_answer": "about 100 million sentences.",
    "slm_pred": "about 100 million sentences.",
    "llm_pred": "about 100 million sentences",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "3a9d391d25cde8af3334ac62d478b36b30079d74",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3a9d391d25cde8af3334ac62d478b36b30079d74",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8d8300d88283c73424c8f301ad9fdd733845eb47",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "confusion matrices of labels between annotators",
    "final_answer": "The annotation experiment is evaluated through agreement statistics, including heatmaps, Cohen's kappa scores, and the use of bootstrap resampling to compare the agreement of different annotator groups.",
    "slm_pred": "The annotation experiment is evaluated through agreement statistics, including heatmaps, Cohen's kappa scores, and the use of bootstrap resampling to compare the agreement of different annotator groups.",
    "llm_pred": "By bootstrap resampling two groups of five annotators per stanza and comparing their labels using Cohen’s kappa across varying selection thresholds.",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "48b12eb53e2d507343f19b8a667696a39b719807",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”",
    "final_answer": "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.",
    "slm_pred": "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia.",
    "llm_pred": "Beauty/Joy, Sadness, Uneasiness, Vitality, Suspense, Awe/Sublime, Humor, Annoyance, and Nostalgia",
    "perf_chosen": 0.0416666667,
    "perf_slm": 0.0416666667,
    "perf_llm": 0.0416666667,
    "cost": 5.0
  },
  {
    "id": "003f884d3893532f8c302431c9f70be6f64d9be8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, implying that their results are based on English data.",
    "slm_pred": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, implying that their results are based on English data.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "003f884d3893532f8c302431c9f70be6f64d9be8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, resulting in 283 English-speaking communities for their analysis.",
    "slm_pred": "yes \n\nThey manually remove communities where the bulk of the contributions are in a foreign language, resulting in 283 English-speaking communities for their analysis.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n",
    "final_answer": "They manifest in systematically varying patterns across different types of communities.",
    "slm_pred": "They manifest in systematically varying patterns across different types of communities.",
    "llm_pred": "They vary systematically by community identity: specialized, dynamic communities show higher retention but larger newcomer–insider (acculturation) gaps, with veterans engaging more in community‑specific and volatile content, whereas in generic communities outsiders engage more with volatile content and retention is lower.",
    "perf_chosen": 0.06315789470000001,
    "perf_slm": 0.06315789470000001,
    "perf_llm": 0.25806451610000003,
    "cost": 5.0
  },
  {
    "id": "eea089baedc0ce80731c8fdcb064b82f584f483a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ",
    "final_answer": "They observe that user engagement varies systematically with a community's distinctiveness and dynamicity, with dynamic communities exhibiting higher rates of user retention and engagement, and distinctive communities fostering strong, focused interest from users at one particular point in time.",
    "slm_pred": "They observe that user engagement varies systematically with a community's distinctiveness and dynamicity, with dynamic communities exhibiting higher rates of user retention and engagement, and distinctive communities fostering strong, focused interest from users at one particular point in time.",
    "llm_pred": "User engagement is highest in dynamic (constantly updating) communities, with distinctiveness boosting short‑term retention but not long‑term; niche, dynamic groups retain users better yet show larger newcomer–veteran linguistic gaps, while in generic communities outsiders engage more with volatile content than established members.",
    "perf_chosen": 0.2891566265,
    "perf_slm": 0.2891566265,
    "perf_llm": 0.3255813953,
    "cost": 5.0
  },
  {
    "id": "edb2d24d6d10af13931b3a47a6543bd469752f0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.",
    "final_answer": "They were selected from a dataset of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate their measures, in at least 4 months of the subreddit's history.",
    "slm_pred": "They were selected from a dataset of all subreddits on Reddit from January 2013 to December 2014, with at least 500 words in the vocabulary used to estimate their measures, in at least 4 months of the subreddit's history.",
    "llm_pred": "They included all 2013–2014 subreddits with ≥500 vocabulary words in at least 4 months and manually excluded non‑English ones, yielding about 300 (283) communities.",
    "perf_chosen": 0.6913580247000001,
    "perf_slm": 0.6913580247000001,
    "perf_llm": 0.3939393939,
    "cost": 5.0
  },
  {
    "id": "edb2d24d6d10af13931b3a47a6543bd469752f0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They collect subreddits from January 2013 to December 2014,2 for which there are at\nleast 500 words in the vocabulary used to estimate the measures,\nin at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.",
    "final_answer": "They selected the 300 Reddit communities for comparison from all subreddits on Reddit from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of activity.",
    "slm_pred": "They selected the 300 Reddit communities for comparison from all subreddits on Reddit from January 2013 to December 2014, excluding communities with less than 500 words in their vocabulary and less than 4 months of activity.",
    "llm_pred": "By including all subreddits (2013–2014) with enough English data—at least 500 vocabulary words in at least four months—and removing foreign-language communities, yielding about 300 (283) subreddits.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.20408163270000002,
    "cost": 5.0
  },
  {
    "id": "938cf30c4f1d14fa182e82919e16072fdbcf2a82",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the average volatility of all utterances",
    "final_answer": "By a language volatility metric: PMI comparing a word’s frequency in a given month of a community to its frequency over that community’s entire history, aggregated to the community level.",
    "slm_pred": "The authors measure a community's temporal dynamicity by computing the pointwise mutual information (PMI) of words in a community relative to its entire history, indicating how frequently a word occurs more in a small window of time than in the entire history, which they call \"volatility.\"",
    "llm_pred": "By a language volatility metric: PMI comparing a word’s frequency in a given month of a community to its frequency over that community’s entire history, aggregated to the community level.",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1153846154,
    "perf_llm": 0.1666666667,
    "cost": 25.0
  },
  {
    "id": "93f4ad6568207c9bd10d712a52f8de25b3ebadd4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " the average specificity of all utterances",
    "final_answer": "The authors measure how distinctive a community is by calculating the average specificity of all utterances in the community, using a framework based on pointwise mutual information (PMI).",
    "slm_pred": "The authors measure how distinctive a community is by calculating the average specificity of all utterances in the community, using a framework based on pointwise mutual information (PMI).",
    "llm_pred": "by the average specificity of its language, computed via PMI comparing word frequencies in the community to those in all communities",
    "perf_chosen": 0.3529411765,
    "perf_slm": 0.3529411765,
    "perf_llm": 0.37037037040000004,
    "cost": 5.0
  },
  {
    "id": "71a7153e12879defa186bfb6dbafe79c74265e10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Chinese general corpus",
    "final_answer": "Chinese general corpus.",
    "slm_pred": "Chinese general corpus.",
    "llm_pred": "the Chinese general corpus (Google-pretrained)",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "71a7153e12879defa186bfb6dbafe79c74265e10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "Chinese general corpus.",
    "slm_pred": "Chinese general corpus.",
    "llm_pred": "Chinese general corpus",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "85d1831c28d3c19c84472589a252e28e9884500f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BERT-Base, QANet",
    "final_answer": "BERT-Base and QANet.",
    "slm_pred": "BERT-Base and QANet.",
    "llm_pred": "QANet and BERT-Base",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "85d1831c28d3c19c84472589a252e28e9884500f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "QANet BIBREF39, BERT-Base BIBREF26",
    "final_answer": "BERT-Base and QANet.",
    "slm_pred": "BERT-Base and QANet.",
    "llm_pred": "QANet and BERT-Base.",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "1959e0ebc21fafdf1dd20c6ea054161ba7446f61",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ",
    "final_answer": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is a result of the query term according to the paragraph text.",
    "slm_pred": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is a result of the query term according to the paragraph text.",
    "llm_pred": "Extracting or generating a key–value pair from a paragraph, where the key is a query term and the value is the corresponding result derived from the text.",
    "perf_chosen": 0.36666666670000003,
    "perf_slm": 0.36666666670000003,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "1959e0ebc21fafdf1dd20c6ea054161ba7446f61",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.",
    "final_answer": "Clinical text structuring is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text.",
    "slm_pred": "Clinical text structuring is defined as extracting or generating a key-value pair from a sequence of paragraph text, where the key is a query term and the value is the result of the query term according to the paragraph text.",
    "llm_pred": "Extracting or generating a key–value pair from the paragraph text, where the key is a query term and the value is its result derived from the text.",
    "perf_chosen": 0.1875,
    "perf_slm": 0.1875,
    "perf_llm": 0.23529411760000002,
    "cost": 5.0
  },
  {
    "id": "77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " three types of questions, namely tumor size, proximal resection margin and distal resection margin",
    "final_answer": "Clinical text structuring tasks.",
    "slm_pred": "Clinical text structuring tasks.",
    "llm_pred": "tumor size, proximal resection margin, and distal resection margin",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.7826086957,
    "cost": 5.0
  },
  {
    "id": "77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The specific tasks being unified are different clinical text structuring tasks.",
    "slm_pred": "The specific tasks being unified are different clinical text structuring tasks.",
    "llm_pred": "tumor size extraction, proximal resection margin extraction, and distal resection margin extraction",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "06095a4dee77e9a570837b35fc38e77228664f91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ",
    "final_answer": "There are unrelated sentences in between questions.",
    "slm_pred": "There are unrelated sentences in between questions.",
    "llm_pred": "There are unrelated sentences between questions.",
    "perf_chosen": 0.1290322581,
    "perf_slm": 0.1290322581,
    "perf_llm": 0.1333333333,
    "cost": 5.0
  },
  {
    "id": "19c9cfbc4f29104200393e848b7b9be41913a7ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "2,714 ",
    "final_answer": "2,714 question-answer pairs.",
    "slm_pred": "2,714 question-answer pairs.",
    "llm_pred": "2,714",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6743c1dd7764fc652cfe2ea29097ea09b5544bc3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The tasks evaluated are clinical text structuring (CTS) and question answering based clinical text structuring (QA-CTS).",
    "slm_pred": "The tasks evaluated are clinical text structuring (CTS) and question answering based clinical text structuring (QA-CTS).",
    "llm_pred": "Tumor size, proximal resection margin, and distal resection margin.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "14323046220b2aea8f15fba86819cbccc389ed8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable\n\nHowever, the article mentions that labeling necessary amount of data for training neural network requires expensive labor cost, implying that there may be concerns about data privacy and security, but it does not explicitly address the issue.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0512820513,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "integrate clinical named entity information into pre-trained language model",
    "final_answer": "They introduce clinical named entity information into the model.",
    "slm_pred": "They introduce clinical named entity information into the model.",
    "llm_pred": "By integrating clinical named‑entity information (one‑hot CNER tag sequences for text and query) into BERT’s contextual representations, e.g., via concatenation.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.20689655170000001,
    "cost": 5.0
  },
  {
    "id": "975a4ac9773a4af551142c324b64a0858670d06e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs",
    "final_answer": "The QA-CTS task dataset contains 17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
    "slm_pred": "The QA-CTS task dataset contains 17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
    "llm_pred": "2,714 question–answer pairs.",
    "perf_chosen": 0.7619047619,
    "perf_slm": 0.7619047619,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "326e08a0f5753b90622902bd4a9c94849a24b773",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs",
    "final_answer": "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
    "slm_pred": "17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.",
    "llm_pred": "17,833 sentences",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "bd78483a746fda4805a7678286f82d9621bc45cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26",
    "final_answer": "QANet and BERT-Base.",
    "slm_pred": "QANet and BERT-Base.",
    "llm_pred": "QANet and BERT-Base.",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.4615384615,
    "cost": 25.0
  },
  {
    "id": "dd155f01f6f4a14f9d25afc97504aefdc6d29c13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",
    "final_answer": "Word-level perplexity, R@3 in next-word prediction, latency, and energy usage.",
    "slm_pred": "Word-level perplexity, R@3 in next-word prediction, latency, and energy usage.",
    "llm_pred": "perplexity, R@3 (next-word prediction), latency, and energy usage",
    "perf_chosen": 0.4166666667,
    "perf_slm": 0.4166666667,
    "perf_llm": 0.4545454545,
    "cost": 5.0
  },
  {
    "id": "a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Kneser–Ney smoothing",
    "final_answer": "Kneser-Ney smoothing and Skip-LSTM are mentioned as classic language models.",
    "slm_pred": "Kneser-Ney smoothing and Skip-LSTM are mentioned as classic language models.",
    "llm_pred": "the modified Kneser–Ney smoothed n-gram model (specifically a 5-gram, KN-5)",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "e07df8f613dbd567a35318cd6f6f4cb959f5c82d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "perplexity",
    "final_answer": "Perplexity and log perplexity are commonly used evaluation metrics for language models, with log perplexity being more human-understandable in terms of recall.",
    "slm_pred": "Perplexity and log perplexity are commonly used evaluation metrics for language models, with log perplexity being more human-understandable in terms of recall.",
    "llm_pred": "perplexity",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0869565217,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e07df8f613dbd567a35318cd6f6f4cb959f5c82d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "perplexity",
    "final_answer": "Perplexity is a commonly used evaluation metric for language models, but the article also mentions that log perplexity provides a more human-understandable indicator of R@3.",
    "slm_pred": "Perplexity is a commonly used evaluation metric for language models, but the article also mentions that log perplexity provides a more human-understandable indicator of R@3.",
    "llm_pred": "perplexity",
    "perf_chosen": 0.0769230769,
    "perf_slm": 0.0769230769,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1a43df221a567869964ad3b275de30af2ac35598",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Yelp Challenge dataset",
    "final_answer": "The Yelp Challenge dataset.",
    "slm_pred": "The Yelp Challenge dataset.",
    "llm_pred": "the Yelp Challenge dataset",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1a43df221a567869964ad3b275de30af2ac35598",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yelp Challenge dataset BIBREF2",
    "final_answer": "The Yelp Challenge dataset.",
    "slm_pred": "The Yelp Challenge dataset.",
    "llm_pred": "the Yelp Challenge dataset",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "98b11f70239ef0e22511a3ecf6e413ecb726f954",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable \n\nThe article does not explicitly mention using a pretrained NMT model, but it mentions using the Yelp Challenge dataset to train the NMT model.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "98b11f70239ef0e22511a3ecf6e413ecb726f954",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention the use of a pretrained NMT model for review generation.",
    "slm_pred": "unanswerable \n\nThe article does not mention the use of a pretrained NMT model for review generation.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d4d771bcb59bab4f3eb9026cda7d182eb582027d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "By conditioning generation on an explicit context (rating, restaurant name, city/state, and cuisine tags) so the NMT model produces text aligned with that context.",
    "slm_pred": "The context is a clear-text sequence of: the review rating, restaurant name, city, state, and food tags.",
    "llm_pred": "By conditioning generation on an explicit context (rating, restaurant name, city/state, and cuisine tags) so the NMT model produces text aligned with that context.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "12f1919a3e8ca460b931c6cacc268a926399dff4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AdaBoost-based classifier",
    "final_answer": "AdaBoost with 200 shallow decision trees.",
    "slm_pred": "AdaBoost with 200 shallow decision trees.",
    "llm_pred": "AdaBoost (an ensemble of shallow decision trees)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "cd1034c183edf630018f47ff70b48d74d2bb1649",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "bd9930a613dd36646e2fc016b6eb21ab34c77621",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "1,006 fake reviews and 994 real reviews",
    "final_answer": "2000",
    "slm_pred": "994 (human reviews) + 1006 (NMT-Fake reviews) = 2000",
    "llm_pred": "2000",
    "perf_chosen": 0.0,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that they reproduce the results from See et al. See2017 to then apply LRP on it.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0421052632,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever the article does mention that they reproduce the results from See et al. See2017 and that they used a baseline model from See et al. See2017, but it does not mention any other baselines they compared.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1261261261,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "one",
    "final_answer": "one",
    "slm_pred": "The model uses a single attention mechanism in the decoder.",
    "llm_pred": "one",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "710c1f8d4c137c8dad9972f5ceacdbf8004db208",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "It depends on the case, as the article shows that saliency maps are truthful to the network's computation in some cases but not in others.",
    "slm_pred": "It depends on the case, as the article shows that saliency maps are truthful to the network's computation in some cases but not in others.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8958465d1eaf81c8b781ba4d764a4f5329f026aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "RIPA, Neighborhood Metric, WEAT",
    "final_answer": "WEAT, RIPA, and the neighborhood bias metric.",
    "slm_pred": "The three measures of bias which are reduced in experiments are geometric bias, neighborhood bias, and RIPA metric.",
    "llm_pred": "WEAT, RIPA, and the neighborhood bias metric.",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.2727272727,
    "perf_llm": 0.7272727273,
    "cost": 25.0
  },
  {
    "id": "31b6544346e9a31d656e197ad01756813ee89422",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "That word embeddings inherently model conditional co‑occurrence probabilities (P(w|c)) and that fairness can be expressed as probabilistic parity (e.g., enforcing p(doctor|man) ≈ p(doctor|woman)).",
    "slm_pred": "The nearest neighbor bias mitigation observations.",
    "llm_pred": "That word embeddings inherently model conditional co‑occurrence probabilities (P(w|c)) and that fairness can be expressed as probabilistic parity (e.g., enforcing p(doctor|man) ≈ p(doctor|woman)).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "347e86893e8002024c2d10f618ca98e14689675f",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "only high-quality data helps",
    "final_answer": "high-quality data",
    "slm_pred": "In a low-resourced setting, high quality data turns out to be more important than high volume data.",
    "llm_pred": "high-quality data",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.0952380952,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "347e86893e8002024c2d10f618ca98e14689675f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "high-quality",
    "final_answer": "High quality data turns out to be more important, especially in a low-resourced setting.",
    "slm_pred": "High quality data turns out to be more important, especially in a low-resourced setting.",
    "llm_pred": "high quality data",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "10091275f777e0c2890c3ac0fd0a7d8e266b57cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The model is improved more by quality than by quantity, as shown by the experiment results where high-quality data helps to significantly improve the quality of the embeddings, while massive but low-quality data does not.",
    "slm_pred": "The model is improved more by quality than by quantity, as shown by the experiment results where high-quality data helps to significantly improve the quality of the embeddings, while massive but low-quality data does not.",
    "llm_pred": "Massive data: little to negative gain (e.g., Yorùbá 0.14→0.07); Quality data: large gains—up to 0.39 (Yorùbá) and 0.44 (Twi), i.e., >170% improvement.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "cbf1137912a47262314c94d36ced3232d5fa1926",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "fastText, CWE-LP",
    "final_answer": "Character Word Embedding (CWE) and BERT.",
    "slm_pred": "Character Word Embedding (CWE) and BERT.",
    "llm_pred": "CBOW and skip-gram",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "519db0922376ce1e87fcdedaa626d665d9f3e8ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "The paper specifically targets the Portuguese language, which is assumed to be European Portuguese given the context of Brazil already having widely diffused the use of machine learning techniques.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "519db0922376ce1e87fcdedaa626d665d9f3e8ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "The paper targets the Portuguese language in general, but since it mentions a \"large corpus representative of the Portuguese language\" and a public word2vec model pre-trained for the Portuguese language, it likely targets European Portuguese, but it does not exclude the possibility of Brazilian Portuguese.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "99a10823623f78dbff9ccecb210f187105a196e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "large Portuguese corpus",
    "final_answer": "A large Portuguese corpus from several sources, including a multi-genre corpus representative of the Portuguese language.",
    "slm_pred": "A large Portuguese corpus from several sources, including a multi-genre corpus representative of the Portuguese language.",
    "llm_pred": "a large multi‑genre Portuguese corpus collected from several sources",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "09f0dce416a1e40cc6a24a8b42a802747d2c9363",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Continuous Bag-of-Words (CBOW)",
    "final_answer": "Portuguese word embeddings.",
    "slm_pred": "Portuguese word embeddings.",
    "llm_pred": "Portuguese word2vec embeddings",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ac706631f2b3fa39bf173cd62480072601e44f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ac706631f2b3fa39bf173cd62480072601e44f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "unanswerable \n\nThe text mentions that the dataset is the result of processing the CzCDC 1.0 dataset, but it does not mention any experiments being performed on the final dataset.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "8b71ede8170162883f785040e8628a97fc6b5bcb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification.",
    "final_answer": "The quality of the citation is measured through the F1 measure in the initial recognition of the text spans and their classification.",
    "slm_pred": "The quality of the citation is measured through the F1 measure in the initial recognition of the text spans and their classification.",
    "llm_pred": "F1-measure (precision and recall).",
    "perf_chosen": 0.42352941180000003,
    "perf_slm": 0.42352941180000003,
    "perf_llm": 0.0298507463,
    "cost": 5.0
  },
  {
    "id": "fa2a384a23f5d0fe114ef6a39dced139bddac20e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "903019 references",
    "final_answer": "The dataset contains 237,723 court decisions issued between 1st January 1993 and 30th September 2018.",
    "slm_pred": "The dataset contains 237,723 court decisions issued between 1st January 1993 and 30th September 2018.",
    "llm_pred": "237,723 court decisions",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "53712f0ce764633dbb034e550bb6604f15c0cacd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nAlthough the article mentions that they use Twitter data, which is typically in English, it does not explicitly state that they only use English datasets.",
    "slm_pred": "unanswerable \n\nAlthough the article mentions that they use Twitter data, which is typically in English, it does not explicitly state that they only use English datasets.",
    "llm_pred": "yes",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.0740740741,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0bffc3d82d02910d4816c16b390125e5df55fd01",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "bdd8368debcb1bdad14c454aaf96695ac5186b09",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",
    "final_answer": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to Dryhootch manual, with four intensity categories: No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD.",
    "slm_pred": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants according to Dryhootch manual, with four intensity categories: No PTSD, Low Risk PTSD, Moderate Risk PTSD, and High Risk PTSD.",
    "llm_pred": "By how many of the three survey tools (DOSPERT, BSSS, VIAS) exceed their thresholds in a week: 0 = none (No PTSD), 1 = low, 2 = moderate, 3 = high.",
    "perf_chosen": 0.5333333333,
    "perf_slm": 0.5333333333,
    "perf_llm": 0.380952381,
    "cost": 5.0
  },
  {
    "id": "bdd8368debcb1bdad14c454aaf96695ac5186b09",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "defined into four categories from high risk, moderate risk, to low risk",
    "final_answer": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants, with four categories: Non-existent, light, moderate, and high PTSD.",
    "slm_pred": "The intensity of PTSD is established based on how many surveys support the existence of PTSD among the participants, with four categories: Non-existent, light, moderate, and high PTSD.",
    "llm_pred": "by how many of the three survey tools (DOSPERT, BSSS, VIAS) exceed their thresholds—0=none (no PTSD), 1=low, 2=moderate, 3=high",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3334f50fe1796ce0df9dd58540e9c08be5856c23",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " For each user, we calculate the proportion of tweets scored positively by each LIWC category.",
    "final_answer": "The LIWC algorithm is used to calculate s-scores, which are probabilities that a word or string of characters was generated by the same underlying process as the training data, and the proportions of tweets scored positively by each LIWC category are used as a feature vector in a loglinear regression model.",
    "slm_pred": "The LIWC algorithm is used to calculate s-scores, which are probabilities that a word or string of characters was generated by the same underlying process as the training data, and the proportions of tweets scored positively by each LIWC category are used as a feature vector in a loglinear regression model.",
    "llm_pred": "By using LIWC-style dictionary categories to compute per-user category proportions and α-scores from tweets, then applying a modified LIWC algorithm to derive survey-based s-scores.",
    "perf_chosen": 0.303030303,
    "perf_slm": 0.303030303,
    "perf_llm": 0.20512820510000002,
    "cost": 5.0
  },
  {
    "id": "3334f50fe1796ce0df9dd58540e9c08be5856c23",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "to calculate the possible scores of each survey question using PTSD Linguistic Dictionary ",
    "final_answer": "LIWC is incorporated into this system through the use of LIWC language models ($clm^{+}$, $ulm^{+}$, $clm^{-}$, and $ulm^{-}$) to calculate $s$-scores for each test tweet, and later through the exact similar method of LIWC to extract $\\alpha$-scores for each dimension and categories using the generated PTSD Linguistic Dictionary.",
    "slm_pred": "LIWC is incorporated into this system through the use of LIWC language models ($clm^{+}$, $ulm^{+}$, $clm^{-}$, and $ulm^{-}$) to calculate $s$-scores for each test tweet, and later through the exact similar method of LIWC to extract $\\alpha$-scores for each dimension and categories using the generated PTSD Linguistic Dictionary.",
    "llm_pred": "By using LIWC’s dictionary framework and algorithm to compute category “alpha” scores from tweets (filtered to first‑person use) and then deriving “s-scores” from these to estimate survey-based PTSD measures.",
    "perf_chosen": 0.2950819672,
    "perf_slm": 0.2950819672,
    "perf_llm": 0.2380952381,
    "cost": 5.0
  },
  {
    "id": "7081b6909cb87b58a7b85017a2278275be58bf60",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "210",
    "final_answer": "210 users are surveyed using the clinically validated survey.",
    "slm_pred": "210 users are surveyed using the clinically validated survey.",
    "llm_pred": "210",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1870f871a5bcea418c44f81f352897a2f53d0971",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "DOSPERT, BSSS and VIAS",
    "final_answer": "The Domain-Specific Risk-Taking (DOSPERT) Scale, The Berlin Social Support Scales (BSSS), and the Values In Action Scale (VIAS).",
    "slm_pred": "The Domain-Specific Risk-Taking (DOSPERT) Scale, The Berlin Social Support Scales (BSSS), and the Values In Action Scale (VIAS).",
    "llm_pred": "DOSPERT, BSSS, and VIAS.",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ce6201435cc1196ad72b742db92abd709e0f9e8d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "unanswerable",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "928828544e38fe26c53d81d1b9c70a9fb1cc3feb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "29,500 documents",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "29,500 documents",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "928828544e38fe26c53d81d1b9c70a9fb1cc3feb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "29,500 documents in the CORD-19 corpus (2020-03-13)",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "29,500 documents",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "4f243056e63a74d1349488983dc1238228ca76a7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "They list 75 fine-grained named entity types.",
    "slm_pred": "They list 75 fine-grained named entity types.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d94ac550dfdb9e4bbe04392156065c072b9d75e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d94ac550dfdb9e4bbe04392156065c072b9d75e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "eeb6e0caa4cf5fdd887e1930e22c816b99306473",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The contexts are manually labelled with WordNet senses of the target words",
    "final_answer": "The different senses are annotated/labeled manually in some cases, and automatically induced from clusters in other cases.",
    "slm_pred": "The different senses are annotated/labeled manually in some cases, and automatically induced from clusters in other cases.",
    "llm_pred": "With a keyword per cluster—the word in that cluster having the highest anti-edge count (most anti-edges) is used to label the sense.",
    "perf_chosen": 0.27586206900000004,
    "perf_slm": 0.27586206900000004,
    "perf_llm": 0.1764705882,
    "cost": 5.0
  },
  {
    "id": "3c0eaa2e24c1442d988814318de5f25729696ef5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThe extrinsic evaluation was carried out through the evaluation of the relatedness scores on benchmark datasets and the performance of the sense vectors on the Task 13 of SemEval-2013 — Word Sense Induction.",
    "slm_pred": "yes \n\nThe extrinsic evaluation was carried out through the evaluation of the relatedness scores on benchmark datasets and the performance of the sense vectors on the Task 13 of SemEval-2013 — Word Sense Induction.",
    "llm_pred": "no",
    "perf_chosen": 0.0571428571,
    "perf_slm": 0.0571428571,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "dc1fe3359faa2d7daa891c1df33df85558bc461b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, the model uses both spectrogram images and raw waveforms as features.",
    "slm_pred": "Yes, the model uses both spectrogram images and raw waveforms as features.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "922f1b740f8b13fdc8371e2a275269a44c86195e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nThe article does not mention comparing the performance against a baseline model.",
    "slm_pred": "unanswerable \n\nThe article does not mention comparing the performance against a baseline model.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "922f1b740f8b13fdc8371e2a275269a44c86195e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The performance of the proposed models is compared against various existing models in the field of spoken language identification.",
    "slm_pred": "The performance of the proposed models is compared against various existing models in the field of spoken language identification.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b39f2249a1489a2cef74155496511cc5d1b2a73d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Answer with content missing: (Table 1)\nPrevious state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",
    "final_answer": "up to 96.0% on four languages (English, German, French, and Spanish) and 95.4% on six languages.",
    "slm_pred": "up to 96.0% on four languages (English, German, French, and Spanish) and 95.4% on six languages.",
    "llm_pred": "96.0%",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "591231d75ff492160958f8aa1e6bfcbbcd85a776",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CNN-mean, CNN-avgmax",
    "final_answer": "Previous state-of-the-art vision-based methods, including CNN-mean and CNN-avgmax, as well as the show-tell model.",
    "slm_pred": "Previous state-of-the-art vision-based methods, including CNN-mean and CNN-avgmax, as well as the show-tell model.",
    "llm_pred": "the CNN-mean and CNN-avgmax vision-based baselines",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "9e805020132d950b54531b1a2620f61552f06114",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CNN-mean, CNN-avgmax",
    "final_answer": "The Adam algorithm is applied to optimize the model, and the caption model is trained with two baseline vision-based methods proposed in BIBREF6, BIBREF7, which are CNN-mean and CNN-avgmax.",
    "slm_pred": "The Adam algorithm is applied to optimize the model, and the caption model is trained with two baseline vision-based methods proposed in BIBREF6, BIBREF7, which are CNN-mean and CNN-avgmax.",
    "llm_pred": "CNN-mean and CNN-avgmax.",
    "perf_chosen": 0.1290322581,
    "perf_slm": 0.1290322581,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "95abda842c4df95b4c5e84ac7d04942f1250b571",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "German-English, French-English, and Japanese-English",
    "final_answer": "English, German, French, and Japanese.",
    "slm_pred": "English, German, French, and Japanese.",
    "llm_pred": "English, German, French, and Japanese",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "95abda842c4df95b4c5e84ac7d04942f1250b571",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "multiple language pairs including German-English, French-English, and Japanese-English.",
    "final_answer": "English, German, French, and Japanese.",
    "slm_pred": "The languages used in the multi-lingual caption model are German, English, French, and Japanese.",
    "llm_pred": "English, German, French, and Japanese.",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.0909090909,
    "perf_llm": 0.1538461538,
    "cost": 25.0
  },
  {
    "id": "2419b38624201d678c530eba877c0c016cccd49f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "Yes.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b99d100d17e2a121c3c8ff789971ce66d1d40a4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)",
    "final_answer": "They do not explicitly compare to previous research due to the existing works using smaller data, outdated methods, or lacking standard benchmarks.",
    "slm_pred": "They do not explicitly compare to previous research due to the existing works using smaller data, outdated methods, or lacking standard benchmarks.",
    "llm_pred": "They didn’t compare to previous research—only to their own baselines (e.g., a small-GRU vs. fine-tuned BERT).",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.18518518520000002,
    "cost": 5.0
  },
  {
    "id": "578d0b23cb983b445b1a256a34f969b34d332075",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34",
    "final_answer": "Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, IDAT@FIRE2019 shared-task, MADAR shared task 1 corpus, and task 1 corpus.",
    "slm_pred": "Arab-Tweet, UBC Twitter Gender Dataset, LAMA-DINA, LAMA-DIST, IDAT@FIRE2019 shared-task, MADAR shared task 1 corpus, and task 1 corpus.",
    "llm_pred": "Arab-Tweet (Arap-Tweet), UBC Twitter Gender Dataset, MADAR Task 2 tweets plus the MADAR Task 1 corpus, LAMA-DINA and LAMA-DIST (combined as LAMA-D2), and the IDAT@FIRE2019 Arabic irony dataset.",
    "perf_chosen": 0.3571428571,
    "perf_slm": 0.3571428571,
    "perf_llm": 0.4545454545,
    "cost": 5.0
  },
  {
    "id": "578d0b23cb983b445b1a256a34f969b34d332075",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects",
    "final_answer": "The datasets used in training include Arab-Tweet, UBC Twitter Gender Dataset, the MADAR shared task corpus, the LAMA-DINA dataset, and the IDAT@FIRE2019 shared-task dataset.",
    "slm_pred": "The datasets used in training include Arab-Tweet, UBC Twitter Gender Dataset, the MADAR shared task corpus, the LAMA-DINA dataset, and the IDAT@FIRE2019 shared-task dataset.",
    "llm_pred": "Arab-Tweet; UBC Twitter Gender; MADAR task‑2 tweets plus the MADAR task‑1 corpus; LAMA‑DINA and LAMA‑DIST (combined as LAMA‑D2); and IDAT@FIRE2019 irony tweets.",
    "perf_chosen": 0.4090909091,
    "perf_slm": 0.4090909091,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "6548db45fc28e8a8b51f114635bad14a13eaec5b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . ",
    "final_answer": "They use the Wasserstein GAN (WGAN) and the energy-based GAN, and also propose two models, weGAN and deGAN, which are extensions of the GAN model.",
    "slm_pred": "They use the Wasserstein GAN (WGAN) and the energy-based GAN, and also propose two models, weGAN and deGAN, which are extensions of the GAN model.",
    "llm_pred": "weGAN and deGAN",
    "perf_chosen": 0.17021276600000002,
    "perf_slm": 0.17021276600000002,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6548db45fc28e8a8b51f114635bad14a13eaec5b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "weGAN, deGAN",
    "final_answer": "They use the Wasserstein GAN (Arjovsky et al., 2017) and also their own models weGAN and deGAN.",
    "slm_pred": "They use the Wasserstein GAN (Arjovsky et al., 2017) and also their own models weGAN and deGAN.",
    "llm_pred": "weGAN and deGAN",
    "perf_chosen": 0.2105263158,
    "perf_slm": 0.2105263158,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "4c4f76837d1329835df88b0921f4fe8bda26606f",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "819d2e97f54afcc7cdb3d894a072bcadfba9b747",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CNN, TIME, 20 Newsgroups, and Reuters-21578",
    "final_answer": "The CNN data set and the 20 Newsgroups data set.",
    "slm_pred": "The CNN data set and the 20 Newsgroups data set.",
    "llm_pred": "CNN news (politics, world, US) and 20 Newsgroups (grouped into religion, computer, cars, sport, science, and politics).",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.347826087,
    "cost": 5.0
  },
  {
    "id": "637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "The answer is yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "4b8257cdd9a60087fa901da1f4250e7d910896df",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "typos in spellings or ungrammatical words",
    "final_answer": "misspelled or wrongly transcribed words (e.g., from noisy tweets or ASR/STT outputs)",
    "slm_pred": "The authors do not explicitly define or exemplify 'incorrect words', but they mention that the Twitter Sentiment Classification dataset contains \"many mistakes, as specified in Table TABREF11\", and that the TTS-STT combinations used to generate incomplete sentences with STT error result in varying rates of missing and incorrect words.",
    "llm_pred": "misspelled or wrongly transcribed words (e.g., from noisy tweets or ASR/STT outputs)",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.10909090910000001,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "7e161d9facd100544fa339b06f656eb2fc64ed28",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The article does not specify the exact number of vanilla transformers used after applying an embedding layer.",
    "slm_pred": "The article does not specify the exact number of vanilla transformers used after applying an embedding layer.",
    "llm_pred": "12",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "yes.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, the authors do not test their approach on a dataset without incomplete data; they test it on datasets with incomplete data, specifically with missing or incorrectly transcribed words.",
    "slm_pred": "No, the authors do not test their approach on a dataset without incomplete data; they test it on datasets with incomplete data, specifically with missing or incorrectly transcribed words.",
    "llm_pred": "yes",
    "perf_chosen": 0.0666666667,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4debd7926941f1a02266b1a7be2df8ba6e79311a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "No.\n\nThe authors state that \"in order for current systems to improve the quality of their services, there is a need for development of robust intelligent systems that are able to understand a user even when faced with incomplete representation in language.\" This implies that their approach should be applied in general, not just when dealing with incomplete data.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.033333333300000004,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4debd7926941f1a02266b1a7be2df8ba6e79311a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No.\n\nThe authors suggest that their approach should be applied generally to improve the robustness and efficiency of BERT when applied to incomplete data, not just when dealing with incomplete data.",
    "slm_pred": "No.\n\nThe authors suggest that their approach should be applied generally to improve the robustness and efficiency of BERT when applied to incomplete data, not just when dealing with incomplete data.",
    "llm_pred": "no",
    "perf_chosen": 0.0625,
    "perf_slm": 0.0625,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average",
    "final_answer": "They outperform other models by 6% to 8% in the sentiment classification task and by 0.94% to 1.89% in the intent classification task with STT error.",
    "slm_pred": "They outperform other models by 6% to 8% in the sentiment classification task and by 0.94% to 1.89% in the intent classification task with STT error.",
    "llm_pred": "About 6–8% on sentiment classification, and by 0.94% (gtts–wit.ai) to 1.89% (macsay–wit.ai) on intent classification with STT error.",
    "perf_chosen": 0.7555555556,
    "perf_slm": 0.7555555556,
    "perf_llm": 0.48648648650000004,
    "cost": 5.0
  },
  {
    "id": "44c7c1fbac80eaea736622913d65fe6453d72828",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "34,432 user conversations",
    "final_answer": "34,432 user conversations.",
    "slm_pred": "34,432 user conversations.",
    "llm_pred": "34,432",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "44c7c1fbac80eaea736622913d65fe6453d72828",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "34,432 ",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "3e0c9469821cb01a75e1818f2acb668d071fcf40",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "overall rating, mean number of turns",
    "final_answer": "The metrics to measure user engagement are overall rating, mean number of turns, and mean word count.",
    "slm_pred": "The metrics to measure user engagement are overall rating, mean number of turns, and mean word count.",
    "llm_pred": "overall rating and mean number of turns",
    "perf_chosen": 0.5217391304,
    "perf_slm": 0.5217391304,
    "perf_llm": 0.9230769231,
    "cost": 5.0
  },
  {
    "id": "3e0c9469821cb01a75e1818f2acb668d071fcf40",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "overall rating, mean number of turns",
    "final_answer": "The metrics to measure user engagement are overall rating, mean number of turns, user's per-utterance word count, and the number of backstory questions asked.",
    "slm_pred": "The metrics to measure user engagement are overall rating, mean number of turns, user's per-utterance word count, and the number of backstory questions asked.",
    "llm_pred": "overall rating and mean number of turns",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.9230769231,
    "cost": 5.0
  },
  {
    "id": "a725246bac4625e6fe99ea236a96ccb21b5f30c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)",
    "final_answer": "The system designs introduced are multi-step language understanding modules, a novel dialog act scheme, and an extensive persona database.",
    "slm_pred": "The system designs introduced are multi-step language understanding modules, a novel dialog act scheme, and an extensive persona database.",
    "llm_pred": "a multi-step NLU with a novel dialog act scheme, a fact/opinion interleaving strategy, and an extensive persona backstory database",
    "perf_chosen": 0.13636363640000002,
    "perf_slm": 0.13636363640000002,
    "perf_llm": 0.0909090909,
    "cost": 5.0
  },
  {
    "id": "516626825e51ca1e8a3e0ac896c538c9d8a747c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "77af93200138f46bb178c02f710944a01ed86481",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "71538776757a32eee930d297f6667cd0ec2e9231",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions",
    "final_answer": "They found a strong positive correlation between the number of backstory questions asked and user satisfaction.",
    "slm_pred": "They found a strong positive correlation between the number of backstory questions asked and user satisfaction.",
    "llm_pred": "By a linear regression of overall (log) rating on the (log) number of backstory questions asked.",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.358974359,
    "cost": 5.0
  },
  {
    "id": "7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "no \n\nThey have several baselines, including ROUGE, WordNCE, WPNCE, WordPPL, WPPPL, and word-overlap metrics like ROUGE-L-mult and ROUGE-L-single.",
    "slm_pred": "no \n\nThey have several baselines, including ROUGE, WordNCE, WPNCE, WordPPL, WPPPL, and word-overlap metrics like ROUGE-L-mult and ROUGE-L-single.",
    "llm_pred": "no",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",
    "final_answer": "no",
    "slm_pred": "No.",
    "llm_pred": "no",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.11764705880000001,
    "cost": 25.0
  },
  {
    "id": "3ac30bd7476d759ea5d9a5abf696d4dfc480175b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "LSTM LMs",
    "final_answer": "They use neural language models (LMs), specifically recurrent neural networks (RNNs) and LSTM LMs.",
    "slm_pred": "They use neural language models (LMs), specifically recurrent neural networks (RNNs) and LSTM LMs.",
    "llm_pred": "neural LSTM language models (both word-based and WordPiece-based)",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "0e57a0983b4731eba9470ba964d131045c8c7ea7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "human judges are asked to rate the fluency of system-generated compressions on an ordinal scale from 1 (disfluent) through 3 (fluent).",
    "slm_pred": "human judges are asked to rate the fluency of system-generated compressions on an ordinal scale from 1 (disfluent) through 3 (fluent).",
    "llm_pred": "content and fluency ratings",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f0317e48dafe117829e88e54ed2edab24b86edb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
    "final_answer": "The model still takes into account and overrides the information brought by the text-based annotations when the attention loses track of the objects in the picture.",
    "slm_pred": "The model still takes into account and overrides the information brought by the text-based annotations when the attention loses track of the objects in the picture.",
    "llm_pred": "Attention sometimes “gets lost” on the image, overriding text-based cues and misleading the translation.",
    "perf_chosen": 0.8421052632,
    "perf_slm": 0.8421052632,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "f0317e48dafe117829e88e54ed2edab24b86edb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
    "final_answer": "The model still takes into account and overrides the information brought by the text-based annotations if the attention loses track of the objects in the picture and \"gets lost\".",
    "slm_pred": "The model still takes into account and overrides the information brought by the text-based annotations if the attention loses track of the objects in the picture and \"gets lost\".",
    "llm_pred": "The attention sometimes “gets lost” on the image, overriding text cues and misleading the translation.",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.2173913043,
    "cost": 5.0
  },
  {
    "id": "ec91b87c3f45df050e4e16018d2bf5b62e4ca298",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The baseline used is the model of Calixto et al. (2017) and also the multimodal baseline of Caglayan et al. (2016).",
    "slm_pred": "The baseline used is the model of Calixto et al. (2017) and also the multimodal baseline of Caglayan et al. (2016).",
    "llm_pred": "the CalixtoLC17b multimodal baseline",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f129c97a81d81d32633c94111018880a7ffe16d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Soft attention, Hard Stochastic attention, Local Attention",
    "final_answer": "Soft attention, hard stochastic attention, and local attention (which is a generalization of soft attention).",
    "slm_pred": "Soft attention, hard stochastic attention, and local attention (which is a generalization of soft attention).",
    "llm_pred": "soft attention; hard stochastic attention; local (predictive alignment, “local-m”) attention",
    "perf_chosen": 0.6363636364,
    "perf_slm": 0.6363636364,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "100cf8b72d46da39fedfe77ec939fb44f25de77f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "100cf8b72d46da39fedfe77ec939fb44f25de77f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Chinese dataset BIBREF0",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8cc56fc44136498471754186cfa04056017b4e54",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the article, we can infer that their system outperforms the lexicon-based models significantly, as it outperforms TF-IDF in all relevance metrics.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0779220779,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8cc56fc44136498471754186cfa04056017b4e54",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, according to the text, our proposed model outperforms the lexicon-based model TF-IDF, which is a popular unsupervised baseline, in both retrieval and generative evaluations.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5fa431b14732b3c47ab6eec373f51f2bca04f614",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TF-IDF, NVDM",
    "final_answer": "TF-IDF and NVDM.",
    "slm_pred": "TF-IDF and NVDM.",
    "llm_pred": "TF-IDF and NVDM",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "33ccbc401b224a48fba4b167e86019ffad1787fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "from 50K to 4.8M",
    "final_answer": "4.5 million human comments in the candidate set.",
    "slm_pred": "4.5 million human comments in the candidate set.",
    "llm_pred": "4.5 million",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "cca74448ab0c518edd5fc53454affd67ac1a201c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "198,112",
    "final_answer": "198,112 news articles.",
    "slm_pred": "198,112 news articles.",
    "llm_pred": "198,112",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b69ffec1c607bfe5aa4d39254e0770a3433a191b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Chinese dataset BIBREF0",
    "final_answer": "The Tencent News dataset with millions of real comments was used.",
    "slm_pred": "The Tencent News dataset with millions of real comments was used.",
    "llm_pred": "the Tencent News dataset",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "f5cf8738e8d211095bb89350ed05ee7f9997eb19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "up to four percentage points in accuracy",
    "final_answer": "up to four percentage points in accuracy.",
    "slm_pred": "up to four percentage points in accuracy.",
    "llm_pred": "up to four percentage points in accuracy",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "bed527bcb0dd5424e69563fba4ae7e6ea1fca26a",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "2019 GermEval shared task on hierarchical text classification",
    "final_answer": "the GermEval 2019 shared task dataset (Blurb Genre Collection, BGC)",
    "slm_pred": "the 2019 GermEval shared task on hierarchical text classification and the Blurb Genre Collection (BGC).",
    "llm_pred": "the GermEval 2019 shared task dataset (Blurb Genre Collection, BGC)",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.6956521739,
    "perf_llm": 0.4444444444,
    "cost": 25.0
  },
  {
    "id": "bed527bcb0dd5424e69563fba4ae7e6ea1fca26a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "GermEval 2019 shared task",
    "final_answer": "The dataset used is the 2019 GermEval shared task on hierarchical text classification.",
    "slm_pred": "The dataset used is the 2019 GermEval shared task on hierarchical text classification.",
    "llm_pred": "the GermEval 2019 Blurb Genre Collection (BGC) dataset",
    "perf_chosen": 0.4705882353,
    "perf_slm": 0.4705882353,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "aeab5797b541850e692f11e79167928db80de1ea",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "all three representations are concatenated and passed into a MLP",
    "final_answer": "They combine text representations with the knowledge graph embeddings by concatenating them and passing them into a multilayer perceptron (MLP) with two layers.",
    "slm_pred": "They combine text representations with the knowledge graph embeddings by concatenating them and passing them into a multilayer perceptron (MLP) with two layers.",
    "llm_pred": "By concatenating the BERT text embedding with the Wikidata author embeddings (and metadata) and feeding the concatenation into an MLP.",
    "perf_chosen": 0.303030303,
    "perf_slm": 0.303030303,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "cda4612b4bda3538d19f4b43dde7bc30c1eda4e5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm",
    "final_answer": "Pattern-based extraction with Bayes/frequency scoring and simple co‑occurrence (popularity) counts.",
    "slm_pred": "unanswerable.",
    "llm_pred": "Pattern-based extraction with Bayes/frequency scoring and simple co‑occurrence (popularity) counts.",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0,
    "perf_llm": 0.0869565217,
    "cost": 25.0
  },
  {
    "id": "cda4612b4bda3538d19f4b43dde7bc30c1eda4e5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TextRank, Word2vec BIBREF19, GloVe BIBREF20",
    "final_answer": "The traditional methods to identifying important attributes are simple co-occurrences and ranking process used to identify relatively more precise attributes from all attribute candidates.",
    "slm_pred": "The traditional methods to identifying important attributes are simple co-occurrences and ranking process used to identify relatively more precise attributes from all attribute candidates.",
    "llm_pred": "Pattern-based attribute extraction with statistical scoring (e.g., Bayes), POS-tag patterns with frequency metrics, and simple co-occurrence/popularity counting.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e12674f0466f8c0da109b6076d9939b30952c7da",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "FastText",
    "final_answer": "FastText.",
    "slm_pred": "FastText.",
    "llm_pred": "FastText",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9fe6339c7027a1a0caffa613adabe8b5bb6a7d4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable (the article does not specify the exact type of user generated text data used)",
    "slm_pred": "unanswerable (the article does not specify the exact type of user generated text data used)",
    "llm_pred": "product enquiries",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b5c3787ab3784214fc35f230ac4926fe184d86ba",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "No, they proposed three characteristic metrics: diversity, density, and homogeneity.",
    "slm_pred": "No, they proposed three characteristic metrics: diversity, density, and homogeneity.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9174aded45bc36915f2e2adb6f352f3c7d9ada8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SST-2 (Stanford Sentiment Treebank, version 2), Snips",
    "final_answer": "SST-2 (Stanford Sentiment Treebank, version 2) and Snips dataset.",
    "slm_pred": "SST-2 (Stanford Sentiment Treebank, version 2) and Snips dataset.",
    "llm_pred": "SST-2 (Stanford Sentiment Treebank v2) and Snips.",
    "perf_chosen": 0.875,
    "perf_slm": 0.875,
    "perf_llm": 0.7142857143,
    "cost": 5.0
  },
  {
    "id": "9174aded45bc36915f2e2adb6f352f3c7d9ada8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SST-2, Snips",
    "final_answer": "The SST-2 (Stanford Sentiment Treebank, version 2) dataset and the Snips dataset were used.",
    "slm_pred": "The SST-2 (Stanford Sentiment Treebank, version 2) dataset and the Snips dataset were used.",
    "llm_pred": "SST-2 (Stanford Sentiment Treebank v2) and the Snips dataset.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "a8f1029f6766bffee38a627477f61457b2d6ed5c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "By visualizing the embeddings (PCA followed by t-SNE) so humans could inspect the clusters.",
    "slm_pred": "unanswerable",
    "llm_pred": "By visualizing the embeddings (PCA followed by t-SNE) so humans could inspect the clusters.",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a2103e7fe613549a9db5e65008f33cf2ee0403bd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "wealth , democracy , population, levels of ODA, conflict ",
    "final_answer": "Wealth, population size, democracy, and official development assistance (ODA) are the country-specific drivers of international development rhetoric, with varying effects on the discussion of sustainable development (Topic 7) and economic development (Topic 2).",
    "slm_pred": "Wealth, population size, democracy, and official development assistance (ODA) are the country-specific drivers of international development rhetoric, with varying effects on the discussion of sustainable development (Topic 7) and economic development (Topic 2).",
    "llm_pred": "wealth (GDP per capita), population size, aid/ODA received, regime type/democracy, conflict exposure, and regional location",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2727272727,
    "cost": 5.0
  },
  {
    "id": "13b36644357870008d70e5601f394ec3c6c07048",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not provide information about the language(s) of the dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "13b36644357870008d70e5601f394ec3c6c07048",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not provide information on the language(s) used in the dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",
    "final_answer": "The main international development topics that states raise are identified through the application of structural topic models (STMs) to the UN General Debate Corpus (UNGDC) dataset.",
    "slm_pred": "The main international development topics that states raise are identified through the application of structural topic models (STMs) to the UN General Debate Corpus (UNGDC) dataset.",
    "llm_pred": "By applying structural topic modeling to UN General Debate speeches (UNGDC).",
    "perf_chosen": 0.1904761905,
    "perf_slm": 0.1904761905,
    "perf_llm": 0.0289855072,
    "cost": 5.0
  },
  {
    "id": "fd0ef5a7b6f62d07776bf672579a99c67e61a568",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs",
    "final_answer": "The authors evaluate their system's performance on datasets across various domains and conduct experiments with active learning to validate its effectiveness.",
    "slm_pred": "The authors evaluate their system's performance on datasets across various domains and conduct experiments with active learning to validate its effectiveness.",
    "llm_pred": "Cross-domain evaluations with managed human judgments of query–QA relevance, comparing KBs with and without chit-chat (and chit-chat-only), reporting F1/AUC.",
    "perf_chosen": 0.3461538462,
    "perf_slm": 0.3461538462,
    "perf_llm": 0.16,
    "cost": 5.0
  },
  {
    "id": "071bcb4b054215054f17db64bfd21f17fd9e1a80",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The QnAMaker WebApp acts as a layer between the Bot, Management APIs, and Azure Search Index, handling ranking on top of retrieved results and feedback management for active learning.",
    "slm_pred": "The QnAMaker WebApp acts as a layer between the Bot, Management APIs, and Azure Search Index, handling ranking on top of retrieved results and feedback management for active learning.",
    "llm_pred": "By retrieving candidate Q&As from an Azure Search index and re-ranking them in the QnAMaker WebApp—using preprocessing plus WordNet, CDSSM, TF‑IDF, and contextual features—to return the best answer.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f399d5a8dbeec777a858f81dc4dd33a83ba341a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot",
    "final_answer": "QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and the Bot.",
    "slm_pred": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot.",
    "llm_pred": "QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and the Bot.",
    "perf_chosen": 0.9166666667000001,
    "perf_slm": 0.7857142857,
    "perf_llm": 0.9166666667000001,
    "cost": 25.0
  },
  {
    "id": "f399d5a8dbeec777a858f81dc4dd33a83ba341a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot",
    "final_answer": "QnAMaker Portal; QnAMaker Management APIs; Azure Search Index; QnAMaker WebApp; and the Bot.",
    "slm_pred": "The QnAMaker is composed of QnAMaker Portal, QnAMaker Management APIs, Azure Search Index, QnAMaker WebApp, and Bot.",
    "llm_pred": "QnAMaker Portal; QnAMaker Management APIs; Azure Search Index; QnAMaker WebApp; and the Bot.",
    "perf_chosen": 0.9166666667000001,
    "perf_slm": 0.7857142857,
    "perf_llm": 0.9166666667000001,
    "cost": 25.0
  },
  {
    "id": "d28260b5565d9246831e8dbe594d4f6211b60237",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "We empirically provide a formula to measure the richness in the scenario of machine translation.",
    "final_answer": "by BLEU score on held-out test sets (e.g., NIST MT04/05)",
    "slm_pred": "unanswerable \n\nHowever, it can be inferred that they test the robustness of the Plackett-Luce model through evaluation on the test data, comparing its performance to MERT and MIRA in two tasks: iterative training and N-best list reranking.",
    "llm_pred": "by BLEU score on held-out test sets (e.g., NIST MT04/05)",
    "perf_chosen": 0.0,
    "perf_slm": 0.1923076923,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d28260b5565d9246831e8dbe594d4f6211b60237",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8670989ca39214eda6c1d1d272457a3f3a92818b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "no",
    "slm_pred": "unanswerable \n\nHowever, the article suggests that the new method is able to alleviate over-fitting in some degree when k is not smaller than 5, which may indicate that it is more robust than MIRAs in certain cases.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0526315789,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "923b12c0a50b0ee22237929559fad0903a098b7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Plackett-Luce Model for SMT Reranking",
    "final_answer": "The experiments performed with large-scale features include full training of machine translation models and N-best list reranking.",
    "slm_pred": "The experiments performed with large-scale features include full training of machine translation models and N-best list reranking.",
    "llm_pred": "N-best list reranking using a hierarchical phrase-based system with about 300 unique hypotheses and 7,491 features.",
    "perf_chosen": 0.0909090909,
    "perf_slm": 0.0909090909,
    "perf_llm": 0.0952380952,
    "cost": 5.0
  },
  {
    "id": "67131c15aceeb51ae1d3b2b8241c8750a19cca8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Oracle ",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "579a0603ec56fc2b4aa8566810041dbb0cd7b5e7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "perform experiments to utilize ASR $n$-best hypotheses during evaluation",
    "final_answer": "Majority Vote, Sort by Score, and Rerank (Oracle)",
    "slm_pred": "unanswerable",
    "llm_pred": "Majority Vote, Sort by Score, and Rerank (Oracle)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c9c85eee41556c6993f40e428fa607af4abe80a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "$\\sim $ 8.7M annotated anonymised user utterances",
    "final_answer": "The work is evaluated over an anonymised user utterances dataset of approximately 8.7M annotated utterances across 23 domains.",
    "slm_pred": "The work is evaluated over an anonymised user utterances dataset of approximately 8.7M annotated utterances across 23 domains.",
    "llm_pred": "an anonymized internal dataset of ~8.7M user utterances across 23 domains",
    "perf_chosen": 0.4166666667,
    "perf_slm": 0.4166666667,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "c9c85eee41556c6993f40e428fa607af4abe80a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "on $\\sim $ 8.7M annotated anonymised user utterances",
    "final_answer": "An anonymized user utterances corpus derived from requests across 23 domains, with approximately 8.7 million annotated utterances and a test set of around 300,000 utterances.",
    "slm_pred": "An anonymized user utterances corpus derived from requests across 23 domains, with approximately 8.7 million annotated utterances and a test set of around 300,000 utterances.",
    "llm_pred": "~8.7M annotated anonymized user utterances across 23 domains (proprietary data)",
    "perf_chosen": 0.1875,
    "perf_slm": 0.1875,
    "perf_llm": 0.4705882353,
    "cost": 5.0
  },
  {
    "id": "982979cb3c71770d8d7d2d1be8f92b66223dec85",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine",
    "final_answer": "topological properties.",
    "slm_pred": "topological properties.",
    "llm_pred": "Topological, rank-based metrics—e.g., checking that for each word all same-class words are closer than any words from other classes, independent of absolute cosine values.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4516129032,
    "cost": 5.0
  },
  {
    "id": "5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Class Membership Tests, Class Distinction Test, Word Equivalence Test",
    "final_answer": "Class Membership Tests, Class Distinction Test, and Word Equivalence Test.",
    "slm_pred": "Class Membership Tests, Class Distinction Test, and Word Equivalence Test.",
    "llm_pred": "Class Membership, Class Distinction, and Word Equivalence tests based on cosine-similarity thresholds.",
    "perf_chosen": 0.9473684211000001,
    "perf_slm": 0.9473684211000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)",
    "final_answer": "The intrinsic evaluation metrics used are Class Membership Tests, Class Distinction Test, and Word Equivalence Test, which involve computing the ratio of pairs correctly classified as distinct, belonging to the same class, or equivalent based on cosine similarity.",
    "slm_pred": "The intrinsic evaluation metrics used are Class Membership Tests, Class Distinction Test, and Word Equivalence Test, which involve computing the ratio of pairs correctly classified as distinct, belonging to the same class, or equivalent based on cosine similarity.",
    "llm_pred": "cosine-similarity–based success rates for three tests: Class Membership, Class Distinction, and Word Equivalence",
    "perf_chosen": 0.28125,
    "perf_slm": 0.28125,
    "perf_llm": 0.0512820513,
    "cost": 5.0
  },
  {
    "id": "7ce7edd06925a943e32b59f3e7b5159ccb7acaf6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "consistent increase in the validation loss after about 15 epochs",
    "final_answer": "The validation loss consistently increases after about 15 epochs when using less than 50% of the data available, indicating overfitting.",
    "slm_pred": "The validation loss consistently increases after about 15 epochs when using less than 50% of the data available, indicating overfitting.",
    "llm_pred": "a consistent rise in validation loss after about 15 epochs when trained with less than 50% of the data (e.g., at 25% and 50%)",
    "perf_chosen": 0.4666666667,
    "perf_slm": 0.4666666667,
    "perf_llm": 0.5294117647000001,
    "cost": 5.0
  },
  {
    "id": "a883bb41449794e0a63b716d9766faea034eb359",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "context is a procedural text, the question and the multiple choice answers are composed of images",
    "final_answer": "The multimodality available in the dataset is text and images.",
    "slm_pred": "The multimodality available in the dataset is text and images.",
    "llm_pred": "images and text",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.3157894737,
    "cost": 5.0
  },
  {
    "id": "a883bb41449794e0a63b716d9766faea034eb359",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "images and text",
    "final_answer": "The answer is multimodal comprehension of cooking recipes.",
    "slm_pred": "The answer is multimodal comprehension of cooking recipes.",
    "llm_pred": "text and images",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5d83b073635f5fd8cd1bdb1895d3f13406583fbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory",
    "final_answer": "Hasty Student and Impatient Reader are previously reported models.",
    "slm_pred": "Hasty Student and Impatient Reader are previously reported models.",
    "llm_pred": "Hasty Student and Impatient Reader",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "171ebfdc9b3a98e4cdee8f8715003285caeb2f39",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59",
    "final_answer": "The new model achieves state-of-the-art results compared to other neural models.",
    "slm_pred": "The new model achieves state-of-the-art results compared to other neural models.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0666666667,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3c3cb51093b5fd163e87a773a857496a4ae71f03",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word",
    "final_answer": "The scoring model works by computing a score for a segmented sentence as the average of word scores and link scores, where word scores are calculated based on the possibility of a segment being a legal word and link scores are calculated based on the rationality of the link between words.",
    "slm_pred": "The scoring model works by computing a score for a segmented sentence as the average of word scores and link scores, where word scores are calculated based on the possibility of a segment being a legal word and link scores are calculated based on the rationality of the link between words.",
    "llm_pred": "Map each segmented sentence to candidate word embeddings, compute a word score (GCNN + information entropy for word legality) and a link score (LSTM for contextual linkage), then average these scores over the sentence to get a sequence score for sample selection.",
    "perf_chosen": 0.325,
    "perf_slm": 0.325,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "3c3cb51093b5fd163e87a773a857496a4ae71f03",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history",
    "final_answer": "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then using a gated neural network and LSTM to evaluate the likelihood of each candidate word being a legal word and its link to previous segmentation history.",
    "slm_pred": "The scoring model works by mapping a segmented sentence to a sequence of candidate word embeddings, then using a gated neural network and LSTM to evaluate the likelihood of each candidate word being a legal word and its link to previous segmentation history.",
    "llm_pred": "Map a segmented sentence to candidate word embeddings, compute a word score (legality) via a GCNN with information entropy and a link score (coherence with history) via LSTM, then average word and link scores over the sentence to get the sequence score for sampling.",
    "perf_chosen": 0.3913043478,
    "perf_slm": 0.3913043478,
    "perf_llm": 0.2365591398,
    "cost": 5.0
  },
  {
    "id": "53a0763eff99a8148585ac642705637874be69d4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",
    "final_answer": "The active learning model works by employing a CRF-based segmenter as the learning engine and a scoring model based on information entropy and neural network as the selection engine, which iteratively selects and labels samples from unlabeled data to improve the accuracy of the classifier.",
    "slm_pred": "The active learning model works by employing a CRF-based segmenter as the learning engine and a scoring model based on information entropy and neural network as the selection engine, which iteratively selects and labels samples from unlabeled data to improve the accuracy of the classifier.",
    "llm_pred": "Train a CRF-based segmenter, auto-label unlabeled data, use an entropy+neural scoring model (word, link, and sequence scores) to select the most informative sentences for human relabeling, update the datasets, retrain, and iterate until convergence.",
    "perf_chosen": 0.5087719298,
    "perf_slm": 0.5087719298,
    "perf_llm": 0.27184466020000003,
    "cost": 5.0
  },
  {
    "id": "0bfed6f9cfe93617c5195c848583e3945f2002ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "gated neural network ",
    "final_answer": "a gated combination neural network (GCNN) and an LSTM",
    "slm_pred": "A gated neural network, a long short-term memory (LSTM) model, and a gated recurrent neural network (GRNN) are employed.",
    "llm_pred": "a gated combination neural network (GCNN) and an LSTM",
    "perf_chosen": 0.5,
    "perf_slm": 0.2727272727,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "352c081c93800df9654315e13a880d6387b91919",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "text understanding, expectation, coreference resolution, common-sense knowledge inference, and the embedding of sentences into context.",
    "slm_pred": "text understanding, expectation, coreference resolution, common-sense knowledge inference, and the embedding of sentences into context.",
    "llm_pred": "Guiding reader expectations, supporting coreference resolution and commonsense inference, and embedding the current sentence into a larger context.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "18fbf9c08075e3b696237d22473c463237d153f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.",
    "final_answer": "The inter-annotator agreement was sufficiently high, and a qualified majority vote resulted in 90.5% agreement between annotators for coreference chain annotation.",
    "slm_pred": "The inter-annotator agreement was sufficiently high, and a qualified majority vote resulted in 90.5% agreement between annotators for coreference chain annotation.",
    "llm_pred": "yes — 90.5% agreement (qualified-majority on coreference pairs)",
    "perf_chosen": 0.44,
    "perf_slm": 0.44,
    "perf_llm": 0.16216216220000002,
    "cost": 5.0
  },
  {
    "id": "18fbf9c08075e3b696237d22473c463237d153f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Moderate agreement of 0.64-0.68 Fleiss’ Kappa over event type labels, 0.77 Fleiss’ Kappa over participant labels, and good agreement of 90.5% over coreference information.",
    "final_answer": "The inter-annotator agreement was found to be sufficiently high, and the percentage of pairs annotated by at least 3 annotators was 90.5%.",
    "slm_pred": "The inter-annotator agreement was found to be sufficiently high, and the percentage of pairs annotated by at least 3 annotators was 90.5%.",
    "llm_pred": "yes — about 90.5% agreement (for coreference pairs)",
    "perf_chosen": 0.1739130435,
    "perf_slm": 0.1739130435,
    "perf_llm": 0.1875,
    "cost": 5.0
  },
  {
    "id": "a37ef83ab6bcc6faff3c70a481f26174ccd40489",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " four different annotators",
    "final_answer": "four",
    "slm_pred": "unanswerable. \n\nHowever, according to the article, \"All annotations were done by undergraduate students of computational linguistics\" and \"The stories from each scenario were distributed among four different annotators.\" This implies that there are four subjects (annotators), but it does not mention how many students were involved in the project.",
    "llm_pred": "four",
    "perf_chosen": 0.5,
    "perf_slm": 0.1153846154,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "bc9c31b3ce8126d1d148b1025c66f270581fde10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",
    "final_answer": "WN18 and YAGO3-10.",
    "slm_pred": "WN18 and YAGO3-10.",
    "llm_pred": "WN18 and YAGO3-10",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "bc9c31b3ce8126d1d148b1025c66f270581fde10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WN18 and YAGO3-10",
    "final_answer": "The datasets used to evaluate this approach are WN18 and YAGO3-10.",
    "slm_pred": "The datasets used to evaluate this approach are WN18 and YAGO3-10.",
    "llm_pred": "WN18 and YAGO3-10",
    "perf_chosen": 0.4285714286,
    "perf_slm": 0.4285714286,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "185841e979373808d99dccdade5272af02b98774",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. ",
    "final_answer": "By identifying the fact that, when removed, results in the least change in the model's prediction, indicating the error triple.",
    "slm_pred": "By identifying the fact that, when removed, results in the least change in the model's prediction, indicating the error triple.",
    "llm_pred": "By finding the neighbor triple with the least influence—i.e., the one whose removal causes the smallest change in the target score—and flagging it as the error.",
    "perf_chosen": 0.3235294118,
    "perf_slm": 0.3235294118,
    "perf_llm": 0.2972972973,
    "cost": 5.0
  },
  {
    "id": "d427e3d41c4c9391192e249493be23926fc5d2e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "330f2cdeab689670b68583fc4125f5c0b26615a8",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.",
    "final_answer": "The proposed model has several advantages, including the ability to jointly model words in documents as arising from a mixture of topics, to learn from multiple annotators and crowds by accounting for their biases and different levels of expertise, and to scale to large datasets using an efficient stochastic variational inference algorithm.",
    "slm_pred": "The proposed model has several advantages, including the ability to jointly model words in documents as arising from a mixture of topics, to learn from multiple annotators and crowds by accounting for their biases and different levels of expertise, and to scale to large datasets using an efficient stochastic variational inference algorithm.",
    "llm_pred": "A fully Bayesian, multi-annotator supervised topic model that jointly learns document topics and latent true labels while estimating per-annotator reliabilities/biases, uses scalable stochastic variational inference, and achieves superior predictive performance (classification and regression) on large, complex datasets.",
    "perf_chosen": 0.1875,
    "perf_slm": 0.1875,
    "perf_llm": 0.049382716,
    "cost": 5.0
  },
  {
    "id": "c87b2dd5c439d5e68841a705dd81323ec0d64c97",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)",
    "final_answer": "State-of-the-art approaches for predicting target variables associated with complex high-dimensional data, such as documents or images, are supervised topic models.",
    "slm_pred": "State-of-the-art approaches for predicting target variables associated with complex high-dimensional data, such as documents or images, are supervised topic models.",
    "llm_pred": "Supervised topic models.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ",
    "final_answer": "The datasets used were 20-Newsgroups, Reuters-21578, and LabelMe.",
    "slm_pred": "The datasets used were 20-Newsgroups, Reuters-21578, and LabelMe.",
    "llm_pred": "20-Newsgroups, Reuters-21578, and LabelMe.",
    "perf_chosen": 0.375,
    "perf_slm": 0.375,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe",
    "final_answer": "20-Newsgroups, Reuters-21578, and LabelMe",
    "slm_pred": "The 20-Newsgroups dataset, Reuters-21578, LabelMe, and we8there dataset were used.",
    "llm_pred": "20-Newsgroups, Reuters-21578, and LabelMe",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.4,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "2376c170c343e2305dac08ba5f5bda47c370357f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ",
    "final_answer": "The dataset was collected through a human-to-human dialogue setting, where two paired workers, a user and a system (wizard), conversed synchronously in a chatroom to accomplish a goal generated by a multi-domain goal generator based on a database of travel information in Beijing.",
    "slm_pred": "The dataset was collected through a human-to-human dialogue setting, where two paired workers, a user and a system (wizard), conversed synchronously in a chatroom to accomplish a goal generated by a multi-domain goal generator based on a database of travel information in Beijing.",
    "llm_pred": "via human-to-human, synchronous dialogues between paired trained workers (user/tourist and wizard/system) based on generated multi-domain goals",
    "perf_chosen": 0.2476190476,
    "perf_slm": 0.2476190476,
    "perf_llm": 0.09836065570000001,
    "cost": 5.0
  },
  {
    "id": "2376c170c343e2305dac08ba5f5bda47c370357f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",
    "final_answer": "The dataset was collected through a human-to-human dialogue setting, where workers were paired to converse according to given goals, and their conversations were then annotated and recorded.",
    "slm_pred": "The dataset was collected through a human-to-human dialogue setting, where workers were paired to converse according to given goals, and their conversations were then annotated and recorded.",
    "llm_pred": "Via synchronous human-to-human chats: pairs of trained workers (user and wizard) conversed in a chatroom following generated multi-domain goals, with states and acts annotated.",
    "perf_chosen": 0.2545454545,
    "perf_slm": 0.2545454545,
    "perf_llm": 0.1923076923,
    "cost": 5.0
  },
  {
    "id": "0137ecebd84a03b224eb5ca51d189283abb5f6d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)",
    "final_answer": "The benchmark models provided are for different components of a pipelined task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG), implemented using ConvLab-2.",
    "slm_pred": "The benchmark models provided are for different components of a pipelined task-oriented dialogue system, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG), implemented using ConvLab-2.",
    "llm_pred": "BERTNLU (NLU); RuleDST and TRADE (DST); a supervised-learning SL policy (dialogue policy); TemplateNLG and SC-LSTM (NLG); plus a rule-based user simulator.",
    "perf_chosen": 0.17241379310000002,
    "perf_slm": 0.17241379310000002,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "5f6fbd57cce47f20a0fda27d954543c00c4344c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories",
    "final_answer": "The corpus was annotated automatically using some rules, and then manually checked by three experts for a subset of 50 dialogues.",
    "slm_pred": "The corpus was annotated automatically using some rules, and then manually checked by three experts for a subset of 50 dialogues.",
    "llm_pred": "Using rule-based automatic labeling (with keyword matching) from user/system states and dialogue history—workers annotated user and system states, and experts manually checked 50 dialogues for quality.",
    "perf_chosen": 0.1923076923,
    "perf_slm": 0.1923076923,
    "perf_llm": 0.24561403510000002,
    "cost": 5.0
  },
  {
    "id": "d6e2b276390bdc957dfa7e878de80cee1f41fbca",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Only Bert base and Bert large are compared to proposed approach.",
    "final_answer": "Attentive Mimicking",
    "slm_pred": "Attentive Mimicking.",
    "llm_pred": "Attentive Mimicking",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "32537fdf0d4f76f641086944b413b2f756097e5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking",
    "final_answer": "An absolute improvement of up to 24% over a BERT baseline is reported.",
    "slm_pred": "An absolute improvement of up to 24% over a BERT baseline is reported.",
    "llm_pred": "About 50% over standalone BERT and 31% over Attentive Mimicking on medium-frequency words (rare words also improve, but no exact figure given).",
    "perf_chosen": 0.0689655172,
    "perf_slm": 0.0689655172,
    "perf_llm": 0.2631578947,
    "cost": 5.0
  },
  {
    "id": "ef081d78be17ef2af792e7e919d15a235b8d7275",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23",
    "final_answer": "MNLI, AG's News, and DBPedia.",
    "slm_pred": "MNLI, AG's News, and DBPedia.",
    "llm_pred": "MNLI, AG’s News, and DBPedia",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "ef081d78be17ef2af792e7e919d15a235b8d7275",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MNLI, AG's News, DBPedia",
    "final_answer": "MNLI, AG's News, and DBPedia.",
    "slm_pred": "MNLI, AG's News, and DBPedia.",
    "llm_pred": "MNLI, AG’s News, and DBPedia",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "537b2d7799124d633892a1ef1a485b3b071b303d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WNLaMPro dataset",
    "final_answer": "WNLaMPro dataset.",
    "slm_pred": "WNLaMPro dataset.",
    "llm_pred": "WNLaMPro",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "9aca4b89e18ce659c905eccc78eda76af9f0072a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The model achieves the state-of-the-art results compared to baselines.",
    "slm_pred": "The model achieves the state-of-the-art results compared to baselines.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b0376a7f67f1568a7926eff8ff557a93f434a253",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the article, it can be inferred that the method achieves the \"state-of-the-art results\" and \"best overall performance\" compared to the baseline methods, indicating a significant performance difference.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1785714286,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "dad8cc543a87534751f9f9e308787e1af06f0627",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI",
    "final_answer": "The datasets used for evaluation include AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI.",
    "slm_pred": "The datasets used for evaluation include AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, and OURSELF-WIKI.",
    "llm_pred": "AIDA-B (AIDA-CoNLL), ACE2004, MSNBC, AQUAINT, WNED-CWEB (CWEB), WNED-WIKI (WIKI), and OURSELF-WIKI.",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.7058823529,
    "cost": 5.0
  },
  {
    "id": "dad8cc543a87534751f9f9e308787e1af06f0627",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI",
    "final_answer": "AIDA-B, AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI, and CWEB.",
    "slm_pred": "AIDA-B, AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI, and CWEB.",
    "llm_pred": "AIDA-B (AIDA-CoNLL), ACE2004, MSNBC, AQUAINT, WNED-CWEB (CWEB), WNED-WIKI (WIKI), and OURSELF-WIKI.",
    "perf_chosen": 0.8235294118,
    "perf_slm": 0.8235294118,
    "perf_llm": 0.7777777778,
    "cost": 5.0
  },
  {
    "id": "0481a8edf795768d062c156875d20b8fb656432c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7",
    "final_answer": "lexical and statistical features such as the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc.",
    "slm_pred": "lexical and statistical features such as the popularity of the entity, the edit distance between the entity description and the mention context, the number of identical words in the entity description and the mention context etc.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.2105263158,
    "perf_slm": 0.2105263158,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b6a4ab009e6f213f011320155a7ce96e713c11cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The author's work ranked among the top submissions on the challenge, as their reinforcement learning approaches received good human evaluation results and were the best of their runs in two batches.",
    "slm_pred": "The author's work ranked among the top submissions on the challenge, as their reinforcement learning approaches received good human evaluation results and were the best of their runs in two batches.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "cfffc94518d64cb3c8789395707e4336676e0345",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "classification, regression, neural methods",
    "final_answer": "Classification and regression approaches using deep learning models.",
    "slm_pred": "Classification and regression approaches using deep learning models.",
    "llm_pred": "classification and regression approaches",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "cfffc94518d64cb3c8789395707e4336676e0345",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models",
    "final_answer": "Classification and regression approaches, and deep learning models, have been tried without reinforcement learning.",
    "slm_pred": "Classification and regression approaches, and deep learning models, have been tried without reinforcement learning.",
    "llm_pred": "classification and regression approaches (including SVM-based and neural models)",
    "perf_chosen": 0.3428571429,
    "perf_slm": 0.3428571429,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "f60629c01f99de3f68365833ee115b95a3388699",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)",
    "final_answer": "Labeling sentences as \"summary\" based on their ROUGE score above a threshold, or the top m input sentences with the highest ROUGE score.",
    "slm_pred": "Labeling sentences as \"summary\" based on their ROUGE score above a threshold, or the top m input sentences with the highest ROUGE score.",
    "llm_pred": "Labeling-based classifiers: (1) mark sentences as “summary” if their ROUGE-SU4 F1 exceeds a threshold t; and (2) mark the top m sentences with highest ROUGE-SU4 F1 as “summary.”",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.1052631579,
    "cost": 5.0
  },
  {
    "id": "a7cb4f8e29fd2f3d1787df64cd981a6318b65896",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "642c4704a71fd01b922a0ef003f234dcc7b223cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources",
    "final_answer": "missing values, language-specific attributes, and multiword expressions",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that some languages have a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian, which could be a source of recall errors.",
    "llm_pred": "missing values, language-specific attributes, and multiword expressions",
    "perf_chosen": 0.064516129,
    "perf_slm": 0.40860215050000004,
    "perf_llm": 0.064516129,
    "cost": 25.0
  },
  {
    "id": "e477e494fe15a978ff9c0a5f1c88712cdaec0c5c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "They do not explicitly state looking for inconsistencies between different languages' annotations in UniMorph, but mention \"teasing out the annotation discrepancies within and across projects.\"",
    "slm_pred": "They do not explicitly state looking for inconsistencies between different languages' annotations in UniMorph, but mention \"teasing out the annotation discrepancies within and across projects.\"",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "04495845251b387335bf2e77e2c423130f43c7d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the UD datasets have been used in the CoNLL shared tasks and that each dataset is an annotated treebank, making it a resource of token-level annotations. But it does not mention looking for inconsistencies between different UD treebanks.",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that the UD datasets have been used in the CoNLL shared tasks and that each dataset is an annotated treebank, making it a resource of token-level annotations. But it does not mention looking for inconsistencies between different UD treebanks.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",
    "final_answer": "The 31 languages with both UD and UniMorph data.",
    "slm_pred": "The 31 languages with both UD and UniMorph data.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish",
    "final_answer": "31 languages with both UD and UniMorph data.",
    "slm_pred": "31 languages with both UD and UniMorph data.",
    "llm_pred": "the 31 languages with both UD and UniMorph data",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.32,
    "cost": 5.0
  },
  {
    "id": "f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "The paper mentions that the personalization of the recognition algorithm (e.g., mean and standard deviation normalization) could help to adapt the classification for specific speakers and thus to reduce the bias towards negative emotional states, but does not explicitly evaluate this adjustment.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "confusion matrices, $\\text{F}_1$ score",
    "final_answer": "Using confusion matrices and macro-averaged F1 scores.",
    "slm_pred": "Using confusion matrices and macro-averaged F1 scores.",
    "llm_pred": "Using confusion matrices and per-class metrics, including micro and macro averages.",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "00bcdffff7e055f99aaf1b05cf41c98e2748e948",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For the emotion recognition from text they use described neural network as baseline.\nFor audio and face there is no baseline.",
    "final_answer": "A neural network with an embedding layer, a bidirectional LSTM, and two dense layers followed by a soft max output layer, inspired by previous work.",
    "slm_pred": "A neural network with an embedding layer, a bidirectional LSTM, and two dense layers followed by a soft max output layer, inspired by previous work.",
    "llm_pred": "a neural network with frozen pretrained embeddings, a bidirectional LSTM, and two dense layers with a softmax output",
    "perf_chosen": 0.1304347826,
    "perf_slm": 0.1304347826,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "f92ee3c5fce819db540bded3cfcc191e21799cb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)",
    "final_answer": "Off-the-shelf emotion recognition tools are used for facial expressions (manufacturer cannot be disclosed) and audio signals (specific manufacturer not mentioned).",
    "slm_pred": "Off-the-shelf emotion recognition tools are used for facial expressions (manufacturer cannot be disclosed) and audio signals (specific manufacturer not mentioned).",
    "llm_pred": "Off‑the‑shelf proprietary emotion recognition software (manufacturer undisclosed) for both face and audio.",
    "perf_chosen": 0.4324324324,
    "perf_slm": 0.4324324324,
    "perf_llm": 0.27586206900000004,
    "cost": 5.0
  },
  {
    "id": "f92ee3c5fce819db540bded3cfcc191e21799cb1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cannot be disclosed due to licensing restrictions",
    "final_answer": "Off-the-shelf tools for emotion recognition from facial expressions and audio signals are used.",
    "slm_pred": "Off-the-shelf tools for emotion recognition from facial expressions and audio signals are used.",
    "llm_pred": "Off‑the‑shelf proprietary emotion recognition software (manufacturer undisclosed) for both face and audio.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",
    "final_answer": "30000 merge operations for BPE subword segmentation.",
    "slm_pred": "30000 merge operations for BPE subword segmentation.",
    "llm_pred": "100k words and 3.2M words",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0869565217,
    "perf_llm": 0.0952380952,
    "cost": 5.0
  },
  {
    "id": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)",
    "final_answer": "100k words and 3.2M words",
    "slm_pred": "30k and 90k subwords, with 30000 merge operations used for BPE subword segmentation.",
    "llm_pred": "100k words and 3.2M words",
    "perf_chosen": 0.4545454545,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.4545454545,
    "cost": 25.0
  },
  {
    "id": "07d7652ad4a0ec92e6b44847a17c378b0d9f57f5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "10.37 BLEU",
    "final_answer": "Our optimized NMT system outperforms the PBSMT system across all data settings, achieving a BLEU score of 9.4 in the ultra-low data condition and 32.8 in the full IWSLT 14 training corpus.",
    "slm_pred": "Our optimized NMT system outperforms the PBSMT system across all data settings, achieving a BLEU score of 9.4 in the ultra-low data condition and 32.8 in the full IWSLT 14 training corpus.",
    "llm_pred": "10.37 BLEU",
    "perf_chosen": 0.058823529400000005,
    "perf_slm": 0.058823529400000005,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9f3444c9fb2e144465d63abf58520cddd4165a01",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "gu-EtAl:2018:EMNLP1",
    "final_answer": "They compare with Phrase-Based Statistical Machine Translation (PBSMT) systems.",
    "slm_pred": "They compare with Phrase-Based Statistical Machine Translation (PBSMT) systems.",
    "llm_pred": "the NMT baseline, the NMT with mainstream improvements, and the fully optimized low‑resource NMT setup (configs 1, 2, and 8)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "2348d68e065443f701d8052018c18daa4ecc120e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "highly data-inefficient, underperform phrase-based statistical machine translation",
    "final_answer": "The paper mentions the following pitfalls: using hyperparameters typical for high-resource settings in low-resource settings, not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions, relying on large amounts of auxiliary data, and using vocabulary sizes that are not optimal for low-resource settings.",
    "slm_pred": "The paper mentions the following pitfalls: using hyperparameters typical for high-resource settings in low-resource settings, not tuning hyperparameters or changing network architectures to optimize NMT for low-resource conditions, relying on large amounts of auxiliary data, and using vocabulary sizes that are not optimal for low-resource settings.",
    "llm_pred": "training NMT with high‑resource hyperparameters without low-resource tuning (e.g., large BPE vocabularies, large batches, weak dropout), overreliance on auxiliary data, and domain/morphology mismatches that impair unsupervised methods",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a939a53cabb4893b2fd82996f3dbe8688fdb7bbb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "8b99767620fd4efe51428b68841cc3ec06699280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The technique used for text analysis and mining is a combination of natural language processing (NLP), argumentation mining, and the use of a semantic frame extractor, specifically the Penelope semantic frame extractor.",
    "slm_pred": "The technique used for text analysis and mining is a combination of natural language processing (NLP), argumentation mining, and the use of a semantic frame extractor, specifically the Penelope semantic frame extractor.",
    "llm_pred": "semantic frame extraction using a FrameNet-based causation frame detector (Penelope)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "312417675b3dc431eb7e7b16a917b7fed98d4376",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Axelrod's causal mapping method",
    "final_answer": "Causal mapping methods employed include Axelrod's causal mapping method, cognitive mapping, and the Penelope semantic frame extractor.",
    "slm_pred": "Causal mapping methods employed include Axelrod's causal mapping method, cognitive mapping, and the Penelope semantic frame extractor.",
    "llm_pred": "automated extraction of causation frames using the Penelope semantic frame extractor (FrameNet Causation frame triggers)",
    "perf_chosen": 0.380952381,
    "perf_slm": 0.380952381,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "792d7b579cbf7bfad8fe125b0d66c2059a174cf9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ternary Trans-CNN",
    "final_answer": "Ternary Trans-CNN model, which uses a Convolution 1D architecture with 3 layers and 2 dense fully connected layers, achieving an accuracy of 83.90%.",
    "slm_pred": "Ternary Trans-CNN model, which uses a Convolution 1D architecture with 3 layers and 2 dense fully connected layers, achieving an accuracy of 83.90%.",
    "llm_pred": "the Ternary Trans-CNN model (three-layer 1D CNN followed by dense layers)",
    "perf_chosen": 0.16,
    "perf_slm": 0.16,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "HEOT , A labelled dataset for a corresponding english tweets",
    "final_answer": "The dataset used is HEOT obtained from a past study done by Mathur et al., which is a labelled dataset for Hinglish tweets, and a labelled dataset for English tweets obtained from a study conducted by Davidson et al.",
    "slm_pred": "The dataset used is HEOT obtained from a past study done by Mathur et al., which is a labelled dataset for Hinglish tweets, and a labelled dataset for English tweets obtained from a study conducted by Davidson et al.",
    "llm_pred": "HEOT",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "HEOT",
    "final_answer": "The dataset used is HEOT, a labelled dataset of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent, which was obtained from a past study by Mathur et al.",
    "slm_pred": "The dataset used is HEOT, a labelled dataset of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent, which was obtained from a past study by Mathur et al.",
    "llm_pred": "HEOT dataset",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "5908d7fb6c48f975c5dfc5b19bb0765581df2b25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3189 rows of text messages",
    "final_answer": "3189 rows of text messages with a range of 1-1295 words.",
    "slm_pred": "3189 rows of text messages with a range of 1-1295 words.",
    "llm_pred": "3189 messages",
    "perf_chosen": 0.625,
    "perf_slm": 0.625,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "5908d7fb6c48f975c5dfc5b19bb0765581df2b25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Resulting dataset was 7934 messages for train and 700 messages for test.",
    "final_answer": "The dataset is 3189 rows of text messages of average length 116 words.",
    "slm_pred": "The dataset is 3189 rows of text messages of average length 116 words.",
    "llm_pred": "3189 messages",
    "perf_chosen": 0.16,
    "perf_slm": 0.16,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "cca3301f20db16f82b5d65a102436bebc88a2026",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al",
    "final_answer": "The dataset, HEOT, is obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from Twitter for the conversations happening in the Indian subcontinent.",
    "slm_pred": "The dataset, HEOT, is obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from Twitter for the conversations happening in the Indian subcontinent.",
    "llm_pred": "Annotated, cleaned tweets obtained from Twitter (Indian subcontinent) from the HEOT dataset by Mathur et al.",
    "perf_chosen": 0.5671641791,
    "perf_slm": 0.5671641791,
    "perf_llm": 0.45833333330000003,
    "cost": 5.0
  },
  {
    "id": "cfd67b9eeb10e5ad028097d192475d21d0b6845b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable\n\nThe article mentions that the text augmentation techniques (Synonym Replacement, Random Insertion, Random Swap, and Random Deletion) were used together, but it does not mention whether they were experimented individually.",
    "slm_pred": "unanswerable\n\nThe article mentions that the text augmentation techniques (Synonym Replacement, Random Insertion, Random Swap, and Random Deletion) were used together, but it does not mention whether they were experimented individually.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e1c681280b5667671c7f78b1579d0069cba72b0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM",
    "final_answer": "a Ternary Trans-CNN and a hybrid multi‑channel CNN–LSTM model",
    "slm_pred": "Previous work uses Transfer Learning based approaches, Ternary Trans-CNN models, Hybrid multi-channel CNN and LSTM models, and other architectures such as Fully Connected dense networks and Convolution based architectures.",
    "llm_pred": "a Ternary Trans-CNN and a hybrid multi‑channel CNN–LSTM model",
    "perf_chosen": 0.5,
    "perf_slm": 0.3888888889,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "58d50567df71fa6c3792a0964160af390556757d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable \n\nHowever, based on the information provided, it can be inferred that the dataset is obtained from Twitter.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "07c79edd4c29635dbc1c2c32b8df68193b7701c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "HEOT , A labelled dataset for a corresponding english tweets ",
    "final_answer": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.",
    "slm_pred": "The HEOT dataset obtained from a past study by Mathur et al. and a labelled dataset for English tweets from a study by Davidson et al.",
    "llm_pred": "HEOT",
    "perf_chosen": 0.4571428571,
    "perf_slm": 0.4571428571,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "66125cfdf11d3bf8e59728428e02021177142c3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.",
    "final_answer": "They demonstrate that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
    "slm_pred": "They demonstrate that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
    "llm_pred": "By aligning words using cosine distances between mBERT token embeddings (minimum-weight edge cover) and showing these alignments outperform FastAlign on gold datasets—while being invariant to language-centroid shifts—demonstrating robust, language-neutral word-level semantics.",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.1707317073,
    "cost": 5.0
  },
  {
    "id": "66125cfdf11d3bf8e59728428e02021177142c3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "explicit projection had a negligible effect on the performance",
    "final_answer": "They demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
    "slm_pred": "They demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment by surpassing the outputs of the standard FastAlign tool.",
    "llm_pred": "By computing alignments using cosine distances between mBERT token embeddings and showing these alignments outperform FastAlign (higher F1), demonstrating that the centering-invariant, language-neutral component captures word-level semantics well enough for high-accuracy alignment.",
    "perf_chosen": 0.0571428571,
    "perf_slm": 0.0571428571,
    "perf_llm": 0.0487804878,
    "cost": 5.0
  },
  {
    "id": "222b2469eede9a0448e0226c6c742e8c91522af3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The article suggests that a sentence representation in mBERT is composed of a language-specific component and a language-neutral component, implying that they are not necessarily disjunctive.",
    "slm_pred": "The article suggests that a sentence representation in mBERT is composed of a language-specific component and a language-neutral component, implying that they are not necessarily disjunctive.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6f8386ad64dce3a20bc75165c5c7591df8f419cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.",
    "final_answer": "They assume that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way, and they try to remove the language-specific information from the representations by centering the representations of sentences in each language.",
    "slm_pred": "They assume that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way, and they try to remove the language-specific information from the representations by centering the representations of sentences in each language.",
    "llm_pred": "By estimating and subtracting a per‑language centroid from mBERT sentence vectors (centering), they showed language identity acts like a near-constant shift—centering lowers language-ID accuracy and improves cross-lingual retrieval—indicating separable language-specific and language-neutral components.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "81dc39ee6cdacf90d5f0f62134bf390a29146c65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks",
    "final_answer": "The representation similarity does not directly show language-neutrality, i.e., to what extent are similar phenomena represented similarly across languages.",
    "slm_pred": "The representation similarity does not directly show language-neutrality, i.e., to what extent are similar phenomena represented similarly across languages.",
    "llm_pred": "Removing language identity while preserving fine‑grained, cross‑lingual semantic equivalence—i.e., going beyond a simple constant-shift correction to capture subtle meaning needed for tasks like QE.",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.13636363640000002,
    "cost": 5.0
  },
  {
    "id": "eeaceee98ef1f6c971dac7b0b8930ee8060d71c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.",
    "final_answer": "They propose two approaches: across models and tasks, and across input space, to develop a more nuanced methodology for defining and evaluating faithfulness.",
    "slm_pred": "They propose two approaches: across models and tasks, and across input space, to develop a more nuanced methodology for defining and evaluating faithfulness.",
    "llm_pred": "Evaluating faithfulness in a graded way via two approaches: (1) across models and tasks, and (2) across the input space (subspaces/neighborhoods or specific instances).",
    "perf_chosen": 0.2686567164,
    "perf_slm": 0.2686567164,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "3371d586a3a81de1552d90459709c57c0b1a2594",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.",
    "final_answer": "The authors propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful.",
    "slm_pred": "The authors propose a graded criteria that measures the extent and likelihood of an interpretation to be faithful.",
    "llm_pred": "a graded criterion measuring the extent and likelihood that an interpretation is faithful in practice",
    "perf_chosen": 0.1290322581,
    "perf_slm": 0.1290322581,
    "perf_llm": 0.0677966102,
    "cost": 5.0
  },
  {
    "id": "d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.",
    "final_answer": "The three assumptions are the Model Assumption, the Prediction Assumption, and the Linearity Assumption.",
    "slm_pred": "The three assumptions are the Model Assumption, the Prediction Assumption, and the Linearity Assumption.",
    "llm_pred": "The Model Assumption, the Prediction Assumption, and the Linearity Assumption.",
    "perf_chosen": 0.1866666667,
    "perf_slm": 0.1866666667,
    "perf_llm": 0.1408450704,
    "cost": 5.0
  },
  {
    "id": "d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.",
    "final_answer": "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption.",
    "slm_pred": "The three assumptions in current approaches for defining faithfulness are: \n\n1. The Model Assumption, \n2. The Prediction Assumption, \n3. The Linearity Assumption.",
    "llm_pred": "the Model assumption, the Prediction assumption, and the Linearity assumption",
    "perf_chosen": 0.1445783133,
    "perf_slm": 0.1445783133,
    "perf_llm": 0.1408450704,
    "cost": 5.0
  },
  {
    "id": "2a859e80d8647923181cb2d8f9a2c67b1c3f4608",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust “inherent interpretability” claims., Faithfulness evaluation of IUI systems should not rely on user performance.",
    "final_answer": "Be explicit in what you evaluate, faithfulness evaluation should not involve human-judgement on the quality of interpretation, faithfulness evaluation should not involve human-provided gold labels, do not trust “inherent interpretability” claims, and faithfulness evaluation of IUI systems should not rely on user performance.",
    "slm_pred": "Be explicit in what you evaluate, faithfulness evaluation should not involve human-judgement on the quality of interpretation, faithfulness evaluation should not involve human-provided gold labels, do not trust “inherent interpretability” claims, and faithfulness evaluation of IUI systems should not rely on user performance.",
    "llm_pred": "Be explicit about evaluating faithfulness (not plausibility); avoid human judgment of explanation quality; avoid human-provided gold labels; don’t assume “inherent interpretability”—evaluate it; and don’t use user performance as a proxy for faithfulness.",
    "perf_chosen": 0.9882352941,
    "perf_slm": 0.9882352941,
    "perf_llm": 0.3513513514,
    "cost": 5.0
  },
  {
    "id": "aceac4ad16ffe1af0f01b465919b1d4422941a6b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we provide an extensive analysis of the state-of-the-art model",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f7070b2e258beac9b09514be2bfcc5a528cc3a0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention the performance of their model, but rather focuses on interpreting the behavior of the model's intermediate layers.",
    "slm_pred": "unanswerable \n\nThe article does not explicitly mention the performance of their model, but rather focuses on interpreting the behavior of the model's intermediate layers.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.08,
    "perf_slm": 0.08,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f7070b2e258beac9b09514be2bfcc5a528cc3a0e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly discuss the performance of their model, but rather focuses on interpreting the behavior of deep learning models in natural language inference tasks.",
    "slm_pred": "unanswerable \n\nThe article does not explicitly discuss the performance of their model, but rather focuses on interpreting the behavior of deep learning models in natural language inference tasks.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0689655172,
    "perf_slm": 0.0689655172,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2efdcebebeb970021233553104553205ce5d6567",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "two LSTM layers",
    "final_answer": "two",
    "slm_pred": "Three main parts are mentioned: input encoding, attention, and inference.",
    "llm_pred": "two",
    "perf_chosen": 0.5,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "4fa851d91388f0803e33f6cfae519548598cd37c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a891039441e008f1fd0a227dbed003f76c140737",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "machine comprehension",
    "final_answer": "The answer is Multiple Perspective Context Matching.",
    "slm_pred": "The answer is Multiple Perspective Context Matching.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "73738e42d488b32c9db89ac8adefc75403fa2653",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " 69.10%/78.38%",
    "final_answer": "About +1.1 EM and +1.0 F1 on SQuAD (from 68.00/77.36 to 69.10/78.38 on the dev set).",
    "slm_pred": "unanswerable \n\nThe article does not provide a clear measure of improvement for the adaptation model, but it does mention that when the number of hidden question types (K) is set to 100, the performance improves to 69.10%/78.38% on the development set.",
    "llm_pred": "About +1.1 EM and +1.0 F1 on SQuAD (from 68.00/77.36 to 69.10/78.38 on the dev set).",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.0476190476,
    "perf_llm": 0.11764705880000001,
    "cost": 25.0
  },
  {
    "id": "6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "word embedding, input encoder, alignment, aggregation, and prediction.",
    "final_answer": "The baseline model is composed of typical components including word embedding, input encoder, alignment, aggregation, and prediction.",
    "slm_pred": "The baseline model is composed of typical components including word embedding, input encoder, alignment, aggregation, and prediction.",
    "llm_pred": "word embedding (char-CNN + GloVe), BiGRU input encoders, soft alignment/attention, aggregation with stacked residual BiGRUs, and a pointer network for span prediction",
    "perf_chosen": 0.64,
    "perf_slm": 0.64,
    "perf_llm": 0.41379310340000003,
    "cost": 5.0
  },
  {
    "id": "6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction.",
    "final_answer": "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components.",
    "slm_pred": "The baseline model is composed of word embedding, input encoder, alignment, aggregation, and prediction components.",
    "llm_pred": "a pipeline of word embeddings (char-CNN + GloVe), BiGRU input encoders, soft attention/alignment, multi-layer BiGRU aggregation, and a pointer-network span predictor",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.9090909091,
    "perf_llm": 0.2631578947,
    "cost": 5.0
  },
  {
    "id": "fa218b297d9cdcae238cef71096752ce27ca8f4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Our model achieves a 68.73% EM score and 77.39% F1 score",
    "final_answer": "Our model achieves a 68.73% EM score and 77.39% F1 score on the SQuAD test set.",
    "slm_pred": "Our model achieves a 68.73% EM score and 77.39% F1 score on the SQuAD test set.",
    "llm_pred": "68.73% Exact Match (EM) on SQuAD.",
    "perf_chosen": 0.8148148148000001,
    "perf_slm": 0.8148148148000001,
    "perf_llm": 0.23529411760000002,
    "cost": 5.0
  },
  {
    "id": "ff28d34d1aaa57e7ad553dba09fc924dc21dd728",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "High correlation results range from 0.472 to 0.936",
    "final_answer": "The three fine-tuned BERT versions clearly outperform all other methods, achieving high correlations with human scores for most linguistic qualities rated, on three different datasets.",
    "slm_pred": "The three fine-tuned BERT versions clearly outperform all other methods, achieving high correlations with human scores for most linguistic qualities rated, on three different datasets.",
    "llm_pred": "Spearman’s ρ, Kendall’s τ, and Pearson’s r.",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ae8354e67978b7c333094c36bf9d561ca0c2d286",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks",
    "final_answer": "They use datasets from the NIST DUC-05, DUC-06, and DUC-07 shared tasks.",
    "slm_pred": "They use datasets from the NIST DUC-05, DUC-06, and DUC-07 shared tasks.",
    "llm_pred": "the NIST DUC-05, DUC-06, and DUC-07 summarization datasets",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.9090909091,
    "perf_llm": 0.7777777778,
    "cost": 5.0
  },
  {
    "id": "02348ab62957cb82067c589769c14d798b1ceec7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction",
    "final_answer": "BiGRU with attention and language model (LM).",
    "slm_pred": "BiGRU with attention and language model (LM).",
    "llm_pred": "BiGRU-with-attention baselines, ROUGE, LM-perplexity baselines (GPT‑2 and BERT masked‑LM), and BERT next‑sentence‑prediction.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.0869565217,
    "cost": 5.0
  },
  {
    "id": "02348ab62957cb82067c589769c14d798b1ceec7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",
    "final_answer": "BiGRU with attention and ROUGE.",
    "slm_pred": "BiGRU with attention and ROUGE.",
    "llm_pred": "BiGRU-with-attention baselines, ROUGE, language-model perplexity (GPT‑2 and BERT masked‑LM), and BERT next-sentence prediction.",
    "perf_chosen": 0.5333333333,
    "perf_slm": 0.5333333333,
    "perf_llm": 0.2608695652,
    "cost": 5.0
  },
  {
    "id": "3748787379b3a7d222c3a6254def3f5bfb93a60e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",
    "final_answer": "The five linguistic quality criteria addressed are given in Figure FIGREF2, and include: $\\mathcal {Q}1$ (Grammaticality), $\\mathcal {Q}2$ (Relevance), $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus), and $\\mathcal {Q}5$ (Structure & Coherence).",
    "slm_pred": "The five linguistic quality criteria addressed are given in Figure FIGREF2, and include: $\\mathcal {Q}1$ (Grammaticality), $\\mathcal {Q}2$ (Relevance), $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus), and $\\mathcal {Q}5$ (Structure & Coherence).",
    "llm_pred": "Grammaticality, Non-redundancy, Referential Clarity, Focus, and Structure & Coherence.",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.9333333333,
    "cost": 5.0
  },
  {
    "id": "6852217163ea678f2009d4726cb6bd03cf6a8f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WN18RR, FB15k-237, YAGO3-10",
    "final_answer": "WN18RR, FB15k-237, and YAGO3-10.",
    "slm_pred": "WN18RR, FB15k-237, and YAGO3-10.",
    "llm_pred": "WN18RR, FB15k-237, and YAGO3-10",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "6852217163ea678f2009d4726cb6bd03cf6a8f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27",
    "final_answer": "WN18RR, FB15k-237, and YAGO3-10.",
    "slm_pred": "WN18RR, FB15k-237, and YAGO3-10.",
    "llm_pred": "WN18RR, FB15k-237, and YAGO3-10",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "cd1ad7e18d8eef8f67224ce47f3feec02718ea1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TransE, DistMult, ComplEx, ConvE, RotatE",
    "final_answer": "TransE, DistMult, ComplEx, ConvE, and RotatE.",
    "slm_pred": "TransE, DistMult, ComplEx, ConvE, and RotatE.",
    "llm_pred": "TransE, DistMult, ComplEx, ConvE, and RotatE",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.9090909091,
    "perf_llm": 0.9090909091,
    "cost": 5.0
  },
  {
    "id": "9c9e90ceaba33242342a5ae7568e89fe660270d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively",
    "final_answer": "HAKE significantly outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task.",
    "slm_pred": "HAKE significantly outperforms several existing state-of-the-art methods on benchmark datasets for the link prediction task.",
    "llm_pred": "It significantly and consistently outperforms state-of-the-art methods (e.g., +0.021 MRR on WN18RR and +0.050 MRR on YAGO3-10 vs. RotatE).",
    "perf_chosen": 0.10169491530000001,
    "perf_slm": 0.10169491530000001,
    "perf_llm": 0.31746031750000003,
    "cost": 5.0
  },
  {
    "id": "2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively",
    "final_answer": "Entities are mapped onto the polar coordinate system by combining the modulus part and the phase part, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information.",
    "slm_pred": "Entities are mapped onto the polar coordinate system by combining the modulus part and the phase part, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information.",
    "llm_pred": "by using modulus as the radial coordinate and phase as the angular coordinate",
    "perf_chosen": 0.5098039216,
    "perf_slm": 0.5098039216,
    "perf_llm": 0.5517241379,
    "cost": 5.0
  },
  {
    "id": "db9021ddd4593f6fadf172710468e2fdcea99674",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "phrase-based word embedding and Abstract Syntax Tree (AST) can be incorporated for improved vocabulary mapping and more accurate target code generation.",
    "slm_pred": "phrase-based word embedding and Abstract Syntax Tree (AST) can be incorporated for improved vocabulary mapping and more accurate target code generation.",
    "llm_pred": "phrase-based word embedding",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "db9021ddd4593f6fadf172710468e2fdcea99674",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "incorporating coding syntax tree model",
    "final_answer": "phrase-based word embedding",
    "slm_pred": "unanswerable",
    "llm_pred": "phrase-based word embedding",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8ea4bd4c1d8a466da386d16e4844ea932c44a412",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A parallel corpus where the source is an English expression of code and the target is Python code.",
    "final_answer": "A text-code parallel corpus with 18805 aligned data, where the source data is in English and the target data is in Python programming language.",
    "slm_pred": "A text-code parallel corpus with 18805 aligned data, where the source data is in English and the target data is in Python programming language.",
    "llm_pred": "a text–code parallel corpus of English descriptions and Python code (18,805 aligned pairs)",
    "perf_chosen": 0.619047619,
    "perf_slm": 0.619047619,
    "perf_llm": 0.5161290323000001,
    "cost": 5.0
  },
  {
    "id": "8ea4bd4c1d8a466da386d16e4844ea932c44a412",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " text-code parallel corpus",
    "final_answer": "They use a text-code parallel corpus with 18805 aligned data in it.",
    "slm_pred": "They use a text-code parallel corpus with 18805 aligned data in it.",
    "llm_pred": "a text–code parallel corpus of 18,805 aligned English descriptions and Python code pairs",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "92240eeab107a4f636705b88f00cefc4f0782846",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThere is no information in the article about comparisons to other models.",
    "slm_pred": "unanswerable \n\nThere is no information in the article about comparisons to other models.",
    "llm_pred": "no",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4196d329061f5a9d147e1e77aeed6a6bd9b35d18",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "seq2seq translation",
    "final_answer": "The architecture of the system is a neural network based machine translation model using a Long Short-Term Memory (LSTM) model with an encoder layer, a decoder layer, and an output layer.",
    "slm_pred": "The architecture of the system is a neural network based machine translation model using a Long Short-Term Memory (LSTM) model with an encoder layer, a decoder layer, and an output layer.",
    "llm_pred": "an encoder–decoder LSTM (seq2seq) with attention, implemented in OpenNMT/PyTorch",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "a37e4a21ba98b0259c36deca0d298194fa611d2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "one line long",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that programmers express a for-loop in a very few specific ways and variable declaration and value assignment expressions are also limited in nature.",
    "llm_pred": "one line long",
    "perf_chosen": 0.0,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "321429282557e79061fe2fe02a9467f3d0118cdd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "phrase-based word embedding, Abstract Syntax Tree(AST)",
    "final_answer": "Phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficial to further improve accuracy.",
    "slm_pred": "Phrase-based word embedding and Abstract Syntax Tree (AST) can be beneficial to further improve accuracy.",
    "llm_pred": "Phrase-based word embeddings and incorporating Abstract Syntax Trees (coding syntax tree models).",
    "perf_chosen": 0.4761904762,
    "perf_slm": 0.4761904762,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "891cab2e41d6ba962778bda297592c916b432226",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Python",
    "final_answer": "Python.",
    "slm_pred": "Python.",
    "llm_pred": "Python",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1eeabfde99594b8d9c6a007f50b97f7f527b0a17",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "validation data",
    "final_answer": "The validation data from the source corpus with 500 lines, which is 3% of the training data, is used to measure accuracy.",
    "slm_pred": "The validation data from the source corpus with 500 lines, which is 3% of the training data, is used to measure accuracy.",
    "llm_pred": "validation data from the source corpus",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "e96adf8466e67bd19f345578d5a6dc68fd0279a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "unsupervised ",
    "final_answer": "The text-to-image synthesis was initially carried out through a search and supervised learning combined process.",
    "slm_pred": "The text-to-image synthesis was initially carried out through a search and supervised learning combined process.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e96adf8466e67bd19f345578d5a6dc68fd0279a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis",
    "final_answer": "The text-to-image synthesis was mainly carried out through a search and supervised learning combined process in the early stages of research.",
    "slm_pred": "The text-to-image synthesis was mainly carried out through a search and supervised learning combined process in the early stages of research.",
    "llm_pred": "supervised (using paired text–image data)",
    "perf_chosen": 0.31746031750000003,
    "perf_slm": 0.31746031750000003,
    "perf_llm": 0.042553191500000004,
    "cost": 5.0
  },
  {
    "id": "c1477a6c86bd1670dd17407590948000c9a6b7c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images",
    "final_answer": "Although the article does not explicitly state the remaining challenges, it mentions that actual AI systems are far from achieving text-to-image synthesis, implying that there are still unresolved challenges in this area.",
    "slm_pred": "Although the article does not explicitly state the remaining challenges, it mentions that actual AI systems are far from achieving text-to-image synthesis, implying that there are still unresolved challenges in this area.",
    "llm_pred": "Generating high-resolution, photo-realistic images from text descriptions.",
    "perf_chosen": 0.10909090910000001,
    "perf_slm": 0.10909090910000001,
    "perf_llm": 0.0666666667,
    "cost": 5.0
  },
  {
    "id": "e020677261d739c35c6f075cde6937d0098ace7f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception",
    "final_answer": "Slightly better than other approaches (e.g., GAN-INT-CLS and StackGAN) on the Oxford-102 Flowers dataset.",
    "slm_pred": "unanswerable",
    "llm_pred": "Slightly better than other approaches (e.g., GAN-INT-CLS and StackGAN) on the Oxford-102 Flowers dataset.",
    "perf_chosen": 0.1555555556,
    "perf_slm": 0.0,
    "perf_llm": 0.1555555556,
    "cost": 25.0
  },
  {
    "id": "7fe48939ce341212c1d801095517dc552b98e7b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "gating mechanism acts upon each dimension of the word and character-level vectors",
    "final_answer": "in the vector gate (vg) used to combine character- and word-level representations",
    "slm_pred": "unanswerable \n\nHowever, the article does mention \"a learned vector gate (also referred to as feature-wise sigmoidal gate)\" but it does not specify where it is employed.",
    "llm_pred": "in the vector gate (vg) used to combine character- and word-level representations",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.0526315789,
    "perf_llm": 0.1666666667,
    "cost": 25.0
  },
  {
    "id": "65ad17f614b7345f0077424c04c94971c831585b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiLSTM with max pooling",
    "final_answer": "BiLSTM with max pooling.",
    "slm_pred": "BiLSTM with max pooling.",
    "llm_pred": "a BiLSTM with max pooling",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "323e100a6c92d3fe503f7a93b96d821408f92109",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BIBREF13 , BIBREF18",
    "final_answer": "They evaluate on 11 downstream transfer tasks, including sentiment classification, subjectivity classification, question-type classification, recognizing textual entailment, estimating semantic relatedness, and measuring textual semantic similarity.",
    "slm_pred": "They evaluate on 11 downstream transfer tasks, including sentiment classification, subjectivity classification, question-type classification, recognizing textual entailment, estimating semantic relatedness, and measuring textual semantic similarity.",
    "llm_pred": "CR, MPQA, MR, SST2, SST5, SUBJ, TREC, SICK E, SICK R, STS16, and STSB.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9f89bff89cea722debc991363f0826de945bc582",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S",
    "final_answer": "They use MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RareWords (RW) datasets for word-level similarity evaluation.",
    "slm_pred": "They use MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RareWords (RW) datasets for word-level similarity evaluation.",
    "llm_pred": "MEN; MTurk287; MTurk771; RG; RW; SimLex999; SimVerb3500; WS353; WS353R; WS353S",
    "perf_chosen": 0.6896551724000001,
    "perf_slm": 0.6896551724000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9f89bff89cea722debc991363f0826de945bc582",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WS353S, SimLex999, SimVerb3500",
    "final_answer": "The authors use the following word-similarity datasets: MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RW.",
    "slm_pred": "The authors use the following word-similarity datasets: MEN, MTurk287, MTurk771, RG, WS353, WS353R, WS353S, SimLex999, SimVerb3500, and RW.",
    "llm_pred": "MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.4615384615,
    "cost": 5.0
  },
  {
    "id": "735f58e28d84ee92024a36bc348cfac2ee114409",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, the New York Times (NYT) datasets are used for experiments, with NYT24 having 24 relations and NYT29 having 29 relations, and both datasets have significantly larger numbers of instances of multiple relation tuples with overlapping entities.",
    "slm_pred": "Yes, the New York Times (NYT) datasets are used for experiments, with NYT24 having 24 relations and NYT29 having 29 relations, and both datasets have significantly larger numbers of instances of multiple relation tuples with overlapping entities.",
    "llm_pred": "Yes—NYT24 (24 relations) and NYT29 (29 relations) are used, but the article does not provide dataset sizes.",
    "perf_chosen": 0.0526315789,
    "perf_slm": 0.0526315789,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "710fa8b3e74ee63d2acc20af19f95f7702b7ce5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WordDecoding (WDec) model",
    "final_answer": "The WordDecoding (WDec) model performed better in experiments, achieving higher F1 scores than the PtrNetDecoding (PNDec) model on the NYT29 and NYT24 datasets.",
    "slm_pred": "The WordDecoding (WDec) model performed better in experiments, achieving higher F1 scores than the PtrNetDecoding (PNDec) model on the NYT29 and NYT24 datasets.",
    "llm_pred": "WordDecoding (WDec) performed better.",
    "perf_chosen": 0.2307692308,
    "perf_slm": 0.2307692308,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "56123dd42cf5c77fc9a88fc311ed2e1eb672126e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention",
    "final_answer": "the previously published baselines: CopyR (BIBREF6), HRL (BIBREF11), and GraphR (BIBREF14)",
    "slm_pred": "Previous work authors refer to researchers BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF9, BIBREF11, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, and BIBREF36.",
    "llm_pred": "the previously published baselines: CopyR (BIBREF6), HRL (BIBREF11), and GraphR (BIBREF14)",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "1898f999626f9a6da637bd8b4857e5eddf2fc729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively",
    "final_answer": "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively.",
    "slm_pred": "Our WordDecoding model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively.",
    "llm_pred": "3.9% and 4.1% higher for WDec, and 3.0% and 1.3% higher for PNDec (vs. HRL).",
    "perf_chosen": 0.6349206349000001,
    "perf_slm": 0.6349206349000001,
    "perf_llm": 0.4210526316,
    "cost": 5.0
  },
  {
    "id": "1898f999626f9a6da637bd8b4857e5eddf2fc729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores",
    "final_answer": "Our WordDecoding (WDec) model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding (PNDec) model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively.",
    "slm_pred": "Our WordDecoding (WDec) model achieves F1 scores that are 3.9% and 4.1% higher than HRL on the NYT29 and NYT24 datasets respectively, and our PtrNetDecoding (PNDec) model achieves F1 scores that are 3.0% and 1.3% higher than HRL on the NYT29 and NYT24 datasets respectively.",
    "llm_pred": "About 3–4% higher (3.9–4.1% for single models; up to 4.2% with ensembles).",
    "perf_chosen": 0.7073170732,
    "perf_slm": 0.7073170732,
    "perf_llm": 0.12244897960000001,
    "cost": 5.0
  },
  {
    "id": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings",
    "final_answer": "- Rank by number of times the citation is mentioned in the document\n- Rank by citation impact (times cited in the literature)\n- Google Scholar “Related Articles” ranking\n- TF*IDF-weighted cosine similarity ranking\n- A learning-to-rank model trained on text similarity rankings",
    "slm_pred": "The baselines were: (1) rank by the number of times a citation is mentioned in the document, (2) rank by the number of times the citation is cited in the literature (citation impact), (3) rank using Google Scholar Related Articles, (4) rank by the TF*IDF weighted cosine similarity, and (5) rank using a learning-to-rank model trained on text similarity rankings.",
    "llm_pred": "- Rank by number of times the citation is mentioned in the document\n- Rank by citation impact (times cited in the literature)\n- Google Scholar “Related Articles” ranking\n- TF*IDF-weighted cosine similarity ranking\n- A learning-to-rank model trained on text similarity rankings",
    "perf_chosen": 0.7415730337,
    "perf_slm": 0.9009009009000001,
    "perf_llm": 0.7415730337,
    "cost": 25.0
  },
  {
    "id": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.",
    "final_answer": "The baselines included ranking by the number of times a citation is mentioned in the document, ranking by the number of times the citation is cited in the literature (citation impact), ranking using Google Scholar Related Articles, ranking by the TF*IDF weighted cosine similarity, and ranking using a learning-to-rank model trained on text similarity rankings.",
    "slm_pred": "The baselines included ranking by the number of times a citation is mentioned in the document, ranking by the number of times the citation is cited in the literature (citation impact), ranking using Google Scholar Related Articles, ranking by the TF*IDF weighted cosine similarity, and ranking using a learning-to-rank model trained on text similarity rankings.",
    "llm_pred": "- Rank by number of times a citation is mentioned in the document\n- Rank by citation impact (times cited in the literature)\n- Google Scholar “Related Articles” ranking\n- TF*IDF-weighted cosine similarity ranking\n- A learning-to-rank model trained on text-similarity rankings",
    "perf_chosen": 0.8288288288000001,
    "perf_slm": 0.8288288288000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "c10f38ee97ed80484c1a70b8ebba9b1fb149bc91",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "SVMRank",
    "final_answer": "SVMRank",
    "slm_pred": "unanswerable \n\nHowever, based on the text, it appears that the authors tested three different supervised models, but they do not explicitly state that they developed a new supervised model.",
    "llm_pred": "SVMRank",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "340501f23ddc0abe344a239193abbaaab938cc3a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations",
    "final_answer": "450 annotated citations",
    "slm_pred": "unanswerable",
    "llm_pred": "450 annotated citations",
    "perf_chosen": 0.1935483871,
    "perf_slm": 0.0,
    "perf_llm": 0.1935483871,
    "cost": 25.0
  },
  {
    "id": "fbb85cbd41de6d2818e77e8f8d4b91e431931faa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "asked the authors to rank by closeness five citations we selected from their paper",
    "final_answer": "unanswerable",
    "slm_pred": "Amazon Mechanical Turk (AMT) and Google Adwords (GA) are mentioned as crowdsourcing platforms, but the article states that the ideal annotators for the task are the authors themselves, and that they used unpaid authors as annotators.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.08,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "1951cde612751410355610074c3c69cec94824c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "autoencoders",
    "final_answer": "The CNN model performed the best with an achieved accuracy of 82.6%.",
    "slm_pred": "The CNN model performed the best with an achieved accuracy of 82.6%.",
    "llm_pred": "CNNs",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "1951cde612751410355610074c3c69cec94824c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CNN",
    "final_answer": "The CNN model performed better with the highest achieved accuracy of 82.6%.",
    "slm_pred": "The CNN model performed better with the highest achieved accuracy of 82.6%.",
    "llm_pred": "CNNs",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4140d8b5a78aea985546aa1e323de12f63d24add",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "61272b1d0338ed7708cf9ed9c63060a6a53e97a2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "accuracy of 82.6%",
    "final_answer": "Their performance on the dataset achieved an accuracy of 82.6% with CNNs, outperforming autoencoders and MLP.",
    "slm_pred": "Their performance on the dataset achieved an accuracy of 82.6% with CNNs, outperforming autoencoders and MLP.",
    "llm_pred": "83.13% accuracy",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "53b02095ba7625d85721692fce578654f66bbdf0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The dataset is not explicitly mentioned as having a specific size, but it is mentioned that a subset of the dataset was used for training (60% of the dataset), and the rest was used for testing and validation (40% of the dataset).",
    "slm_pred": "The dataset is not explicitly mentioned as having a specific size, but it is mentioned that a subset of the dataset was used for training (60% of the dataset), and the rest was used for testing and validation (40% of the dataset).",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0cd0755ac458c3bafbc70e4268c1e37b87b9721b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThey used Amazon Mechanical Turk (MTurk) to collect the Talk The Walk dataset.",
    "slm_pred": "yes \n\nThey used Amazon Mechanical Turk (MTurk) to collect the Talk The Walk dataset.",
    "llm_pred": "yes",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0cd0755ac458c3bafbc70e4268c1e37b87b9721b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "c1ce652085ef9a7f02cb5c363ce2b8757adbe213",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)",
    "final_answer": "The dataset was collected by manually capturing 360-views of several neighborhoods in New York City and crowd-sourcing the collection of the dataset on Amazon Mechanical Turk.",
    "slm_pred": "The dataset was collected by manually capturing 360-views of several neighborhoods in New York City and crowd-sourcing the collection of the dataset on Amazon Mechanical Turk.",
    "llm_pred": "Crowdsourced on Amazon Mechanical Turk using ParlAI’s interface, pairing Turkers as tourist and guide to interact in a 360° NYC street-view grid environment.",
    "perf_chosen": 0.48648648650000004,
    "perf_slm": 0.48648648650000004,
    "perf_llm": 0.2941176471,
    "cost": 5.0
  },
  {
    "id": "96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English",
    "final_answer": "natural language",
    "slm_pred": "natural language",
    "llm_pred": "natural language",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b85ab5f862221fac819cf2fef239bcb08b9cafc6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "localization accuracy",
    "final_answer": "The authors looked at F1 scores, precision, recall, and accuracy as evaluation metrics.",
    "slm_pred": "The authors looked at F1 scores, precision, recall, and accuracy as evaluation metrics.",
    "llm_pred": "localization accuracy and task success rate, plus F1 (with precision/recall) for landmark classification",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "7e34501255b89d64b9598b409d73f96489aafe45",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " dataset on Mechanical Turk involving human perception, action and communication",
    "final_answer": "They used a dataset collected from Amazon Mechanical Turk (MTurk) called Talk The Walk, which consists of over 10k successful dialogues.",
    "slm_pred": "They used a dataset collected from Amazon Mechanical Turk (MTurk) called Talk The Walk, which consists of over 10k successful dialogues.",
    "llm_pred": "The Talk The Walk dataset crowd‑sourced on Amazon Mechanical Turk.",
    "perf_chosen": 0.1935483871,
    "perf_slm": 0.1935483871,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e854edcc5e9111922e6e120ae17d062427c27ec1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "yes",
    "slm_pred": "unanswerable",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e854edcc5e9111922e6e120ae17d062427c27ec1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article mentions the use of the Quora duplicate question dataset, but does not specify the language of this dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0909090909,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "bd6cec2ab620e67b3e0e7946fc045230e6906020",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems",
    "final_answer": "The accuracy of the system is measured using a formula that rewards clustering which matches the article clustering and the total number of claims clustered.",
    "slm_pred": "The accuracy of the system is measured using a formula that rewards clustering which matches the article clustering and the total number of claims clustered.",
    "llm_pred": "By a custom scoring formula that rewards claim clusters aligning with article (story) clusters and the proportion/number of claims included in clusters.",
    "perf_chosen": 0.1694915254,
    "perf_slm": 0.1694915254,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "4b0ba460ae3ba7a813f204abd16cf631b871baca",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "text clustering on the embeddings of texts",
    "final_answer": "automatically retrieved by comparing the incoming claim to existing factchecks in the database.",
    "slm_pred": "automatically retrieved by comparing the incoming claim to existing factchecks in the database.",
    "llm_pred": "By embedding the claim (USE Large), measuring its distance to all stored claims, linking those within a threshold, and returning the ones in the same Louvain-derived cluster.",
    "perf_chosen": 0.1,
    "perf_slm": 0.1,
    "perf_llm": 0.058823529400000005,
    "cost": 5.0
  },
  {
    "id": "63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Quora duplicate question dataset BIBREF22",
    "final_answer": "The Quora duplicate question dataset and the Full Fact database of news articles.",
    "slm_pred": "The Quora duplicate question dataset and the Full Fact database of news articles.",
    "llm_pred": "Quora duplicate question dataset",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "d27f23bcd80b12f6df8e03e65f9b150444925ecf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "claim detection and claim clustering",
    "slm_pred": "The components in the factchecking algorithm include claim detection, claim clustering, and a graph-based method using Louvain Community Detection for community formation.",
    "llm_pred": "claim detection and claim clustering",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b11ee27f3de7dd4a76a1f158dc13c2331af37d9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " path ranking-based KGC (PRKGC)",
    "final_answer": "The baseline is a simple, transparent, and interpretable model created to highlight the challenges and nature of RC-QED.",
    "slm_pred": "The baseline is a simple, transparent, and interpretable model created to highlight the challenges and nature of RC-QED.",
    "llm_pred": "a path ranking–based knowledge graph completion (PRKGC) model",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "7aba5e4483293f5847caad144ee0791c77164917",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WikiHop",
    "final_answer": "The WikiHop dataset was used in the experiment.",
    "slm_pred": "The WikiHop dataset was used in the experiment.",
    "llm_pred": "WikiHop",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "565d668947ffa6d52dad019af79289420505889b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "565d668947ffa6d52dad019af79289420505889b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d83304c70fe66ae72e78aa1d183e9f18b7484cd6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (“Not stated in the article” or “Other”), The “summary” text boxes",
    "final_answer": "The dataset was annotated through a crowdsourcing framework where workers were asked to judge whether a statement could be derived from a given article, and if so, to provide justification explanations in the form of natural language summaries.",
    "slm_pred": "The dataset was annotated through a crowdsourcing framework where workers were asked to judge whether a statement could be derived from a given article, and if so, to provide justification explanations in the form of natural language summaries.",
    "llm_pred": "Crowdsourced on Amazon Mechanical Turk: three workers per instance judged statements (True/Likely/Unsure), selected supporting sentences, and wrote summarized natural language derivations (NLDs).",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e90ac9ee085dc2a9b6fe132245302bbce5f3f5ab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "WikiHop, an entity-based multi-hop QA dataset.",
    "slm_pred": "WikiHop, an entity-based multi-hop QA dataset.",
    "llm_pred": "WikiHop",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5b029ad0d20b516ec11967baaf7d2006e8d7199f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " two labels ",
    "final_answer": "Two labels are considered in the multi-label task, being sentiment polarity and the candidate/category in consideration.",
    "slm_pred": "Two labels are considered in the multi-label task, being sentiment polarity and the candidate/category in consideration.",
    "llm_pred": "two",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "79bd2ad4cb5c630ce69d5a859ed118132cae62d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d3a1a53521f252f869fdae944db986931d9ffe48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "political pundits of the Washington Post",
    "final_answer": "The Washington Post political pundits.",
    "slm_pred": "The Washington Post political pundits.",
    "llm_pred": "the political pundits of the Washington Post",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.9090909091,
    "perf_llm": 0.9230769231,
    "cost": 5.0
  },
  {
    "id": "d3a1a53521f252f869fdae944db986931d9ffe48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the experts in the field",
    "final_answer": "Washington Post political pundits.",
    "slm_pred": "Washington Post political pundits.",
    "llm_pred": "the political pundits of the Washington Post",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "38e11663b03ac585863742044fd15a0e875ae9ab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " peoples' sentiments expressed over social media",
    "final_answer": "The crowd in these experiments refers to the users of social media, specifically those who post and share tweets on Twitter.",
    "slm_pred": "The crowd in these experiments refers to the users of social media, specifically those who post and share tweets on Twitter.",
    "llm_pred": "Twitter users (the public posting on social media microblogs)",
    "perf_chosen": 0.14814814810000002,
    "perf_slm": 0.14814814810000002,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "14421b7ae4459b647033b3ccba635d4ba7bb114b",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "experts in Washington Post",
    "final_answer": "the Washington Post experts’ declared winners",
    "slm_pred": "The ground truth of who won a debate is established by the results given by the experts in Washington Post.",
    "llm_pred": "the Washington Post experts’ declared winners",
    "perf_chosen": 0.4,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "52f7e42fe8f27d800d1189251dfec7446f0e1d3b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",
    "final_answer": "Significantly better, as shown by the experimental results.",
    "slm_pred": "Significantly better, as shown by the experimental results.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we use t-SNE tool BIBREF27 to visualize the learned embedding",
    "final_answer": "The learned embeddings are further analyzed using t-SNE tool to visualize the learned embedding, and efficiency evaluation is also done to show the test accuracy with increasing epoch and different embedding sizes.",
    "slm_pred": "The learned embeddings are further analyzed using t-SNE tool to visualize the learned embedding, and efficiency evaluation is also done to show the test accuracy with increasing epoch and different embedding sizes.",
    "llm_pred": "t-SNE visualization of the learned embeddings",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.375,
    "cost": 5.0
  },
  {
    "id": "aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26",
    "final_answer": "Three typical knowledge graph embedding methods (TransE, TransR, and TransH), two path-based models (PTransE and ALL-PATHS), and three attribute-incorporated methods (R-GCN and KR-EAR) along with four variants of KANE are used for comparison.",
    "slm_pred": "Three typical knowledge graph embedding methods (TransE, TransR, and TransH), two path-based models (PTransE and ALL-PATHS), and three attribute-incorporated methods (R-GCN and KR-EAR) along with four variants of KANE are used for comparison.",
    "llm_pred": "TransE, TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR.",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "cf0085c1d7bd9bc9932424e4aba4e6812d27f727",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "FB24K, DBP24K, Game30K",
    "final_answer": "The three datasets used to measure performance are DBP24K, Game30K, and FB24K.",
    "slm_pred": "The three datasets used to measure performance are DBP24K, Game30K, and FB24K.",
    "llm_pred": "DBP24K, FB24K, and Game30K.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "cf0085c1d7bd9bc9932424e4aba4e6812d27f727",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph",
    "final_answer": "DBP24K, Game30K, and FB24K.",
    "slm_pred": "DBP24K, Game30K, and FB24K.",
    "llm_pred": "DBP24K, Game30K, and FB24K.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "586b7470be91efe246c3507b05e30651ea6b9832",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.",
    "final_answer": "KANE captures both high-order structural and attribute information of KGs through recursive embedding propagation based on relation triples and multi-head attention-based aggregation of attribute triples.",
    "slm_pred": "KANE captures both high-order structural and attribute information of KGs through recursive embedding propagation based on relation triples and multi-head attention-based aggregation of attribute triples.",
    "llm_pred": "By recursively propagating embeddings along relation triples to capture high-order structure and using multi‑head attention to aggregate attribute triples within a GCN framework.",
    "perf_chosen": 0.45,
    "perf_slm": 0.45,
    "perf_llm": 0.15789473680000002,
    "cost": 5.0
  },
  {
    "id": "31b20a4bab09450267dfa42884227103743e3426",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24",
    "final_answer": "TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR.",
    "slm_pred": "TransH, TransR, PTransE, ALL-PATHS, R-GCN, and KR-EAR.",
    "llm_pred": "TransH and TransR",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "45306b26447ea4b120655d6bb2e3636079d3d6e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "Yes",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0c08af6e4feaf801185f2ec97c4da04c8b767ad6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The authors mention a few confounds such as \"incorrect hashtag supervision\" and \"pragmatic difficulty\" in their study.",
    "slm_pred": "The authors mention a few confounds such as \"incorrect hashtag supervision\" and \"pragmatic difficulty\" in their study.",
    "llm_pred": "yes—noisy hashtag-based supervision and the challenge of selecting true “sober” negatives (e.g., mislabeled/retrospective tweets)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6412e97373e8e9ae3aa20aa17abef8326dc05450",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Human evaluators",
    "final_answer": "The baseline model used is SVM (Support Vector Machine) classifier.",
    "slm_pred": "The baseline model used is SVM (Support Vector Machine) classifier.",
    "llm_pred": "Support Vector Machine (SVM)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "957bda6b421ef7d2839c3cec083404ac77721f14",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio",
    "final_answer": "capitalization patterns, spelling errors, POS tag ratios, named entity counts, discourse connectors, repeated characters (e.g., “happpy”), emoticon counts, sentiment ratio (positive/negative words), and tweet length",
    "slm_pred": "Spelling errors, capitalisation, POS ratios, length, and sentiment ratios.",
    "llm_pred": "capitalization patterns, spelling errors, POS tag ratios, named entity counts, discourse connectors, repeated characters (e.g., “happpy”), emoticon counts, sentiment ratio (positive/negative words), and tweet length",
    "perf_chosen": 0.6086956522,
    "perf_slm": 0.4666666667,
    "perf_llm": 0.6086956522,
    "cost": 25.0
  },
  {
    "id": "957bda6b421ef7d2839c3cec083404ac77721f14",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.",
    "final_answer": "capitalization patterns, spelling errors, POS tag ratios, named-entity mentions, discourse connectors, repeated characters (elongations), emoticon counts/presence, sentiment ratio (positive/negative word proportion), and tweet length",
    "slm_pred": "Spelling errors, POS ratios, length, sentiment ratios, capitalisation, and repeated characters.",
    "llm_pred": "capitalization patterns, spelling errors, POS tag ratios, named-entity mentions, discourse connectors, repeated characters (elongations), emoticon counts/presence, sentiment ratio (positive/negative word proportion), and tweet length",
    "perf_chosen": 0.5909090909,
    "perf_slm": 0.4516129032,
    "perf_llm": 0.5909090909,
    "cost": 25.0
  },
  {
    "id": "368317b4fd049511e00b441c2e9550ded6607c37",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes. \n\nThe article states that the authors use hashtag-based supervision, but also mention that the hashtags are removed so that they provide labels, but do not act as features. This suggests that the labels provided by the hashtags are verified by the authors themselves.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.044444444400000005,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b3ec918827cd22b16212265fcdd5b3eadee654ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "387970ebc7ef99f302f318d047f708274c0e8f21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The authors equate drunk tweeting with drunk texting, as they refer to both as \"drunk-texting\" throughout the paper.",
    "slm_pred": "The authors equate drunk tweeting with drunk texting, as they refer to both as \"drunk-texting\" throughout the paper.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2fffff59e57b8dbcaefb437a6b3434fc137f813b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining",
    "final_answer": "The text corpora (S) from aristo2016:combining.",
    "slm_pred": "The text corpora (S) from aristo2016:combining.",
    "llm_pred": "the Sentence corpus (S) from Aristo (BIBREF6)",
    "perf_chosen": 0.2608695652,
    "perf_slm": 0.2608695652,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "2fffff59e57b8dbcaefb437a6b3434fc137f813b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The text corpora (S) from BIBREF6 aristo2016:combining.",
    "slm_pred": "The text corpora (S) from BIBREF6 aristo2016:combining.",
    "llm_pred": "the Sentence corpus (S) used in BIBREF6 (Aristo)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "eb95af36347ed0e0808e19963fe4d058e2ce3c9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge",
    "final_answer": "The proposed technique, TupleInf, achieves a statistically significant improvement of 11.8% over the state-of-the-art system, TableILP, on a broad set of over 1,300 science questions.",
    "slm_pred": "The proposed technique, TupleInf, achieves a statistically significant improvement of 11.8% over the state-of-the-art system, TableILP, on a broad set of over 1,300 science questions.",
    "llm_pred": "61.4%",
    "perf_chosen": 0.0701754386,
    "perf_slm": 0.0701754386,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "cd1792929b9fa5dd5b1df0ae06fc6aece4c97424",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "yes \n\nThis is based on the information that the article uses Open IE v4 to extract tuples from text corpora, which involves entity linking.",
    "slm_pred": "yes \n\nThis is based on the information that the article uses Open IE v4 to extract tuples from text corpora, which involves entity linking.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "65d34041ffa4564385361979a08706b10b92ebc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "no \n\nThe article states that \"We define a tuple as (subject; predicate; objects) with zero or more objects.\"",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.1052631579,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$",
    "final_answer": "ElasticSearch query against a text corpora (S), followed by running Open IE v4.",
    "slm_pred": "ElasticSearch query against a text corpora (S), followed by running Open IE v4.",
    "llm_pred": "Open IE v4",
    "perf_chosen": 0.23529411760000002,
    "perf_slm": 0.23529411760000002,
    "perf_llm": 0.1034482759,
    "cost": 5.0
  },
  {
    "id": "a8545f145d5ea2202cb321c8f93e75ad26fcf4aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "The answer is unanswerable, as the article discusses the method's ability to answer complex questions that require multiple facts, but it does not explicitly state whether it can answer multi-hop questions. However, it does mention the possibility of adding inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates, which could be related to multi-hop reasoning.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "417dabd43d6266044d38ed88dbcb5fdd7a426b22",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining",
    "final_answer": "The textual source to which OpenIE was applied is the text corpora (S) from aristo2016:combining.",
    "slm_pred": "The textual source to which OpenIE was applied is the text corpora (S) from aristo2016:combining.",
    "llm_pred": "the sentence corpus S (domain-targeted sentences and web text from Aristo)",
    "perf_chosen": 0.1875,
    "perf_slm": 0.1875,
    "perf_llm": 0.4285714286,
    "cost": 5.0
  },
  {
    "id": "fed230cef7c130f6040fb04304a33bbc17ca3a36",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$",
    "final_answer": "Open IE v4.",
    "slm_pred": "Open IE v4.",
    "llm_pred": "Open IE v4",
    "perf_chosen": 0.1034482759,
    "perf_slm": 0.1034482759,
    "perf_llm": 0.1034482759,
    "cost": 5.0
  },
  {
    "id": "7917d44e952b58ea066dc0b485d605c9a1fe3dda",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The answer is yes.",
    "slm_pred": "The answer is yes.",
    "llm_pred": "no",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7d5ba230522df1890619dedcfb310160958223c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The primary reason for the dense mode outperforming the sparse one is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.",
    "slm_pred": "The primary reason for the dense mode outperforming the sparse one is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.",
    "llm_pred": "Yes—because averaging dense word vectors implicitly handles semantically similar words, providing smoothing that reduces sparsity.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a48cc6d3d322a7b159ff40ec162a541bf74321eb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Word Sense Induction & Disambiguation",
    "final_answer": "An evaluation using the SemEval 2010 Task 14 (Word Sense Induction & Disambiguation) methodology, measured by adjusted Rand index (ARI).",
    "slm_pred": "The evaluation is conducted using the SemEval 2010 Task 14 evaluation methodology, specifically the adjusted Rand index (ARI) as the quality measure.",
    "llm_pred": "An evaluation using the SemEval 2010 Task 14 (Word Sense Induction & Disambiguation) methodology, measured by adjusted Rand index (ARI).",
    "perf_chosen": 0.347826087,
    "perf_slm": 0.0,
    "perf_llm": 0.347826087,
    "cost": 25.0
  },
  {
    "id": "2bc0bb7d3688fdd2267c582ca593e2ce72718a91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wiktionary",
    "final_answer": "RuThes, a large-scale lexical ontology for Russian, and Wiktionary referred as Joint Exp SWN.",
    "slm_pred": "RuThes, a large-scale lexical ontology for Russian, and Wiktionary referred as Joint Exp SWN.",
    "llm_pred": "Watlink, RuThes, and RuWordNet.",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "cosine similarity",
    "final_answer": "cosine similarity",
    "slm_pred": "Cosine similarity.",
    "llm_pred": "cosine similarity",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "dcb18516369c3cf9838e83168357aed6643ae1b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.",
    "final_answer": "The Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 were used for Quasar-T, while for Quasar-S, Lucene retrieval was used.",
    "slm_pred": "The Sliding Window (SW) and Sliding Window + Distance (SW+D) baselines proposed in BIBREF13 were used for Quasar-T, while for Quasar-S, Lucene retrieval was used.",
    "llm_pred": "Apache Lucene",
    "perf_chosen": 0.1395348837,
    "perf_slm": 0.1395348837,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f46a907360d75ad566620e7f6bf7746497b6e4a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Kyubyong Park, Edouard Grave et al BIBREF11",
    "final_answer": "The word embeddings used were pre-trained word embeddings for Vietnamese created by Kyubyong Park and Edouard Grave, specifically vectors of 100 dimensions and 300 dimensions, respectively, generated from Wikipedia database backup dumps.",
    "slm_pred": "The word embeddings used were pre-trained word embeddings for Vietnamese created by Kyubyong Park and Edouard Grave, specifically vectors of 100 dimensions and 300 dimensions, respectively, generated from Wikipedia database backup dumps.",
    "llm_pred": "pre-trained Vietnamese embeddings by Kyubyong Park (word2vec/fastText, 100-d) and by Edouard Grave et al. (fastText, 300-d)",
    "perf_chosen": 0.20512820510000002,
    "perf_slm": 0.20512820510000002,
    "perf_llm": 0.5217391304,
    "cost": 5.0
  },
  {
    "id": "79d999bdf8a343ce5b2739db3833661a1deab742",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag",
    "final_answer": "The BLSTM-CNN-CRF system produced five types of errors: No extraction, No annotation, Wrong range, Wrong tag, and Wrong range and tag.",
    "slm_pred": "The BLSTM-CNN-CRF system produced five types of errors: No extraction, No annotation, Wrong range, Wrong tag, and Wrong range and tag.",
    "llm_pred": "No extraction; No annotation; Wrong range; Wrong tag; Wrong range and tag.",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "71d59c36225b5ee80af11d3568bdad7425f17b0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the BLSTM-CNN-CRF performed better than the BLSTM-CRF as it was used in the analysis experiments and achieved a 91.62 F1 score in comparison to another model that used BLSTM-CRF which achieved a 90.94 F1 score.",
    "slm_pred": "unanswerable \n\nHowever, it can be inferred that the BLSTM-CNN-CRF performed better than the BLSTM-CRF as it was used in the analysis experiments and achieved a 91.62 F1 score in comparison to another model that used BLSTM-CRF which achieved a 90.94 F1 score.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1509433962,
    "perf_slm": 0.1509433962,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "efc65e5032588da4a134d121fe50d49fe8fe5e8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",
    "final_answer": "Task A is used as a main task, and tasks B and C are used as auxiliary tasks for multitask learning.",
    "slm_pred": "Task A is used as a main task, and tasks B and C are used as auxiliary tasks for multitask learning.",
    "llm_pred": "tasks A and B (question–comment relevance and question–question relevance)",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "a30958c7123d1ad4723dcfd19d8346ccedb136d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "08333e4dd1da7d6b5e9b645d40ec9d502823f5d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",
    "final_answer": "The article does not explicitly mention the performance gap between their approach and the strong handcrafted method.",
    "slm_pred": "The article does not explicitly mention the performance gap between their approach and the strong handcrafted method.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "bc1bc92920a757d5ec38007a27d0f49cb2dde0d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the context, it seems that the article is comparing different systems, and the feature-rich based system is mentioned as a strong system that performs well on tasks A and C.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0571428571,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "942eb1f7b243cdcfd47f176bcc71de2ef48a17c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, the authors performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task.",
    "slm_pred": "Yes, the authors performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task.",
    "llm_pred": "yes—preliminary experiments on the Arabic portion of SemEval-2016",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9bffc9a9c527e938b2a95ba60c483a916dbd1f6b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes. \n\nThe article states that the Multi-Head Attention (MHA) can learn n_head different scores in parallel child spaces and is very powerful for alignments.",
    "slm_pred": "Yes. \n\nThe article states that the Multi-Head Attention (MHA) can learn n_head different scores in parallel child spaces and is very powerful for alignments.",
    "llm_pred": "yes",
    "perf_chosen": 0.08,
    "perf_slm": 0.08,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8434974090491a3c00eed4f22a878f0b70970713",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Proposed model has 1.16 million parameters and 11.04 MB.",
    "final_answer": "AEN-GloVe has a relatively small model size, ranking second after MemNet, and is significantly smaller than most other models, including AEN-GloVe-BiLSTM.",
    "slm_pred": "AEN-GloVe has a relatively small model size, ranking second after MemNet, and is significantly smaller than most other models, including AEN-GloVe-BiLSTM.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b67420da975689e47d3ea1c12b601851018c4071",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.",
    "final_answer": "The proposed model is different from BERT as it employs attentional encoder layers and a label smoothing regularization to enhance performance on targeted sentiment classification tasks.",
    "slm_pred": "The proposed model is different from BERT as it employs attentional encoder layers and a label smoothing regularization to enhance performance on targeted sentiment classification tasks.",
    "llm_pred": "A lightweight, task‑specific attentional encoder that models target–context interactions (intra/inter multi‑head attention with point‑wise conv and label smoothing) rather than relying on BERT’s generic encoder.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.2264150943,
    "cost": 5.0
  },
  {
    "id": "a4e66e842be1438e5cd8d7cb2a2c589f494aee27",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Depeche + SVM",
    "final_answer": "Depeche++ emotion lexicons + Linear SVM",
    "slm_pred": "unanswerable",
    "llm_pred": "Depeche++ emotion lexicons + Linear SVM",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.0,
    "perf_llm": 0.5714285714,
    "cost": 25.0
  },
  {
    "id": "cb78e280e3340b786e81636431834b75824568c3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "9",
    "final_answer": "Nine emotions (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, and Neutral).",
    "slm_pred": "Nine emotions (Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, and Neutral).",
    "llm_pred": "9",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2941874356e98eb2832ba22eae9cb08ec8ce0308",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT",
    "final_answer": "Bag-of-words-based benchmarks using methods such as Classification with TF-IDF + Linear SVM, Classification with Depeche++ Emotion lexicons + Linear SVM, Classification with NRC Emotion lexicons + Linear SVM, and Combination of TF-IDF and NRC Emotion lexicons + Linear SVM.",
    "slm_pred": "Bag-of-words-based benchmarks using methods such as Classification with TF-IDF + Linear SVM, Classification with Depeche++ Emotion lexicons + Linear SVM, Classification with NRC Emotion lexicons + Linear SVM, and Combination of TF-IDF and NRC Emotion lexicons + Linear SVM.",
    "llm_pred": "TF‑IDF + SVM; Depeche++ emotion lexicons + SVM; NRC emotion lexicons + SVM; TF‑IDF + NRC + SVM; Doc2Vec + SVM; Hierarchical BiLSTM; BiRNN with self‑attention; ELMo + BiRNN; and fine‑tuned BERT.",
    "perf_chosen": 0.2641509434,
    "perf_slm": 0.2641509434,
    "perf_llm": 0.6046511628,
    "cost": 5.0
  },
  {
    "id": "4e50e9965059899d15d3c3a0c0a2d73e0c5802a0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words",
    "final_answer": "The dataset contains a total of 9710 passages.",
    "slm_pred": "The dataset contains a total of 9710 passages.",
    "llm_pred": "9710 passages",
    "perf_chosen": 0.20689655170000001,
    "perf_slm": 0.20689655170000001,
    "perf_llm": 0.1739130435,
    "cost": 5.0
  },
  {
    "id": "67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3 ",
    "final_answer": "There were 3 unique annotators per passage, with in-house annotators also being used for consolidating labels when necessary.",
    "slm_pred": "There were 3 unique annotators per passage, with in-house annotators also being used for consolidating labels when necessary.",
    "llm_pred": "3",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aecb485ea7d501094e50ad022ade4f0c93088d80",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "2fea3c955ff78220b2c31a8ad1322bc77f6706f8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them",
    "final_answer": "Our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them.",
    "slm_pred": "Our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them.",
    "llm_pred": "That the method effectively controls the morphological realization of first‑ and second‑person pronouns and their associated verbs/adjectives in translation.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Speaker's Gender Effects, Interlocutors' Gender and Number Effects",
    "final_answer": "Fine-grained syntactic analysis is performed, focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements.",
    "slm_pred": "Fine-grained syntactic analysis is performed, focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements.",
    "llm_pred": "dependency parsing (using a Hebrew dependency parser)",
    "perf_chosen": 0.0625,
    "perf_slm": 0.0625,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "da068b20988883bc324e55c073fb9c1a5c39be33",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference",
    "final_answer": "It is demonstrated through a fine-grained syntactic analysis of the translations generated using the proposed method, as well as through quantitative results showing an improvement in BLEU scores when the correct information is provided.",
    "slm_pred": "It is demonstrated through a fine-grained syntactic analysis of the translations generated using the proposed method, as well as through quantitative results showing an improvement in BLEU scores when the correct information is provided.",
    "llm_pred": "By adding gender/number-marked prefixes (e.g., “She said to them:”) and verifying—via a Hebrew dependency parse of the outputs—that verbs and pronouns show the expected feminine/plural morphology (also reflected in higher BLEU).",
    "perf_chosen": 0.24657534250000002,
    "perf_slm": 0.24657534250000002,
    "perf_llm": 0.22857142860000002,
    "cost": 5.0
  },
  {
    "id": "0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Google's machine translation system (GMT)",
    "final_answer": "Google's machine translation system (GMT) accessed through its Cloud API.",
    "slm_pred": "Google's machine translation system (GMT) accessed through its Cloud API.",
    "llm_pred": "Google’s machine translation system (Google Translate)",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "edcde2b675cf8a362a63940b2bbdf02c150fe01f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences",
    "final_answer": "The black-box context injection system consists of a pre-defined textual hint and a wrapper method that injects the desired information into the input sentence to influence the output of a trained NMT system.",
    "slm_pred": "The black-box context injection system consists of a pre-defined textual hint and a wrapper method that injects the desired information into the input sentence to influence the output of a trained NMT system.",
    "llm_pred": "pre-processing to add gender/number-indicating prefixes to the input and post-processing to strip the translated prefix from the output",
    "perf_chosen": 0.2127659574,
    "perf_slm": 0.2127659574,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "d20d6c8ecd7cb0126479305d27deb0c8b642b09f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)",
    "final_answer": "Cepstral mean normalization (CMN) and variance with mean normalization (CMVN) are mentioned.",
    "slm_pred": "Cepstral mean normalization (CMN) and variance with mean normalization (CMVN) are mentioned.",
    "llm_pred": "- Cepstral mean normalization (CMN)\n- Cepstral mean and variance normalization (CMVN)\n- Batch normalization",
    "perf_chosen": 0.7826086957,
    "perf_slm": 0.7826086957,
    "perf_llm": 0.6956521739,
    "cost": 5.0
  },
  {
    "id": "11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram",
    "final_answer": "They experiment with 40 mel-scaled log filterbank energies (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector), and also spectrogram.",
    "slm_pred": "They experiment with 40 mel-scaled log filterbank energies (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector), and also spectrogram.",
    "llm_pred": "mel filterbank energies (with deltas/delta-deltas) and spectrograms, under various normalizations (CMN/CMVN)",
    "perf_chosen": 0.8235294118,
    "perf_slm": 0.8235294118,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "2677b88c2def3ed94e25a776599555a788d197f2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "6-layer bLSTM with 1024 hidden units",
    "final_answer": "A 6-layer bidirectional LSTM with 1024 hidden units.",
    "slm_pred": "A 6-layer bidirectional LSTM with 1024 hidden units.",
    "llm_pred": "6-layer bidirectional LSTM (bLSTM) with 1024 hidden units.",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "8ca31caa34cc5b65dc1d01d0d1f36bf8c4928805",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "Conversational speech.",
    "slm_pred": "Conversational speech.",
    "llm_pred": "conversational speech",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9ab43f941c11a4b09a0e4aea61b4a5b4612e7933",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span",
    "final_answer": "Previous models, such as MTMSN, used approaches like training a categorical variable to predict the number of spans to extract and generalizing the single-span head method using non-maximum suppression (NMS) algorithm.",
    "slm_pred": "Previous models, such as MTMSN, used approaches like training a categorical variable to predict the number of spans to extract and generalizing the single-span head method using non-maximum suppression (NMS) algorithm.",
    "llm_pred": "Predict the number of spans and then generalize single-span extraction using non‑maximum suppression to select multiple non‑overlapping spans.",
    "perf_chosen": 0.4324324324,
    "perf_slm": 0.4324324324,
    "perf_llm": 0.2950819672,
    "cost": 5.0
  },
  {
    "id": "5a02a3dd26485a4e4a77411b50b902d2bda3731b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span",
    "final_answer": "They use sequence tagging by individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually.",
    "slm_pred": "They use sequence tagging by individually tagging each token with a categorical tag, relying on the tokens' contextual representation to bridge the information gap resulting from the tokens being tagged individually.",
    "llm_pred": "By tagging each token with BIO labels over the concatenated question–passage, then decoding the BIO sequence (via beam search/greedy) to yield the set of answer spans.",
    "perf_chosen": 0.16279069770000001,
    "perf_slm": 0.16279069770000001,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "579941de2838502027716bae88e33e79e69997a6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",
    "final_answer": "The proposed model does not hinder performance on other question types, actually improving results on these types compared to the NABERT+ baseline.",
    "slm_pred": "The proposed model does not hinder performance on other question types, actually improving results on these types compared to the NABERT+ baseline.",
    "llm_pred": "similar results",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9a65cfff4d99e4f9546c72dece2520cae6231810",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",
    "final_answer": "Our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.",
    "slm_pred": "Our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.",
    "llm_pred": "Best overall on DROP, slightly eclipsing the prior state of the art.",
    "perf_chosen": 0.23529411760000002,
    "perf_slm": 0.23529411760000002,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "a9def7958eac7b9a780403d4f136927f756bab83",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MTMSN BIBREF4",
    "final_answer": "MTMSN.",
    "slm_pred": "MTMSN.",
    "llm_pred": "MTMSN",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "547be35cff38028648d199ad39fb48236cfb99ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, it is implied that the XR model has access to a large collection of un-annotated text from the same distribution, whereas the fully supervised model does not.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0666666667,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e42fbf6c183abf1c6c2321957359c7683122b48e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n",
    "final_answer": "The aspect based sentiment classifier trained only using the XR loss achieves a F1 score of 53.81 with a classifier trained on 100 labeled sentences.",
    "slm_pred": "The aspect based sentiment classifier trained only using the XR loss achieves a F1 score of 53.81 with a classifier trained on 100 labeled sentences.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e574f0f733fb98ecef3c64044004aa7a320439be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "DISPLAYFORM0",
    "final_answer": "The expectation regularization loss is defined as the KL-divergence between the model's posterior distribution over a set and the expected label distribution, given by D(P || Q) = ∑[p(x)log(p(x)/q(x))], where p(x) is the expected label distribution and q(x) is the model's posterior distribution.",
    "slm_pred": "The expectation regularization loss is defined as the KL-divergence between the model's posterior distribution over a set and the expected label distribution, given by D(P || Q) = ∑[p(x)log(p(x)/q(x))], where p(x) is the expected label distribution and q(x) is the model's posterior distribution.",
    "llm_pred": "KL(qS || pS) over each set S, i.e., LXR(S) = −∑k qkS log pkS with pkS = (1/|S|)∑x∈S pθ(y=k|x).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The Lemming model in BIBREF17",
    "final_answer": "The Lemming model (a log-linear joint tagger–lemmatizer with a second‑order CRF and edit‑tree pruning).",
    "slm_pred": "The Lemming model, a log-linear model that performs joint morphological tagging and lemmatization, was used as a non-neural baseline for Task 2.",
    "llm_pred": "The Lemming model (a log-linear joint tagger–lemmatizer with a second‑order CRF and edit‑tree pruning).",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.3157894737,
    "cost": 25.0
  },
  {
    "id": "bd3ccb63fd8ce5575338d7332e96def7a3fabad6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ROMULUS dataset, NLU-Benchmark dataset",
    "final_answer": "The NLU-Benchmark (NLU-BM) dataset, which contains 25,716 utterances, and the ROMULUS dataset, which contains 1,431 sentences, are both publicly available and used in the research.",
    "slm_pred": "The NLU-Benchmark (NLU-BM) dataset, which contains 25,716 utterances, and the ROMULUS dataset, which contains 1,431 sentences, are both publicly available and used in the research.",
    "llm_pred": "NLU-Benchmark (NLU-BM)",
    "perf_chosen": 0.27586206900000004,
    "perf_slm": 0.27586206900000004,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "7c794fa0b2818d354ca666969107818a2ffdda0c",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities",
    "final_answer": "Intent F1 and Exact Match (EM) accuracy of the entire sequence of labels.",
    "slm_pred": "Intent F1 and Exact Match (EM) accuracy of the entire sequence of labels.",
    "llm_pred": "intent F1 and the combined intent-and-entity F1",
    "perf_chosen": 0.6046511628,
    "perf_slm": 0.6046511628,
    "perf_llm": 0.2162162162,
    "cost": 5.0
  },
  {
    "id": "1ef5fc4473105f1c72b4d35cf93d312736833d3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, they do not provide decision sequences as supervision while training models, but rather use reinforcement learning to train the models.",
    "slm_pred": "No, they do not provide decision sequences as supervision while training models, but rather use reinforcement learning to train the models.",
    "llm_pred": "no",
    "perf_chosen": 0.0909090909,
    "perf_slm": 0.0909090909,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)",
    "final_answer": "SQuAD and NewsQA, which are repurposed for the interactive MRC task.",
    "slm_pred": "SQuAD and NewsQA, which are repurposed for the interactive MRC task.",
    "llm_pred": "SQuAD and NewsQA.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.1,
    "cost": 5.0
  },
  {
    "id": "b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).",
    "final_answer": "They use a combination of reinforcement learning (Rainbow algorithm) and supervised learning to train the models.",
    "slm_pred": "They use a combination of reinforcement learning (Rainbow algorithm) and supervised learning to train the models.",
    "llm_pred": "With a hybrid scheme: the action generator is trained via Rainbow DQN with experience replay (sample 64 from a 500k buffer every 5 steps), and the question answerer is trained supervised with NLL on ground-truth spans from a separate replay buffer (sampled every 5 steps), reporting test results from the checkpoint with best validation performance.",
    "perf_chosen": 0.2325581395,
    "perf_slm": 0.2325581395,
    "perf_llm": 0.0487804878,
    "cost": 5.0
  },
  {
    "id": "e9cf1b91f06baec79eb6ddfd91fc5d434889f652",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "previous, next, Ctrl+F $<$query$>$, stop",
    "final_answer": "Ctrl+F (search for token) and stop.",
    "slm_pred": "Ctrl+F (search for token) and stop.",
    "llm_pred": "Ctrl+F (search), next, previous, and stop.",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "6976296126e4a5c518e6b57de70f8dc8d8fde292",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)",
    "final_answer": "The proposed multimodal models are the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
    "slm_pred": "The proposed multimodal models are the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
    "llm_pred": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), and Textual Kernels Model (TKM).",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 0.96,
    "cost": 5.0
  },
  {
    "id": "53640834d68cf3b86cf735ca31f1c70aa0006b72",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not mention the language of the tweets.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b2b0321b0aaf58c3aa9050906ade6ef35874c5c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " $150,000$ tweets",
    "final_answer": "The dataset, called MMHS150K, is formed by 150,000 tweets.",
    "slm_pred": "The dataset, called MMHS150K, is formed by 150,000 tweets.",
    "llm_pred": "150,000 tweets",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4e9684fd68a242cb354fa6961b0e3b5c35aae4b6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ",
    "final_answer": "The multimodal models do not outperform the unimodal textual models.",
    "slm_pred": "The multimodal models do not outperform the unimodal textual models.",
    "llm_pred": "They do not outperform unimodal (text-only) models, yielding similar performance.",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.0666666667,
    "cost": 5.0
  },
  {
    "id": "2e632eb5ad611bbd16174824de0ae5efe4892daf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples",
    "final_answer": "The author believes it is due to noisy data, complexity and diversity of multimodal relations, and a small set of multimodal examples.",
    "slm_pred": "The author believes it is due to noisy data, complexity and diversity of multimodal relations, and a small set of multimodal examples.",
    "llm_pred": "Noisy, subjective annotations plus the complex, background-knowledge–heavy visual–text relations and too few truly multimodal hate examples lead models to rely mostly on text, so they don’t beat text-only methods.",
    "perf_chosen": 0.7428571429,
    "perf_slm": 0.7428571429,
    "perf_llm": 0.2380952381,
    "cost": 5.0
  },
  {
    "id": "d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)",
    "final_answer": "The F-score, the Area Under the ROC Curve (AUC), and the mean accuracy (ACC) are used to benchmark the results.",
    "slm_pred": "The F-score, the Area Under the ROC Curve (AUC), and the mean accuracy (ACC) are used to benchmark the results.",
    "llm_pred": "F-score, area under the ROC curve (AUC), and mean accuracy (ACC).",
    "perf_chosen": 0.5106382979,
    "perf_slm": 0.5106382979,
    "perf_llm": 0.5263157895,
    "cost": 5.0
  },
  {
    "id": "24c0f3d6170623385283dfda7f2b6ca2c7169238",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Twitter API",
    "final_answer": "The data is collected using the Twitter API.",
    "slm_pred": "The data is collected using the Twitter API.",
    "llm_pred": "Twitter API",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "$150,000$ tweets",
    "final_answer": "Correct, 150,000.",
    "slm_pred": "Correct, 150,000.",
    "llm_pred": "150,000 tweets",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification",
    "final_answer": "The unimodal detection models used were a CNN for image features extraction and a single layer LSTM for tweet text classification.",
    "slm_pred": "The unimodal detection models used were a CNN for image features extraction and a single layer LSTM for tweet text classification.",
    "llm_pred": "a CNN (Inception v3) for images and a single-layer LSTM for text (including OCR’d image text)",
    "perf_chosen": 0.3529411765,
    "perf_slm": 0.3529411765,
    "perf_llm": 0.20689655170000001,
    "cost": 5.0
  },
  {
    "id": "b0799e26152197aeb3aa3b11687a6cc9f6c31011",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)",
    "final_answer": "The different models for multimodal detection proposed were the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
    "slm_pred": "The different models for multimodal detection proposed were the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).",
    "llm_pred": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), and Textual Kernels Model (TKM).",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.96,
    "cost": 5.0
  },
  {
    "id": "4ce4db7f277a06595014db181342f8cb5cb94626",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities",
    "final_answer": "hate or not.",
    "slm_pred": "hate or not.",
    "llm_pred": "Each tweet has a hate vs. not-hate label (and subcategory labels: racist, sexist, homophobic, religion-based, or other).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "62a6382157d5f9c1dce6e6c24ac5994442053002",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "accuracy, normalized mutual information",
    "final_answer": "The evaluation metrics used were accuracy (ACC) and the normalized mutual information metric (NMI).",
    "slm_pred": "The evaluation metrics used were accuracy (ACC) and the normalized mutual information metric (NMI).",
    "llm_pred": "accuracy (ACC) and normalized mutual information (NMI)",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "9e04730907ad728d62049f49ac828acb4e0a1a2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",
    "final_answer": "Their performance results are reported in Tables TABREF43, TABREF44, TABREF46, and TABREF47, showing significant improvements over baseline methods.",
    "slm_pred": "Their performance results are reported in Tables TABREF43, TABREF44, TABREF46, and TABREF47, showing significant improvements over baseline methods.",
    "llm_pred": "They performed equally well (no clear winner), both showing large improvements over LPI.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5a0841cc0628e872fe473874694f4ab9411a1d10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of the performance improvement of the proposed methods over the other methods, but rather discusses the performance enhancements of the proposed methods compared to other baseline methods.",
    "slm_pred": "unanswerable \n\nThe article does not provide a direct comparison of the performance improvement of the proposed methods over the other methods, but rather discusses the performance enhancements of the proposed methods compared to other baseline methods.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a5dd569e6d641efa86d2c2b2e970ce5871e0963f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods",
    "final_answer": "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector–based clustering methods.",
    "slm_pred": "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector based clustering methods, as well as four baseline clustering methods based on unsupervised dimensionality reduction methods such as Laplacian Eigenmaps (LE), Locality Preserving Indexing (LPI), and Autoencoder (AE), and Latent Semantic Analysis (LSA).",
    "llm_pred": "K-means, Skip-thought Vectors, Recursive Neural Network, and Paragraph Vector–based clustering methods.",
    "perf_chosen": 0.8695652174,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.8695652174,
    "cost": 25.0
  },
  {
    "id": "785c054f6ea04701f4ab260d064af7d124260ccc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SearchSnippets, StackOverflow, Biomedical",
    "final_answer": "They used three public short text datasets: SearchSnippets, StackOverflow, and Biomedical.",
    "slm_pred": "They used three public short text datasets: SearchSnippets, StackOverflow, and Biomedical.",
    "llm_pred": "SearchSnippets, StackOverflow (question titles), and Biomedical (BioASQ) paper titles.",
    "perf_chosen": 0.4285714286,
    "perf_slm": 0.4285714286,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "3f6610d1d68c62eddc2150c460bf1b48a064e5e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "no.",
    "slm_pred": "no.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4c854d33a832f3f729ce73b206ff90677e131e48",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3",
    "final_answer": "Three configurations of Transformer networks are explored: a small to medium-sized network with 4 Transformer layers, a small network with 2 Transformer layers, and a minimal network with 1 Transformer layer.",
    "slm_pred": "Three configurations of Transformer networks are explored: a small to medium-sized network with 4 Transformer layers, a small network with 2 Transformer layers, and a minimal network with 1 Transformer layer.",
    "llm_pred": "Three Transformer setups: 4 layers (8 heads, d=512, FF=1024), 2 layers (8 heads, d=256, FF=1024), and 1 layer (8 heads, d=256, FF=512).",
    "perf_chosen": 0.37037037040000004,
    "perf_slm": 0.37037037040000004,
    "perf_llm": 0.1777777778,
    "cost": 5.0
  },
  {
    "id": "163c15da1aa0ba370a00c5a09294cd2ccdb4b96d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "90dd5c0f5084a045fd6346469bc853c33622908f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets",
    "final_answer": "Sparse categorical cross-entropy is used for identifying classes from a feature set, and evaluation is the metric of performance, which assumes a large target classification set.",
    "slm_pred": "Sparse categorical cross-entropy is used for identifying classes from a feature set, and evaluation is the metric of performance, which assumes a large target classification set.",
    "llm_pred": "By BLEU-2 translation scores and exact-match accuracy (correct vs. incorrect) of the generated equation/answer.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0740740741,
    "cost": 5.0
  },
  {
    "id": "095888f6e10080a958d9cd3f779a339498f3a109",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20",
    "final_answer": "The authors use four individual datasets: AI2, CC, IL, and MAWPS.",
    "slm_pred": "The authors use four individual datasets: AI2, CC, IL, and MAWPS.",
    "llm_pred": "AI2, Common Core (CC), Illinois (IL), and MAWPS.",
    "perf_chosen": 0.4210526316,
    "perf_slm": 0.4210526316,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "57e783f00f594e08e43a31939aedb235c9d5a102",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AUC-ROC",
    "final_answer": "The evaluation metrics used include AUC-ROC, accuracy, user interruption rate, reuse of VVA skills, number of active dialogs, top-1 accuracy, and relative change with respect to the popularity model.",
    "slm_pred": "The evaluation metrics used include AUC-ROC, accuracy, user interruption rate, reuse of VVA skills, number of active dialogs, top-1 accuracy, and relative change with respect to the popularity model.",
    "llm_pred": "AUC-ROC, accuracy, and top‑1 accuracy (offline); plus, in A/B tests, user interruption rate, reuse of this and other skills, and number of active dialogs.",
    "perf_chosen": 0.0666666667,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.08,
    "cost": 5.0
  },
  {
    "id": "9646fa1abbe3102a0364f84e0a55d107d45c97f0",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "29983f4bc8a5513a198755e474361deee93d4ab6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "five-minute reuse and one-day return",
    "final_answer": "Five-minute reuse and one-day return, which label an instance as positive if it is followed by a new joke request within a specified time interval.",
    "slm_pred": "Five-minute reuse and one-day return, which label an instance as positive if it is followed by a new joke request within a specified time interval.",
    "llm_pred": "five-minute reuse and one-day return",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6c0f97807cd83a94a4d26040286c6f89c4a0f8e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "finite sequence of terms",
    "final_answer": "Vector space representation with TF-IDF weights.",
    "slm_pred": "Vector space representation with TF-IDF weights.",
    "llm_pred": "TF‑IDF–weighted vectors in a vector space model.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "13ca4bf76565564c8ec3238c0cbfacb0b41e14d2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "14 TDs, BIBREF15",
    "final_answer": "A dataset of 14 TDs, with subjects on aliens, stories, law, and news.",
    "slm_pred": "A dataset of 14 TDs, with subjects on aliens, stories, law, and news.",
    "llm_pred": "a dataset of 14 textual documents on aliens, stories, law, and news",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.1333333333,
    "cost": 5.0
  },
  {
    "id": "70797f66d96aa163a3bee2be30a328ba61c40a18",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SRCC",
    "final_answer": "They compare the similarity values produced by each of the similarity measures CS, SRCC, and PCC.",
    "slm_pred": "They compare the similarity values produced by each of the similarity measures CS, SRCC, and PCC.",
    "llm_pred": "By comparing SRCC against cosine similarity and Pearson correlation on a 14‑document corpus (via similarity-value analyses and clustering/classification experiments).",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.1,
    "cost": 5.0
  },
  {
    "id": "71f2b368228a748fd348f1abf540236568a61b07",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "unshuffled version of the French OSCAR corpus",
    "final_answer": "The Masked Language Modeling (MLM) task on the French OSCAR corpus.",
    "slm_pred": "The Masked Language Modeling (MLM) task on the French OSCAR corpus.",
    "llm_pred": "the French OSCAR Common Crawl corpus",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa",
    "final_answer": "none",
    "slm_pred": "unanswerable",
    "llm_pred": "none",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "63723c6b398100bba5dc21754451f503cb91c9b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)",
    "final_answer": "CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches.",
    "slm_pred": "CamemBERT improves the state of the art for most tasks over previous monolingual and multilingual approaches.",
    "llm_pred": "large pretrained language models (e.g., BERT/RoBERTa)",
    "perf_chosen": 0.0487804878,
    "perf_slm": 0.0487804878,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5471766ca7c995dd7f0f449407902b32ac9db269",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT",
    "final_answer": "The results of CamemBERT outperformed previous results by a large margin for POS tagging and dependency parsing, with error reductions of up to 2.96% and 3.33% respectively, and showed a 2.36 point increase in the F1 score for named entity recognition.",
    "slm_pred": "The results of CamemBERT outperformed previous results by a large margin for POS tagging and dependency parsing, with error reductions of up to 2.96% and 3.33% respectively, and showed a 2.36 point increase in the F1 score for named entity recognition.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.3779527559,
    "perf_slm": 0.3779527559,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "dc49746fc98647445599da9d17bc004bafdc4579",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "The article does not directly compare CamemBERT against multilingual BERT, but it mentions that CamemBERT outperforms multilingual language models such as mBERT on certain tasks.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "8720c096c8b990c7b19f956ee4930d5f2c019e2b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "CamemBERT was pre-trained for 100k steps during 17 hours on 256 Nvidia V100 GPUs.",
    "slm_pred": "CamemBERT was pre-trained for 100k steps during 17 hours on 256 Nvidia V100 GPUs.",
    "llm_pred": "17 hours",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b573b36936ffdf1d70e66f9b5567511c989b46b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "unshuffled version of the French OSCAR corpus",
    "final_answer": "The unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.",
    "slm_pred": "The unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens.",
    "llm_pred": "the French OSCAR corpus extracted from Common Crawl (Nov 2018), unshuffled, ~138GB of text",
    "perf_chosen": 0.5384615385,
    "perf_slm": 0.5384615385,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "bf25a202ac713a34e09bf599b3601058d9cace46",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Randomwalk, Walktrap, Louvain clustering",
    "final_answer": "The state of the art measures are structure-based controversy measures developed by Garimella et al.",
    "slm_pred": "The state of the art measures are structure-based controversy measures developed by Garimella et al.",
    "llm_pred": "structure-based controversy measures",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "political events such as elections, corruption cases or justice decisions",
    "final_answer": "netanyahu; ukraine; Kavanaugh (Oct 3); Bolsonaro (Oct 27); @mauriciomacri (Jan 1–11 and Mar 11–18)",
    "slm_pred": "The article does not explicitly list the specific controversial topics experimented with, but it mentions that they include topics such as elections, corruption cases, and justice decisions, as well as specific examples like the Brazilian presidential elections in 2018, the Ukraine conflict, and the Kavanaugh confirmation hearings.",
    "llm_pred": "netanyahu; ukraine; Kavanaugh (Oct 3); Bolsonaro (Oct 27); @mauriciomacri (Jan 1–11 and Mar 11–18)",
    "perf_chosen": 0.0,
    "perf_slm": 0.24561403510000002,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "2df910c9806f0c379d7bb1bc2be2610438e487dc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. ",
    "final_answer": "thirty different Twitter discussions (15 controversial and 15 non‑controversial) collected between March 2015 and June 2019",
    "slm_pred": "They used thirty different discussions that took place between March 2015 and June 2019, in four languages and five regions, and included both controversial and non-controversial discussions.",
    "llm_pred": "thirty different Twitter discussions (15 controversial and 15 non‑controversial) collected between March 2015 and June 2019",
    "perf_chosen": 0.1739130435,
    "perf_slm": 0.35087719300000003,
    "perf_llm": 0.1739130435,
    "cost": 25.0
  },
  {
    "id": "a2a3af59f3f18a28eb2ca7055e1613948f395052",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Twitter",
    "final_answer": "Twitter.",
    "slm_pred": "Twitter.",
    "llm_pred": "Twitter",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d92f1c15537b33b32bfc436e6d017ae7d9d6c29a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "four different languages: English, Portuguese, Spanish and French",
    "final_answer": "Four languages: English, Portuguese, Spanish, and French.",
    "slm_pred": "Four languages: English, Portuguese, Spanish, and French.",
    "llm_pred": "four",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "fa3663567c48c27703e09c42930e51bacfa54905",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "deep convolutional networks BIBREF53 , BIBREF54",
    "final_answer": "unanswerable \n\nThe article does not provide information about the current state-of-the-art (SOTA) for sentiment analysis on Twitter at the time of writing.",
    "slm_pred": "unanswerable \n\nThe article does not provide information about the current state-of-the-art (SOTA) for sentiment analysis on Twitter at the time of writing.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7997b9971f864a504014110a708f215c84815941",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",
    "final_answer": "The language of Twitter proved to be quite challenging due to its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.",
    "slm_pred": "The language of Twitter proved to be quite challenging due to its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations.",
    "llm_pred": "Informal, noisy, and very short texts—e.g., creative spelling/punctuation, misspellings, slang, new words, URLs, hashtags/abbreviations—plus phenomena like sarcasm, multiple topics per tweet, and tricky negation handling.",
    "perf_chosen": 0.6274509804,
    "perf_slm": 0.6274509804,
    "perf_llm": 0.4166666667,
    "cost": 5.0
  },
  {
    "id": "0d1408744651c3847469c4a005e4a9dccbd89cf1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the given text, it is mentioned that \"quantification is not a mere byproduct of classification, since a good classifier is not necessarily a good quantifier, and vice versa\" which implies that specific evaluation measures are required for quantification, but the article does not specify the exact metrics.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0384615385,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a3d83c2a1b98060d609e7ff63e00112d36ce2607",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "27.41 transformation on average of single seed sentence is available in dataset.",
    "final_answer": "approximately 28.4",
    "slm_pred": "unanswerable \n\nHowever, we can find the number of unique sentences with multiple transformations. The article states that out of 4262 distinct sentences, only 188 was recorded more than once, which means that for most sentences, there is only one transformation available.",
    "llm_pred": "approximately 28.4",
    "perf_chosen": 0.0,
    "perf_slm": 0.1509433962,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "aeda22ae760de7f5c0212dad048e4984cd613162",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",
    "final_answer": "The dataset COSTRA 1.0 includes 293 annotations from 12 annotators, which include paraphrases, minimal change, nonsense, past, future, ban, and other transformations.",
    "slm_pred": "The dataset COSTRA 1.0 includes 293 annotations from 12 annotators, which include paraphrases, minimal change, nonsense, past, future, ban, and other transformations.",
    "llm_pred": "Labels for 15 sentence-transformation types (e.g., paraphrase and other modifications) for each transformed sentence.",
    "perf_chosen": 0.1025641026,
    "perf_slm": 0.1025641026,
    "perf_llm": 0.3225806452,
    "cost": 5.0
  },
  {
    "id": "d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes, as new sentences.",
    "final_answer": "They are represented as 15 different paraphrases of each sentence.",
    "slm_pred": "They are represented as 15 different paraphrases of each sentence.",
    "llm_pred": "as distinct sentences generated by annotators applying specified transformation types to seed sentences",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.23529411760000002,
    "cost": 5.0
  },
  {
    "id": "2d536961c6e1aec9f8491e41e383dc0aac700e0a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that 15 modification types were selected for the second round of annotation, but it does not explicitly list them.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "18482658e0756d69e39a77f8fcb5912545a72b9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes, available at http://hdl.handle.net/11234/1-3123",
    "slm_pred": "yes, available at http://hdl.handle.net/11234/1-3123",
    "llm_pred": "yes",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9d336c4c725e390b6eba8bb8fe148997135ee981",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "016b59daa84269a93ce821070f4f5c1a71752a8a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, they briefly describe the analysis of the LASER embeddings, noting that the minimal change operation and tense changes resulted in vectors very similar to the original sentence.",
    "slm_pred": "Yes, they briefly describe the analysis of the LASER embeddings, noting that the minimal change operation and tense changes resulted in vectors very similar to the original sentence.",
    "llm_pred": "yes",
    "perf_chosen": 0.0689655172,
    "perf_slm": 0.0689655172,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "771b373d09e6eb50a74fffbf72d059ad44e73ab0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " we were looking for original and uncommon sentence change suggestions",
    "final_answer": "By sourcing seed sentences from both Global Voices and OpenSubtitles to cover journalistic and colloquial (spoken) styles.",
    "slm_pred": "They introduce language variation by wrapping COSTRA 1.0 into an API, such as SentEval, and extending the corpus in Czech and adding other language variants.",
    "llm_pred": "By sourcing seed sentences from both Global Voices and OpenSubtitles to cover journalistic and colloquial (spoken) styles.",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.0571428571,
    "perf_llm": 0.0740740741,
    "cost": 25.0
  },
  {
    "id": "efb52bda7366d2b96545cf927f38de27de3b5b77",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, they use external resources such as Korektor, a statistical spellchecker and grammar checker, to minimize the influence of typos on the performance of embedding methods.",
    "slm_pred": "Yes, they use external resources such as Korektor, a statistical spellchecker and grammar checker, to minimize the influence of typos on the performance of embedding methods.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1a7d28c25bb7e7202230e1b70a885a46dac8a384",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article mentions that the domain-specific embeddings are related to the domains of the documents, but it does not specify the size of the dataset.",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that the domain-specific embeddings are related to the domains of the documents, but it does not specify the size of the dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0714285714,
    "perf_slm": 0.0714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6bc45d4f908672945192390642da5a2760971c40",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "48cc41c372d44b69a477998be449f8b81384786b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "They aren’t better—SemSentSum matches or outperforms prior state-of-the-art.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "efb3a87845460655c53bd7365bcb8393c99358ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",
    "final_answer": "Our model outperformed the state-of-the-art models on all three datasets: STS Corpus, Sanders dataset, and HCR dataset.",
    "slm_pred": "Our model outperformed the state-of-the-art models on all three datasets: STS Corpus, Sanders dataset, and HCR dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.34482758620000004,
    "perf_slm": 0.34482758620000004,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0619fc797730a3e59ac146a5a4575c81517cc618",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. ",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "846a1992d66d955fa1747bca9a139141c19908e8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)",
    "final_answer": "They used three datasets: Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR).",
    "slm_pred": "They used three datasets: Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR).",
    "llm_pred": "Stanford Twitter Sentiment (STS), Sanders Twitter Sentiment, and Health Care Reform (HCR) datasets.",
    "perf_chosen": 0.8484848485,
    "perf_slm": 0.8484848485,
    "perf_llm": 0.8148148148000001,
    "cost": 5.0
  },
  {
    "id": "1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "12d77ac09c659d2e04b5e3955a283101c3ad1058",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)",
    "final_answer": "The Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset are used for experiments.",
    "slm_pred": "The Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset are used for experiments.",
    "llm_pred": "Stanford Twitter Sentiment (STS) Corpus, Sanders Twitter Sentiment Corpus, and Health Care Reform (HCR) dataset.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8965517241000001,
    "cost": 5.0
  },
  {
    "id": "d60a3887a0d434abc0861637bbcd9ad0c596caf4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "rules that compute polarity of words after POS tagging or parsing steps",
    "final_answer": "The proposed semantic rules set includes five rules, displayed in Table TABREF15, which are used to capture important information and remove unessential parts in a tweet.",
    "slm_pred": "The proposed semantic rules set includes five rules, displayed in Table TABREF15, which are used to capture important information and remove unessential parts in a tweet.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0526315789,
    "perf_slm": 0.0526315789,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "69a7a6675c59a4c5fb70006523b9fe0f01ca415c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "link prediction , triplet classification",
    "final_answer": "They experiment with link prediction and triplet classification tasks.",
    "slm_pred": "They experiment with link prediction and triplet classification tasks.",
    "llm_pred": "link prediction and triplet classification",
    "perf_chosen": 0.6153846154,
    "perf_slm": 0.6153846154,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "60cb756d382b3594d9e1f4a5e2366db407e378ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, they evaluate LAN by comparing it with different scoring functions and analyzing its performance on sparse knowledge graphs and case studies on neighbors' weights.",
    "slm_pred": "Yes, they evaluate LAN by comparing it with different scoring functions and analyzing its performance on sparse knowledge graphs and case studies on neighbors' weights.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "352a1bf734b2d7f0618e9e2b0dbed4a3f1787160",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "The results validate the superiority of LAN w.r.t. the three properties, but the article does not explicitly state that they evaluate existing methods in terms of these properties.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "045dbdbda5d96a672e5c69442e30dbf21917a1ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "Unlike GANs’ adversarial generator–discriminator setup, NetAb uses two cooperative CNNs—one predicts clean labels and the other estimates a noise transition matrix to handle label noise, trained with cross-entropy rather than adversarial loss.",
    "slm_pred": "unanswerable",
    "llm_pred": "Unlike GANs’ adversarial generator–discriminator setup, NetAb uses two cooperative CNNs—one predicts clean labels and the other estimates a noise transition matrix to handle label noise, trained with cross-entropy rather than adversarial loss.",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c20b012ad31da46642c553ce462bc0aad56912db",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source",
    "final_answer": "the movie sentence polarity dataset and the SemEval‑2016 Laptop and Restaurant datasets",
    "slm_pred": "The dataset used to train the model is a combination of three clean-labeled datasets (movie sentence polarity dataset, laptop and restaurant datasets collected from SemEval-2016) and three noisy-labeled training datasets (2,000 reviews for each domain: movie, laptop, and restaurant).",
    "llm_pred": "the movie sentence polarity dataset and the SemEval‑2016 Laptop and Restaurant datasets",
    "perf_chosen": 0.48648648650000004,
    "perf_slm": 0.5,
    "perf_llm": 0.48648648650000004,
    "cost": 25.0
  },
  {
    "id": "13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",
    "final_answer": "The proposed model NetAb outperforms a wide range of baselines markedly in sentence-level sentiment classification, especially in handling noisy labels.",
    "slm_pred": "The proposed model NetAb outperforms a wide range of baselines markedly in sentence-level sentiment classification, especially in handling noisy labels.",
    "llm_pred": "It outperforms the state of the art, achieving the best accuracy and F1 on all datasets (except negative-class F1 on Laptop).",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "89b9a2389166b992c42ca19939d750d88c5fa79b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "dccc3b182861fd19ccce5bd00ce9c3f40451ed6e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "yes",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "98ba7a7aae388b1a77dd6cab890977251d906359",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention if STES (Semantic Text Exchange Score) has been previously used in the literature, it only introduces it as a new metric for evaluating overall performance of a model on STE (Semantic Text Exchange).",
    "slm_pred": "unanswerable \n\nThe article does not mention if STES (Semantic Text Exchange Score) has been previously used in the literature, it only introduces it as a new metric for evaluating overall performance of a model on STE (Semantic Text Exchange).",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3da9a861dfa25ed486cff0ef657d398fdebf8a93",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)",
    "final_answer": "NWN-STEM, GWN-STEM, and W2V-STEM.",
    "slm_pred": "NWN-STEM, GWN-STEM, and W2V-STEM.",
    "llm_pred": "NWN-STEM, GWN-STEM, and W2V-STEM.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "8c0a0747a970f6ea607ff9b18cfeb738502d9a95",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ERR of 19.05 with i-vectors and 15.52 with x-vectors",
    "final_answer": "The performance of both the i-vector system and the x-vector system on CN-Celeb(E) was much worse than on SITW and SITW(S), indicating a significant difference between the two datasets.",
    "slm_pred": "The performance of both the i-vector system and the x-vector system on CN-Celeb(E) was much worse than on SITW and SITW(S), indicating a significant difference between the two datasets.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "529dabe7b4a8a01b20ee099701834b60fb0c43b0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",
    "final_answer": "real-world settings including ambient noise, background babbling, music, cheers, and laugh, with strong and overlapped background speakers in some cases.",
    "slm_pred": "real-world settings including ambient noise, background babbling, music, cheers, and laugh, with strong and overlapped background speakers in some cases.",
    "llm_pred": "unconstrained conditions",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",
    "final_answer": "The 11 genres covered are: entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement.",
    "slm_pred": "The 11 genres covered are: entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement.",
    "llm_pred": "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation, and advertisement",
    "perf_chosen": 0.8125,
    "perf_slm": 0.8125,
    "perf_llm": 0.9629629630000001,
    "cost": 5.0
  },
  {
    "id": "5699996a7a2bb62c68c1e62e730cabf1e3186eef",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "944d5dbe0cfc64bf41ea36c11b1d378c408d40b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "x-vector",
    "final_answer": "unanswerable \n\nThe article compares the performance of i-vector and x-vector systems on CN-Celeb, but it does not conclude which one works better overall.",
    "slm_pred": "unanswerable \n\nThe article compares the performance of i-vector and x-vector systems on CN-Celeb, but it does not conclude which one works better overall.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0833333333,
    "perf_slm": 0.0833333333,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "327e6c6609fbd4c6ae76284ca639951f03eb4a4c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",
    "final_answer": "unanswerable, as the article does not provide a direct comparison of the two performances. However, it can be inferred that the performance on CN-Celeb is significantly inferior to VoxCeleb based on the experimental results.",
    "slm_pred": "unanswerable, as the article does not provide a direct comparison of the two performances. However, it can be inferred that the performance on CN-Celeb is significantly inferior to VoxCeleb based on the experimental results.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "df8cc1f395486a12db98df805248eb37c087458b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types",
    "final_answer": "The new model, conditional BERT, is evaluated on six text classification datasets, including SST, Subj, MPQA, RT, TREC, and SST2.",
    "slm_pred": "The new model, conditional BERT, is evaluated on six text classification datasets, including SST, Subj, MPQA, RT, TREC, and SST2.",
    "llm_pred": "SST2, SST5, Subj, MPQA, RT, and TREC.",
    "perf_chosen": 0.375,
    "perf_slm": 0.375,
    "perf_llm": 0.22857142860000002,
    "cost": 5.0
  },
  {
    "id": "6e97c06f998f09256be752fa75c24ba853b0db24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Accuracy across six datasets",
    "final_answer": "The authors measure performance using accuracy on six benchmark classification datasets.",
    "slm_pred": "The authors measure performance using accuracy on six benchmark classification datasets.",
    "llm_pred": "classification accuracy on benchmark text classification datasets (using CNN and LSTM-RNN classifiers)",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "de2d33760dc05f9d28e9dabc13bab2b3264cadb7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The conditional masked language model (C-MLM) objective performs better than the original masked language model (MLM) objective.",
    "slm_pred": "The conditional masked language model (C-MLM) objective performs better than the original masked language model (MLM) objective.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "63bb39fd098786a510147f8ebc02408de350cb7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, the article only mentions BERT as the pre-trained language model evaluated for contextual augmentation.",
    "slm_pred": "No, the article only mentions BERT as the pre-trained language model evaluated for contextual augmentation.",
    "llm_pred": "no",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6333845facb22f862ffc684293eccc03002a4830",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "unanswerable \n\nHowever, the article does mention performance on tasks with data augmentation.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a12a08099e8193ff2833f79ecf70acf132eda646",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "unanswerable",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "999b20dc14cb3d389d9e3ba5466bc3869d2d6190",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Kim et al. (2019)",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the content of the article, it appears that the survey is not referencing a specific, newly-published paper, but rather providing a comprehensive overview of recent developments in the field of NQG.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ca4b66ffa4581f9491442dcec78ca556253c8146",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b3ff166bd480048e099d09ba4a96e2e32b42422b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable.",
    "slm_pred": "unanswerable.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3703433d434f1913307ceb6a8cfb9a07842667dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Considering \"What\" and \"How\" separately versus jointly optimizing for both.",
    "final_answer": "The learning paradigms covered in this survey include the change of learning paradigm, the broadening of the input spectrum, and the generation of deep questions.",
    "slm_pred": "The learning paradigms covered in this survey include the change of learning paradigm, the broadening of the input spectrum, and the generation of deep questions.",
    "llm_pred": "The traditional reductionist pipeline (separate content selection and question construction) and the neural end-to-end Seq2Seq paradigm, including multi-task learning.",
    "perf_chosen": 0.0571428571,
    "perf_slm": 0.0571428571,
    "perf_llm": 0.0689655172,
    "cost": 5.0
  },
  {
    "id": "f7c34b128f8919e658ba4d5f1f3fc604fb7ff793",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Textual inputs, knowledge bases, and images.",
    "final_answer": "textual inputs, especially declarative sentences, knowledge bases, and images.",
    "slm_pred": "textual inputs, especially declarative sentences, knowledge bases, and images.",
    "llm_pred": "text, knowledge bases, and images",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "d42031893fd4ba5721c7d37e1acb1c8d229ffc21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "No, they mainly focus on neural methods for question generation.",
    "slm_pred": "No, they mainly focus on neural methods for question generation.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a999761aa976458bbc7b4f330764796446d030ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cross-lingual NE recognition",
    "final_answer": "Their model is a hypersphere-based named entity recognition model that uses Euclidean distance and a hypersphere representation to classify entities into different types.",
    "slm_pred": "Their model is a hypersphere-based named entity recognition model that uses Euclidean distance and a hypersphere representation to classify entities into different types.",
    "llm_pred": "a hypersphere-based NE model that represents each entity type as a hypersphere in embedding space (center + radius)",
    "perf_chosen": 0.0769230769,
    "perf_slm": 0.0769230769,
    "perf_llm": 0.1,
    "cost": 5.0
  },
  {
    "id": "f229069bcb05c2e811e4786c89b0208af90d9a25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThe article states that they evaluate the hypersphere model on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0.",
    "slm_pred": "yes \n\nThe article states that they evaluate the hypersphere model on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0.",
    "llm_pred": "yes—on CoNLL-2003 and ONTONOTES 5.0",
    "perf_chosen": 0.0952380952,
    "perf_slm": 0.0952380952,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6b55b558ed581759425ede5d3a6fcdf44b8082ac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Naive Bayes, SVM, Maximum Entropy classifiers",
    "final_answer": "The baseline method, Distant Supervision, and a model proposed by Ayushi Dalmia.",
    "slm_pred": "The baseline method, Distant Supervision, and a model proposed by Ayushi Dalmia.",
    "llm_pred": "the Distant Supervision baseline of Go et al. (Naive Bayes/SVM/MaxEnt with unigram/bigram/POS features)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.1052631579,
    "cost": 5.0
  },
  {
    "id": "3e3f5254b729beb657310a5561950085fa690e83",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x.",
    "final_answer": "The Effective Word Score (EFWS) of a word x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x.",
    "slm_pred": "The Effective Word Score (EFWS) of a word x is calculated as EFWS(x) = N(+x) - N(-x), where N(x) is the number of words in the tweet with polarity score x.",
    "llm_pred": "EFWS(x) = N(+x) − N(−x), where N(x) is the number of words in the tweet with polarity score x.",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.7111111111,
    "cost": 5.0
  },
  {
    "id": "5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The tweet subjectivity is measured using TextBlob, which provides a subjectivity level between 0 and 1, or Opinion Finder Tool, which identifies subjective and objective parts of the text.",
    "slm_pred": "The tweet subjectivity is measured using TextBlob, which provides a subjectivity level between 0 and 1, or Opinion Finder Tool, which identifies subjective and objective parts of the text.",
    "llm_pred": "Using TextBlob’s subjectivity score (a 0–1 value).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "129c03acb0963ede3915415953317556a55f34ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.",
    "final_answer": "Because the original DMN requires supporting facts to be labeled during training.",
    "slm_pred": "Because the original DMN requires supporting facts to be labeled during training.",
    "llm_pred": "Because the original DMN learns its attention over facts using labeled supporting facts; without that supervision it struggles to select the relevant facts from the story.",
    "perf_chosen": 0.1230769231,
    "perf_slm": 0.1230769231,
    "perf_llm": 0.1518987342,
    "cost": 5.0
  },
  {
    "id": "58b3b630a31fcb9bffb510390e1ec30efe87bfbf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " the facts that are relevant for answering a particular question) are labeled during training.",
    "final_answer": "Providing labeled facts relevant for answering a question during training.",
    "slm_pred": "Providing labeled facts relevant for answering a question during training.",
    "llm_pred": "labels indicating which facts/sentences are relevant to answer each question during training",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.3846153846,
    "cost": 5.0
  },
  {
    "id": "141dab98d19a070f1ce7e7dc384001d49125d545",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer",
    "final_answer": "They proposed a new input module using a two-level encoder with a sentence reader and input fusion layer to allow for information flow between sentences.",
    "slm_pred": "They proposed a new input module using a two-level encoder with a sentence reader and input fusion layer to allow for information flow between sentences.",
    "llm_pred": "A new two‑level input encoder: a sentence reader plus an input fusion layer (bi-directional GRU) to allow information flow between sentences.",
    "perf_chosen": 0.3018867925,
    "perf_slm": 0.3018867925,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "afdad4c9bdebf88630262f1a9a86ac494f06c4c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images.",
    "final_answer": "They proposed and implemented several improvements to the DMN, including an input fusion layer to allow interactions between input facts, a novel attention-based GRU for logical reasoning, and an untied model with a linear layer and ReLU activation for memory update.",
    "slm_pred": "They proposed and implemented several improvements to the DMN, including an input fusion layer to allow interactions between input facts, a novel attention-based GRU for logical reasoning, and an untied model with a linear layer and ReLU activation for memory update.",
    "llm_pred": "A new two‑level input module with an input‑fusion layer, an attention‑augmented GRU for the memory (with untied passes and a ReLU memory update), and an image input module.",
    "perf_chosen": 0.2337662338,
    "perf_slm": 0.2337662338,
    "perf_llm": 0.21875000000000003,
    "cost": 5.0
  },
  {
    "id": "bfd4fc82ffdc5b2b32c37f4222e878106421ce2a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. ",
    "final_answer": "The model learns to select the important facts from a larger set without requiring labeled supporting facts during training.",
    "slm_pred": "The model learns to select the important facts from a larger set without requiring labeled supporting facts during training.",
    "llm_pred": "By learning to select the important facts from the larger set using attention.",
    "perf_chosen": 0.18604651160000002,
    "perf_slm": 0.18604651160000002,
    "perf_llm": 0.2162162162,
    "cost": 5.0
  },
  {
    "id": "1ce26783f0ff38925bfc07bbbb65d206e52c2d21",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9213159f874b3bdd9b4de956a88c703aac988411",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "accuracy",
    "final_answer": "Annotators rated fluency on a 0–5 scale (0=incoherent, 5=well‑written) and performed style classification (which-of-3 and which-of-2) to judge whether outputs matched the target style.",
    "slm_pred": "They perform manual evaluation by having three English Literature PhD candidates score the fluency of outputs on a 0-5 scale, and also have the annotators classify transferred sentences by style.",
    "llm_pred": "Annotators rated fluency on a 0–5 scale (0=incoherent, 5=well‑written) and performed style classification (which-of-3 and which-of-2) to judge whether outputs matched the target style.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a234bcbf2e41429422adda37d9e926b49ef66150",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "classification accuracy, BLEU scores, model perplexities of the reconstruction",
    "final_answer": "BLEU scores and perplexity.",
    "slm_pred": "BLEU scores and perplexity.",
    "llm_pred": "BLEU, perplexity, classifier accuracy, and the control-realization rates (Exact, Direction, Atomic).",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.30000000000000004,
    "cost": 5.0
  },
  {
    "id": "c383fa9170ae00a4a24a8e39358c38395c5f034b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " words found in the control word lists are then removed, The remaining words, which represent the content",
    "final_answer": "They use part-of-speech (POS) tags to identify content words.",
    "slm_pred": "They use part-of-speech (POS) tags to identify content words.",
    "llm_pred": "By POS tagging and heuristics—treating nouns, verbs, and adjectives (the words left after removing predefined function-word/control lists) as content.",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "83251fd4a641cea8b180b49027e74920bca2699a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style",
    "final_answer": "They model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions, by extracting and counting surface-level features, often counts of function words.",
    "slm_pred": "They model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions, by extracting and counting surface-level features, often counts of function words.",
    "llm_pred": "As a heuristic vector of count-based control features (e.g., counts of function words and syntactic markers like SBAR) embedded as decoder feature embeddings.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.3188405797,
    "cost": 5.0
  },
  {
    "id": "5d70c32137e82943526911ebdf78694899b3c28a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not explicitly state that they only report results on English data, but it does mention that the data was collected from Twitter and other platforms, which are predominantly English. However, it does not exclude the possibility of other languages being present in the data.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0408163265,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "97dac7092cf8082a6238aaa35f4b185343b914af",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression",
    "final_answer": "The article provides insights into the relationship between demographics and mental health, specifically that women are more likely to be diagnosed with depression, while men have higher suicide rates, and depression triggers vary across different age groups.",
    "slm_pred": "The article provides insights into the relationship between demographics and mental health, specifically that women are more likely to be diagnosed with depression, while men have higher suicide rates, and depression triggers vary across different age groups.",
    "llm_pred": "Depression risk varies by demographics: females show higher depressive behavior (and higher prevalence) while men have higher suicide rates; age shapes both triggers and expression—youth are less analytically oriented with lower clout (confidence) and higher authenticity, whereas analytic thinking and clout rise with age and authenticity declines.",
    "perf_chosen": 0.253968254,
    "perf_slm": 0.253968254,
    "perf_llm": 0.1095890411,
    "cost": 5.0
  },
  {
    "id": "195611926760d1ceec00bd043dfdc8eba2df5ad1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Random Forest classifier",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "445e792ce7e699e960e2cb4fe217aeacdd88d392",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Demographic information is predicted using weighted lexicon of terms.",
    "final_answer": "The framework facilitates demographic inference from social media by integrating heterogeneous features from different modalities, including aesthetic features, language features, sociability, and user engagement, to detect likely depressed individuals and infer demographic information such as age and gender.",
    "slm_pred": "The framework facilitates demographic inference from social media by integrating heterogeneous features from different modalities, including aesthetic features, language features, sociability, and user engagement, to detect likely depressed individuals and infer demographic information such as age and gender.",
    "llm_pred": "By integrating multimodal signals—visual features from images/profile photos (including CNN-based face age/gender), textual and profile-description cues (lexicon-based language features and self-disclosures), screen names, and sociability/engagement metrics—to infer demographics like age and gender.",
    "perf_chosen": 0.08510638300000001,
    "perf_slm": 0.08510638300000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a3b1520e3da29d64af2b6e22ff15d330026d0b36",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns",
    "final_answer": "- Images: computational aesthetics (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) and face-based cues (facial presence/expression, age/gender from profile photos; OCR text sentiment).\n- Text/profile text: n-grams and psycholinguistic emotion/sentiment features (e.g., LIWC), plus screen name.\n- Social/behavior: ego-network sociability and user engagement metrics.",
    "slm_pred": "The features used from each data type are as follows: \n- Aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) \n- Choice of profile picture (for gender, age, and facial expression) \n- Screen name \n- Language features from both textual content and profile's description (n-gram, emotion, sentiment) \n- Sociability from ego-network \n- User engagement \n- General image features (normalized red, green, blue, mean of original colors, brightness, contrast, luminance, hue-saturation-value color space, mean and variance for saturation and hue) \n- Facial presence features \n- Facial expression features (Ekman's model of six emotions: anger, disgust, fear, joy, sadness, and surprise) \n- Qualitative language analysis features (analytical thinking, clout, authenticity, emotional tone, descriptors categories, informal language markers, and linguistic aspects)",
    "llm_pred": "- Images: computational aesthetics (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness) and face-based cues (facial presence/expression, age/gender from profile photos; OCR text sentiment).\n- Text/profile text: n-grams and psycholinguistic emotion/sentiment features (e.g., LIWC), plus screen name.\n- Social/behavior: ego-network sociability and user engagement metrics.",
    "perf_chosen": 0.0625,
    "perf_slm": 0.2686567164,
    "perf_llm": 0.0625,
    "cost": 25.0
  },
  {
    "id": "2cf8825639164a842c3172af039ff079a8448592",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The data are self-reported by Twitter users and then verified by two human experts.",
    "final_answer": "The data is annotated using self-reported depressed users, a lexicon of depression symptoms, and human judges who verified a subset of users.",
    "slm_pred": "The data is annotated using self-reported depressed users, a lexicon of depression symptoms, and human judges who verified a subset of users.",
    "llm_pred": "Using self-disclosure cues (e.g., profile text) with labels verified by two human judges.",
    "perf_chosen": 0.3888888889,
    "perf_slm": 0.3888888889,
    "perf_llm": 0.2962962963,
    "cost": 5.0
  },
  {
    "id": "36b25021464a9574bf449e52ae50810c4ac7b642",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "From Twitter profile descriptions of the users.",
    "final_answer": "Self-disclosure in Twitter profile descriptions, such as age and gender explicitly mentioned in the text, and also from user-provided information like \"16 years old suicidal girl\".",
    "slm_pred": "Self-disclosure in Twitter profile descriptions, such as age and gender explicitly mentioned in the text, and also from user-provided information like \"16 years old suicidal girl\".",
    "llm_pred": "users’ self-disclosed Twitter profile descriptions",
    "perf_chosen": 0.303030303,
    "perf_slm": 0.303030303,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "98515bd97e4fae6bfce2d164659cd75e87a9fc89",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Sociability from ego-network on Twitter",
    "final_answer": "Twitter",
    "slm_pred": "unanswerable",
    "llm_pred": "Twitter",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "53bf6238baa29a10f4ff91656c470609c16320e1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Users' tweets",
    "final_answer": "Twitter",
    "slm_pred": "Twitter.",
    "llm_pred": "Twitter",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b27f7993b1fe7804c5660d1a33655e424cea8d10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Profile pictures from the Twitter users' profiles.",
    "final_answer": "Twitter.",
    "slm_pred": "Twitter.",
    "llm_pred": "Twitter (users’ profile and posted images)",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "e21a8581cc858483a31c6133e53dd0cfda76ae4c",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9f6e877e3bde771595e8aee10c2656a0e7b9aeb2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a3783e42c2bf616c8a07bd3b3d503886660e4344",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0d0959dba3f7c15ee4f5cdee51682656c4abbd8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes",
    "final_answer": "A sememe is a basic unit of meaning in a word, a fundamental concept or idea that contributes to the overall meaning of a word.",
    "slm_pred": "A sememe is a basic unit of meaning in a word, a fundamental concept or idea that contributes to the overall meaning of a word.",
    "llm_pred": "an atomic unit of meaning used in HowNet to annotate concepts",
    "perf_chosen": 0.3043478261,
    "perf_slm": 0.3043478261,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "589be705a5cc73a23f30decba23ce58ec39d313b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Dutch section of the OSCAR corpus",
    "final_answer": "They pre-trained their model on the Dutch section of the OSCAR corpus.",
    "slm_pred": "They pre-trained their model on the Dutch section of the OSCAR corpus.",
    "llm_pred": "the Dutch section of the OSCAR corpus (from Common Crawl)",
    "perf_chosen": 0.7368421053,
    "perf_slm": 0.7368421053,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "6e962f1f23061f738f651177346b38fd440ff480",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT",
    "final_answer": "RobBERT, a Dutch RoBERTa-based pre-trained language model, outperforms previous models and other BERT models on several tasks, including sentiment analysis and die/dat disambiguation.",
    "slm_pred": "RobBERT, a Dutch RoBERTa-based pre-trained language model, outperforms previous models and other BERT models on several tasks, including sentiment analysis and die/dat disambiguation.",
    "llm_pred": "BERT-based transformer models",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "594a6bf37eab64a16c6a05c365acc100e38fcff1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "sentiment analysis, the disambiguation of demonstrative pronouns,",
    "final_answer": "sentiment analysis and Dutch “die/dat” pronoun disambiguation",
    "slm_pred": "They experimented on sentiment analysis, die/dat disambiguation, and possibly other Dutch language tasks.",
    "llm_pred": "sentiment analysis and Dutch “die/dat” pronoun disambiguation",
    "perf_chosen": 0.4285714286,
    "perf_slm": 0.30000000000000004,
    "perf_llm": 0.4285714286,
    "cost": 25.0
  },
  {
    "id": "d79d897f94e666d5a6fcda3b0c7e807c8fad109e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances",
    "final_answer": "Results of the experiments suggest that natural language based agents are more robust to task-nuisances and allow for better transfer.",
    "slm_pred": "Results of the experiments suggest that natural language based agents are more robust to task-nuisances and allow for better transfer.",
    "llm_pred": "They still outperformed vision and feature representations when the environment was flooded with task-nuisances (long, 250+ word descriptions), showing high robustness to added noise.",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.2325581395,
    "cost": 5.0
  },
  {
    "id": "599d9ca21bbe2dbe95b08cf44dfc7537bde06f98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The natural language based agents outperform the other methods, including vision-based and feature-based representations, achieving better performance, faster convergence, and more robust solutions.",
    "slm_pred": "The natural language based agents outperform the other methods, including vision-based and feature-based representations, achieving better performance, faster convergence, and more robust solutions.",
    "llm_pred": "They outperform visual and feature-based agents, with faster convergence and greater robustness and transfer across scenarios.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "827464c79f33e69959de619958ade2df6f65fdee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThis is because the article does not provide information about the convergence rate of the natural language agents compared to other agents. It only mentions that GloVe pretrained embedding vectors were used, which speeds up the training process.",
    "slm_pred": "unanswerable \n\nThis is because the article does not provide information about the convergence rate of the natural language agents compared to other agents. It only mentions that GloVe pretrained embedding vectors were used, which speeds up the training process.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.05,
    "perf_slm": 0.05,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8e857e44e4233193c7b2d538e520d37be3ae1552",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios",
    "final_answer": "The authors perform experiments on several tasks in the Doom environment to compare the performance of natural language representation against visual-based and feature representations.",
    "slm_pred": "The authors perform experiments on several tasks in the Doom environment to compare the performance of natural language representation against visual-based and feature representations.",
    "llm_pred": "They compared natural language state representations to raw image, feature-vector, and semantic-segmentation inputs on multiple VizDoom scenarios (basic, health gathering, take cover, defend the center/line, and a mixed “super” task) using DQN and PPO agents.",
    "perf_chosen": 0.18461538460000002,
    "perf_slm": 0.18461538460000002,
    "perf_llm": 0.2631578947,
    "cost": 5.0
  },
  {
    "id": "084fb7c80a24b341093d4bf968120e3aff56f693",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " represent the state using natural language",
    "final_answer": "The state is represented via natural language by converting labels and features of the environment into descriptive sentences that summarize the state, using techniques such as patching and parsing.",
    "slm_pred": "The state is represented via natural language by converting labels and features of the environment into descriptive sentences that summarize the state, using techniques such as patching and parsing.",
    "llm_pred": "By parsing each frame’s semantic segmentation into sentences that describe counts, distances, and directions of objects relative to the agent—built by dividing the screen into patches and concatenating their descriptions.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0555555556,
    "cost": 5.0
  },
  {
    "id": "babe72f0491e65beff0e5889380e8e32d7a81f78",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )",
    "final_answer": "Our TL-TranSum method outperforms the Maximal Relevance Minimal Redundancy (MRMR) baseline by 7% of ROUGE-SU4 score.",
    "slm_pred": "Our TL-TranSum method outperforms the Maximal Relevance Minimal Redundancy (MRMR) baseline by 7% of ROUGE-SU4 score.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.3870967742,
    "perf_slm": 0.3870967742,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "31ee92e521be110b6a5a8d08cc9e6f90a3a97aae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "No, the paper does not discuss previous models which have been applied to the same task, but it does mention existing research on detecting moral sentiment in online text and bias in NLP.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "737397f66751624bcf4ef891a10b29cfc46b0520",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n",
    "final_answer": "The datasets used in the paper include the Google N-grams corpus, COHA, WordNet, and the valence ratings for the nearly 14,000 English nouns collected by BIBREF28.",
    "slm_pred": "The datasets used in the paper include the Google N-grams corpus, COHA, WordNet, and the valence ratings for the nearly 14,000 English nouns collected by BIBREF28.",
    "llm_pred": "- Google N-grams (Google Books) and COHA corpora (for diachronic embeddings)\n- Moral Foundations Dictionary (MFD)\n- Human valence (and concreteness) ratings for ~14,000 English words\n- WordNet (for noun validation)\n- Historical word-frequency lists for 1800–1999 (top 10,000 nouns)\n- Word frequency and word-length norms",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.2978723404,
    "cost": 5.0
  },
  {
    "id": "87cb19e453cf7e248f24b5f7d1ff9f02d87fc261",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;",
    "final_answer": "The parameter-free model works by summarizing each set of seed words by its expected vector in embedding space, and classifying concepts into the class of closest expected embedding in Euclidean distance following a softmax rule.",
    "slm_pred": "The parameter-free model works by summarizing each set of seed words by its expected vector in embedding space, and classifying concepts into the class of closest expected embedding in Euclidean distance following a softmax rule.",
    "llm_pred": "By averaging the seed-word embeddings per class to form centroids and assigning a query to the class with the nearest centroid in embedding space (via a softmax over Euclidean distances).",
    "perf_chosen": 0.5849056604,
    "perf_slm": 0.5849056604,
    "perf_llm": 0.3366336634,
    "cost": 5.0
  },
  {
    "id": "5fb6a21d10adf4e81482bb5c1ec1787dc9de260d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence",
    "final_answer": "They quantify moral relevance by using a set of seed words approximating moral irrelevance based on valence ratings, and then using a Centroid model or Naïve Bayes model to classify concepts into morally relevant and morally irrelevant categories.",
    "slm_pred": "They quantify moral relevance by using a set of seed words approximating moral irrelevance based on valence ratings, and then using a Centroid model or Naïve Bayes model to classify concepts into morally relevant and morally irrelevant categories.",
    "llm_pred": "As the model’s posterior probability p(c1 | q) that a concept belongs to the “morally relevant” class, computed from word embeddings using relevant vs. irrelevant seed words.",
    "perf_chosen": 0.44827586210000003,
    "perf_slm": 0.44827586210000003,
    "perf_llm": 0.2608695652,
    "cost": 5.0
  },
  {
    "id": "542a87f856cb2c934072bacaa495f3c2645f93be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation",
    "final_answer": "They showcase the fine-grained moral dimensions of \"Care / Harm\", \"Fairness / Cheating\", \"Loyalty / Betrayal\", \"Authority / Subversion\", and \"Sanctity / Degradation\".",
    "slm_pred": "They showcase the fine-grained moral dimensions of \"Care / Harm\", \"Fairness / Cheating\", \"Loyalty / Betrayal\", \"Authority / Subversion\", and \"Sanctity / Degradation\".",
    "llm_pred": "Care/Harm; Fairness/Cheating; Loyalty/Betrayal; Authority/Subversion; Sanctity/Degradation",
    "perf_chosen": 0.7586206897000001,
    "perf_slm": 0.7586206897000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "the Google Books N-grams diachronic corpus",
    "slm_pred": "They use the Google N-grams corpus and the Moral Foundations Dictionary, as well as data from the Pew Research Center's 2013 Global Attitudes survey, and WordNet, and BIBREF30 (unspecified source) for their analysis.",
    "llm_pred": "the Google Books N-grams diachronic corpus",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c180f44667505ec03214d44f4970c0db487a8bae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all",
    "final_answer": "The system performed well, particularly in the fairy-tale genre, where the neural approach was deemed as coherent as the human-made game.",
    "slm_pred": "The system performed well, particularly in the fairy-tale genre, where the neural approach was deemed as coherent as the human-made game.",
    "llm_pred": "Generally better than the rule-based and random baselines—especially in fairy tales—and as coherent as the human-made game, though the human-made version was best overall and the rules-based game was judged more interesting in mystery.",
    "perf_chosen": 0.2790697674,
    "perf_slm": 0.2790697674,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "76d62e414a345fe955dc2d99562ef5772130bc7e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique",
    "final_answer": "The information is extracted using two methods: a neural question-answering technique and a rule-based information extraction technique, specifically OpenIE5.",
    "slm_pred": "The information is extracted using two methods: a neural question-answering technique and a rule-based information extraction technique, specifically OpenIE5.",
    "llm_pred": "By querying a QA model (ALBERT) with targeted questions to iteratively extract entities/relations (AskBERT), or via a rule‑based OpenIE5 triple extractor with NER/POS postprocessing.",
    "perf_chosen": 0.5405405405,
    "perf_slm": 0.5405405405,
    "perf_llm": 0.2380952381,
    "cost": 5.0
  },
  {
    "id": "6b9310b577c6232e3614a1612cbbbb17067b3886",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d484a71e23d128f146182dccc30001df35cdf93f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50.",
    "final_answer": "unanswerable \n\nThe article does not provide a direct comparison of the proposed model's perplexity and BLEU score with those of typical UMT models.",
    "slm_pred": "unanswerable \n\nThe article does not provide a direct comparison of the proposed model's perplexity and BLEU score with those of typical UMT models.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.2553191489,
    "perf_slm": 0.2553191489,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5787ac3e80840fe4cf7bfae7e8983fa6644d6220",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We collected a corpus of poems and a corpus of vernacular literature from online resources",
    "final_answer": "A corpus of poems and a corpus of vernacular literature from online resources.",
    "slm_pred": "A corpus of poems and a corpus of vernacular literature from online resources.",
    "llm_pred": "An unaligned corpus collected online: 163K Tang/Song quatrain poems and 337K vernacular paragraphs from 281 books.",
    "perf_chosen": 0.9285714286000001,
    "perf_slm": 0.9285714286000001,
    "perf_llm": 0.4516129032,
    "cost": 5.0
  },
  {
    "id": "ee31c8a94e07b3207ca28caef3fbaf9a38d94964",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5",
    "final_answer": "BLEU and Micro Entity F1.",
    "slm_pred": "BLEU and Micro Entity F1.",
    "llm_pred": "BLEU and Micro Entity F1 (and human ratings on correctness, fluency, and human-likeness).",
    "perf_chosen": 0.3846153846,
    "perf_slm": 0.3846153846,
    "perf_llm": 0.5294117647000001,
    "cost": 5.0
  },
  {
    "id": "66d743b735ba75589486e6af073e955b6bb9d2a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR",
    "final_answer": "The baseline systems included Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, and DSR.",
    "slm_pred": "The baseline systems included Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, and DSR.",
    "llm_pred": "Attn seq2seq; Ptr-UNK; KV Net; Mem2Seq; DSR",
    "perf_chosen": 0.7368421053,
    "perf_slm": 0.7368421053,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b9f852256113ef468d60e95912800fab604966f6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Camrest, InCar Assistant",
    "final_answer": "InCar dataset and CamRest dataset.",
    "slm_pred": "InCar dataset and CamRest dataset.",
    "llm_pred": "InCar and CamRest datasets.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "88f8ab2a417eae497338514142ac12c3cec20876",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "05e3b831e4c02bbd64a6e35f6c52f0922a41539a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "equi-distant segments of the video.",
    "slm_pred": "equi-distant segments of the video.",
    "llm_pred": "from multiple equal-duration (equi-distant) segments of the video",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "They initialize their word embeddings using 300-dimensional GloVe and then fine-tune them during training.",
    "slm_pred": "They initialize their word embeddings using 300-dimensional GloVe and then fine-tune them during training.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6472f9d0a385be81e0970be91795b1b97aa5a9cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Answer with content missing: (list missing) \nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\n\nYes.",
    "final_answer": "no",
    "slm_pred": "yes \n\nThe article mentions that the authors also add an auxiliary decoding module, not concatenate the question, answer pairs before encoding them, and use the Time-Extended FiLM module for feature extraction, and running hyperparameter optimization over the validation set to select hyperparameters, giving them the observed performance boost.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "2173809eb117570d289cefada6971e946b902bd6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "293e9a0b30670f4f0a380e574a416665a8c55bc2",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article mentions that the authors have an Android application ready for their system, but it does not provide information on whether it is publicly available.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0714285714,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "17de58c17580350c9da9c2f3612784b432154d11",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "multi-class Naive Bayes",
    "final_answer": "A multi-class Naive Bayes classifier is used for emergency categorization.",
    "slm_pred": "A multi-class Naive Bayes classifier is used for emergency categorization.",
    "llm_pred": "Naive Bayes (multi-class)",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ff27d6e6eb77e55b4d39d343870118d1a6debd5e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SVM",
    "final_answer": "Support Vector Machines (SVM) is used for the first stage of emergency detection, while Naive Bayes (NB) is used for the second stage classification model for categorizing tweets into specific emergency types.",
    "slm_pred": "Support Vector Machines (SVM) is used for the first stage of emergency detection, while Naive Bayes (NB) is used for the second stage classification model for categorizing tweets into specific emergency types.",
    "llm_pred": "Support Vector Machine (SVM)",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "29772ba04886bee2d26b7320e1c6d9b156078891",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "unanswerable \n\nThe article does not provide information about the source of the tweets, but mentions that the authors collected data using the Twitter API for saved data available for public use.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "94dc437463f7a7d68b8f6b57f1e3606eacc4490a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the article, it can be inferred that there are at least 4 categories: fire, earthquake, accident, and robbery, as well as a category for drunk driving and drunk driving accidents.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0571428571,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9d9d84822a9c42eb0257feb7c18086d390dae3be",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, it can be inferred that the baseline was likely the conventional methods of emergency response, such as calling emergency numbers like 100 or 911, but the article does not explicitly mention this.",
    "slm_pred": "unanswerable \n\nHowever, it can be inferred that the baseline was likely the conventional methods of emergency response, such as calling emergency numbers like 100 or 911, but the article does not explicitly mention this.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0571428571,
    "perf_slm": 0.0571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d27e3a099954e917b6491e81b2e907478d7f1233",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, the tweets are specific to a region, as the system is currently collecting tweets with a location filter for the city of \"Mumbai\" and displays its map location on the interface.",
    "slm_pred": "Yes, the tweets are specific to a region, as the system is currently collecting tweets with a location filter for the city of \"Mumbai\" and displays its map location on the interface.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article does mention the creation of a dataset called MED, but it doesn't explicitly state whether it is released or not.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "dfc393ba10ec4af5a17e5957fcbafdffdb1a6443",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, BERT",
    "final_answer": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.",
    "slm_pred": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.",
    "llm_pred": "BiMPM, ESIM, Decomposable Attention Model, KIM, and BERT.",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.9333333333,
    "cost": 5.0
  },
  {
    "id": "311a7fa62721e82265f4e0689b4adc05f6b74215",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",
    "final_answer": "Upward reasoning allows an inference from a more general concept to a more specific one, while downward reasoning allows an inference from a more specific concept to a more general one.",
    "slm_pred": "Upward reasoning allows an inference from a more general concept to a more specific one, while downward reasoning allows an inference from a more specific concept to a more general one.",
    "llm_pred": "Upward reasoning: entailment holds when a phrase is replaced by a more general one; downward reasoning: entailment holds when a phrase is replaced by a more specific one.",
    "perf_chosen": 0.625,
    "perf_slm": 0.625,
    "perf_llm": 0.4590163934,
    "cost": 5.0
  },
  {
    "id": "82bcacad668351c0f81bd841becb2dbf115f000e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures",
    "final_answer": "The ability to capture the interaction between lexical and syntactic structures, and to determine the direction of inference based on the presence of monotonicity operators and their arguments.",
    "slm_pred": "The ability to capture the interaction between lexical and syntactic structures, and to determine the direction of inference based on the presence of monotonicity operators and their arguments.",
    "llm_pred": "A form of logical inference that tests entailment by replacing phrases with more general or specific ones according to the upward/downward polarity set by monotonicity operators and syntax.",
    "perf_chosen": 0.5833333333,
    "perf_slm": 0.5833333333,
    "perf_llm": 0.2083333333,
    "cost": 5.0
  },
  {
    "id": "5937ebbf04f62d41b48cbc6b5c38fc309e5c2328",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration', Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset, Acknowledgements (b) are mostly with positive or neutral, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP), Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral, No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny)., Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'",
    "final_answer": "The analysis found a key relationship between dialogue acts and emotional states, including Accept/Agree with Joy, Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc.",
    "slm_pred": "The analysis found a key relationship between dialogue acts and emotional states, including Accept/Agree with Joy, Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc.",
    "llm_pred": "A strong correlation between MELD’s emotion and sentiment labels.",
    "perf_chosen": 0.2331288344,
    "perf_slm": 0.2331288344,
    "perf_llm": 0.042253521100000004,
    "cost": 5.0
  },
  {
    "id": "dcd6f18922ac5c00c22cef33c53ff5ae08b42298",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.",
    "final_answer": "The ensemble annotator extracts the final label by giving priority to the labels that are perfectly matching in all the neural annotators, then to the context-based models, and finally by ranking the labels with their confidence values.",
    "slm_pred": "The ensemble annotator extracts the final label by giving priority to the labels that are perfectly matching in all the neural annotators, then to the context-based models, and finally by ranking the labels with their confidence values.",
    "llm_pred": "By first taking labels that match across all models, then preferring consensus among the context models; if none, ranking labels by softmax confidence and choosing the highest agreed label; otherwise assigning “xx” (unknown).",
    "perf_chosen": 0.3163841808,
    "perf_slm": 0.3163841808,
    "perf_llm": 0.21965317920000002,
    "cost": 5.0
  },
  {
    "id": "2965c86467d12b79abc16e1457d848cb6ca88973",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Dialogue Act Markup in Several Layers (DAMSL) tag set",
    "final_answer": "The dialogue act labels were defined based on the DAMSL (Dialogue Act Markup in Several Layers) tag set, which includes forward-looking functions (such as Statement, Info-request, Thanking) and backwards-looking functions (such as Accept, Reject, Answer).",
    "slm_pred": "The dialogue act labels were defined based on the DAMSL (Dialogue Act Markup in Several Layers) tag set, which includes forward-looking functions (such as Statement, Info-request, Thanking) and backwards-looking functions (such as Accept, Reject, Answer).",
    "llm_pred": "According to the DAMSL tag set (as in the Switchboard DA corpus), with forward- and backward-looking functions.",
    "perf_chosen": 0.4090909091,
    "perf_slm": 0.4090909091,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "b99948ac4810a7fe3477f6591b8cf211d6398e67",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "five",
    "final_answer": "Five neural models were trained and used for dialogue act labeling.",
    "slm_pred": "Five neural models were trained and used for dialogue act labeling.",
    "llm_pred": "five",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "73d657d6faed0c11c65b1ab60e553db57f4971ca",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "They do not directly compare their model against other models, but they mention SyncNet, a previous neural network model that achieves 99% accuracy on lip video synchronisation, and argue that their data is more challenging.",
    "slm_pred": "They do not directly compare their model against other models, but they mention SyncNet, a previous neural network model that achieves 99% accuracy on lip video synchronisation, and argue that their data is more challenging.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9ef182b61461d0d8b6feb1d6174796ccde290a15",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Use an existing one",
    "final_answer": "They annotate their own dataset using self-supervision, generating and labelling pairs of ultrasound and audio segments as positive (correctly synchronised) or negative (randomly desynchronised).",
    "slm_pred": "They annotate their own dataset using self-supervision, generating and labelling pairs of ultrasound and audio segments as positive (correctly synchronised) or negative (randomly desynchronised).",
    "llm_pred": "an existing dataset (UltraSuite)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "f6f8054f327a2c084a73faca16cf24a180c094ae",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "unanswerable \n\nHowever, the model predicts the synchronisation offset by calculating the average distance for each candidate offset across utterance segments and selecting the one with the minimum average distance.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b8f711179a468fec9a0d8a961fb0f51894af4b31",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CNN",
    "final_answer": "A two-stream neural network architecture.",
    "slm_pred": "A two-stream neural network architecture.",
    "llm_pred": "a two‑stream (Siamese-style) neural network architecture",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3bf429633ecbbfec3d7ffbcfa61fa90440cc918b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "apply an ensemble of deep learning and linguistics t",
    "final_answer": "Aspects are identified as feature related to an opinion target.",
    "slm_pred": "Aspects are identified as feature related to an opinion target.",
    "llm_pred": "By combining a CNN-based sequence tagger (using word embeddings and POS/context) with linguistic pattern rules and taking tokens tagged B–A/I–A as aspect terms.",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.0625,
    "cost": 5.0
  },
  {
    "id": "94e0cf44345800ef46a8c7d52902f074a1139e1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",
    "final_answer": "The MSM 2013 corpus, the UMBC Twitter corpus, and the Ritter NER corpus are used for the analysis.",
    "slm_pred": "The MSM 2013 corpus, the UMBC Twitter corpus, and the Ritter NER corpus are used for the analysis.",
    "llm_pred": "Ritter (Twitter), MSM 2013 (Twitter), and the UMBC Twitter corpus.",
    "perf_chosen": 0.24,
    "perf_slm": 0.24,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1000 hours of WSJ audio data",
    "final_answer": "WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them.",
    "slm_pred": "WSJ corpus, an 80 hour subset of clean Librispeech, the full 960 hour Librispeech training set, or a combination of all of them.",
    "llm_pred": "Unlabeled audio from WSJ and LibriSpeech (the 80‑hour clean subset or the full 960‑hour set), or their combination.",
    "perf_chosen": 0.1379310345,
    "perf_slm": 0.1379310345,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "12eaaf3b6ebc51846448c6e1ad210dbef7d25a96",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "wav2vec has 12 convolutional layers",
    "final_answer": "12",
    "slm_pred": "The encoder network has five convolutional layers, and the context network has seven convolutional layers.",
    "llm_pred": "12",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.30000000000000004,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "828615a874512844ede9d7f7d92bdc48bb48b18d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "They explore that more data for pre-training improves performance, but a specific threshold or amount of data needed for a certain magnitude of improvement is not explicitly stated.",
    "slm_pred": "They explore that more data for pre-training improves performance, but a specific threshold or amount of data needed for a certain magnitude of improvement is not explicitly stated.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a43c400ae37a8705ff2effb4828f4b0b177a74c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "shared character embeddings for taggers in both languages together through optimization of a joint loss function",
    "final_answer": "The individual embeddings for a given character are shared among the languages jointly trained on.",
    "slm_pred": "The individual embeddings for a given character are shared among the languages jointly trained on.",
    "llm_pred": "By sharing character embeddings in a common vector space learned jointly across languages.",
    "perf_chosen": 0.3870967742,
    "perf_slm": 0.3870967742,
    "perf_llm": 0.34482758620000004,
    "cost": 5.0
  },
  {
    "id": "4056ee2fd7a0a0f444275e627bb881134a1c2a10",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . ",
    "final_answer": "The Universal Dependencies (UD) treebanks datasets.",
    "slm_pred": "The Universal Dependencies (UD) treebanks datasets.",
    "llm_pred": "Universal Dependencies (UD) treebanks",
    "perf_chosen": 0.375,
    "perf_slm": 0.375,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "f6496b8d09911cdf3a9b72aec0b0be6232a6dba1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5c90e1ed208911dbcae7e760a553e912f8c237a5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents",
    "final_answer": "The datasets are of varying sizes, with the in-house dataset containing 6 languages, 56 entity types, and 53 relation types, and the ACE05 dataset containing 3 languages, 7 entity types, and 6 relation types.",
    "slm_pred": "The datasets are of varying sizes, with the in-house dataset containing 6 languages, 56 entity types, and 53 relation types, and the ACE05 dataset containing 3 languages, 7 entity types, and 6 relation types.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.2173913043,
    "perf_slm": 0.2173913043,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3c3b4797e2b21e2c31cf117ad9e52f327721790f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese",
    "final_answer": "They experiment on 7 target languages, including German, Spanish, Portuguese, Chinese, Italian, Japanese, and Arabic.",
    "slm_pred": "They experiment on 7 target languages, including German, Spanish, Portuguese, Chinese, Italian, Japanese, and Arabic.",
    "llm_pred": "German, Spanish, Italian, Portuguese, Chinese, Japanese, and Arabic.",
    "perf_chosen": 0.6153846154,
    "perf_slm": 0.6153846154,
    "perf_llm": 0.8421052632,
    "cost": 5.0
  },
  {
    "id": "a7d72f308444616a0befc8db7ad388b3216e2143",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in-house dataset, ACE05 dataset ",
    "final_answer": "The in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset are used.",
    "slm_pred": "The in-house dataset and the ACE (Automatic Content Extraction) 2005 multilingual dataset are used.",
    "llm_pred": "An in-house multilingual RE dataset (EN, DE, ES, IT, JA, PT) and the ACE 2005 multilingual dataset (EN, AR, ZH).",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "dfb0351e8fa62ceb51ce77b0f607885523d1b8e8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not provide a direct comparison of auto-completion performance between using both language and vision and only language.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0909090909,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a130aa735de3b65c71f27018f20d3c068bafb826",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "16k images and 740k corresponding region descriptions",
    "final_answer": "The dataset provided by this research is large, consisting of 16k images and 740k region descriptions from the Visual Genome dataset, and 9k images and 54k referring expressions from the ReferIt dataset.",
    "slm_pred": "The dataset provided by this research is large, consisting of 16k images and 740k region descriptions from the Visual Genome dataset, and 9k images and 54k referring expressions from the ReferIt dataset.",
    "llm_pred": "approximately 1.73 million samples",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c1663a7f7750b399f40ef7b4bf19d5c598890ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "we replace user embeddings with a low-dimensional image representation",
    "final_answer": "By using a modified FactorCell LSTM that conditions on CNN-extracted image features (fed via the adaptation matrix) to generate character-level completions with beam search.",
    "slm_pred": "They complete a user query prefix conditioned upon an image by using a modified FactorCell LSTM language model that incorporates image features from a pre-trained CNN.",
    "llm_pred": "By using a modified FactorCell LSTM that conditions on CNN-extracted image features (fed via the adaptation matrix) to generate character-level completions with beam search.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1714285714,
    "perf_llm": 0.1818181818,
    "cost": 25.0
  },
  {
    "id": "aa800b424db77e634e82680f804894bfa37f2a34",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention whether the collection process used a WoZ method.",
    "slm_pred": "unanswerable \n\nThe article does not mention whether the collection process used a WoZ method.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fbd47705262bfa0a2ba1440a2589152def64cbbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models",
    "final_answer": "Their model outperformed the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.",
    "slm_pred": "Their model outperformed the baseline by 35% in Exact Match and 25% in Goal Match in previously seen environments.",
    "llm_pred": "35%",
    "perf_chosen": 0.36,
    "perf_slm": 0.36,
    "perf_llm": 0.0625,
    "cost": 5.0
  },
  {
    "id": "51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search",
    "final_answer": "A standard sequence-to-sequence model augmented with an attention mechanism, a two-step baseline approach that divides the task into path generation and path verification, and an ablation model that only takes natural language instructions as input.",
    "slm_pred": "A standard sequence-to-sequence model augmented with an attention mechanism, a two-step baseline approach that divides the task into path generation and path verification, and an ablation model that only takes natural language instructions as input.",
    "llm_pred": "A two-step seq2seq+attention baseline with DFS-based path verification, plus two ablations of their model: (1) no graph input (no attention/FC, no masking) and (2) the same ablation but with the output-layer masking.",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "3aee5c856e0ee608a7664289ffdd11455d153234",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",
    "final_answer": "The proposed model, \"Ours with Mask and Ordered Triplets\", outperformed the Baseline and Ablation models on all metrics in previously seen environments.",
    "slm_pred": "The proposed model, \"Ours with Mask and Ordered Triplets\", outperformed the Baseline and Ablation models on all metrics in previously seen environments.",
    "llm_pred": "It achieved the best performance on the dataset, outperforming all baselines.",
    "perf_chosen": 0.0714285714,
    "perf_slm": 0.0714285714,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f42d470384ca63a8e106c7caf1cb59c7b92dbc27",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "exact match, f1 score, edit distance and goal match",
    "final_answer": "The evaluation metrics used are EM (Exact Match), F1 (harmonic average of precision and recall), ED (minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence), and GM (Goal Match).",
    "slm_pred": "The evaluation metrics used are EM (Exact Match), F1 (harmonic average of precision and recall), ED (minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence), and GM (Goal Match).",
    "llm_pred": "Exact Match (EM), F1 score, Edit Distance (ED), and Goal Match (GM).",
    "perf_chosen": 0.24,
    "perf_slm": 0.24,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "29bdd1fb20d013b23b3962a065de3a564b14f0fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The authors used Mechanical Turk for collecting their dataset.",
    "slm_pred": "The authors used Mechanical Turk for collecting their dataset.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25b2ae2d86b74ea69b09c140a41593c00c47a82b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "using Amazon Mechanical Turk using simulated environments with topological maps",
    "final_answer": "The navigation instructions were collected through Mechanical Turk using 100 simulated environments with a corresponding topological map.",
    "slm_pred": "The navigation instructions were collected through Mechanical Turk using 100 simulated environments with a corresponding topological map.",
    "llm_pred": "Via Amazon Mechanical Turk, using 100 simulated environments with corresponding topological maps.",
    "perf_chosen": 0.5185185185,
    "perf_slm": 0.5185185185,
    "perf_llm": 0.8181818182,
    "cost": 5.0
  },
  {
    "id": "fd7f13b63f6ba674f5d5447b6114a201fe3137cb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "english language",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable\n\nHowever, based on the context, it can be inferred that the experiment is likely done in English.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c82e945b43b2e61c8ea567727e239662309e9508",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort",
    "final_answer": "Clinical sentiment features and structured biomarkers/assessments, including EEG, MRI anatomy (gray matter volume, cortical thickness, surface area), social/role functioning, NEO-FFI, and symptom scales (PANSS, MADRS, YMRS).",
    "slm_pred": "paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, plus several others, including clinical narratives, EEG biomarkers, brain anatomy, social functioning assessments, personality assessment, and symptom scales.",
    "llm_pred": "Clinical sentiment features and structured biomarkers/assessments, including EEG, MRI anatomy (gray matter volume, cortical thickness, surface area), social/role functioning, NEO-FFI, and symptom scales (PANSS, MADRS, YMRS).",
    "perf_chosen": 0.125,
    "perf_slm": 0.1153846154,
    "perf_llm": 0.125,
    "cost": 25.0
  },
  {
    "id": "fbee81a9d90ff23603ee4f5986f9e8c0eb035b52",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",
    "final_answer": "Their initial results show a Fleiss's Kappa of 0.575, indicating moderate to substantial agreement among annotators.",
    "slm_pred": "Their initial results show a Fleiss's Kappa of 0.575, indicating moderate to substantial agreement among annotators.",
    "llm_pred": "Strongest topic coherence in themes of substance use, suicidality, and anxiety disorders.",
    "perf_chosen": 0.0408163265,
    "perf_slm": 0.0408163265,
    "perf_llm": 0.1333333333,
    "cost": 5.0
  },
  {
    "id": "39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)",
    "final_answer": "The authors used two datasets: a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, and a corpus of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms from the Research Patient Data Registry (RPDR).",
    "slm_pred": "The authors used two datasets: a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, and a corpus of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms from the Research Patient Data Registry (RPDR).",
    "llm_pred": "EHRs from McLean Hospital’s OnTrack program (Meditech; 240k paragraphs from 220 patients) and psychiatric EHRs from the Partners HealthCare Research Patient Data Registry (RPDR; ~30,000 patients).",
    "perf_chosen": 0.5981308411,
    "perf_slm": 0.5981308411,
    "perf_llm": 0.3611111111,
    "cost": 5.0
  },
  {
    "id": "1f6180bba0bc657c773bd3e4269f87540a520ead",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "57388bf2693d71eb966d42fa58ab66d7f595e55f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A BPE model is applied to the stem after morpheme segmentation.",
    "final_answer": "The morphology knowledge is implemented by segmenting the words into morpheme units and then applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
    "slm_pred": "The morphology knowledge is implemented by segmenting the words into morpheme units and then applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
    "llm_pred": "By segmenting words into stems and suffixes via morphological analysis and then applying BPE only to the stem (marking stems with “##”, suffixes with “$$”, and BPE continuations with “@@”).",
    "perf_chosen": 0.3783783784,
    "perf_slm": 0.3783783784,
    "perf_llm": 0.1951219512,
    "cost": 5.0
  },
  {
    "id": "47796c7f0a7de76ccb97ccbd43dc851bb8a613d5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12",
    "final_answer": "The word segmentation method works by segmenting complex words into simple morpheme units, either by morpheme segmentation or by applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
    "slm_pred": "The word segmentation method works by segmenting complex words into simple morpheme units, either by morpheme segmentation or by applying Byte Pair Encoding (BPE) on the stem units after morpheme segmentation.",
    "llm_pred": "It splits each word into a stem and suffix morphemes, applies BPE to the stem only, and marks boundaries (“##” for intact stems, “@@” for non-final stem subwords, “$$” for suffixes) to preserve morphology for NMT.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.08510638300000001,
    "cost": 5.0
  },
  {
    "id": "9d5153a7553b7113716420a6ddceb59f877eb617",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nThe article does not mention if the word segmentation method is independently evaluated or not.",
    "slm_pred": "unanswerable \n\nThe article does not mention if the word segmentation method is independently evaluated or not.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "55c840a2f1f663ab2bff984ae71501b17429d0c0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5357c56ea80a21a7ca88a80f21711c5431042c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "36",
    "final_answer": "24 layers, with an additional feature re-presentation layer.",
    "slm_pred": "24 layers, with an additional feature re-presentation layer.",
    "llm_pred": "36 layers",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "35915166ab2fd3d39c0297c427d4ac00e8083066",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, they use the total objective function which is a combination of the auxiliary loss and the main loss, scaled by a coefficient lambda.",
    "slm_pred": "No, they use the total objective function which is a combination of the auxiliary loss and the main loss, scaled by a coefficient lambda.",
    "llm_pred": "Yes—via a weighted sum (main loss plus λ=0.3 times the auxiliary losses across selected layers).",
    "perf_chosen": 0.08,
    "perf_slm": 0.08,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e6c872fea474ea96ca2553f7e9d5875df4ef55cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, it can be inferred that the proposed model might require more computational resources due to the additional feature re-presentation layer and the iterated use of the objective function at intermediate layers, which could potentially increase training time.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.05,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fc29bb14f251f18862c100e0d3cd1396e8f2c3a1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, agglutinative languages are used in the prediction of both prefixing and suffixing languages, as indicated by the results showing that models pretrained on HUN (agglutinative) perform well for both English (suffixing) and Zulu (prefixing).",
    "slm_pred": "Yes, agglutinative languages are used in the prediction of both prefixing and suffixing languages, as indicated by the results showing that models pretrained on HUN (agglutinative) perform well for both English (suffixing) and Zulu (prefixing).",
    "llm_pred": "yes",
    "perf_chosen": 0.0555555556,
    "perf_slm": 0.0555555556,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f3e96c5487d87557a661a65395b0162033dc05b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Zulu",
    "final_answer": "Navajo, an Athabaskan language.",
    "slm_pred": "Navajo, an Athabaskan language.",
    "llm_pred": "Zulu",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "74db8301d42c7e7936eb09b2171cd857744c52eb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors",
    "final_answer": "The performance on the task is evaluated using test accuracy, as shown in Tables TABREF18 and TABREF19, and by manually annotating and categorizing errors in the validation set.",
    "slm_pred": "The performance on the task is evaluated using test accuracy, as shown in Tables TABREF18 and TABREF19, and by manually annotating and categorizing errors in the validation set.",
    "llm_pred": "by accuracy on held-out sets (exact-match inflection accuracy)",
    "perf_chosen": 0.2608695652,
    "perf_slm": 0.2608695652,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "587885bc86543b8f8b134c20e2c62f6251195571",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English, Spanish and Zulu",
    "final_answer": "English, Spanish, and Zulu.",
    "slm_pred": "English, Spanish, and Zulu.",
    "llm_pred": "English, Spanish, and Zulu",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256",
    "final_answer": "The baselines are residual-connected models, such as BASE-4L for IWSLT14 German-English and Turkish-English, and the best performance model provided in BIBREF2 for WMT14 English-German.",
    "slm_pred": "The baselines are residual-connected models, such as BASE-4L for IWSLT14 German-English and Turkish-English, and the best performance model provided in BIBREF2 for WMT14 English-German.",
    "llm_pred": "residual-connected encoder–decoder models (e.g., BASE-4L and the WMT14 baseline from BIBREF2)",
    "perf_chosen": 0.2105263158,
    "perf_slm": 0.2105263158,
    "perf_llm": 0.16,
    "cost": 5.0
  },
  {
    "id": "8c0621016e96d86a7063cb0c9ec20c76a2dba678",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f1214a05cc0e6d870c789aed24a8d4c768e1db2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "German-English, Turkish-English, English-German",
    "final_answer": "The article explores the following language pairs: German-English, Turkish-English, and English-German.",
    "slm_pred": "The article explores the following language pairs: German-English, Turkish-English, and English-German.",
    "llm_pred": "German–English, Turkish–English, and English–German",
    "perf_chosen": 0.4285714286,
    "perf_slm": 0.4285714286,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "41d3ab045ef8e52e4bbe5418096551a22c5e9c43",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",
    "final_answer": "The article mentions the following datasets were used:\n\n1. IWSLT14 German-English dataset\n2. IWSLT14 Turkish-English dataset\n3. SETimes corpus\n4. WMT14 English-German dataset\n5. dev2010\n6. tst2010\n7. tst2011\n8. tst2012\n9. tst2013\n10. tst2014",
    "slm_pred": "The article mentions the following datasets were used:\n\n1. IWSLT14 German-English dataset\n2. IWSLT14 Turkish-English dataset\n3. SETimes corpus\n4. WMT14 English-German dataset\n5. dev2010\n6. tst2010\n7. tst2011\n8. tst2012\n9. tst2013\n10. tst2014",
    "llm_pred": "IWSLT14 German–English, Turkish–English (IWSLT14 + SETimes), and WMT14 English–German",
    "perf_chosen": 0.2926829268,
    "perf_slm": 0.2926829268,
    "perf_llm": 0.4285714286,
    "cost": 5.0
  },
  {
    "id": "62736ad71c76a20aee8e003c462869bab4ab4b1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time",
    "final_answer": "The movement of binomials is tracked by the difference in ordinality between the earliest and latest years in the dataset.",
    "slm_pred": "The movement of binomials is tracked by the difference in ordinality between the earliest and latest years in the dataset.",
    "llm_pred": "By computing yearly ordinality o_{x,y,t} and defining movement as the range over time: max_t o_{x,y,t} − min_t o_{x,y,t}.",
    "perf_chosen": 0.1739130435,
    "perf_slm": 0.1739130435,
    "perf_llm": 0.1492537313,
    "cost": 5.0
  },
  {
    "id": "aaf50a6a9f449389ef212d25d0fae59c10b0df92",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "news publications, wine reviews, and Reddit",
    "final_answer": "news publications, wine reviews, and Reddit posts",
    "slm_pred": "News publications, wine reviews, Reddit, and sports communities.",
    "llm_pred": "news publications, wine reviews, and Reddit posts",
    "perf_chosen": 0.9230769231,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.9230769231,
    "cost": 25.0
  },
  {
    "id": "a1917232441890a89b9a268ad8f987184fa50f7a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Trinomials are likely to appear in exactly one order",
    "final_answer": "Yes, trinomials show more structure than binomials, with 36% being completely frozen, compared to 21% of binomials, and the last word often keeping the same position in different orderings.",
    "slm_pred": "Yes, trinomials show more structure than binomials, with 36% being completely frozen, compared to 21% of binomials, and the last word often keeping the same position in different orderings.",
    "llm_pred": "yes — trinomials show more structure: many are completely frozen, and when not, the last word typically remains fixed, unlike binomials",
    "perf_chosen": 0.15789473680000002,
    "perf_slm": 0.15789473680000002,
    "perf_llm": 0.1333333333,
    "cost": 5.0
  },
  {
    "id": "574f17134e4dd041c357ebb75a7ef590da294d22",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "null model ",
    "final_answer": "The new model proposed is a directed graph where an edge from $i$ to $j$ means that $i$ tends to come before $j$ in binomials.",
    "slm_pred": "The new model proposed is a directed graph where an edge from $i$ to $j$ means that $i$ tends to come before $j$ in binomials.",
    "llm_pred": "a null model in which binomial orderings are randomly assigned according to their global asymmetry values",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.0740740741,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "41fd359b8c1402b31b6f5efd660143d1414783a0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " close to random,",
    "final_answer": "The most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general.",
    "slm_pred": "The most common theories on frozen binomial ordering were largely ineffective at predicting binomial ordering in general.",
    "llm_pred": "Largely ineffective—previously proposed rules performed poorly at scale.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d216d715ec27ee2d4949f9e908895a18fb3238e2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "word length, number of phonemes, number of syllables, alphabetical order, and frequency",
    "final_answer": "The previously proposed rules for predicting binomial ordering mentioned in the article include phonological rules (such as rhythm and Panini's Law), semantic rules (such as the \"Me First\" principle and categorical tendencies), and frequency-based rules, among others.",
    "slm_pred": "The previously proposed rules for predicting binomial ordering mentioned in the article include phonological rules (such as rhythm and Panini's Law), semantic rules (such as the \"Me First\" principle and categorical tendencies), and frequency-based rules, among others.",
    "llm_pred": "phonological, semantic, and metadata (frequency) rules",
    "perf_chosen": 0.0408163265,
    "perf_slm": 0.0408163265,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "ba973b13f26cd5eb1da54663c0a72842681d5bf5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "news publications, wine reviews, and Reddit",
    "final_answer": "News publications, wine reviews, and Reddit are used to test binomial lists.",
    "slm_pred": "News publications, wine reviews, and Reddit are used to test binomial lists.",
    "llm_pred": "Reddit, news articles, and wine reviews.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.8333333333,
    "cost": 5.0
  },
  {
    "id": "508580af51483b5fb0df2630e8ea726ff08d537b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.",
    "final_answer": "They model a city description using embeddings by computing the TF-IDF weighted element-wise mean of the token-level representations from pretrained language models, such as ELMo, BERT, and GloVe, and then reducing the dimensionality of the city embeddings to 40 using PCA.",
    "slm_pred": "They model a city description using embeddings by computing the TF-IDF weighted element-wise mean of the token-level representations from pretrained language models, such as ELMo, BERT, and GloVe, and then reducing the dimensionality of the city embeddings to 40 using PCA.",
    "llm_pred": "By taking the TF‑IDF‑weighted element‑wise mean of token embeddings (ELMo/BERT/GloVe) for each city and then reducing to 40 dimensions with PCA.",
    "perf_chosen": 0.5416666667,
    "perf_slm": 0.5416666667,
    "perf_llm": 0.2894736842,
    "cost": 5.0
  },
  {
    "id": "89d1687270654979c53d0d0e6a845cdc89414c67",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Using crowdsourcing ",
    "final_answer": "They collect human judgements through an \"odd-one-out\" task, where crowd workers are asked to identify the intruder as the city with the maximum Euclidean distance from the other two in a triplet of cities.",
    "slm_pred": "They collect human judgements through an \"odd-one-out\" task, where crowd workers are asked to identify the intruder as the city with the maximum Euclidean distance from the other two in a triplet of cities.",
    "llm_pred": "by crowdsourcing an odd-one-out task where workers pick the intruder among triplets of city descriptions",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.11764705880000001,
    "cost": 5.0
  },
  {
    "id": "fc6cfac99636adda28654e1e19931c7394d76c7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. ",
    "final_answer": "They devise a simple clustering algorithm that iteratively proposes random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence.",
    "slm_pred": "They devise a simple clustering algorithm that iteratively proposes random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence.",
    "llm_pred": "a custom exchange-based clustering algorithm that iteratively swaps memberships to maximize “cluster strength” (intra- vs inter-group distance) under equal-size constraints",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.24657534250000002,
    "cost": 5.0
  },
  {
    "id": "ed7a3e7fc1672f85a768613e7d1b419475950ab4",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "single-domain setting",
    "final_answer": "single-domain setting",
    "slm_pred": "The article does not explicitly state that the approach performs better in the multi-domain setting, but it does mention that the model achieves a joint goal accuracy of 45.72% on the multi-domain dataset MultiWoZ, which is significant better than most of the previous models, whereas on the single-domain dataset WoZ2.0, it maintains performance at the level of the state-of-the-art with a marginal drop of 0.3% compared with previous work.",
    "llm_pred": "single-domain setting",
    "perf_chosen": 1.0,
    "perf_slm": 0.0563380282,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72ceeb58e783e3981055c70a3483ea706511fac3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "joint goal accuracy",
    "final_answer": "The metric of joint goal accuracy is used to compare the model to previous work, and cross-entropy loss is used during training.",
    "slm_pred": "The metric of joint goal accuracy is used to compare the model to previous work, and cross-entropy loss is used during training.",
    "llm_pred": "- Joint goal accuracy\n- Inference Time Multiplier (ITM)\n- Joint domain accuracy (ablation)\n- Joint domain-slot accuracy (ablation)",
    "perf_chosen": 0.24,
    "perf_slm": 0.24,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "9bfa46ad55136f2a365e090ce585fc012495393c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ",
    "final_answer": "The WoZ2.0 dataset and the MultiWoZ dataset are used to evaluate performance.",
    "slm_pred": "The WoZ2.0 dataset and the MultiWoZ dataset are used to evaluate performance.",
    "llm_pred": "WoZ2.0 and MultiWoZ",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "42812113ec720b560eb9463ff5e74df8764d1bff",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "By applying a standard first‑order logic theorem prover to the FOL translations of the sentences using a fixed background ontology/axioms.",
    "slm_pred": "unanswerable",
    "llm_pred": "By applying a standard first‑order logic theorem prover to the FOL translations of the sentences using a fixed background ontology/axioms.",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4f4892f753b1d9c5e5e74c7c94d8c9b6ef523e7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not address why the model cannot learn first-order logic on natural language, but rather discusses its ability to learn on an artificial language and its compositional generalization capabilities.",
    "slm_pred": "unanswerable \n\nThe article does not address why the model cannot learn first-order logic on natural language, but rather discusses its ability to learn on an artificial language and its compositional generalization capabilities.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f258ada8577bb71873581820a94695f4a2c223b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "70,000",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "05bb75a1e1202850efa9191d6901de0a34744af0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "documents from the CommonCrawl dataset that has the most overlapping n-grams with the question",
    "final_answer": "STORIES.",
    "slm_pred": "STORIES.",
    "llm_pred": "STORIES (the customized CommonCrawl subset)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.2105263158,
    "cost": 5.0
  },
  {
    "id": "770aeff30846cd3d0d5963f527691f3685e8af02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "unanswerable \n\nAlthough they mention training on various text corpora and using ensemble methods, they do not explicitly mention fine-tuning their model on the end task.",
    "slm_pred": "unanswerable \n\nAlthough they mention training on various text corpora and using ensemble methods, they do not explicitly mention fine-tuning their model on the end task.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f7817b949605fb04b1e4fec9dd9ca8804fb92ae9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Because, unlike other languages, English does not mark grammatical genders",
    "final_answer": "Because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement.",
    "slm_pred": "Because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement.",
    "llm_pred": "Because many languages have rich gender agreement, not like English; simply swapping gendered words doesn’t update all agreeing forms, making sentences ungrammatical.",
    "perf_chosen": 0.4375,
    "perf_slm": 0.4375,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "8255f74cae1352e5acb2144fb857758dda69be02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by calculating log ratio of grammatical phrase over ungrammatical phrase",
    "final_answer": "They measure grammaticality by computing the log ratio of the grammatical phrase over the ungrammatical phrase.",
    "slm_pred": "They measure grammaticality by computing the log ratio of the grammatical phrase over the ungrammatical phrase.",
    "llm_pred": "the LM log-likelihood ratio of a grammatical phrase to its ungrammatical counterpart",
    "perf_chosen": 0.6923076923,
    "perf_slm": 0.6923076923,
    "perf_llm": 0.4545454545,
    "cost": 5.0
  },
  {
    "id": "db62d5d83ec187063b57425affe73fef8733dd28",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Markov random field with an optional neural parameterization",
    "final_answer": "The inflection model of D18-1473 is used to convert between masculine-inflected and feminine-inflected lemmata.",
    "slm_pred": "The inflection model of D18-1473 is used to convert between masculine-inflected and feminine-inflected lemmata.",
    "llm_pred": "a Markov random field (for morpho-syntactic agreement)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "946676f1a836ea2d6fe98cb4cfc26b9f4f81984d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe paper discusses the experiments and results of several models, but does not provide a single performance metric that can be used to describe the performance achieved by the model.",
    "slm_pred": "unanswerable \n\nThe paper discusses the experiments and results of several models, but does not provide a single performance metric that can be used to describe the performance achieved by the model.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0625,
    "perf_slm": 0.0625,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3b090b416c4ad7d9b5b05df10c5e7770a4590f6a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that supervision afforded by the RNNG and ActionLSTM models did not translate into more robust or humanlike learning outcomes.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0769230769,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a1e07c7563ad038ee2a7de5093ea08efdd6077d4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark",
    "final_answer": "The sizes of the datasets employed vary, including a relatively small corpus, the Penn Treebank, the French Treebank, the Billion Word benchmark, English Wikipedia, and a random subset of the frWaC dataset (approximately 4 million sentences, 138 million word tokens).",
    "slm_pred": "The sizes of the datasets employed vary, including a relatively small corpus, the Penn Treebank, the French Treebank, the Billion Word benchmark, English Wikipedia, and a random subset of the frWaC dataset (approximately 4 million sentences, 138 million word tokens).",
    "llm_pred": "about 4 million sentences (≈138 million word tokens)",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.6086956522,
    "cost": 5.0
  },
  {
    "id": "a1c4f9e8661d4d488b8684f055e0ee0e2275f767",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)",
    "final_answer": "The baseline models are the Recurrent Neural Network (RNN) models, specifically the two-layer recurrent neural language models with long short-term memory architecture (LSTM).",
    "slm_pred": "The baseline models are the Recurrent Neural Network (RNN) models, specifically the two-layer recurrent neural language models with long short-term memory architecture (LSTM).",
    "llm_pred": "two-layer LSTM language models (LSTM (PTB) and LSTM (FTB))",
    "perf_chosen": 0.3529411765,
    "perf_slm": 0.3529411765,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c5171daf82107fce0f285fa18f19e91fbd1215c5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the evaluation metrics include BLEU and ROUGE (1, 2, L) scores",
    "final_answer": "BLEU and ROUGE (1, 2, L) scores with multiple references.",
    "slm_pred": "BLEU and ROUGE (1, 2, L) scores with multiple references.",
    "llm_pred": "BLEU and ROUGE (ROUGE‑1, ROUGE‑2, ROUGE‑L) scores.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "baeb6785077931e842079e9d0c9c9040947ffa4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The E2E NLG challenge dataset BIBREF21",
    "final_answer": "The E2E NLG challenge dataset, which is a crowd-sourced dataset of 50k instances in the restaurant domain.",
    "slm_pred": "The E2E NLG challenge dataset, which is a crowd-sourced dataset of 50k instances in the restaurant domain.",
    "llm_pred": "the E2E NLG Challenge dataset (restaurant domain)",
    "perf_chosen": 0.4347826087,
    "perf_slm": 0.4347826087,
    "perf_llm": 0.7692307692,
    "cost": 5.0
  },
  {
    "id": "bb570d4a1b814f508a07e74baac735bf6ca0f040",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "better sentence pair representations",
    "final_answer": "The model's better performance is attributed to its deeper representations, enriched by different levels of text representation, including subword-level embeddings, pre-trained ELMo embeddings, and bi-attention modules.",
    "slm_pred": "The model's better performance is attributed to its deeper representations, enriched by different levels of text representation, including subword-level embeddings, pre-trained ELMo embeddings, and bi-attention modules.",
    "llm_pred": "Because it augments word embeddings (subword + ELMo) and uses a deep residual bi-attention encoder, yielding richer, better sentence-pair representations.",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.1739130435,
    "cost": 5.0
  },
  {
    "id": "1771a55236823ed44d3ee537de2e85465bf03eaf",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, we can say that recall is \"great\" for the proposed model but \"not good\" for the other models, as mentioned in the article.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1509433962,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "1d74fd1d38a5532d20ffae4abbadaeda225b6932",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",
    "final_answer": "F1: unanswerable; recall: about 90%",
    "slm_pred": "The article does not explicitly mention the f1 score and recall values for the model. However, it mentions that precision is not a priority in their case, and they stopped training when it crossed 70% precision, 90% recall on training and testing sets. \n\nIn the results section, it is mentioned that recall is great for their model but precision is not good, as the model is also trying to detect new potential topics which are not there even in reference Wikipedia-Titles and NER sets. \n\nTherefore, I will not be able to provide a specific answer to the question.",
    "llm_pred": "F1: unanswerable; recall: about 90%",
    "perf_chosen": 0.16,
    "perf_slm": 0.1355932203,
    "perf_llm": 0.16,
    "cost": 25.0
  },
  {
    "id": "da8bda963f179f5517a864943dc0ee71249ee1ce",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "4 layers",
    "final_answer": "one",
    "slm_pred": "unanswerable",
    "llm_pred": "one",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5c059a13d59947f30877bed7d0180cca20a83284",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "A dump of 15 million English news articles published in the past 4 years.",
    "slm_pred": "A dump of 15 million English news articles published in the past 4 years.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a1885f807753cff7a59f69b5cf6d0fdef8484057",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "English wikipedia dataset has more than 18 million, a dump of 15 million English news articles ",
    "final_answer": "about 3 million news articles (≈150 million sentences)",
    "slm_pred": "Around 3 million news articles and 10 million Wikipedia titles.",
    "llm_pred": "about 3 million news articles (≈150 million sentences)",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.3846153846,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "c2553166463b7b5ae4d9786f0446eb06a90af458",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).",
    "final_answer": "The BIBREF5 sieve system (RULE), BIBREF6 system (STAT), and the BIBREF11 deep reinforcement system (NEURAL) are tested.",
    "slm_pred": "The BIBREF5 sieve system (RULE), BIBREF6 system (STAT), and the BIBREF11 deep reinforcement system (NEURAL) are tested.",
    "llm_pred": "the RULE sieve system, the STAT statistical system, and the NEURAL deep reinforcement system",
    "perf_chosen": 0.6086956522,
    "perf_slm": 0.6086956522,
    "perf_llm": 0.6046511628,
    "cost": 5.0
  },
  {
    "id": "cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.",
    "final_answer": "The proposed model achieves new state-of-the-art (SOTA) performances on both datasets, with significant improvements compared to the previous SOTA without BERT on SParC, improving Ques. Match and Int. Match by 10.6 and 5.4 points, respectively.",
    "slm_pred": "The proposed model achieves new state-of-the-art (SOTA) performances on both datasets, with significant improvements compared to the previous SOTA without BERT on SParC, improving Ques. Match and Int. Match by 10.6 and 5.4 points, respectively.",
    "llm_pred": "10.6 and 5.4 percentage points (Ques.Match and Int.Match on SParC, vs prior SOTA).",
    "perf_chosen": 0.6071428571,
    "perf_slm": 0.6071428571,
    "perf_llm": 0.5882352941,
    "cost": 5.0
  },
  {
    "id": "69e678666d11731c9bfa99953e2cd5a5d11a4d4f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SParC BIBREF2 and CoSQL BIBREF6",
    "final_answer": "SParC and CoSQL.",
    "slm_pred": "SParC and CoSQL.",
    "llm_pred": "SParC and CoSQL",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "471d624498ab48549ce492ada9e6129da05debac",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Concat\nTurn\nGate\nAction Copy\nTree Copy\nSQL Attn\nConcat + Action Copy\nConcat + Tree Copy\nConcat + SQL Attn\nTurn + Action Copy\nTurn + Tree Copy\nTurn + SQL Attn\nTurn + SQL Attn + Action Copy",
    "final_answer": "Concat, Gate, Turn, Tree Copy, SQL Attn, Action Copy, and a combination of these methods.",
    "slm_pred": "Concat, Gate, Turn, Tree Copy, SQL Attn, Action Copy, and a combination of these methods.",
    "llm_pred": "Concat; Turn; Gate; SQL Attention; Tree Copy; Action Copy",
    "perf_chosen": 0.3829787234,
    "perf_slm": 0.3829787234,
    "perf_llm": 0.3902439024,
    "cost": 5.0
  },
  {
    "id": "f858031ebe57b6139af46ee0f25c10870bb00c3c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SParC BIBREF2 and CoSQL BIBREF6",
    "final_answer": "SParC and CoSQL.",
    "slm_pred": "SParC and CoSQL.",
    "llm_pred": "SParC and CoSQL",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "1763a029daca7cab10f18634aba02a6bd1b6faa7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain",
    "final_answer": "The improvement over the state-of-the-art results varies, with a 2.4% and 1.6% improvement in the “DS” part of the two datasets respectively, and a very large margin of +11.4% and +4.9% in the “HDS” part.",
    "slm_pred": "The improvement over the state-of-the-art results varies, with a 2.4% and 1.6% improvement in the “DS” part of the two datasets respectively, and a very large margin of +11.4% and +4.9% in the “HDS” part.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.5652173913,
    "perf_slm": 0.5652173913,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f9de9ddea0c70630b360167354004ab8cbfff041",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "There were hierarchical and non-hierarchical baselines; BERT was one of those baselines",
    "final_answer": "no",
    "slm_pred": "Yes",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "58e65741184c81c9e7fe0ca15832df2d496beb6f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "269b05b74d5215b09c16e95a91ae50caedd9e2aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "agreement rates, Kappa value",
    "final_answer": "Kappa value.",
    "slm_pred": "Kappa value.",
    "llm_pred": "Cohen’s kappa (kappa value)",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "0d7f514f04150468b2d1de9174c12c28e52c5511",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "agreement of 0.85 and Kappa value of 0.83",
    "final_answer": "High, with Kappa values ranging from 0.83 to 0.99 for different aspects of annotation.",
    "slm_pred": "High, with Kappa values ranging from 0.83 to 0.99 for different aspects of annotation.",
    "llm_pred": "≥ 0.9 (Cohen’s kappa)",
    "perf_chosen": 0.2727272727,
    "perf_slm": 0.2727272727,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "4d223225dbf84a80e2235448a4d7ba67bfb12490",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "removing AltLexC and adding Progression into our sense hierarchy",
    "final_answer": "Adapted to take account of properties of Chinese described in Section 3.",
    "slm_pred": "Adapted to take account of properties of Chinese described in Section 3.",
    "llm_pred": "By modifying the PDTB-3 scheme for Chinese—e.g., removing AltLexC, adding a Progression sense, and adjusting argument scope and connective treatment.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.41379310340000003,
    "cost": 5.0
  },
  {
    "id": "ca26cfcc755f9d0641db0e4d88b4109b903dbb26",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.",
    "final_answer": "The results show significant improvement over baseline models, with some achieving better performance by up to $55.98\\%$ in F1 score.",
    "slm_pred": "The results show significant improvement over baseline models, with some achieving better performance by up to $55.98\\%$ in F1 score.",
    "llm_pred": "Significantly better than the baselines (p<0.001).",
    "perf_chosen": 0.1951219512,
    "perf_slm": 0.1951219512,
    "perf_llm": 0.0740740741,
    "cost": 5.0
  },
  {
    "id": "6cdd61ebf84aa742155f4554456cc3233b6ae2bf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SVM with RBF kernel",
    "final_answer": "The majority baseline, SVM with RBF kernel, FastText, and BiLSTM with Attention.",
    "slm_pred": "The majority baseline, SVM with RBF kernel, FastText, and BiLSTM with Attention.",
    "llm_pred": "FastText and a BiLSTM with attention (both using only the claim text).",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "8e8097cada29d89ca07166641c725e0f8fed6676",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.",
    "final_answer": "The pragmatic and discourse context is added to the dataset by structuring the argument path, which only consists of claims and corresponds to a particular line of reasoning for the given controversial topic.",
    "slm_pred": "The pragmatic and discourse context is added to the dataset by structuring the argument path, which only consists of claims and corresponds to a particular line of reasoning for the given controversial topic.",
    "llm_pred": "By including each claim’s argument-path context—its predecessor claims from the Kialo argument tree (from the thesis down to the claim).",
    "perf_chosen": 0.36923076920000003,
    "perf_slm": 0.36923076920000003,
    "perf_llm": 0.2307692308,
    "cost": 5.0
  },
  {
    "id": "951098f0b7169447695b47c142384f278f451a1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact",
    "final_answer": "The dataset includes impact votes and the corresponding context of the argument, where users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact, and very high impact.",
    "slm_pred": "The dataset includes impact votes and the corresponding context of the argument, where users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact, and very high impact.",
    "llm_pred": "impact votes for each claim and the corresponding argument context (its position in the argument tree)",
    "perf_chosen": 0.6896551724000001,
    "perf_slm": 0.6896551724000001,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "07c59824f5e7c5399d15491da3543905cfa5f751",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "4,261  days for France and 4,748 for the UK",
    "final_answer": "The training set contains data up to December 31st, 2013 (2,557 days for France and 2,922 for the UK), the validation set contains data from 2014 and 2015 (730 days), and the test set contains data from January 1st, 2016 (974 days for France and 1,096 for the UK).",
    "slm_pred": "The training set contains data up to December 31st, 2013 (2,557 days for France and 2,922 for the UK), the validation set contains data from 2014 and 2015 (730 days), and the test set contains data from January 1st, 2016 (974 days for France and 1,096 for the UK).",
    "llm_pred": "Training: 2,557 days (France) and 2,922 days (UK); Testing: 974 days (France) and 1,096 days (UK).",
    "perf_chosen": 0.2413793103,
    "perf_slm": 0.2413793103,
    "perf_llm": 0.32,
    "cost": 5.0
  },
  {
    "id": "77f04cd553df691e8f4ecbe19da89bc32c7ac734",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$, which is an example of geometric property visible for context similarity between words.",
    "slm_pred": "Yes, the vector $\\overrightarrow{king} - \\overrightarrow{man} + \\overrightarrow{woman}$ is expected to be very close to the vector $\\overrightarrow{queen}$, which is an example of geometric property visible for context similarity between words.",
    "llm_pred": "Yes—e.g., the vector king − man + woman is close to queen.",
    "perf_chosen": 0.0666666667,
    "perf_slm": 0.0666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "728a55c0f628f2133306b6bd88af00eb54017b12",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.",
    "final_answer": "Geometric properties pertaining to the behavior of the time series, as well as grouping of words by topic, such as winter, summer, or day of the week clusters.",
    "slm_pred": "Geometric properties pertaining to the behavior of the time series, as well as grouping of words by topic, such as winter, summer, or day of the week clusters.",
    "llm_pred": "They encode linguistic similarity as vector-space geometry (e.g., king − man + woman ≈ queen).",
    "perf_chosen": 0.2608695652,
    "perf_slm": 0.2608695652,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d5498d16e8350c9785782b57b1e5a82212dbdaad",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Relative error is less than 5%",
    "final_answer": "Achieving less than 5% of MAPE was deemed impressive by expert electricity forecasters.",
    "slm_pred": "Achieving less than 5% of MAPE was deemed impressive by expert electricity forecasters.",
    "llm_pred": "less than 5% relative error (MAPE)",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.8333333333,
    "cost": 5.0
  },
  {
    "id": "3e839783d8a4f2fe50ece4a9b476546f0842b193",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "F1 score of 66.66%",
    "final_answer": "an improvement of about 5 F-score points over the prior state-of-the-art on SSEC",
    "slm_pred": "unanswerable",
    "llm_pred": "an improvement of about 5 F-score points over the prior state-of-the-art on SSEC",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.0,
    "perf_llm": 0.11764705880000001,
    "cost": 25.0
  },
  {
    "id": "2869d19e54fb554fcf1d6888e526135803bb7d75",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "F1 score of 82.10%",
    "final_answer": "The proposed system improves the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.",
    "slm_pred": "The proposed system improves the existing best system of SemEval 2016 task 6 by 3.2 F-score points for sentiment analysis.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0833333333,
    "perf_slm": 0.0833333333,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "894c086a2cbfe64aa094c1edabbb1932a3d7c38a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",
    "final_answer": "unanswerable \n\nThe article does not provide a clear definition of the state-of-the-art systems, but it mentions that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis.",
    "slm_pred": "unanswerable \n\nThe article does not provide a clear definition of the state-of-the-art systems, but it mentions that the proposed system improves the existing state-of-the-art systems for sentiment and emotion analysis.",
    "llm_pred": "Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN.",
    "perf_chosen": 0.1960784314,
    "perf_slm": 0.1960784314,
    "perf_llm": 0.3571428571,
    "cost": 5.0
  },
  {
    "id": "722e9b6f55971b4c48a60f7a9fe37372f5bf3742",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism",
    "final_answer": "The proposed system performs multi-tasking by using a two-layered multi-task attention based neural network, where emotion analysis is utilized as an auxiliary task to improve the efficiency of sentiment analysis.",
    "slm_pred": "The proposed system performs multi-tasking by using a two-layered multi-task attention based neural network, where emotion analysis is utilized as an auxiliary task to improve the efficiency of sentiment analysis.",
    "llm_pred": "By jointly training sentiment and emotion analysis with a shared BiLSTM word encoder and task‑specific two-level attentions, feeding each to separate output layers.",
    "perf_chosen": 0.33802816900000004,
    "perf_slm": 0.33802816900000004,
    "perf_llm": 0.34375000000000006,
    "cost": 5.0
  },
  {
    "id": "9c2f306044b3d1b3b7fdd05d1c046e887796dd7a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15",
    "final_answer": "The benchmark dataset of SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC).",
    "slm_pred": "The benchmark dataset of SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC).",
    "llm_pred": "SemEval 2016 Task 6 and the Stance Sentiment Emotion Corpus (SSEC)",
    "perf_chosen": 0.6923076923,
    "perf_slm": 0.6923076923,
    "perf_llm": 0.8181818182,
    "cost": 5.0
  },
  {
    "id": "3d99bc8ab2f36d4742e408f211bec154bc6696f7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article does provide some details about the model's architecture, including the number of hidden state vectors and the dimensions of the context vector.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0740740741,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9219eef636ddb020b9d394868959325562410f83",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not specify the previous state-of-the-art model.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ff83eea2df9976c1a01482818340871b17ad4f8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on the previous state-of-the-art performance.",
    "slm_pred": "unanswerable \n\nThe article does not provide information on the previous state-of-the-art performance.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0ee20a3a343e1e251b74a804e9aa1393d17b46d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets",
    "final_answer": "The classifier can significantly speed up the annotation process, making large-scale content analysis more feasible.",
    "slm_pred": "The classifier can significantly speed up the annotation process, making large-scale content analysis more feasible.",
    "llm_pred": "By pre-filtering tweets to surface likely polarized (pro-Russian/pro-Ukrainian) cases, increasing the hit rate and speeding manual annotation.",
    "perf_chosen": 0.2553191489,
    "perf_slm": 0.2553191489,
    "perf_llm": 0.1632653061,
    "cost": 5.0
  },
  {
    "id": "f0e8f045e2e33a2129e67fb32f356242db1dc280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "applying reasoning BIBREF36 or irony detection methods BIBREF37",
    "final_answer": "Category I errors need further investigation, and category II and III errors could be improved by applying reasoning or irony detection methods.",
    "slm_pred": "Category I errors need further investigation, and category II and III errors could be improved by applying reasoning or irony detection methods.",
    "llm_pred": "Applying reasoning and irony-detection methods and better handling Twitter-specific/unknown words (e.g., via more data and improved representations).",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.24,
    "cost": 5.0
  },
  {
    "id": "b6c235d5986914b380c084d9535a7b01310c0278",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly",
    "final_answer": "The classifiers use category I, II, and III errors, which are further categorized into direct inference, event-specific knowledge, and interpretation of the speaker's intention, respectively.",
    "slm_pred": "The classifiers use category I, II, and III errors, which are further categorized into direct inference, event-specific knowledge, and interpretation of the speaker's intention, respectively.",
    "llm_pred": "Category I, II, and III errors.",
    "perf_chosen": 0.11428571430000001,
    "perf_slm": 0.11428571430000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " convolutional neural network (CNN) BIBREF29",
    "final_answer": "A convolutional neural network (CNN).",
    "slm_pred": "A convolutional neural network (CNN).",
    "llm_pred": "a convolutional neural network (CNN)",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "1e4450e23ec81fdd59821055f998fd9db0398b16",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The hashtag-based baseline uses hashtags identified by the pointwise mutual information (pmi) between a hashtag and a class in the annotated dataset.",
    "slm_pred": "The hashtag-based baseline uses hashtags identified by the pointwise mutual information (pmi) between a hashtag and a class in the annotated dataset.",
    "llm_pred": "class-indicative hashtags selected via PMI from the annotated dataset",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "02ce4c288df14a90a210cb39973c6ac0fb4cec59",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "English",
    "final_answer": "English and Russian",
    "slm_pred": "unanswerable \n\nHowever, the dataset includes English tweets, as mentioned in the section \"Dataset\".",
    "llm_pred": "English and Russian",
    "perf_chosen": 0.5,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "60726d9792d301d5ff8e37fbb31d5104a520dea3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MH17 Twitter dataset",
    "final_answer": "The MH17 Twitter dataset introduced by BIBREF4, a dataset collected to study the flow of (dis)information about the MH17 plane crash on Twitter.",
    "slm_pred": "The MH17 Twitter dataset introduced by BIBREF4, a dataset collected to study the flow of (dis)information about the MH17 plane crash on Twitter.",
    "llm_pred": "the MH17 Twitter dataset",
    "perf_chosen": 0.2307692308,
    "perf_slm": 0.2307692308,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "e39d90b8d959697d9780eddce3a343e60543be65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content",
    "final_answer": "Previous studies analysed information based on manually labeled content, such as television transcripts or tweets, and also used distant annotations, for example, classifying tweets based on the domain of a URL or a hashtag.",
    "slm_pred": "Previous studies analysed information based on manually labeled content, such as television transcripts or tweets, and also used distant annotations, for example, classifying tweets based on the domain of a URL or a hashtag.",
    "llm_pred": "Domains of shared URLs and hashtags.",
    "perf_chosen": 0.39080459770000003,
    "perf_slm": 0.39080459770000003,
    "perf_llm": 0.0677966102,
    "cost": 5.0
  },
  {
    "id": "5f7850254b723adf891930c6faced1058b99bd57",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. ",
    "final_answer": "The HMM models use spaces, indentation, special characters, and state transitions as features, and those features are somewhat interpretable due to the visualization of the HMM states and LSTM components.",
    "slm_pred": "The HMM models use spaces, indentation, special characters, and state transitions as features, and those features are somewhat interpretable due to the visualization of the HMM states and LSTM components.",
    "llm_pred": "Either raw characters (discrete emissions) or LSTM hidden-state vectors (continuous emissions); the resulting HMM states are quite interpretable, often aligning with human-readable cues like spaces, indentation, punctuation, and comment symbols.",
    "perf_chosen": 0.380952381,
    "perf_slm": 0.380952381,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "4d05a264b2353cff310edb480a917d686353b007",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The HMM can identify punctuation or pick up on vowels.",
    "final_answer": "The HMMs learn about the state transitions and emission parameters, which are related to spaces, indentation, and special characters in the data.",
    "slm_pred": "The HMMs learn about the state transitions and emission parameters, which are related to spaces, indentation, and special characters in the data.",
    "llm_pred": "spaces, indentation, and punctuation/comment symbols (e.g., code comments)",
    "perf_chosen": 0.0625,
    "perf_slm": 0.0625,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7cdce4222cea6955b656c1a3df1129bb8119e2d0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters",
    "final_answer": "The authors use visualizations, such as Figures 3, 3, and 2, to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
    "slm_pred": "The authors use visualizations, such as Figures 3, 3, and 2, to show how the LSTM and HMM components of the hybrid algorithm complement each other in terms of features learned in the data.",
    "llm_pred": "By visualization: color‑coding the text with HMM state posteriors and with k‑means clusters of LSTM hidden states (supplemented by decision‑tree probes of individual LSTM units).",
    "perf_chosen": 0.2413793103,
    "perf_slm": 0.2413793103,
    "perf_llm": 0.3265306122,
    "cost": 5.0
  },
  {
    "id": "6ea63327ffbab2fc734dd5c2414e59d3acc56ea5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.",
    "final_answer": "The gap in performance between the HMMs and the LSTMs is relatively small, as the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension on all text data sets.",
    "slm_pred": "The gap in performance between the HMMs and the LSTMs is relatively small, as the hybrid algorithm performs a bit better than the standalone LSTM with the same LSTM state dimension on all text data sets.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1739130435,
    "perf_slm": 0.1739130435,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "50690b72dc61748e0159739a9a0243814d37f360",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "Yes, the article does not mention any non-English data.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8266642303fbc6a1138b4e23ee1d859a6f584fbb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BIBREF3, BIBREF4, BIBREF9",
    "final_answer": "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset are used.",
    "slm_pred": "The Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset are used.",
    "llm_pred": "Waseem and Hovy’s Sexist/Racist (SR), Davidson et al.’s Hate (HATE), and Golbeck et al.’s Harassment (HAR) Twitter datasets.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3685bf2409b23c47bfd681989fb4a763bcab6be2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "300 Dimensional Glove",
    "final_answer": "The embedding algorithm used is Glove Common Crawl Embeddings (840B Token) with a dimension size of 300.",
    "slm_pred": "The embedding algorithm used is Glove Common Crawl Embeddings (840B Token) with a dimension size of 300.",
    "llm_pred": "GloVe (Common Crawl), 300-dimensional",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "19225e460fff2ac3aebc7fe31fcb4648eda813fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Common Crawl ",
    "final_answer": "840B Token Common Crawl Embeddings.",
    "slm_pred": "840B Token Common Crawl Embeddings.",
    "llm_pred": "Common Crawl (GloVe, 840B tokens)",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "f37026f518ab56c859f6b80b646d7f19a7b684fa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "our model requires 100k parameters , while BIBREF8 requires 250k parameters",
    "final_answer": "They found that their model required 100k parameters, while BIBREF8's model required 250k parameters, indicating a 150k parameter difference.",
    "slm_pred": "They found that their model required 100k parameters, while BIBREF8's model required 250k parameters, indicating a 150k parameter difference.",
    "llm_pred": "about 150,000 fewer parameters",
    "perf_chosen": 0.41379310340000003,
    "perf_slm": 0.41379310340000003,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "1231934db6adda87c1b15e571468b8e9d225d6fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Excluding the embedding weights, our model requires 100k parameters",
    "final_answer": "excluding the word embeddings, our model requires 100k parameters.",
    "slm_pred": "excluding the word embeddings, our model requires 100k parameters.",
    "llm_pred": "100k parameters (excluding embedding weights)",
    "perf_chosen": 0.7777777778,
    "perf_slm": 0.7777777778,
    "perf_llm": 0.7142857143,
    "cost": 5.0
  },
  {
    "id": "81303f605da57ddd836b7c121490b0ebb47c60e7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Sexist/Racist (SR) data set, HATE dataset, HAR",
    "final_answer": "The datasets used were the Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset.",
    "slm_pred": "The datasets used were the Sexist/Racist (SR) dataset, the HATE dataset, and the HAR dataset.",
    "llm_pred": "the Sexist/Racist (SR), HATE, and HAR (Harassment) Twitter datasets",
    "perf_chosen": 0.4545454545,
    "perf_slm": 0.4545454545,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "a3f108f60143d13fe38d911b1cc3b17bdffde3bd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",
    "final_answer": "The system's F1 performance outperformed previous results by as much as 12 F1 points, achieving a new state of the art for classifying hate speech.",
    "slm_pred": "The system's F1 performance outperformed previous results by as much as 12 F1 points, achieving a new state of the art for classifying hate speech.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1025641026,
    "perf_slm": 0.1025641026,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "118ff1d7000ea0d12289d46430154cc15601fd8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "logistic regression",
    "final_answer": "The baseline was a logistic regression model using character n-grams and word unigrams with TF*IDF weighting.",
    "slm_pred": "The baseline was a logistic regression model using character n-grams and word unigrams with TF*IDF weighting.",
    "llm_pred": "a logistic regression using TF–IDF-weighted character n-grams and word unigrams",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "102a0439739428aac80ac11795e73ce751b93ea1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "KFTT BIBREF12 and BTEC BIBREF13",
    "final_answer": "The English-to-Japanese language pair datasets used were KFTT and BTEC, as well as the larger Japanese-English ASPEC dataset.",
    "slm_pred": "The English-to-Japanese language pair datasets used were KFTT and BTEC, as well as the larger Japanese-English ASPEC dataset.",
    "llm_pred": "KFTT, BTEC, and ASPEC",
    "perf_chosen": 0.2608695652,
    "perf_slm": 0.2608695652,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English-Japanese",
    "final_answer": "English-to-Japanese, specifically on the KFTT and BTEC datasets.",
    "slm_pred": "English-to-Japanese, specifically on the KFTT and BTEC datasets.",
    "llm_pred": "English–Japanese (and Japanese–English for ASPEC)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "04f72eddb1fc73dd11135a80ca1cf31e9db75578",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "278 more annotations",
    "final_answer": "25% more roles.",
    "slm_pred": "25% more roles.",
    "llm_pred": "25% more coverage.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "f74eaee72cbd727a6dffa1600cdf1208672d713e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "QA pairs per predicate",
    "final_answer": "The coverage was measured by evaluating against expertly annotated data and comparing with PropBank.",
    "slm_pred": "The coverage was measured by evaluating against expertly annotated data and comparing with PropBank.",
    "llm_pred": "by evaluating recall against an expertly annotated sample and via comparison with PropBank",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "068dbcc117c93fa84c002d3424bafb071575f431",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",
    "final_answer": "Quality was measured through F1 scores, specifically using Unlabeled Argument Detection (UA) and Labeled Argument Detection (LA) metrics, as well as Inter-Annotator Agreement (IAA) for dataset consistency.",
    "slm_pred": "Quality was measured through F1 scores, specifically using Unlabeled Argument Detection (UA) and Labeled Argument Detection (LA) metrics, as well as Inter-Annotator Agreement (IAA) for dataset consistency.",
    "llm_pred": "an additional validation step in which workers judged the validity of the questions (but not the answers)",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "96526a14820b7debfd6f7c5beeade0a854b93d1a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " trained annotators BIBREF4, crowdsourcing BIBREF5 ",
    "final_answer": "The corpus was obtained through crowdsourcing with 133K verbs annotated by 2 trained workers independently and a third consolidator.",
    "slm_pred": "The corpus was obtained through crowdsourcing with 133K verbs annotated by 2 trained workers independently and a third consolidator.",
    "llm_pred": "by crowdsourcing (on Amazon Mechanical Turk)",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "extensive personal feedback",
    "final_answer": "Through several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback, and 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations.",
    "slm_pred": "Through several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback, and 1 out of 3 participants were selected after exhibiting good performance, tested against expert annotations.",
    "llm_pred": "Through several short training rounds (up to 15 predicates each) with extensive personal feedback, selecting workers who perform well against expert annotations.",
    "perf_chosen": 0.15789473680000002,
    "perf_slm": 0.15789473680000002,
    "perf_llm": 0.24,
    "cost": 5.0
  },
  {
    "id": "78c010db6413202b4063dc3fb6e3cc59ec16e7e3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a trained worker consolidates existing annotations ",
    "final_answer": "Systematic screening of workers, concise guidelines, and a short training procedure.",
    "slm_pred": "Systematic screening of workers, concise guidelines, and a short training procedure.",
    "llm_pred": "Screened, trained workers annotate each verb twice independently, and a third “consolidator” merges/validates their QAs under refined guidelines (e.g., handling modality/negation and single-entity answers).",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.1333333333,
    "cost": 5.0
  },
  {
    "id": "a69af5937cab861977989efd72ad1677484b5c8c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the annotation machinery of BIBREF5",
    "final_answer": "The previous dataset was annotated by trained annotators, but later resorted to crowdsourcing with a single QA-generating worker and validation by two others.",
    "slm_pred": "The previous dataset was annotated by trained annotators, but later resorted to crowdsourcing with a single QA-generating worker and validation by two others.",
    "llm_pred": "By non-expert crowd workers after a brief training procedure.",
    "perf_chosen": 0.0714285714,
    "perf_slm": 0.0714285714,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8847f2c676193189a0f9c0fe3b86b05b5657b76a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "1593 annotations",
    "final_answer": "133K verbs",
    "slm_pred": "unanswerable",
    "llm_pred": "133K verbs",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "05196588320dfb0b9d9be7d64864c43968d329bc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e930f153c89dfe9cff75b7b15e45cd3d700f4c72",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nHowever, the article does mention that the dataset contains 36 million En → Fr sentence pairs, which was used to train a multilingual NMT model, but it does not provide any information about the relationship between the training data size and the performance of the multilingual encoder.",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that the dataset contains 36 million En → Fr sentence pairs, which was used to train a multilingual NMT model, but it does not provide any information about the relationship between the training data size and the performance of the multilingual encoder.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0408163265,
    "perf_slm": 0.0408163265,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "545ff2f76913866304bfacdb4cc10d31dbbd2f37",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "WMT 2014 En-Fr parallel corpus",
    "final_answer": "English–French parallel translation data.",
    "slm_pred": "unanswerable",
    "llm_pred": "English–French parallel translation data.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.0,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "cf93a209c8001ffb4ef505d306b6ced5936c6b63",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "late 2014",
    "final_answer": "Between 2014-2016.",
    "slm_pred": "Between 2014-2016.",
    "llm_pred": "2014–2016",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "96-97.6% using the objects color or shape and 79% using shape alone",
    "final_answer": "The task success rate achieved is 97.6% when using the object's color to uniquely identify it and 96.0% when using the object's shape.",
    "slm_pred": "The task success rate achieved is 97.6% when using the object's color to uniquely identify it and 96.0% when using the object's shape.",
    "llm_pred": "97.6%",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "1e2ffa065b640e912d6ed299ff713a12195e12c4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command",
    "final_answer": "The authors perform a simulated binning task in which a robot is tasked to place a cube into a bowl as outlined by the verbal command.",
    "slm_pred": "The authors perform a simulated binning task in which a robot is tasked to place a cube into a bowl as outlined by the verbal command.",
    "llm_pred": "a simulated binning task where a robot places a cube into a specified bowl based on language and vision",
    "perf_chosen": 0.9387755102,
    "perf_slm": 0.9387755102,
    "perf_llm": 0.4761904762,
    "cost": 5.0
  },
  {
    "id": "28b2a20779a78a34fb228333dc4b93fd572fda15",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "supervised learning",
    "final_answer": "unanswerable \n\nThe article does not explicitly state whether the proposed approach is reinforcement or supervised learning, but it is based on imitation learning which is typically a supervised learning methodology.",
    "slm_pred": "unanswerable \n\nThe article does not explicitly state whether the proposed approach is reinforcement or supervised learning, but it is based on imitation learning which is typically a supervised learning methodology.",
    "llm_pred": "supervised (imitation) learning from demonstrations",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "b367b823c5db4543ac421d0057b02f62ea16bf9f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nHowever, it's only exploited for tackling unknown words of Japanese texts, not for Vietnamese texts due to the unavailability of Vietnamese WordNet.",
    "slm_pred": "yes \n\nHowever, it's only exploited for tackling unknown words of Japanese texts, not for Vietnamese texts due to the unavailability of Vietnamese WordNet.",
    "llm_pred": "yes",
    "perf_chosen": 0.0833333333,
    "perf_slm": 0.0833333333,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "84737d871bde8058d8033e496179f7daec31c2d3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7b3d207ed47ae58286029b62fd0c160a0145e73d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not explicitly mention the dataset used in the paper. However, it mentions various sources such as financial reports, press releases, earnings call transcripts, credit agreements, news articles, customer interaction logs, and social data that are used in the study.",
    "slm_pred": "unanswerable \n\nThe article does not explicitly mention the dataset used in the paper. However, it mentions various sources such as financial reports, press releases, earnings call transcripts, credit agreements, news articles, customer interaction logs, and social data that are used in the study.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0454545455,
    "perf_slm": 0.0454545455,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d58c264068d8ca04bb98038b4894560b571bab3e",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe paper discusses various methods and models for anomaly detection from text in the financial domain, but it does not provide specific information about their performance.",
    "slm_pred": "unanswerable \n\nThe paper discusses various methods and models for anomaly detection from text in the financial domain, but it does not provide specific information about their performance.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0714285714,
    "perf_slm": 0.0714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f80d89fb905b3e7e17af1fe179b6f441405ad79b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5f6fac08c97c85d5f4f4d56d8b0691292696f8e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe paper does mention a study that improved the performance of standard methods for fake-news detection, but it does not provide a baseline for the task.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "6adec34d86095643e6b89cda5c7cd94f64381acc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.",
    "final_answer": "Non-contextual properties of a word, derived directly from the word, and capture the general tendency of a word being echoed in explanations.",
    "slm_pred": "Non-contextual properties of a word, derived directly from the word, and capture the general tendency of a word being echoed in explanations.",
    "llm_pred": "Features derived directly from the word itself—e.g., its frequency, length (#characters), and WordNet depth.",
    "perf_chosen": 0.8095238095,
    "perf_slm": 0.8095238095,
    "perf_llm": 0.4117647059,
    "cost": 5.0
  },
  {
    "id": "62ba1fefc1eb826fe0cbac092d37a3e2098967e9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "random method , LSTM ",
    "final_answer": "The random model with a 15% echo rate.",
    "slm_pred": "The random model with a 15% echo rate.",
    "llm_pred": "a random model that predicts “echoed” with 15% probability",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "93ac147765ee2573923f68aa47741d4bcbf88fa8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties",
    "final_answer": "The proposed features are: non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
    "slm_pred": "The proposed features are: non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
    "llm_pred": "non-contextual word properties; word usage features in the OP and in the PC; features capturing OP–PC connection/differences; and general OP/PC conversation properties",
    "perf_chosen": 0.8679245283,
    "perf_slm": 0.8679245283,
    "perf_llm": 0.5106382979,
    "cost": 5.0
  },
  {
    "id": "14c0328e8ec6360a913b8ecb3e50cb27650ff768",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)",
    "final_answer": "Our models outperform the random baseline by a wide margin, with the best F1 score more than doubling that of the random baseline (0.286 vs. 0.116).",
    "slm_pred": "Our models outperform the random baseline by a wide margin, with the best F1 score more than doubling that of the random baseline (0.286 vs. 0.116).",
    "llm_pred": "Random baseline: predict the positive label with 0.15 probability (F1 ≈ 0.116 on content words).",
    "perf_chosen": 0.8214285714,
    "perf_slm": 0.8214285714,
    "perf_llm": 0.3111111111,
    "cost": 5.0
  },
  {
    "id": "6073fa9050da76eeecd8aa3ccc7ecb16a238d83f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "F1 score",
    "final_answer": "The F1 score is used as the evaluation metric, especially since the problem is imbalanced.",
    "slm_pred": "The F1 score is used as the evaluation metric, especially since the problem is imbalanced.",
    "llm_pred": "F1 score",
    "perf_chosen": 0.23529411760000002,
    "perf_slm": 0.23529411760000002,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "eacd7e540cc34cb45770fcba463f4bf968681d59",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The article provides explanations for intriguing patterns of word being echoed.",
    "slm_pred": "The article provides explanations for intriguing patterns of word being echoed.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1124804c3702499b78cf0678bab5867e81284b6c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties",
    "final_answer": "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
    "slm_pred": "Non-contextual properties of a word, word usage in an OP or PC, how a word connects an OP and PC, and general OP/PC properties.",
    "llm_pred": "- Non-contextual word features\n- Word usage features in the OP and in the PC\n- Features capturing OP–PC differences (how a word connects OP and PC)\n- General OP/PC conversation properties",
    "perf_chosen": 0.9387755102,
    "perf_slm": 0.9387755102,
    "perf_llm": 0.641509434,
    "cost": 5.0
  },
  {
    "id": "2b78052314cb730824836ea69bc968df7964b4e4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SQUAD",
    "final_answer": "The SQUAD dataset denoted as $\\mathcal {S}$, and the SQUAD dataset split into ${\\mathcal {S}}^{tr}$, ${\\mathcal {S}}^{val}$, and ${\\mathcal {S}}^{te}$ for training, validation, and testing respectively.",
    "slm_pred": "The SQUAD dataset denoted as $\\mathcal {S}$, and the SQUAD dataset split into ${\\mathcal {S}}^{tr}$, ${\\mathcal {S}}^{val}$, and ${\\mathcal {S}}^{te}$ for training, validation, and testing respectively.",
    "llm_pred": "SQuAD (Stanford Question Answering Dataset).",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.0740740741,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "11d2f0d913d6e5f5695f8febe2b03c6c125b667c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "using the BLEU score as a quantitative metric and human evaluation for quality",
    "final_answer": "The performance of this system is measured using the BLEU metric, human evaluation, and usability study.",
    "slm_pred": "The performance of this system is measured using the BLEU metric, human evaluation, and usability study.",
    "llm_pred": "By BLEU on the VQG model’s validation set and human evaluation/usability surveys for the chatbot’s dialogue quality.",
    "perf_chosen": 0.48275862070000003,
    "perf_slm": 0.48275862070000003,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "1c85a25ec9d0c4f6622539f48346e23ff666cd5f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "5 questions per image",
    "final_answer": "5 questions per image.",
    "slm_pred": "5 questions per image.",
    "llm_pred": "5",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "37d829cd42db9ae3d56ab30953a7cf9eda050841",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, the VQG model uses a similar Encoder-Decoder architecture with attention, similar to image captioning systems.",
    "slm_pred": "Yes, the VQG model uses a similar Encoder-Decoder architecture with attention, similar to image captioning systems.",
    "llm_pred": "yes",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4b41f399b193d259fd6e24f3c6e95dc5cae926dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",
    "final_answer": "A total of 15,000 images with 75,000 questions from MS COCO, Bing, and Flickr datasets are used for training the VQG model, while the chatbot model is trained on 162,064 utterances over 10,907 dialogues from Persona-chat dataset and 304,713 utterances over 220,579 conversational exchanges from Cornell-movie corpus.",
    "slm_pred": "A total of 15,000 images with 75,000 questions from MS COCO, Bing, and Flickr datasets are used for training the VQG model, while the chatbot model is trained on 162,064 utterances over 10,907 dialogues from Persona-chat dataset and 304,713 utterances over 220,579 conversational exchanges from Cornell-movie corpus.",
    "llm_pred": "15,000 images with 75,000 questions",
    "perf_chosen": 0.41791044780000003,
    "perf_slm": 0.41791044780000003,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "76377e5bb7d0a374b0aefc54697ac9cd89d2eba8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "By considering words as vertices and generating directed edges between neighboring words within a sentence",
    "final_answer": "They obtain word lattices from words by treating all possible substrings of a sentence as vertices, connecting neighbor words by directed edges according to their positions in the original sentence, and using a lookup vocabulary to decide which substrings can be considered as words.",
    "slm_pred": "They obtain word lattices from words by treating all possible substrings of a sentence as vertices, connecting neighbor words by directed edges according to their positions in the original sentence, and using a lookup vocabulary to decide which substrings can be considered as words.",
    "llm_pred": "By taking all substrings that appear in a lookup vocabulary as nodes and connecting neighboring words with directed edges according to their positions in the sentence.",
    "perf_chosen": 0.3389830508,
    "perf_slm": 0.3389830508,
    "perf_llm": 0.4390243902,
    "cost": 5.0
  },
  {
    "id": "85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Precision@1, Mean Average Precision, Mean Reciprocal Rank",
    "final_answer": "P@1 (Precision@1), MAP (Mean Average Precision), and MRR (Mean Reciprocal Rank) are used for DBQA, while P@1 and MRR are used for KBRE.",
    "slm_pred": "P@1 (Precision@1), MAP (Mean Average Precision), and MRR (Mean Reciprocal Rank) are used for DBQA, while P@1 and MRR are used for KBRE.",
    "llm_pred": "Precision@1 (P@1), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR)",
    "perf_chosen": 0.4666666667,
    "perf_slm": 0.4666666667,
    "perf_llm": 0.7777777778,
    "cost": 5.0
  },
  {
    "id": "4b128f9e94d242a8e926bdcb240ece279d725729",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "DBQA, KBRE",
    "final_answer": "They evaluate on two Chinese question answering datasets, DBQA (document based question answering) and KBRE (knowledge based relation extraction).",
    "slm_pred": "They evaluate on two Chinese question answering datasets, DBQA (document based question answering) and KBRE (knowledge based relation extraction).",
    "llm_pred": "DBQA and KBRE (two Chinese QA datasets from NLPCC-2016)",
    "perf_chosen": 0.1904761905,
    "perf_slm": 0.1904761905,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "f8f13576115992b0abb897ced185a4f9d35c5de9",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "two competing languages (language 1 and language 2)",
    "slm_pred": "unanswerable",
    "llm_pred": "two competing languages (language 1 and language 2)",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "1fdcc650c65c11908f6bde67d5052087245f3dde",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "yes",
    "slm_pred": "The article does not specify the language of the data, but it mentions that the dataset (UXTD) contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, who are speaking English. \n\nHowever, the data selection for this investigation defines a simplified phonetic segment classification task using English phonemes, such as /p, b, v, f, /, and /r/. \n\nSo, the answer to your question is 'yes', but with the caveat that the language of the data is only specified as English in the context of the phonetic segment classification task.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "abad9beb7295d809d7e5e1407cbf673c9ffffd19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, they suggest using recurrent architectures, evaluating the complementarity of audio and ultrasound signals, and extending the classification system to more fine-grained place of articulation.",
    "slm_pred": "Yes, they suggest using recurrent architectures, evaluating the complementarity of audio and ultrasound signals, and extending the classification system to more fine-grained place of articulation.",
    "llm_pred": "Adding the speaker’s mean ultrasound frame as an extra input (e.g., a second channel in CNNs) to aid speaker normalization.",
    "perf_chosen": 0.0769230769,
    "perf_slm": 0.0769230769,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "265c9b733e4dfffb76acfbade4c0c9b14d3ccde1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)",
    "final_answer": "The dataset, UXTD, contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, with 31 female and 27 male participants.",
    "slm_pred": "The dataset, UXTD, contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old, with 31 female and 27 male participants.",
    "llm_pred": "UltraSuite UXTD: synchronized acoustic–ultrasound from 58 typically developing children (5–12 years; 31F/27M), phone-aligned B-mode midsagittal tongue images recorded on a SonixRP at 121 fps with 135° FOV, each frame 63×412 samples, using UXTD type A and B utterances.",
    "perf_chosen": 0.5405405405,
    "perf_slm": 0.5405405405,
    "perf_llm": 0.29885057470000004,
    "cost": 5.0
  },
  {
    "id": "0f928732f226185c76ad5960402e9342c0619310",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "feedforward neural networks (DNNs), convolutional neural networks (CNNs)",
    "final_answer": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification.",
    "slm_pred": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used for classification.",
    "llm_pred": "feedforward neural networks and convolutional neural networks",
    "perf_chosen": 0.7619047619,
    "perf_slm": 0.7619047619,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "11c5b12e675cfd8d1113724f019d8476275bd700",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "They differ from previous work in focusing on child speech with a larger number of speakers (58 children) and using cross-validation to evaluate speaker-independent systems across all speakers.",
    "slm_pred": "They differ from previous work in focusing on child speech with a larger number of speakers (58 children) and using cross-validation to evaluate speaker-independent systems across all speakers.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d24acc567ebaec1efee52826b7eaadddc0a89e8b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "10700",
    "final_answer": "10700 training examples with roughly 2000 to 3000 examples per class.",
    "slm_pred": "10700 training examples with roughly 2000 to 3000 examples per class.",
    "llm_pred": "10,700",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2d62a75af409835e4c123a615b06235a352a67fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "feedforward neural networks, convolutional neural networks",
    "final_answer": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used, with DNNs consisting of 3 hidden layers and CNNs with 2 convolutional and max pooling layers.",
    "slm_pred": "Feedforward neural networks (DNNs) and convolutional neural networks (CNNs) are used, with DNNs consisting of 3 hidden layers and CNNs with 2 convolutional and max pooling layers.",
    "llm_pred": "feedforward (DNN) and convolutional (CNN) neural networks",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "58",
    "final_answer": "58 speakers, specifically 31 female and 27 male children.",
    "slm_pred": "58 speakers, specifically 31 female and 27 male children.",
    "llm_pred": "58",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "c034f38a570d40360c3551a6469486044585c63c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.",
    "final_answer": "Significant improvements in perplexity (with p-value < 0.05) were obtained by the proposed method compared to the baselines.",
    "slm_pred": "Significant improvements in perplexity (with p-value < 0.05) were obtained by the proposed method compared to the baselines.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.12121212120000001,
    "perf_slm": 0.12121212120000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9cbea686732b5b85f77868ca47d2f93cf34516ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution",
    "final_answer": "The multi-turn dialog system learns by directly from the data, without human-defined heuristic rules, and with the help of an external text analysis program called the Linguistic Inquiry and Word Count (LIWC) to capture the emotion information carried in the context.",
    "slm_pred": "The multi-turn dialog system learns by directly from the data, without human-defined heuristic rules, and with the help of an external text analysis program called the Linguistic Inquiry and Word Count (LIWC) to capture the emotion information carried in the context.",
    "llm_pred": "In an end-to-end, data-driven way, learning emotional interactions directly from multi-turn dialog data without hand-crafted rules.",
    "perf_chosen": 0.4096385542,
    "perf_slm": 0.4096385542,
    "perf_llm": 0.1034482759,
    "cost": 5.0
  },
  {
    "id": "6aee16c4f319a190c2a451c1c099b66162299a28",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness",
    "final_answer": "Human evaluation is performed by recruiting English-speaking students to rate the responses generated by the models based on three criteria: grammatical correctness, contextual coherence, and emotional appropriateness.",
    "slm_pred": "Human evaluation is performed by recruiting English-speaking students to rate the responses generated by the models based on three criteria: grammatical correctness, contextual coherence, and emotional appropriateness.",
    "llm_pred": "By four English-speaking students who rated each model’s generated response (given the first three turns of 100 four-turn dialogs) on grammatical correctness, contextual coherence, and emotional appropriateness using a 0–2 scale.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.30000000000000004,
    "cost": 5.0
  },
  {
    "id": "4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "180047e1ccfc7c98f093b8d1e1d0479a4cca99cc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " sequence-to-sequence model (denoted as S2S), HRAN",
    "final_answer": "The vanilla sequence-to-sequence model (S2S) and HRAN are used as baseline models.",
    "slm_pred": "The vanilla sequence-to-sequence model (S2S) and HRAN are used as baseline models.",
    "llm_pred": "the vanilla sequence-to-sequence model (S2S) and HRAN (Hierarchical Recurrent Attention Network)",
    "perf_chosen": 0.5555555556,
    "perf_slm": 0.5555555556,
    "perf_llm": 0.4705882353,
    "cost": 5.0
  },
  {
    "id": "fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "unanswerable \n\nAlthough the article mentions that deep neural networks can be used for \"a range of collective natural language tasks, including chunking and extraction of named entities and relationships\", it does not specifically mention relation extraction as a task being evaluated.",
    "slm_pred": "unanswerable \n\nAlthough the article mentions that deep neural networks can be used for \"a range of collective natural language tasks, including chunking and extraction of named entities and relationships\", it does not specifically mention relation extraction as a task being evaluated.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b5d6357d3a9e3d5fdf9b344ae96cddd11a407875",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6",
    "final_answer": "the neural-parser-based baseline",
    "slm_pred": "The neural-parser-based system.",
    "llm_pred": "the neural-parser-based baseline",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f33a21c6a9c75f0479ffdbb006c40e0739134716",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "syntax-based system may generate correct syntactic analyses for partial grammatical fragments",
    "final_answer": "The main reason is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, providing crucial information for SRL.",
    "slm_pred": "The main reason is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, providing crucial information for SRL.",
    "llm_pred": "yes — because parsers can recover correct structure for grammatical fragments in L2 texts, providing crucial cues that improve SRL.",
    "perf_chosen": 0.6111111111,
    "perf_slm": 0.6111111111,
    "perf_llm": 0.25806451610000003,
    "cost": 5.0
  },
  {
    "id": "8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Authors",
    "final_answer": "Two senior students majoring in Applied Linguistics.",
    "slm_pred": "Two senior students majoring in Applied Linguistics.",
    "llm_pred": "two senior students majoring in Applied Linguistics",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "17f5f4a5d943c91d46552fb75940b67a72144697",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X",
    "final_answer": "36.4% in rank-correlation with respect to human attention in the VQA-HAT dataset.",
    "slm_pred": "36.4% in rank-correlation with respect to human attention in the VQA-HAT dataset.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.4242424242,
    "perf_slm": 0.4242424242,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "83f22814aaed9b5f882168e22a3eac8f5fda3882",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "rank-correlation BIBREF25",
    "final_answer": "They measure the correlation using rank-correlation, as in BIBREF4, BIBREF24.",
    "slm_pred": "They measure the correlation using rank-correlation, as in BIBREF4, BIBREF24.",
    "llm_pred": "by computing rank-correlation between human and model attention maps",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "ed11b4ff7ca72dd80a792a6028e16ba20fccff66",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "they are available in the Visual Genome dataset",
    "final_answer": "They obtain region descriptions and object annotations from the Visual Genome dataset.",
    "slm_pred": "They obtain region descriptions and object annotations from the Visual Genome dataset.",
    "llm_pred": "from the Visual Genome dataset",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "a48c6d968707bd79469527493a72bfb4ef217007",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "MultiNLI",
    "final_answer": "MultiNLI",
    "slm_pred": "SNLI + MultiNLI, however, the article specifically states that even training on this combined dataset, accuracy drops significantly when tested on the SICK dataset.",
    "llm_pred": "MultiNLI",
    "perf_chosen": 1.0,
    "perf_slm": 0.0833333333,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b69897deb5fb80bf2adb44f9cbf6280d747271b3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BERT",
    "final_answer": "BERT.",
    "slm_pred": "BERT.",
    "llm_pred": "BERT",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ad1f230f10235413d1fe501e414358245b415476",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",
    "final_answer": "six high-performing models covering sentence encoding models, cross-sentence attention models, and fine-tuned pre-trained language models, specifically: BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT.",
    "slm_pred": "six high-performing models covering sentence encoding models, cross-sentence attention models, and fine-tuned pre-trained language models, specifically: BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT.",
    "llm_pred": "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0a521541b9e2b5c6d64fb08eb318778eba8ac9f7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SNLI, MultiNLI and SICK",
    "final_answer": "The datasets used were SNLI, MultiNLI, and SICK.",
    "slm_pred": "The datasets used were SNLI, MultiNLI, and SICK.",
    "llm_pred": "SNLI, MultiNLI, and SICK",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "11e376f98df42f487298ec747c32d485c845b5cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, we can make an inference that the baseline was not sufficient as the article states \"However, their effort is not enough since such efforts are primarily based on manual moderation to identify and delete offensive materials. The process is labour intensive, time consuming, and not sustainable or scalable in reality\"",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0377358491,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "284ea817fd79bc10b7a82c88d353e8f8a9d7e93c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "c0122190119027dc3eb51f0d4b4483d2dbedc696",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN",
    "final_answer": "They use a Stacking method as their ensemble technique.",
    "slm_pred": "They use a Stacking method as their ensemble technique.",
    "llm_pred": "a stacking classifier using a simple fully connected neural network (dense model) as the meta-model",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "1ed6acb88954f31b78d2821bb230b722374792ed",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5a33ec23b4341584a8079db459d89a4e23420494",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "1b9119813ea637974d21862a8ace83bc1acbab8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).",
    "final_answer": "VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD, and text crawled from Facebook.",
    "slm_pred": "VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD, and text crawled from Facebook.",
    "llm_pred": "the VLSP 2019 Hate Speech Detection (HSD) dataset",
    "perf_chosen": 0.0975609756,
    "perf_slm": 0.0975609756,
    "perf_llm": 0.12121212120000001,
    "cost": 5.0
  },
  {
    "id": "8abb96b2450ebccfcc5c98772cec3d86cd0f53e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nAlthough the article discusses Twitter health-related data, there is no mention of the language of the data.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f52ec4d68de91dba66668f0affc198706949ff90",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Women-Yoga",
    "final_answer": "Yoga–Veganism and Women–Yoga.",
    "slm_pred": "The article does not explicitly state the other interesting correlations observed, but it mentions the discovery of correlations such as \"Yoga-Veganism\" and \"Women-Yoga\" in Topic 2.",
    "llm_pred": "Yoga–Veganism and Women–Yoga.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0740740741,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "225a567eeb2698a9d3f1024a8b270313a6d15f82",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix",
    "final_answer": "The baselines were a basic RNN text comprehension model well-trained on a large dataset and models pretrained on large data.",
    "slm_pred": "The baselines were a basic RNN text comprehension model well-trained on a large dataset and models pretrained on large data.",
    "llm_pred": "Evidence Averaging, and Evidence Averaging augmented with a BM25 word-overlap feature.",
    "perf_chosen": 0.1395348837,
    "perf_slm": 0.1395348837,
    "perf_llm": 0.11764705880000001,
    "cost": 5.0
  },
  {
    "id": "35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ensemble of hand-crafted syntactic and frame-semantic features BIBREF16",
    "final_answer": "The state of the art for ranking MC Test answers is the averaging-based model, which can beat even the best previously reported model (HABCNN-TE) on the MC-500 dataset.",
    "slm_pred": "The state of the art for ranking MC Test answers is the averaging-based model, which can beat even the best previously reported model (HABCNN-TE) on the MC-500 dataset.",
    "llm_pred": "an ensemble of hand-crafted syntactic and frame-semantic features",
    "perf_chosen": 0.0555555556,
    "perf_slm": 0.0555555556,
    "perf_llm": 0.875,
    "cost": 5.0
  },
  {
    "id": "f5eac66c08ebec507c582a2445e99317a83e9ebe",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The final size of the dataset is outlined in Fig. FIGREF8.",
    "slm_pred": "The final size of the dataset is outlined in Fig. FIGREF8.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "62613aca3d7c7d534c9f6d8cb91ff55626bb8695",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset",
    "final_answer": "The Argus dataset, AI2-8grade/CK12 dataset, and MCTest dataset were used.",
    "slm_pred": "The Argus dataset, AI2-8grade/CK12 dataset, and MCTest dataset were used.",
    "llm_pred": "Argus (their question–evidence dataset), AI2-8grade/CK12 science QA, and MCTest (MC-160 and MC-500)",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "6e4505609a280acc45b0a821755afb1b3b518ffd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The BLEU metric ",
    "final_answer": "The BLEU metric is adopted to evaluate the model performance during evaluation.",
    "slm_pred": "The BLEU metric is adopted to evaluate the model performance during evaluation.",
    "llm_pred": "BLEU",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "9bd938859a8b063903314a79f09409af8801c973",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets",
    "final_answer": "WMT14 En–Fr, WMT14 En–De, IWSLT 2014 De–En, and IWSLT 2015 En–Vi.",
    "slm_pred": "The WMT 2014 English-French and English-German translation datasets, the IWSLT 2014 German-English translation dataset, the IWSLT 2015 English-Vietnamese translation dataset, and two small IWSLT datasets.",
    "llm_pred": "WMT14 En–Fr, WMT14 En–De, IWSLT 2014 De–En, and IWSLT 2015 En–Vi.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "68ba5bf18f351e8c83fae7b444cc50bef7437f13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "De-En, En-Fr and En-Vi translation tasks",
    "final_answer": "WMT14 En-Fr and En-De datasets, IWSLT De-En dataset, and IWSLT 2015 En-Vi dataset.",
    "slm_pred": "WMT14 En-Fr and En-De datasets, IWSLT De-En dataset, and IWSLT 2015 En-Vi dataset.",
    "llm_pred": "WMT14 English–French, WMT14 English–German, and IWSLT2014 German–English.",
    "perf_chosen": 0.4210526316,
    "perf_slm": 0.4210526316,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "f6a1125c5621a2f32c9bcdd188dff14efa096083",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "2.2 BLEU gains",
    "final_answer": "MUSE achieves 2.2 BLEU gains in En-Fr translation compared to Evolved Transformer.",
    "slm_pred": "MUSE achieves 2.2 BLEU gains in En-Fr translation compared to Evolved Transformer.",
    "llm_pred": "2.2 BLEU points (on En–Fr).",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "282aa4e160abfa7569de7d99b8d45cabee486ba4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the weighted sum of the new opinion representations, according to their associations with the current aspect representation",
    "final_answer": "The opinion summary is determined via a bi-linear term to calculate the association score between the opinion representation and each aspect, and then the improved opinion summary is obtained via the weighted sum of the opinion representations.",
    "slm_pred": "The opinion summary is determined via a bi-linear term to calculate the association score between the opinion representation and each aspect, and then the improved opinion summary is obtained via the weighted sum of the opinion representations.",
    "llm_pred": "By applying bi-linear attention to aspect-transformed opinion representations and taking the weighted sum of those representations.",
    "perf_chosen": 0.40740740740000003,
    "perf_slm": 0.40740740740000003,
    "perf_llm": 0.4242424242,
    "cost": 5.0
  },
  {
    "id": "ecfb2e75eb9a8eba8f640a039484874fa0d2fceb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a6950c22c7919f86b16384facc97f2cf66e5941d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.",
    "final_answer": "They use the SemEval ABSA challenge datasets from 2014 to 2016, as well as MPQA, to train the model.",
    "slm_pred": "They use the SemEval ABSA challenge datasets from 2014 to 2016, as well as MPQA, to train the model.",
    "llm_pred": "the four SemEval ABSA ATE datasets: SemEval 2014 Laptop and the Restaurant sets from SemEval 2014, 2015, and 2016",
    "perf_chosen": 0.2173913043,
    "perf_slm": 0.2173913043,
    "perf_llm": 0.5652173913,
    "cost": 5.0
  },
  {
    "id": "54be3541cfff6574dba067f1e581444537a417db",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.",
    "final_answer": "The proposed framework achieves 5.0%, 1.6%, 1.4%, and 1.3% absolute gains on the four datasets compared to the winning systems of SemEval ABSA.",
    "slm_pred": "The proposed framework achieves 5.0%, 1.6%, 1.4%, and 1.3% absolute gains on the four datasets compared to the winning systems of SemEval ABSA.",
    "llm_pred": "5.0%, 1.6%, 1.4%, and 1.3% absolute gains (across the four datasets).",
    "perf_chosen": 0.7234042553000001,
    "perf_slm": 0.7234042553000001,
    "perf_llm": 0.4571428571,
    "cost": 5.0
  },
  {
    "id": "221e9189a9d2431902d8ea833f486a38a76cbd8e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The average number of utterances per dialog is about 23 ",
    "final_answer": "about 23.",
    "slm_pred": "about 23.",
    "llm_pred": "about 23",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "a276d5931b989e0a33f2a0bc581456cca25658d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2",
    "final_answer": "Several baseline models, including state-of-the-art neural seq2seq architectures such as n-gram, convolution, LSTM, and Transformer models, are offered.",
    "slm_pred": "Several baseline models, including state-of-the-art neural seq2seq architectures such as n-gram, convolution, LSTM, and Transformer models, are offered.",
    "llm_pred": "n‑gram (3- and 4-gram), convolutional seq2seq (fconv), LSTM (with and without attention), Transformer, GPT‑2; and for argument prediction, a seq2seq Transformer and a copy-augmented Transformer.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.380952381,
    "cost": 5.0
  },
  {
    "id": "c21d26130b521c9596a1edd7b9ef3fe80a499f1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations",
    "final_answer": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations.",
    "slm_pred": "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks, and making restaurant reservations.",
    "llm_pred": "ordering pizza; creating auto repair appointments; setting up ride service; ordering movie tickets; ordering coffee drinks; making restaurant reservations",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.9743589744000001,
    "cost": 5.0
  },
  {
    "id": "ec8043290356fcb871c2f5d752a9fe93a94c2f71",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques",
    "final_answer": "other text classification tasks",
    "slm_pred": "unanswerable",
    "llm_pred": "other text classification tasks",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.0,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "728c2fb445173fe117154a2a5482079caa42fe24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach",
    "final_answer": "The traditional co-occurrence networks fail to establish links between similar words whenever they appear distant in the text because they only connect adjacent words, overlooking long-range syntactical links and semantically similar words not sharing the same lemma.",
    "slm_pred": "The traditional co-occurrence networks fail to establish links between similar words whenever they appear distant in the text because they only connect adjacent words, overlooking long-range syntactical links and semantically similar words not sharing the same lemma.",
    "llm_pred": "Because word adjacency networks connect only adjacent words, they ignore long‑range relationships, so distant semantically similar words are not linked.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.20512820510000002,
    "cost": 5.0
  },
  {
    "id": "23d32666dfc29ed124f3aa4109e2527efa225fbc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "They use it as addition to previous model - they add new edge between words if word embeddings are similar.",
    "final_answer": "They augment the co-occurrence network with embedding-based virtual edges; they don’t replace the existing features.",
    "slm_pred": "They complement and add to the previous features of the model, rather than replacing them.",
    "llm_pred": "They augment the co-occurrence network with embedding-based virtual edges; they don’t replace the existing features.",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.2941176471,
    "perf_llm": 0.11764705880000001,
    "cost": 25.0
  },
  {
    "id": "076928bebde4dffcb404be216846d9d680310622",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks",
    "final_answer": "word adjacency networks (window-based co-occurrence graphs linking adjacent words)",
    "slm_pred": "The previous co-occurrence networks are based on simple models such as word adjacency networks and traditional word co-occurrence networks.",
    "llm_pred": "word adjacency networks (window-based co-occurrence graphs linking adjacent words)",
    "perf_chosen": 0.30000000000000004,
    "perf_slm": 0.36,
    "perf_llm": 0.30000000000000004,
    "cost": 25.0
  },
  {
    "id": "f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "balanced accuracy, i.e., the average of the three accuracies on each class",
    "final_answer": "Yes—via manual human evaluation of explanation relevance (percentage of explanations judged relevant).",
    "slm_pred": "The model explanation output is evaluated using BLEU score, but it is mentioned that BLEU score is not an appropriate measure for the quality of explanations, and human evaluation is suggested instead.",
    "llm_pred": "Yes—via manual human evaluation of explanation relevance (percentage of explanations judged relevant).",
    "perf_chosen": 0.0833333333,
    "perf_slm": 0.13636363640000002,
    "perf_llm": 0.0833333333,
    "cost": 25.0
  },
  {
    "id": "66bf0d61ffc321f15e7347aaed191223f4ce4b4a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "2,060 workers",
    "final_answer": "three",
    "slm_pred": "unanswerable \n\nThe article does not mention the number of annotators used to write natural language explanations to SNLI-VE-2.0.",
    "llm_pred": "three",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5dfa59c116e0ceb428efd99bab19731aa3df4bbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Totally 6980 validation and test image-sentence pairs have been corrected.",
    "final_answer": "one per training instance and three per validation/test instance",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that the authors randomly selected 100 image-sentence pairs in the validation set of SNLI-VE and their corresponding explanations in e-SNLI and examined how relevant these explanations are for the VTE task.",
    "llm_pred": "one per training instance and three per validation/test instance",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.17021276600000002,
    "perf_llm": 0.1052631579,
    "cost": 25.0
  },
  {
    "id": "0c557b408183630d1c6c325b5fb9ff1573661290",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set",
    "final_answer": "The performance difference of the existing model between the original and corrected corpus is not significant, with a slight decrease of 0.50% in the corrected corpus.",
    "slm_pred": "The performance difference of the existing model between the original and corrected corpus is not significant, with a slight decrease of 0.50% in the corrected corpus.",
    "llm_pred": "0.16 percentage points",
    "perf_chosen": 0.13636363640000002,
    "perf_slm": 0.13636363640000002,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a08b5018943d4428f067c08077bfff1af3de9703",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "neutral class",
    "final_answer": "The neutral class in SNLI-VE has a substantial labelling error rate of approximately 31%.",
    "slm_pred": "The neutral class in SNLI-VE has a substantial labelling error rate of approximately 31%.",
    "llm_pred": "neutral",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "9447ec36e397853c04dcb8f67492ca9f944dbd4b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words",
    "final_answer": "A dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
    "slm_pred": "A dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
    "llm_pred": "A 2.6‑GB Italian corpus from the 2019‑04‑01 Italian Wikipedia dump, Italian Google News categories, and anonymized Laila customer‑care chats.",
    "perf_chosen": 0.375,
    "perf_slm": 0.375,
    "perf_llm": 0.3225806452,
    "cost": 5.0
  },
  {
    "id": "12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "Yes.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "32c149574edf07b1a96d7f6bc49b95081de1abd2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3de27c81af3030eb2d9de1df5ec9bfacdef281a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences",
    "final_answer": "The dataset used to train Word2Vec for the Italian Language is composed of 2.6 GB of raw text, including 421,829,960 words divided into 17,305,401 sentences.",
    "slm_pred": "The dataset used to train Word2Vec for the Italian Language is composed of 2.6 GB of raw text, including 421,829,960 words divided into 17,305,401 sentences.",
    "llm_pred": "2.6 GB of raw text",
    "perf_chosen": 0.3870967742,
    "perf_slm": 0.3870967742,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "cc680cb8f45aeece10823a3f8778cf215ccc8af0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others",
    "final_answer": "Different parameter settings, such as the number of epochs, window size, and negative sampling value, have a significant impact on the performance and semantic capacity of the resulting model, leading to oscillatory trends and varying levels of accuracy in the syntactic and semantic macro-areas.",
    "slm_pred": "Different parameter settings, such as the number of epochs, window size, and negative sampling value, have a significant impact on the performance and semantic capacity of the resulting model, leading to oscillatory trends and varying levels of accuracy in the syntactic and semantic macro-areas.",
    "llm_pred": "Performance depends strongly on tuning: more epochs generally improve results; high negative sampling—especially with small windows—hurts accuracy and causes oscillations; the best K depends on window size; Skip‑gram outperforms CBOW; and semantics are captured more easily than syntax (3COSMUL best overall, 3COSADD slightly better in limited-learning/semantic cases).",
    "perf_chosen": 0.1714285714,
    "perf_slm": 0.1714285714,
    "perf_llm": 0.1369863014,
    "cost": 5.0
  },
  {
    "id": "fab4ec639a0ea1e07c547cdef1837c774ee1adb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not compare the findings for Italian language with the English language version.",
    "slm_pred": "unanswerable \n\nThe article does not compare the findings for Italian language with the English language version.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9190c56006ba84bf41246a32a3981d38adaf422c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)",
    "final_answer": "The dataset used for training Word2Vec in Italian language was obtained from a dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
    "slm_pred": "The dataset used for training Word2Vec in Italian language was obtained from a dump of the Italian Wikipedia, Italian Google News, and anonymized chats between users and a customer care chatbot (Laila).",
    "llm_pred": "a combined corpus of Italian Wikipedia (2019-04-01 dump), Italian Google News articles, and anonymized Laila customer-care chat logs",
    "perf_chosen": 0.6197183099,
    "perf_slm": 0.6197183099,
    "perf_llm": 0.4210526316,
    "cost": 5.0
  },
  {
    "id": "7aab78e90ba1336950a2b0534cc0cb214b96b4fd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.",
    "final_answer": "By an attention mechanism over the morphology table, whose weighted affix embeddings are concatenated to the decoder’s last layer to guide prediction.",
    "slm_pred": "The auxiliary signals from the morphology table are incorporated in the decoder through an attention module that assigns different weights to the affixes in the table.",
    "llm_pred": "By an attention mechanism over the morphology table, whose weighted affix embeddings are concatenated to the decoder’s last layer to guide prediction.",
    "perf_chosen": 0.25,
    "perf_slm": 0.3181818182,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "b7fe91e71da8f4dc11e799b3bd408d253230e8c6",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "target-side affixes",
    "final_answer": "target-language affixes (affix embeddings)",
    "slm_pred": "Affixes of the target language.",
    "llm_pred": "target-language affixes (affix embeddings)",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "16fa6896cf4597154363a6c9a98deb49fffef15f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nThe article does not explicitly mention the language of the Twitter data used, but it does mention using the \"twint\" scraping tool and \"GloVe word embeddings\" which are typically used for English text.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0f60864503ecfd5b048258e21d548ab5e5e81772",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "Yes, the authors mention several confounds, including the skew in the model due to the use of \"influential\" Twitter users who rarely make spelling errors or use sarcasm, which leads to a high rate of false negatives, and the potential bias in the study due to the influx of climate change deniers who tweet about hurricanes only after the event.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fe578842021ccfc295209a28cf2275ca18f8d155",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel",
    "final_answer": "The machine learning models used include neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel).",
    "slm_pred": "The machine learning models used include neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel).",
    "llm_pred": "RNNs (including an LSTM), CNNs, Naive Bayes with Laplace smoothing, k‑clustering, and linear‑kernel SVMs.",
    "perf_chosen": 0.6153846154,
    "perf_slm": 0.6153846154,
    "perf_llm": 0.5384615385,
    "cost": 5.0
  },
  {
    "id": "00ef9cc1d1d60f875969094bb246be529373cb1d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.",
    "final_answer": "The methodology used to compensate for limited labelled data is the \"influential tweet\" labeling technique, where a set of influential Twitter users who are certain to have a classifiable sentiment regarding the topic are used to label tweets in bulk.",
    "slm_pred": "The methodology used to compensate for limited labelled data is the \"influential tweet\" labeling technique, where a set of influential Twitter users who are certain to have a classifiable sentiment regarding the topic are used to label tweets in bulk.",
    "llm_pred": "Using distant supervision: label tweets by “influential” accounts with known pro/anti climate stances, assigning binary labels to their tweets to generate training data.",
    "perf_chosen": 0.5633802817,
    "perf_slm": 0.5633802817,
    "perf_llm": 0.18518518520000002,
    "cost": 5.0
  },
  {
    "id": "279b633b90fa2fd69e84726090fadb42ebdf4c02",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires",
    "final_answer": "The East Coast Bomb Cyclone, the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, and the California Camp Fires.",
    "slm_pred": "The East Coast Bomb Cyclone, the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, and the California Camp Fires.",
    "llm_pred": "East Coast Bomb Cyclone; Mendocino (California) wildfires; Hurricane Florence; Hurricane Michael; California Camp Fires.",
    "perf_chosen": 0.9714285714,
    "perf_slm": 0.9714285714,
    "perf_llm": 0.9032258065000001,
    "cost": 5.0
  },
  {
    "id": "0106bd9d54e2f343cc5f30bb09a5dbdd171e964b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "twitter ",
    "final_answer": "Twitter",
    "slm_pred": "Twitter.",
    "llm_pred": "Twitter",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e015d033d4ee1c83fe6f192d3310fb820354a553",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BIBREF8 a refined collection of tweets gathered from twitter",
    "final_answer": "They used a refined collection of tweets gathered from Twitter, specifically a dataset labeled for named entity recognition task containing 8,257 tweets with 12,784 entities in total.",
    "slm_pred": "They used a refined collection of tweets gathered from Twitter, specifically a dataset labeled for named entity recognition task containing 8,257 tweets with 12,784 entities in total.",
    "llm_pred": "the BIBREF8 Twitter NER dataset (8,257 labeled tweets)",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "8a871b136ccef78391922377f89491c923a77730",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "af60462881b2d723adeb4acb5fbc07ea27b6bde2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.",
    "final_answer": "Strong correlations were discovered between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s).",
    "slm_pred": "Strong correlations were discovered between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s).",
    "llm_pred": "Harassment was more frequent at night; conductors/drivers and then friends/relatives were leading identified harassers; young perpetrators tended to act in groups, mostly on streets, while adults more often acted alone, especially on public transportation; and there were strong correlations between perpetrator age and location, group size and location, and age and group size.",
    "perf_chosen": 0.2762430939,
    "perf_slm": 0.2762430939,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "3c378074111a6cc7319c0db0aced5752c30bfffb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b464bc48f176a5945e54051e3ffaea9a6ad886d7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant",
    "final_answer": "They are experimenting with booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.",
    "slm_pred": "They are experimenting with booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.",
    "llm_pred": "Booking flights (United), renting homes (Airbnb), buying bus tickets (Greyhound), and making restaurant reservations (OpenTable).",
    "perf_chosen": 0.6071428571,
    "perf_slm": 0.6071428571,
    "perf_llm": 0.3137254902,
    "cost": 5.0
  },
  {
    "id": "3b40799f25dbd98bba5b526e0a1d0d0bb51173e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3c16d4cf5dc23223980d9c0f924cb9e4e6943f13",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "AMS method.",
    "final_answer": "They select answer candidates by searching for triples in ConceptNet that share the same concept as the masked concept and relation.",
    "slm_pred": "They select answer candidates by searching for triples in ConceptNet that share the same concept as the masked concept and relation.",
    "llm_pred": "By using the masked concept as the correct answer and choosing four distractors from ConceptNet triples that share the same relation and the other concept—i.e., from (∗, r, c2) or (c1, r, ∗).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4c822bbb06141433d04bbc472f08c48bc8378865",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",
    "final_answer": "They use natural language processing and computational linguistics tools, including text mining procedures, Granger causality, and Latent Dirichlet Allocation (LDA) topic modeling.",
    "slm_pred": "They use natural language processing and computational linguistics tools, including text mining procedures, Granger causality, and Latent Dirichlet Allocation (LDA) topic modeling.",
    "llm_pred": "By keyword filtering—selecting tweets that explicitly use causal verbs like “caused,” “causes,” or “causing,” then analyzing surrounding n-grams.",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.1379310345,
    "cost": 5.0
  },
  {
    "id": "1baf87437b70cc0375b8b7dc2cfc2830279bc8b5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Randomly selected from a Twitter dump, temporally matched to causal documents",
    "final_answer": "A random selection of tweets from Twitter, matched temporally to the causal corpus.",
    "slm_pred": "A random selection of tweets from Twitter, matched temporally to the causal corpus.",
    "llm_pred": "Random, time-matched English tweets from the 2013 Twitter Gardenhose that exclude “caused/causing/causes” and bidirectional words.",
    "perf_chosen": 0.5833333333,
    "perf_slm": 0.5833333333,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "0b31eb5bb111770a3aaf8a3931d8613e578e07a8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Presence of only the exact unigrams 'caused', 'causing', or 'causes'",
    "final_answer": "The selection criteria for \"causal statements\" are that they are with high certainty causal statements, without attempting to study the factual correctness of these statements or offer any degree of verification.",
    "slm_pred": "The selection criteria for \"causal statements\" are that they are with high certainty causal statements, without attempting to study the factual correctness of these statements or offer any degree of verification.",
    "llm_pred": "Tweets containing explicit causal verbs—e.g., “caused,” “causes,” or “causing”—to ensure high‑certainty causal statements.",
    "perf_chosen": 0.1463414634,
    "perf_slm": 0.1463414634,
    "perf_llm": 0.0869565217,
    "cost": 5.0
  },
  {
    "id": "7348e781b2c3755b33df33f4f0cab4b94fcbeb9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Only automatic methods",
    "final_answer": "crowdsourcing",
    "slm_pred": "They use a combination of crowdsourcing, automatic methods, and external resources, including the Stanford CoreNLP toolkit and the labMT sentiment scores, which were crowdsourced.",
    "llm_pred": "crowdsourcing",
    "perf_chosen": 0.0,
    "perf_slm": 0.14814814810000002,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f68bd65b5251f86e1ed89f0c858a8bb2a02b233a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Randomly from a Twitter dump",
    "final_answer": "They collect a comparable corpus by gathering the same number of statements selected at random, but controlling for time of year.",
    "slm_pred": "They collect a comparable corpus by gathering the same number of statements selected at random, but controlling for time of year.",
    "llm_pred": "By sampling an equal number of random tweets from the same time periods (controlling for time of year) as the causal set.",
    "perf_chosen": 0.0769230769,
    "perf_slm": 0.0769230769,
    "perf_llm": 0.0740740741,
    "cost": 5.0
  },
  {
    "id": "e111925a82bad50f8e83da274988b9bea8b90005",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Randomly from Twitter",
    "final_answer": "They collected the control corpus by selecting random documents that did not contain causal words or bidirectional words, and matched them temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013.",
    "slm_pred": "They collected the control corpus by selecting random documents that did not contain causal words or bidirectional words, and matched them temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013.",
    "llm_pred": "Randomly selected English tweets that exclude “caused/causes/causing” and bidirectional words, time-matched to the causal set to have the same count in each 15‑minute interval.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0740740741,
    "cost": 5.0
  },
  {
    "id": "ba48c095c496d01c7717eaa271470c3406bf2d7c",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Chinese",
    "final_answer": "Chinese",
    "slm_pred": "unanswerable \n\n(Note: The article does not mention the languages used in the experiments.)",
    "llm_pred": "Chinese",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "42a61773aa494f7b12838f71a949034c12084de1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6",
    "final_answer": "MemN2N and Attentive and Impatient Readers.",
    "slm_pred": "MemN2N and Attentive and Impatient Readers.",
    "llm_pred": "MemN2N, Attentive Reader, and Impatient Reader.",
    "perf_chosen": 0.7692307692,
    "perf_slm": 0.7692307692,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "48c3e61b2ed7b3f97706e2a522172bf9b51ec467",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "correctness of all the question answer pairs are verified by at least two annotators",
    "final_answer": "The type and correctness of all the question answer pairs were verified by at least two annotators.",
    "slm_pred": "The type and correctness of all the question answer pairs were verified by at least two annotators.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.8387096774,
    "perf_slm": 0.8387096774,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "61fba3ab10f7b6906e27b028fb1d42ec601c3fb8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "80de3baf97a55ea33e0fe0cafa6f6221ba347d0a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "unanswerable \n\nHowever, based on the context it seems they are not hand-crafted, as it is mentioned that \"we determine $\\pi _j$ for each mention $m_j$ in the following way\".",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",
    "final_answer": "Resolution mode variables, denoted as $\\Pi = \\lbrace \\pi _1, \\ldots, \\pi _n\\rbrace $, are introduced to distinguish mentions resolved by different categories of information, specifically indicating in which mode the mention should be resolved, such as string-matching (str), precise-construct (prec), or attribute-matching (attr).",
    "slm_pred": "Resolution mode variables, denoted as $\\Pi = \\lbrace \\pi _1, \\ldots, \\pi _n\\rbrace $, are introduced to distinguish mentions resolved by different categories of information, specifically indicating in which mode the mention should be resolved, such as string-matching (str), precise-construct (prec), or attribute-matching (attr).",
    "llm_pred": "Variables πj that indicate, for each mention, which resolution mode to use—string-matching (str), precise-construct (prec), or attribute-matching (attr).",
    "perf_chosen": 0.44827586210000003,
    "perf_slm": 0.44827586210000003,
    "perf_llm": 0.4117647059,
    "cost": 5.0
  },
  {
    "id": "e76139c63da0f861c097466983fbe0c94d1d9810",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No, supervised models perform better for this task.",
    "final_answer": "unanswerable \n\nHowever, the paper does mention that the model outperforms the baseline systems and comes close to the state-of-the-art supervised systems, but it does not explicitly claim to be the state-of-the-art.",
    "slm_pred": "unanswerable \n\nHowever, the paper does mention that the model outperforms the baseline systems and comes close to the state-of-the-art supervised systems, but it does not explicitly claim to be the state-of-the-art.",
    "llm_pred": "no",
    "perf_chosen": 0.0512820513,
    "perf_slm": 0.0512820513,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "b8b588ca1e876b3094ae561a875dd949c8965b2e",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the article, one of the problems found with the evaluation scheme for open domain chit-chat systems is the difficulty in constructing a gold standard (a reference set) to evaluate a response generated by such a system due to the infinite number of possible responses.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.301369863,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "2ec640e6b4f1ebc158d13ee6589778b4c08a04c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, it can be inferred that the data is annotated by the voice resource department of the iFLYTEK Corporation for task 2, and possibly by Lingzhi Li, Yangzi Zhang, Jiaqi Zhu, and Xiaoming Shi for task 1, as mentioned in the acknowledgements section.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.044444444400000005,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ab0bb4d0a9796416d3d7ceba0ba9ab50c964e9d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The collection steps for task 1 include releasing data for training and developing, allowing participants to collect external data for training and developing, and considering two sub tasks, one for a closed evaluation using only the released data and the other for an open evaluation allowing external data collection.",
    "slm_pred": "The collection steps for task 1 include releasing data for training and developing, allowing participants to collect external data for training and developing, and considering two sub tasks, one for a closed evaluation using only the released data and the other for an open evaluation allowing external data collection.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0460019eb2186aef835f7852fc445b037bd43bb7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "two",
    "final_answer": "31",
    "slm_pred": "unanswerable",
    "llm_pred": "31",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "96c09ece36a992762860cde4c110f1653c110d96",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",
    "final_answer": "unanswerable \n\nThe article does not explicitly state the result of the highest performing system.",
    "slm_pred": "unanswerable \n\nThe article does not explicitly state the result of the highest performing system.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.044444444400000005,
    "perf_slm": 0.044444444400000005,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a9cc4b17063711c8606b8fc1c5eaf057b317a0c9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input",
    "final_answer": "F1-score is used for task 1, and for task 2, the metrics are: task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, and guidance ability for out of scope input.",
    "slm_pred": "F1-score is used for task 1, and for task 2, the metrics are: task completion ratio, user satisfaction degree, response fluency, number of dialogue turns, and guidance ability for out of scope input.",
    "llm_pred": "Task 1: F1-score; Task 2: task completion ratio, user satisfaction, response fluency, number of dialogue turns, and guidance ability for out-of-scope input.",
    "perf_chosen": 0.7931034483,
    "perf_slm": 0.7931034483,
    "perf_llm": 0.7659574468,
    "cost": 5.0
  },
  {
    "id": "6ead576ee5813164684a8cdda36e6a8c180455d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Rouge-L, Bleu-1",
    "final_answer": "They measure the quality of summaries using Rouge-L, which is a metric for evaluating the similarity between generated and reference summaries.",
    "slm_pred": "They measure the quality of summaries using Rouge-L, which is a metric for evaluating the similarity between generated and reference summaries.",
    "llm_pred": "ROUGE‑L.",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0869565217,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0117aa1266a37b0d2ef429f1b0653b9dde3677fe",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "The answer is yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "well-formed sentences vs concise answers",
    "final_answer": "They refer to the different formats in which answers can be generated, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context.",
    "slm_pred": "They refer to the different formats in which answers can be generated, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context.",
    "llm_pred": "Different formats of the answer—e.g., concise phrases (Q&A) versus well‑formed sentences (NLG) that are understandable without the question’s context.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "6c80bc3ed6df228c8ca6e02c0a8a1c2889498688",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "no \n\nThe article mentions that the model \"Masque\" can generate answers in multiple styles, and that it is capable of answering one question in multiple styles, such as concise phrases and well-formed sentences.",
    "slm_pred": "no \n\nThe article mentions that the model \"Masque\" can generate answers in multiple styles, and that it is capable of answering one question in multiple styles, such as concise phrases and well-formed sentences.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "2d274c93901c193cf7ad227ab28b1436c5f410af",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",
    "final_answer": "MCAN, extractive approaches, and a model trained with the WFA set consisting of the single style.",
    "slm_pred": "MCAN, extractive approaches, and a model trained with the WFA set consisting of the single style.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e63bde5c7b154fbe990c3185e2626d13a1bad171",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "cb8a6f5c29715619a137e21b54b29e9dd48dad7d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "well-formed sentences vs concise answers",
    "final_answer": "The answer styles refer to different ways of answering a question, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question.",
    "slm_pred": "The answer styles refer to different ways of answering a question, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question.",
    "llm_pred": "the desired format of the generated answer, e.g., a concise phrase vs. a well‑formed standalone sentence",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.1904761905,
    "cost": 5.0
  },
  {
    "id": "8a7bd9579d2783bfa81e055a7a6ebc3935da9d20",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WAS, LipCH-Net-seq, CSSMCM-w/o video",
    "final_answer": "LipCH-Net was the previous state of the art model for this task, but it only performed word classification for Chinese Mandarin lip reading, not at the complete sentence level.",
    "slm_pred": "LipCH-Net was the previous state of the art model for this task, but it only performed word classification for Chinese Mandarin lip reading, not at the complete sentence level.",
    "llm_pred": "LipCH-Net",
    "perf_chosen": 0.060606060600000004,
    "perf_slm": 0.060606060600000004,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "27b01883ed947b457d3bab0c66de26c0736e4f90",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "syllables",
    "final_answer": "sequence-to-sequence learning problem.",
    "slm_pred": "sequence-to-sequence learning problem.",
    "llm_pred": "a sequence-to-sequence architecture (with dual attention)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9714cb7203c18a0c53805f6c889f2e20b4cab5dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "video sequence is first fed into the VGG model BIBREF9 to extract visual feature",
    "final_answer": "The visible movements of the neck, head, and mouth.",
    "slm_pred": "The visible movements of the neck, head, and mouth.",
    "llm_pred": "visible movements of the neck, head, and mouth",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0869565217,
    "perf_llm": 0.0909090909,
    "cost": 5.0
  },
  {
    "id": "a22b900fcd76c3d36b5679691982dc6e9a3d34bf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nSince the provided text does not mention any information about the language of the data being used, we cannot determine if the results are reported only on English data.",
    "slm_pred": "unanswerable \n\nSince the provided text does not mention any information about the language of the data being used, we cannot determine if the results are reported only on English data.",
    "llm_pred": "yes",
    "perf_chosen": 0.064516129,
    "perf_slm": 0.064516129,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fb2593de1f5cc632724e39d92e4dd82477f06ea1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "performances of a purely content-based model naturally stays stable",
    "final_answer": "They demonstrate the robustness of their results by simulating the effects of an overly context-sensitive classifier in a cross-document setting and comparing the performances of different model types.",
    "slm_pred": "They demonstrate the robustness of their results by simulating the effects of an overly context-sensitive classifier in a cross-document setting and comparing the performances of different model types.",
    "llm_pred": "By showing their models significantly outperform the majority-class baseline (macro F1) across all prediction settings and type variables.",
    "perf_chosen": 0.2162162162,
    "perf_slm": 0.2162162162,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "476d0b5579deb9199423bb843e584e606d606bc7",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "BIBREF13, majority baseline",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "eddabb24bc6de6451bcdaa7940f708e925010912",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.",
    "final_answer": "With tree-based sentiment scores encoded as a 5-class one-hot vector, assigned to the highest parse node covering the span.",
    "slm_pred": "unanswerable",
    "llm_pred": "With tree-based sentiment scores encoded as a 5-class one-hot vector, assigned to the highest parse node covering the span.",
    "perf_chosen": 0.2127659574,
    "perf_slm": 0.0,
    "perf_llm": 0.2127659574,
    "cost": 25.0
  },
  {
    "id": "f0946fb9df9839977f4d16c43476e4c2724ff772",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "They are defined as units that bear great similarity with elementary discourse units (EDUs) in RST.",
    "slm_pred": "They are defined as units that bear great similarity with elementary discourse units (EDUs) in RST.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e51d0c2c336f255e342b5f6c3cf2a13231789fed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They collected tweets in Russian language using a heuristic query specific to Russian",
    "final_answer": "Twitter corpus acquired from 2015/07/21 till 2015/08/04.",
    "slm_pred": "Twitter corpus acquired from 2015/07/21 till 2015/08/04.",
    "llm_pred": "the Russian Twitter stream (15 days, 2015/07/21–2015/08/04)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.1,
    "cost": 5.0
  },
  {
    "id": "5b6aec1b88c9832075cd343f59158078a91f3597",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",
    "final_answer": "The proposed word embeddings outperform Sindhi fastText word representations in intrinsic evaluation matrices and have a larger vocabulary.",
    "slm_pred": "The proposed word embeddings outperform Sindhi fastText word representations in intrinsic evaluation matrices and have a larger vocabulary.",
    "llm_pred": "They outperform SdfastText, achieving higher intrinsic similarity scores with a larger, cleaner vocabulary (SG best), while SdfastText has limited vocabulary and lower performance.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.12244897960000001,
    "cost": 5.0
  },
  {
    "id": "a6717e334c53ebbb87e5ef878a77ef46866e3aed",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "yes",
    "slm_pred": "yes, they are used for various NLP tasks such as parts-of-speech tagging, named entity recognition, and text classification.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a1064307a19cd7add32163a70b6623278a557946",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "908456 unique words are available in collected corpus.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that the corpus contains a \"large corpus of more than 61 million words\" and \"unique tokens\" but does not specify the exact number of unique words.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8cb9006bcbd2f390aadc6b70d54ee98c674e45cc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter",
    "final_answer": "Using a web-scrapy-based crawler, from daily Kawish and Awami Awaz (Sindhi) newspapers, Wikipedia dumps, Wichaar social blog, Focus WordPress blog, Sindh Salamat literary sites, Sindhi Adabi Board (novels/history/religion), and Twitter (news/sports).",
    "slm_pred": "The data was collected from multiple web resources, including news columns of daily Sindhi newspapers, Wikipedia dumps, short stories, sports news from social blogs, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board, and tweets regarding news and sports from Twitter.",
    "llm_pred": "Using a web-scrapy-based crawler, from daily Kawish and Awami Awaz (Sindhi) newspapers, Wikipedia dumps, Wichaar social blog, Focus WordPress blog, Sindh Salamat literary sites, Sindhi Adabi Board (novels/history/religion), and Twitter (news/sports).",
    "perf_chosen": 0.5542168675,
    "perf_slm": 0.8073394495,
    "perf_llm": 0.5542168675,
    "cost": 25.0
  },
  {
    "id": "75043c17a2cddfce6578c3c0e18d4b7cf2f18933",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious",
    "final_answer": "The trends found in musical preferences are that audiences want more contemporary, intense, and novel music, but less mellow and unpretentious.",
    "slm_pred": "The trends found in musical preferences are that audiences want more contemporary, intense, and novel music, but less mellow and unpretentious.",
    "llm_pred": "Audiences increasingly prefer music that is contemporary, intense, and somewhat novel/sophisticated, and less mellow and unpretentious.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.4615384615,
    "cost": 5.0
  },
  {
    "id": "95bb3ea4ebc3f2174846e8d422abc076e1407d6a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "between 1900s and 2010s",
    "final_answer": "The decades looked at were the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, and 1990s.",
    "slm_pred": "The decades looked at were the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, and 1990s.",
    "llm_pred": "the 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, 1990s, and 2000s",
    "perf_chosen": 0.1111111111,
    "perf_slm": 0.1111111111,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "3ebdc15480250f130cf8f5ab82b0595e4d870e2f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "77 genres",
    "final_answer": "77 genres.",
    "slm_pred": "77 genres.",
    "llm_pred": "77",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "bbc58b193c08ccb2a1e8235a36273785a3b375fb",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3c34187a248d179856b766e9534075da1aa5d1cf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)",
    "final_answer": "The CRF model achieves an F1 score of 89.60 on the development set, 87.82 on the test set, and 71.49 on the supplemental test set.",
    "slm_pred": "The CRF model achieves an F1 score of 89.60 on the development set, 87.82 on the test set, and 71.49 on the supplemental test set.",
    "llm_pred": "F1 ≈ 89.6 on the dev set, 87.8 on the test set, and 71.5 on the supplemental test set.",
    "perf_chosen": 0.6808510638,
    "perf_slm": 0.6808510638,
    "perf_llm": 0.5853658537,
    "cost": 5.0
  },
  {
    "id": "8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data",
    "final_answer": "The paper motivates the use of CRF as the baseline model because it was the most popular model in both Shared Tasks on Language Identification for Code-Switched Data.",
    "slm_pred": "The paper motivates the use of CRF as the baseline model because it was the most popular model in both Shared Tasks on Language Identification for Code-Switched Data.",
    "llm_pred": "yes",
    "perf_chosen": 0.488372093,
    "perf_slm": 0.488372093,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "97757a69d9fc28b260e68284fd300726fbe358d0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)",
    "final_answer": "The handcrafted features used are: Bias feature, Token feature, Uppercase feature, Titlecase feature, Character trigram feature, Quotation feature, Word suffix feature, POS tag, Word shape, and Word embedding.",
    "slm_pred": "The handcrafted features used are: Bias feature, Token feature, Uppercase feature, Titlecase feature, Character trigram feature, Quotation feature, Word suffix feature, POS tag, Word shape, and Word embedding.",
    "llm_pred": "Bias, token, uppercase (y/n), titlecase (y/n), character trigrams, quotation (y/n), word suffix (last 3 chars), POS tag, word shape, and word embedding.",
    "perf_chosen": 0.6567164179,
    "perf_slm": 0.6567164179,
    "perf_llm": 0.5901639344,
    "cost": 5.0
  },
  {
    "id": "41830ebb8369a24d490e504b7cdeeeaa9b09fd9c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "using an external pre-trained style classifier to measure transfer accuracy",
    "slm_pred": "unanswerable \n\nThe article does not provide a clear definition of the current state of the art method.",
    "llm_pred": "using an external pre-trained style classifier to measure transfer accuracy",
    "perf_chosen": 0.0,
    "perf_slm": 0.1111111111,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4904ef32a8f84cf2f53b1532ccf7aa77273b3d19",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "They outperform state of the art in terms of BLEU between output and human-written reformulations.",
    "slm_pred": "They outperform state of the art in terms of BLEU between output and human-written reformulations.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches",
    "final_answer": "The three new proposed architectures are: \n\n1. A model with a special dedicated discriminator added to control that the latent representation does not contain stylistic information.\n2. A shifted autoencoder (SAE) that feeds the \"soft\" generated sentence into an encoder and checks how close is the representation to the original representation in terms of the cosine distance.\n3. A combination of both the above approaches.",
    "slm_pred": "The three new proposed architectures are: \n\n1. A model with a special dedicated discriminator added to control that the latent representation does not contain stylistic information.\n2. A shifted autoencoder (SAE) that feeds the \"soft\" generated sentence into an encoder and checks how close is the representation to the original representation in terms of the cosine distance.\n3. A combination of both the above approaches.",
    "llm_pred": "a model with a style-removal discriminator (Dz), a shifted autoencoder (SAE), and the combination of both",
    "perf_chosen": 0.5652173913,
    "perf_slm": 0.5652173913,
    "perf_llm": 0.4186046512,
    "cost": 5.0
  },
  {
    "id": "d6a27c41c81f12028529e97e255789ec2ba39eaa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points",
    "final_answer": "Accuracy can change up to 5 percentage points, while BLEU can vary up to 8 points.",
    "slm_pred": "Accuracy can change up to 5 percentage points, while BLEU can vary up to 8 points.",
    "llm_pred": "up to 5 percentage points",
    "perf_chosen": 0.9375,
    "perf_slm": 0.9375,
    "perf_llm": 0.4761904762,
    "cost": 5.0
  },
  {
    "id": "2d3bf170c1647c5a95abae50ee3ef3b404230ce4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "standard parametrized attention and a non-attention baseline",
    "final_answer": "The standard parametrized attention and the non-attention baseline are used as baseline methods.",
    "slm_pred": "The standard parametrized attention and the non-attention baseline are used as baseline methods.",
    "llm_pred": "the standard (parametrized) attention-based model and a non-attention baseline",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "6e8c587b6562fafb43a7823637b84cd01487059a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ranges from 44.22 to 100.00 depending on K and the sequence length.",
    "final_answer": "unanswerable, as the article does not provide a single BLEU score but rather presents various results in tables and figures.",
    "slm_pred": "unanswerable, as the article does not provide a single BLEU score but rather presents various results in tables and figures.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ab9453fa2b927c97b60b06aeda944ac5c1bfef1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Sequence Copy Task and WMT'17",
    "final_answer": "The article does not explicitly list all the datasets used in experiments, but it mentions that four large machine translation datasets of WMT'17 are used, specifically English-Czech, English-German, English-Finish, and English-Turkish. Additionally, the \"Toy Copy task\" and a \"toy data\" are also mentioned, but the specific dataset used for this task is not specified.",
    "slm_pred": "The article does not explicitly list all the datasets used in experiments, but it mentions that four large machine translation datasets of WMT'17 are used, specifically English-Czech, English-German, English-Finish, and English-Turkish. Additionally, the \"Toy Copy task\" and a \"toy data\" are also mentioned, but the specific dataset used for this task is not specified.",
    "llm_pred": "- Synthetic Toy Copy datasets (sequence lengths L ∈ {10, 50, 100, 200})\n- WMT’17 MT datasets: English–Czech, English–German, English–Finnish, English–Turkish (train), with newstest2015 for validation and newstest2016 for testing",
    "perf_chosen": 0.1355932203,
    "perf_slm": 0.1355932203,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "3a8d65eb8e1dbb995981a0e02d86ebf3feab107a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "an adversarial loss ($\\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\\ell _{cycle}$) on each side",
    "final_answer": "A cycle consistency loss.",
    "slm_pred": "A cycle consistency loss.",
    "llm_pred": "Cycle consistency losses (using average cosine similarity) in both directions.",
    "perf_chosen": 0.32,
    "perf_slm": 0.32,
    "perf_llm": 0.1935483871,
    "cost": 5.0
  },
  {
    "id": "d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "New best results of accuracy (P@1) on Vecmap:\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",
    "final_answer": "The proposed model achieves the best results on a standard benchmark.",
    "slm_pred": "The proposed model achieves the best results on a standard benchmark.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1764705882,
    "perf_slm": 0.1764705882,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "54c7fc08598b8b91a8c0399f6ab018c45e259f79",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Proposed method vs best baseline result on Vecmap (Accuracy P@1):\nEN-IT: 50 vs 50\nIT-EN: 42.67 vs 42.67\nEN-DE: 51.6 vs 51.47\nDE-EN: 47.22 vs 46.96\nEN-FI: 35.88 vs 36.24\nFI-EN: 39.62 vs 39.57\nEN-ES: 39.47 vs 39.30\nES-EN: 36.43 vs 36.06",
    "final_answer": "Our model significantly outperforms competitive baselines, obtaining the best published results.",
    "slm_pred": "Our model significantly outperforms competitive baselines, obtaining the best published results.",
    "llm_pred": "significantly better, achieving the best published results compared to competitive baselines",
    "perf_chosen": 0.0377358491,
    "perf_slm": 0.0377358491,
    "perf_llm": 0.0377358491,
    "cost": 5.0
  },
  {
    "id": "5112bbf13c7cf644bf401daecb5e3265889a4bfc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on the size of the data used in the experiments.",
    "slm_pred": "unanswerable \n\nThe article does not provide information on the size of the data used in the experiments.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1111111111,
    "perf_slm": 0.1111111111,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "03ce42ff53aa3f1775bc57e50012f6eb1998c480",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI",
    "final_answer": "English–Italian, English–German, English–Finnish, English–Spanish, English–Esperanto, and English–Malay.",
    "slm_pred": "unanswerable \n\nHowever, based on the information provided, the following language pairs are experimented on:\n\n- English $\\leftrightarrow $ Italian\n- English $\\leftrightarrow $ Malay (MS)\n- English $\\leftrightarrow $ English-Esperanto (EO)\n- English $\\leftrightarrow $ German (DE)\n- English $\\leftrightarrow $ Finnish (FI)\n- English $\\leftrightarrow $ Spanish (ES)",
    "llm_pred": "English–Italian, English–German, English–Finnish, English–Spanish, English–Esperanto, and English–Malay.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ebeedbb8eecdf118d543fdb5224ae610eef212c8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT",
    "final_answer": "unanswerable \n\nThis is because the article does not provide information about current state-of-the-art methods that consider the two tasks independently. It does mention that current state-of-the-art methods are often adversarial and do not consider symmetry, and that the proposed method is a simple yet effective approach that outperforms these methods.",
    "slm_pred": "unanswerable \n\nThis is because the article does not provide information about current state-of-the-art methods that consider the two tasks independently. It does mention that current state-of-the-art methods are often adversarial and do not consider symmetry, and that the proposed method is a simple yet effective approach that outperforms these methods.",
    "llm_pred": "Conneau et al. (2018)’s adversarial MUSE model (Adv-C)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9efd025cfa69c6ff2777528bd158f79ead9353d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "559c1307610a15427caeb8aff4d2c01ae5c9de20",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .",
    "final_answer": "The FEVER baseline system and the Enhanced Sequential Inference Model (ESIM) are compared to the Decomposable Attention baseline.",
    "slm_pred": "The FEVER baseline system and the Enhanced Sequential Inference Model (ESIM) are compared to the Decomposable Attention baseline.",
    "llm_pred": "the FEVER baseline using Decomposable Attention (AllenNLP) on five concatenated sentences (FEVER Five/Oracle)",
    "perf_chosen": 0.3181818182,
    "perf_slm": 0.3181818182,
    "perf_llm": 0.20512820510000002,
    "cost": 5.0
  },
  {
    "id": "4ecb6674bcb4162bf71aea8d8b82759255875df3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BIBREF5",
    "final_answer": "A specific transformer network released by OpenAI, pre-trained for language modeling on the BookCorpus dataset.",
    "slm_pred": "A specific transformer network released by OpenAI, pre-trained for language modeling on the BookCorpus dataset.",
    "llm_pred": "OpenAI’s pre-trained GPT (12-layer) language model trained on BookCorpus.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "eacc1eb65daad055df934e0e878f417b73b2ecc1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem",
    "final_answer": "The FEVER task is a large-scale challenge that tests a combination of retrieval and textual entailment capabilities.",
    "slm_pred": "The FEVER task is a large-scale challenge that tests a combination of retrieval and textual entailment capabilities.",
    "llm_pred": "Verifying textual claims against Wikipedia by retrieving evidence sentences and classifying each claim as supported, refuted, or not enough info.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "d353a6bbdc66be9298494d0c853e0d8d752dec4b",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "22c36082b00f677e054f0f0395ed685808965a02",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)",
    "final_answer": "The English$\\rightarrow $Italian/German portions of the MuST-C corpus, along with additional public and proprietary data.",
    "slm_pred": "The English$\\rightarrow $Italian/German portions of the MuST-C corpus, along with additional public and proprietary data.",
    "llm_pred": "the English→Italian/German portions of the MuST-C corpus (TED Talks)",
    "perf_chosen": 0.4905660377,
    "perf_slm": 0.4905660377,
    "perf_llm": 0.2553191489,
    "cost": 5.0
  },
  {
    "id": "90bc60320584ebba11af980ed92a309f0c1b5507",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).",
    "final_answer": "They enrich the positional embedding with length information by computing the distance from every position to the end of the sentence.",
    "slm_pred": "They enrich the positional embedding with length information by computing the distance from every position to the end of the sentence.",
    "llm_pred": "By adding to each decoder timestep an extra positional vector that encodes distance to the target’s end—using len−pos (absolute) or a quantized relative proportion—and summing it with the target embedding.",
    "perf_chosen": 0.2105263158,
    "perf_slm": 0.2105263158,
    "perf_llm": 0.08510638300000001,
    "cost": 5.0
  },
  {
    "id": "f52b2ca49d98a37a6949288ec5f281a3217e5ae8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",
    "final_answer": "They condition the output to a given target-source class by prepending a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$).",
    "slm_pred": "They condition the output to a given target-source class by prepending a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$).",
    "llm_pred": "By prepending a special length token to the source (e.g., <short>, <normal>, <long>) indicating the desired target/source length-ratio class.",
    "perf_chosen": 0.28,
    "perf_slm": 0.28,
    "perf_llm": 0.18604651160000002,
    "cost": 5.0
  },
  {
    "id": "228425783a4830e576fb98696f76f4c7c0a1b906",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "two translation directions (En-It and En-De)",
    "final_answer": "English, Italian, and German.",
    "slm_pred": "English, Italian, and German.",
    "llm_pred": "English–Italian and English–German (English, Italian, and German)",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "9d1135303212356f3420ed010dcbe58203cc7db4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)",
    "final_answer": "The MuST-C corpus, along with additional data for English-Italian and English-German.",
    "slm_pred": "The MuST-C corpus, along with additional data for English-Italian and English-German.",
    "llm_pred": "the English→Italian/German portions of the MuST-C corpus (TED Talks)",
    "perf_chosen": 0.3673469388,
    "perf_slm": 0.3673469388,
    "perf_llm": 0.2553191489,
    "cost": 5.0
  },
  {
    "id": "d8bf4a29c7af213a9a176eb1503ec97d01cc8f51",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "73abb173a3cc973ab229511cf53b426865a2738b",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model",
    "final_answer": "The proposed approach is compared against several state-of-the-art models, including the Collective Matrix Factorization model, the Maximum Entropy (MaxEnt) model, and a deep neural network (DNN) architecture, as well as four baseline trackers provided by the DSTC organisers.",
    "slm_pred": "The proposed approach is compared against several state-of-the-art models, including the Collective Matrix Factorization model, the Maximum Entropy (MaxEnt) model, and a deep neural network (DNN) architecture, as well as four baseline trackers provided by the DSTC organisers.",
    "llm_pred": "Baseline, Focus, HWU, HWU+, MaxEnt, and a DNN model.",
    "perf_chosen": 0.3859649123,
    "perf_slm": 0.3859649123,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "093039f974805952636c19c12af3549aa422ec43",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "It uses deep learning framework (pytorch)",
    "final_answer": "The library is framework agnostic, but specifically integrates with PyTorch and TensorFlow.",
    "slm_pred": "The library is framework agnostic, but specifically integrates with PyTorch and TensorFlow.",
    "llm_pred": "a PyTorch library (not framework-agnostic)",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "8df89988adff57279db10992846728ec4f500eaa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient",
    "final_answer": "The self-critical baseline.",
    "slm_pred": "The self-critical baseline.",
    "llm_pred": "self-critical baseline",
    "perf_chosen": 0.0384615385,
    "perf_slm": 0.0384615385,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "94edac71eea1e78add678fb5ed2d08526b51016b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations",
    "final_answer": "Parallel Scan Inference, Vectorized Parsing, and Semiring Matrix Operations.",
    "slm_pred": "Parallel Scan Inference, Vectorized Parsing, and Semiring Matrix Operations.",
    "llm_pred": "parallel scan inference, vectorized parsing, and efficient semiring matrix operations (via custom CUDA kernels)",
    "perf_chosen": 0.9411764706,
    "perf_slm": 0.9411764706,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "9c4ed8ca59ba6d240f031393b01f634a9dc3615d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "VecMap, Muse, Barista",
    "final_answer": "The Sent baseline, which assumes all targets in each sentence to be of the same polarity.",
    "slm_pred": "The Sent baseline, which assumes all targets in each sentence to be of the same polarity.",
    "llm_pred": "the Sent baseline—assuming all targets in a sentence have the same polarity",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ca7e71131219252d1fab69865804b8f89a2c0a8f",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d77c9ede2727c28e0b5a240b2521fd49a19442e0",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "word embeddings",
    "final_answer": "The input representation of OpenIE tuples into the model is the concatenation of word embedding and another embedding indicating whether this word is a predicate: $\\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]$.",
    "slm_pred": "The input representation of OpenIE tuples into the model is the concatenation of word embedding and another embedding indicating whether this word is a predicate: $\\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]$.",
    "llm_pred": "A BIO tagging scheme: O for non-span words, Bp/Ip for the predicate span, and Ba_i/Ia_i for each argument span.",
    "perf_chosen": 0.058823529400000005,
    "perf_slm": 0.058823529400000005,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a9610cbcca813f4376fbfbf21cc14689c7fbd677",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
    "final_answer": "The VIST dataset consists of 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
    "slm_pred": "The VIST dataset consists of 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
    "llm_pred": "40,071 training, 4,988 validation, and 5,050 testing stories.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "64ab2b92e986e0b5058bf4f1758e849f6a41168b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "The FHVAE-based disentangled speech representation learning is more prominent than speaker adversarial training in improving the robustness of unsupervised feature learning towards speaker variation.",
    "slm_pred": "The FHVAE-based disentangled speech representation learning is more prominent than speaker adversarial training in improving the robustness of unsupervised feature learning towards speaker variation.",
    "llm_pred": "FHVAE-based disentangled representation learning outperforms adversarial training—both help, but FHVAE yields larger (more prominent) ABX error reductions, with only marginal gains from combining them.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "bcd6befa65cab3ffa6334c8ecedd065a4161028b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect",
    "final_answer": "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect.",
    "slm_pred": "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect.",
    "llm_pred": "Wordplay jokes where a word or phrase suggests multiple meanings by exploiting polysemy, homonymy, or phonological similarity for humorous or rhetorical effect.",
    "perf_chosen": 0.9736842105,
    "perf_slm": 0.9736842105,
    "perf_llm": 0.6779661017,
    "cost": 5.0
  },
  {
    "id": "479fc9e6d6d80e69f425d9e82e618e6b7cd12764",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "intra-sequential and intra-word",
    "final_answer": "Intra-sentential and intra-word code-mixed puns.",
    "slm_pred": "Intra-sentential and intra-word code-mixed puns.",
    "llm_pred": "intra-sentential and intra-word",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "bc26eee4ef1c8eff2ab8114a319901695d044edb",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction",
    "final_answer": "The dialogue is guided through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world, and the history.",
    "slm_pred": "The dialogue is guided through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world, and the history.",
    "llm_pred": "By using an FSM-driven Wizard interface that restricts the Wizard to predefined, context-appropriate actions/messages.",
    "perf_chosen": 0.2816901408,
    "perf_slm": 0.2816901408,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "9c94ff8c99d3e51c256f2db78c34b2361f26b9c2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard.",
    "final_answer": "The semiguided dialogue refers to a type of dialogue where the Wizard is provided with a list of valid and relevant dialogue task actions, but still has some freedom to make choices and interact with the Operator. \n\nIn this type of dialogue, the Wizard interface is guided, as it provides possible actions on the right-hand side of the browser window, but the Wizard still has the option to type a message freely or select from predefined messages.",
    "slm_pred": "The semiguided dialogue refers to a type of dialogue where the Wizard is provided with a list of valid and relevant dialogue task actions, but still has some freedom to make choices and interact with the Operator. \n\nIn this type of dialogue, the Wizard interface is guided, as it provides possible actions on the right-hand side of the browser window, but the Wizard still has the option to type a message freely or select from predefined messages.",
    "llm_pred": "A dialogue where only the Wizard’s side is constrained—guided by an FSM with predefined message/action options—while free text and the Operator’s turns remain unguided.",
    "perf_chosen": 0.3414634146,
    "perf_slm": 0.3414634146,
    "perf_llm": 0.2571428571,
    "cost": 5.0
  },
  {
    "id": "8e9de181fa7d96df9686d0eb2a5c43841e6400fa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",
    "final_answer": "The article mentions that the CRWIZ framework has been used for a data collection and the results are compared to a similar dataset collected in a more controlled lab setting with a single Wizard.",
    "slm_pred": "The article mentions that the CRWIZ framework has been used for a data collection and the results are compared to a similar dataset collected in a more controlled lab setting with a single Wizard.",
    "llm_pred": "Yes—CRWIZ has been used on AMT to collect emergency-response dialogues and generate a dataset compared against a prior lab-set dataset; it demonstrates feasible, scalable collection, but no specific quantitative results are reported.",
    "perf_chosen": 0.2990654206,
    "perf_slm": 0.2990654206,
    "perf_llm": 0.15238095240000002,
    "cost": 5.0
  },
  {
    "id": "ff1595a388769c6429423a75b6e1734ef88d3e46",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.",
    "final_answer": "The CRWIZ Intelligent Wizard Interface ensures compliance with procedures by providing a restricted list of valid and relevant dialogue task actions that changes dynamically based on the context.",
    "slm_pred": "The CRWIZ Intelligent Wizard Interface ensures compliance with procedures by providing a restricted list of valid and relevant dialogue task actions that changes dynamically based on the context.",
    "llm_pred": "By constraining the Wizard to context‑dependent actions in an FSM—offering only a restricted set of predefined, valid dialogue options.",
    "perf_chosen": 0.1797752809,
    "perf_slm": 0.1797752809,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "dd2046f5481f11b7639a230e8ca92904da75feed",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "maximum of two scores assigned by the two separate models, average score",
    "final_answer": "They combine the models using two strategies: Max Score Ensemble and Average Score Ensemble.",
    "slm_pred": "They combine the models using two strategies: Max Score Ensemble and Average Score Ensemble.",
    "llm_pred": "By ensembling their prediction scores—taking either the maximum score (Max Score Ensemble) or the average score (Average Score Ensemble).",
    "perf_chosen": 0.3846153846,
    "perf_slm": 0.3846153846,
    "perf_llm": 0.3225806452,
    "cost": 5.0
  },
  {
    "id": "47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Logistic regression model with character-level n-gram features",
    "final_answer": "The logistic regression model with character-level n-gram features.",
    "slm_pred": "The logistic regression model with character-level n-gram features.",
    "llm_pred": "logistic regression with character-level n-gram features",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.9230769231,
    "cost": 5.0
  },
  {
    "id": "569ad21441e99ae782d325d5f5e1ac19e08d5e76",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "title of the news article, screen name of the user",
    "final_answer": "The context used includes user screen names, comments in the same thread, and the news article the comment is written for.",
    "slm_pred": "The context used includes user screen names, comments in the same thread, and the news article the comment is written for.",
    "llm_pred": "the news article title and the user’s screen name",
    "perf_chosen": 0.3870967742,
    "perf_slm": 0.3870967742,
    "perf_llm": 0.7368421053,
    "cost": 5.0
  },
  {
    "id": "90741b227b25c42e0b81a08c279b94598a25119d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation",
    "final_answer": "The language that explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.",
    "slm_pred": "The language that explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation.",
    "llm_pred": "language that explicitly or implicitly threatens or demeans a person or group based on an identity facet (e.g., gender, ethnicity, sexual orientation)",
    "perf_chosen": 0.9454545455000001,
    "perf_slm": 0.9454545455000001,
    "perf_llm": 0.7346938776,
    "cost": 5.0
  },
  {
    "id": "1d739bb8e5d887fdfd1f4b6e39c57695c042fa25",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "three parallel LSTM BIBREF21 layers",
    "final_answer": "The neural network model mainly consists of three parallel LSTM layers with three different inputs.",
    "slm_pred": "The neural network model mainly consists of three parallel LSTM layers with three different inputs.",
    "llm_pred": "Three parallel bi-directional LSTMs (attention on the comment branch), whose outputs are concatenated and fed to a sigmoid layer.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "5c70fdd3d6b67031768d3e28336942e49bf9a500",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages",
    "final_answer": "The model consumes human interaction through the Storyline Planner and Story Writer modules, which process user input, edit and re-generate text, and guide generation by providing a topic and tweaking decoding parameters to control novelty.",
    "slm_pred": "The model consumes human interaction through the Storyline Planner and Story Writer modules, which process user input, edit and re-generate text, and guide generation by providing a topic and tweaking decoding parameters to control novelty.",
    "llm_pred": "User inputs are lowercased and tokenized (via spaCy), with OOVs mapped via WordNet hypernym/hyponym search, then passed as conditioning to the Storyline Planner and Story Writer.",
    "perf_chosen": 0.2191780822,
    "perf_slm": 0.2191780822,
    "perf_llm": 0.1875,
    "cost": 5.0
  },
  {
    "id": "f27502c3ece9ade265389d5ace90ca9ca42b46f3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "separate set of Turkers to rate the stories for overall quality and the three improvement areas",
    "final_answer": "They evaluate generated stories through self-reported engagement, satisfaction, and perception of story quality from Mechanical Turk workers, as well as independent ratings from a separate set of Turkers for overall quality and specific improvement areas.",
    "slm_pred": "They evaluate generated stories through self-reported engagement, satisfaction, and perception of story quality from Mechanical Turk workers, as well as independent ratings from a separate set of Turkers for overall quality and specific improvement areas.",
    "llm_pred": "By crowdworker (MTurk) ratings on a five-point scale for overall quality and for creativity, relevance, and causal/temporal coherence (two ratings per story, discarding pairs differing by >2).",
    "perf_chosen": 0.43137254900000005,
    "perf_slm": 0.43137254900000005,
    "perf_llm": 0.18604651160000002,
    "cost": 5.0
  },
  {
    "id": "ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "aa4b38f601cc87bf93849245d5f65124da3dc112",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Title-to-Story system",
    "final_answer": "The turn-taking baseline that mimics the interaction of previous work, and the Title-to-Story system, which generates directly from topic.",
    "slm_pred": "The turn-taking baseline that mimics the interaction of previous work, and the Title-to-Story system, which generates directly from topic.",
    "llm_pred": "the Title-to-Story model and a turn-taking interaction baseline modeled on prior work",
    "perf_chosen": 0.1904761905,
    "perf_slm": 0.1904761905,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "08b87a90139968095433f27fc88f571d939cd433",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12",
    "final_answer": "The spelling features described in BIBREF12.",
    "slm_pred": "The spelling features described in BIBREF12.",
    "llm_pred": "Judging tokens as IOCs using only spelling features (a spelling-rule baseline).",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.3125,
    "cost": 5.0
  },
  {
    "id": "ef872807cb0c9974d18bbb886a7836e793727c3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.",
    "final_answer": "The contextual features used are the frequency of the token in the whole corpus, the frequency of contextual keywords from the windowed portions of the texts centering on the token in the whole corpus, and the size of the window.",
    "slm_pred": "The contextual features used are the frequency of the token in the whole corpus, the frequency of contextual keywords from the windowed portions of the texts centering on the token in the whole corpus, and the size of the window.",
    "llm_pred": "Frequencies of automatically extracted contextual keywords co-occurring within a fixed window around each token (a co‑occurrence vector over those keywords).",
    "perf_chosen": 0.31746031750000003,
    "perf_slm": 0.31746031750000003,
    "perf_llm": 0.2790697674,
    "cost": 5.0
  },
  {
    "id": "4db3c2ca6ddc87209c31b20763b7a3c1c33387bc",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018",
    "final_answer": "crawled online from 35 cybersecurity blogs published from 2001 to 2018.",
    "slm_pred": "crawled online from 35 cybersecurity blogs published from 2001 to 2018.",
    "llm_pred": "687 English APT reports and 5,427 Chinese articles crawled from 35 cybersecurity blogs.",
    "perf_chosen": 0.37037037040000004,
    "perf_slm": 0.37037037040000004,
    "perf_llm": 0.20689655170000001,
    "cost": 5.0
  },
  {
    "id": "63337fd803f6fdd060ebd0f53f9de79d451810cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "human-crafted features that heavily rely on specific language knowledge such as dependency structure.",
    "slm_pred": "human-crafted features that heavily rely on specific language knowledge such as dependency structure.",
    "llm_pred": "Human-crafted linguistic features, e.g., dependency-structure-based features.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "63496705fff20c55d4b3d8cdf4786f93e742dd3d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7b44bee49b7cb39cb7d5eec79af5773178c27d4d",
    "dataset": "qasper_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",
    "final_answer": "The data in RAFAEL is labelled manually by assigning information to each question.",
    "slm_pred": "The data in RAFAEL is labelled manually by assigning information to each question.",
    "llm_pred": "manually",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6d54bad91b6ccd1108d1ddbff1d217c6806e0842",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "only the first word sense (usually the most common) is taken into account",
    "final_answer": "They take only the first word sense (usually the most common) into account.",
    "slm_pred": "They take only the first word sense (usually the most common) into account.",
    "llm_pred": "They use only the first (most common) WordNet sense for polysemous words.",
    "perf_chosen": 0.8461538462,
    "perf_slm": 0.8461538462,
    "perf_llm": 0.48,
    "cost": 5.0
  },
  {
    "id": "238ec3c1e1093ce2f5122ee60209b969f7669fae",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.\n4) Computing the mean m and the sample variance σ for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",
    "final_answer": "It is measured using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor.",
    "slm_pred": "It is measured using the concept of surrounding uniformity, which roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor.",
    "llm_pred": "by surrounding uniformity (SU): |w + Σ a_i| divided by (|w| + Σ |a_i|) for the N nearest neighbors",
    "perf_chosen": 0.22058823530000002,
    "perf_slm": 0.22058823530000002,
    "perf_llm": 0.157480315,
    "cost": 5.0
  },
  {
    "id": "f704d182c9e01a2002381b76bf21e4bb3c0d3efc",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "fine-tuning with forgetting cost regularization",
    "slm_pred": "unanswerable \n\nThe article does not provide a direct comparison of various transfer learning techniques to determine which one yields the best performance. However, it does mention that mere fine-tuning reaches state-of-the-art results for factoid questions.",
    "llm_pred": "fine-tuning with forgetting cost regularization",
    "perf_chosen": 0.0,
    "perf_slm": 0.0555555556,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "da544015511e535503dee2eaf4912a5e36c806cd",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14",
    "final_answer": "unanswerable",
    "slm_pred": "The article does not explicitly describe the architecture of the model, but it mentions the use of a sequence-to-sequence Spanish-English ST model and Nonnegative Matrix Factorization (NMF) for topic modeling.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.1951219512,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7bc993b32484d6ae3c86d0b351a68e59fd2757a5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Spanish",
    "final_answer": "Spanish.",
    "slm_pred": "Spanish.",
    "llm_pred": "Spanish",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "LDC corpus",
    "final_answer": "from the training corpus (LDC data)",
    "slm_pred": "unanswerable \n\nHowever, based on the article, it can be inferred that the vocabulary is derived from the training corpus, as the article mentions that the word frequency is ranked from greatest to least in the training corpus to determine the segmentation point for the hybrid word-characters level.",
    "llm_pred": "from the training corpus (LDC data)",
    "perf_chosen": 0.5,
    "perf_slm": 0.0408163265,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "e44a5514d7464993997212341606c2c0f3a72eb4",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "word-level granularity",
    "slm_pred": "Character level.",
    "llm_pred": "word-level granularity",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "310e61b9dd4d75bc1bebbcb1dae578f55807cd04",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)",
    "final_answer": "They used the LDC corpus for training data, and the NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as test sets for Chinese-to-English translation, and the NIST 2003(MT03) dataset as the validation set, and the NIST 2008(MT08) as test set for English-to-Chinese translation.",
    "slm_pred": "They used the LDC corpus for training data, and the NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as test sets for Chinese-to-English translation, and the NIST 2003(MT03) dataset as the validation set, and the NIST 2008(MT08) as test set for English-to-Chinese translation.",
    "llm_pred": "LDC corpus",
    "perf_chosen": 0.4285714286,
    "perf_slm": 0.4285714286,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "bdc6664cec2b94b0b3769bc70a60914795f39574",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values",
    "final_answer": "The classic information retrieval metrics of precision (P), recall (R), and F1 are used to measure the correctness of the answers given by participants in the DQA experiments and the QALD system.",
    "slm_pred": "The classic information retrieval metrics of precision (P), recall (R), and F1 are used to measure the correctness of the answers given by participants in the DQA experiments and the QALD system.",
    "llm_pred": "by precision, recall, and F1-score",
    "perf_chosen": 0.0526315789,
    "perf_slm": 0.0526315789,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "e40df8c685a28b98006c47808f506def68f30e26",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "yes",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "9653c89a93ac5c717a0a26cf80e9aa98a5ccf910",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8",
    "final_answer": "gAnswer, QAKiS, Platypus, and WDAqua.",
    "slm_pred": "gAnswer, QAKiS, Platypus, and WDAqua.",
    "llm_pred": "WDAqua, QAKiS, gAnswer, and Platypus (askplatyp.us).",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "b921a1771ed0ba9dbeff9da000336ecf2bb38322",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "412aff0b2113b7d61c914edf90b90f2994390088",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "no",
    "slm_pred": "Yes.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "010e3793eb1342225857d3f95e147d8f8467192a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains.",
    "final_answer": "103,871 sentences (Europarl) and 1,269,091 sentences (SoNaR).",
    "slm_pred": "The Europarl dataset consists of 70,057 dat-labeled and 33,814 die-labeled sentences, while the SoNaR dataset has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled sentences.",
    "llm_pred": "103,871 sentences (Europarl) and 1,269,091 sentences (SoNaR).",
    "perf_chosen": 0.1290322581,
    "perf_slm": 0.32142857140000003,
    "perf_llm": 0.1290322581,
    "cost": 25.0
  },
  {
    "id": "c20bb0847ced490a793657fbaf6afb5ef54dad81",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "To account for multiple-testing, Bonferroni correction was applied, so the researchers drew a random sample of 503 songs.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ff8557d93704120b65d9b597a4fab40b49d24b6d",
    "dataset": "qasper_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "447eb98e602616c01187960c9c3011c62afd7c27",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Table TABREF10 displays the twenty resulting topics",
    "final_answer": "Topics typically ascribed to metal lyrics include sadness, death, freedom, nature, occultism, unpleasant/disgusting objects, harsh, gloomy, dystopian, or satanic themes.",
    "slm_pred": "Topics typically ascribed to metal lyrics include sadness, death, freedom, nature, occultism, unpleasant/disgusting objects, harsh, gloomy, dystopian, or satanic themes.",
    "llm_pred": "sadness, death, freedom, nature, occultism, and unpleasant/disgusting objects",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.0740740741,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f398587b9a0008628278a5ea858e01d3f5559f65",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "SPNet vs best baseline:\nROUGE-1: 90.97 vs 90.68\nCIC: 70.45 vs 70.25",
    "final_answer": "SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics with a big margin.",
    "slm_pred": "SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics with a big margin.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.08,
    "perf_slm": 0.08,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair",
    "final_answer": "ROUGE, ROUGE-1, ROUGE-2, ROUGE-L, Critical Information Completeness (CIC), relevance, conciseness, and readability are used for automatic and human evaluation metrics to compare SPNet to its counterparts.",
    "slm_pred": "ROUGE, ROUGE-1, ROUGE-2, ROUGE-L, Critical Information Completeness (CIC), relevance, conciseness, and readability are used for automatic and human evaluation metrics to compare SPNet to its counterparts.",
    "llm_pred": "ROUGE-1/ROUGE-2/ROUGE-L and CIC (automatic); human ratings of relevance, conciseness, and readability, plus pairwise ranking.",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.375,
    "cost": 5.0
  },
  {
    "id": "58f3bfbd01ba9768172be45a819faaa0de2ddfa4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable \n\nThe article does not provide information on whether the proposed abstractive dialog summarization dataset is open source.",
    "slm_pred": "unanswerable \n\nThe article does not provide information on whether the proposed abstractive dialog summarization dataset is open source.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "73633afbefa191b36cca594977204c6511f9dad4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Not at the moment, but summaries can be additionaly extended with this annotations.",
    "final_answer": "The answer is partially, as speaker role scaffold is natural, but semantic slot scaffold is seldom explicitly annotated.",
    "slm_pred": "The answer is partially, as speaker role scaffold is natural, but semantic slot scaffold is seldom explicitly annotated.",
    "llm_pred": "No—speaker role and domain are often available, but semantic slot annotations are rarely provided.",
    "perf_chosen": 0.1290322581,
    "perf_slm": 0.1290322581,
    "perf_llm": 0.14814814810000002,
    "cost": 5.0
  },
  {
    "id": "db39a71080e323ba2ddf958f93778e2b875dcd24",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework.",
    "final_answer": "SPNet utilizes additional speaker role, semantic slot, and dialog domain annotations through separate encoding for different speakers, delexicalization and slot filling, and multi-task learning with domain classification.",
    "slm_pred": "SPNet utilizes additional speaker role, semantic slot, and dialog domain annotations through separate encoding for different speakers, delexicalization and slot filling, and multi-task learning with domain classification.",
    "llm_pred": "By using them as semantic scaffolds: separate encoders for user/system roles, delexicalized template generation with copy/pointing to fill slot values, and an auxiliary multi-label domain classification task jointly trained with summarization.",
    "perf_chosen": 0.406779661,
    "perf_slm": 0.406779661,
    "perf_llm": 0.1904761905,
    "cost": 5.0
  },
  {
    "id": "6da2cb3187d3f28b75ac0a61f6562a8adf716109",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Pointer-Generator, Transformer",
    "final_answer": "sequence-to-sequence neural models with attention (e.g., pointer-generator)",
    "slm_pred": "Pointer-Generator and Transformer, which achieved high-quality summaries of news documents.",
    "llm_pred": "sequence-to-sequence neural models with attention (e.g., pointer-generator)",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "c47e87efab11f661993a14cf2d7506be641375e4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities",
    "final_answer": "The new evaluation metric, CIC (Critical Information Completeness), considers critical informative entities by counting the recall of semantic slot information between a candidate summary and a reference summary.",
    "slm_pred": "The new evaluation metric, CIC (Critical Information Completeness), considers critical informative entities by counting the recall of semantic slot information between a candidate summary and a reference summary.",
    "llm_pred": "By using CIC, which measures the recall of semantic slot values (delexicalized critical entities) matched between the candidate and reference summaries.",
    "perf_chosen": 0.1379310345,
    "perf_slm": 0.1379310345,
    "perf_llm": 0.11764705880000001,
    "cost": 5.0
  },
  {
    "id": "14684ad200915ff1e3fc2a89cb614e472a1a2854",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "yes",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself).",
    "final_answer": "The annotators were educated to rank results by their coverage, fluency, and plausibility in daily life.",
    "slm_pred": "The annotators were educated to rank results by their coverage, fluency, and plausibility in daily life.",
    "llm_pred": "coverage, fluency, and plausibility in daily life",
    "perf_chosen": 0.1304347826,
    "perf_slm": 0.1304347826,
    "perf_llm": 0.0540540541,
    "cost": 5.0
  },
  {
    "id": "5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore",
    "final_answer": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore are used for this task.",
    "slm_pred": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore are used for this task.",
    "llm_pred": "BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, and BERTScore.",
    "perf_chosen": 0.625,
    "perf_slm": 0.625,
    "perf_llm": 0.9090909091,
    "cost": 5.0
  },
  {
    "id": "c34a15f1d113083da431e4157aceb11266e9a1b2",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "no \n\nThe article mentions that rationales are provided as an optional resource, and the models are not required to generate them.",
    "slm_pred": "no \n\nThe article mentions that rationales are provided as an optional resource, and the models are not required to generate them.",
    "llm_pred": "no",
    "perf_chosen": 0.0909090909,
    "perf_slm": 0.0909090909,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "061682beb3dbd7c76cfa26f7ae650e548503d977",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "unanswerable",
    "slm_pred": "no \n\nThe article states that the annotators are required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes, implying that the rationales are generated simultaneously with the sentences.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "3518d8eb84f6228407cfabaf509fd63d60351203",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "No, the crowd-sourced sentences were created by annotators who were given the concept-sets as the only signal, not shown the associated images or videos.",
    "slm_pred": "No, the crowd-sourced sentences were created by annotators who were given the concept-sets as the only signal, not shown the associated images or videos.",
    "llm_pred": "Yes—for the dev/test sets, sentences were written by humans shown only the concept-sets.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "617c77a600be5529b3391ab0c21504cd288cc7c7",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "These concept-sets are sampled from several large corpora of image/video captions",
    "final_answer": "They are sampled from several large corpora of image/video captions.",
    "slm_pred": "They are sampled from several large corpora of image/video captions.",
    "llm_pred": "from large corpora of image/video captions (e.g., VATEX, LSMDC, ActivityNet, SNLI)",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "53d6cbee3606dd106494e2e98aa93fdd95920375",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "test accuracy of 88.9%, which exceeds the previous best by 16.9%",
    "final_answer": "at least 16% in absolute accuracy for all the MCQA datasets (except the SemEval dataset).",
    "slm_pred": "at least 16% in absolute accuracy for all the MCQA datasets (except the SemEval dataset).",
    "llm_pred": "At least 16% absolute accuracy (e.g., 16.9% on DREAM), except SemEval.",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "9dc844f82f520daf986e83466de0c84d93953754",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MultiNLI BIBREF15 and SNLI BIBREF16 ",
    "final_answer": "MultiNLI and SNLI datasets.",
    "slm_pred": "MultiNLI and SNLI datasets.",
    "llm_pred": "MultiNLI and SNLI",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "FTLM++, BERT-large, XLNet",
    "final_answer": "Previous SOTA models, as well as RoBERTa-Large models.",
    "slm_pred": "Previous SOTA models, as well as RoBERTa-Large models.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "36d892460eb863220cd0881d5823d73bbfda172c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11",
    "final_answer": "The four representative MCQA datasets used for benchmark are DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.",
    "slm_pred": "The four representative MCQA datasets used for benchmark are DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.",
    "llm_pred": "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11.",
    "perf_chosen": 0.6086956522,
    "perf_slm": 0.6086956522,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4cbc56d0d53c4c03e459ac43e3c374b75fd48efe",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "LSTM, SCIBERT",
    "final_answer": "the existing ebm-nlp leaderboard models and SCIBERT results",
    "slm_pred": "unanswerable",
    "llm_pred": "the existing ebm-nlp leaderboard models and SCIBERT results",
    "perf_chosen": 0.2,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "e5a965e7a109ae17a42dd22eddbf167be47fca75",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Some sentences are associated to ambiguous dimensions in the hidden state output",
    "final_answer": "Ambiguity in labelled PIO data, ambiguous sentence labels, and the assignment of a single label limiting the quality of predictions.",
    "slm_pred": "Ambiguity in labelled PIO data, ambiguous sentence labels, and the assignment of a single label limiting the quality of predictions.",
    "llm_pred": "Ambiguous, single-label sentence annotations—often noisy or wrong—because sentences can contain multiple PICO elements, which limits prediction quality.",
    "perf_chosen": 0.1875,
    "perf_slm": 0.1875,
    "perf_llm": 0.1379310345,
    "cost": 5.0
  },
  {
    "id": "7d59374d9301a0c09ea5d023a22ceb6ce07fb490",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by number of distinct n-grams",
    "final_answer": "They measure the diversity of inferences by using the distinct n-gram score, which is normalized to [0, 1] by dividing the total number of generated tokens.",
    "slm_pred": "They measure the diversity of inferences by using the distinct n-gram score, which is normalized to [0, 1] by dividing the total number of generated tokens.",
    "llm_pred": "by normalized distinct n-gram counts (e.g., distinct-1/2)",
    "perf_chosen": 0.25806451610000003,
    "perf_slm": 0.25806451610000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "8e2b125426d1220691cceaeaf1875f76a6049cbd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",
    "final_answer": "unanswerable\n\nHowever, the article does state that they \"outperform baseline methods in both the accuracy and diversity of inferences\" but it does not provide a specific percentage or numerical value of the improvement.",
    "slm_pred": "unanswerable\n\nHowever, the article does state that they \"outperform baseline methods in both the accuracy and diversity of inferences\" but it does not provide a specific percentage or numerical value of the improvement.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.16,
    "perf_slm": 0.16,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, based on the context it can be inferred that the baseline models used are RNN-based encoder-decoder models, but the specific models used as baselines on the Atomic dataset are not explicitly mentioned in the article.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.046511627900000005,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fb76e994e2e3fa129f1e94f1b043b274af8fb84c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",
    "final_answer": "By training on an auxiliary dataset with rich event background knowledge in the pretrain stage.",
    "slm_pred": "By training on an auxiliary dataset with rich event background knowledge in the pretrain stage.",
    "llm_pred": "By pretraining on narrative corpora with a context-conditioned latent variable z_c and minimizing the distance between z_c and z_{c′} to transfer the background knowledge into z_{c′}.",
    "perf_chosen": 0.3389830508,
    "perf_slm": 0.3389830508,
    "perf_llm": 0.22857142860000002,
    "cost": 5.0
  },
  {
    "id": "99ef97336c0112d9f60df108f58c8b04b519a854",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "The Atomic dataset is not explicitly stated in the article, but it is mentioned that it \"scales up the size of dataset and expands the scope to nine types of inference dimensions\" compared to the Event2Mind dataset, which contains 25K base events and 300K targets.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "95d8368b1055d97250df38d1e8c4a2b283d2b57e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "pipeline that is used at Microsoft for production data",
    "final_answer": "a vendor-based transcription pipeline used at Microsoft for production data.",
    "slm_pred": "a vendor-based transcription pipeline used at Microsoft for production data.",
    "llm_pred": "Microsoft’s vendor-based two-pass transcription pipeline (first-pass transcription with second-pass QC).",
    "perf_chosen": 0.7368421053,
    "perf_slm": 0.7368421053,
    "perf_llm": 0.1052631579,
    "cost": 5.0
  },
  {
    "id": "a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",
    "final_answer": "Significant improvement.",
    "slm_pred": "Significant improvement.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "LSTMs",
    "final_answer": "They used 2-layer LSTMs for both the source and target sides in the NMT system.",
    "slm_pred": "They used 2-layer LSTMs for both the source and target sides in the NMT system.",
    "llm_pred": "2-layer LSTM encoder–decoder with attention (NMT).",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4f12b41bd3bb2610abf7d7835291496aa69fb78c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Appending the domain tag “<2domain>\" to the source sentences of the respective corpora",
    "final_answer": "They appended the domain tag \"<2domain>\" to the source sentences of the respective corpora to prime the NMT decoder to generate sentences for the specific domain.",
    "slm_pred": "They appended the domain tag \"<2domain>\" to the source sentences of the respective corpora to prime the NMT decoder to generate sentences for the specific domain.",
    "llm_pred": "By appending a special token (e.g., <2domain>) to each source sentence to indicate its domain during training.",
    "perf_chosen": 0.5641025641,
    "perf_slm": 0.5641025641,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency",
    "final_answer": "The potential for ongoing mixed-initiative multi-turn dialogues that do not follow a particular plan or pursue a particular fixed information need.",
    "slm_pred": "The potential for ongoing mixed-initiative multi-turn dialogues that do not follow a particular plan or pursue a particular fixed information need.",
    "llm_pred": "because they’re open-ended and lack a fixed plan or information need, forcing the system to maintain coherence and choose among many possible actions while integrating diverse content in real time",
    "perf_chosen": 0.3571428571,
    "perf_slm": 0.3571428571,
    "perf_llm": 0.27956989250000003,
    "cost": 5.0
  },
  {
    "id": "b54fc86dc2cc6994e10c1819b6405de08c496c7b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred",
    "final_answer": "reaction delay, which is defined as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred.",
    "slm_pred": "reaction delay, which is defined as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred.",
    "llm_pred": "as reaction delay—the time between the original post/tweet and the reaction comment/tweet",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "b43a8a0f4b8496b23c89730f0070172cd5dca06a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.",
    "final_answer": "Their model combines a text sequence sub-network with a vector representation sub-network, which are concatenated through a \"late fusion\" approach, consisting of an embedding layer, two 1-dimensional convolution layers, a max-pooling layer, and several dense layers.",
    "slm_pred": "Their model combines a text sequence sub-network with a vector representation sub-network, which are concatenated through a \"late fusion\" approach, consisting of an embedding layer, two 1-dimensional convolution layers, a max-pooling layer, and several dense layers.",
    "llm_pred": "a linguistically infused late‑fusion neural net: a CNN text branch (200‑d GloVe → two 1D convs → max‑pool → dense) combined with a LIWC feature branch (two dense layers), concatenating features from the reaction and its parent",
    "perf_chosen": 0.4193548387,
    "perf_slm": 0.4193548387,
    "perf_llm": 0.28800000000000003,
    "cost": 5.0
  },
  {
    "id": "b161febf86cdd58bd247a934120410068b24b7d1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other",
    "final_answer": "answer, elaboration, and question, etc., as well as agreement, disagreement, appreciation, humor, and \"other\".",
    "slm_pred": "answer, elaboration, and question, etc., as well as agreement, disagreement, appreciation, humor, and \"other\".",
    "llm_pred": "agreement; answer; appreciation; disagreement; elaboration; humor; negative reaction; question; other",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d40662236eed26f17dd2a3a9052a4cee1482d7d6",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a vector of frame-level acoustic features",
    "final_answer": "39-dimensional MFCCs.",
    "slm_pred": "39-dimensional MFCCs.",
    "llm_pred": "39‑dimensional MFCCs with delta and delta‑delta coefficients (per-frame).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "1d791713d1aa77358f11501f05c108045f53c8aa",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1061",
    "final_answer": "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061.",
    "slm_pred": "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061.",
    "llm_pred": "1061",
    "perf_chosen": 0.0952380952,
    "perf_slm": 0.0952380952,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6b6360fab2edc836901195c0aba973eae4891975",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Switchboard conversational English corpus",
    "final_answer": "The Switchboard conversational English corpus.",
    "slm_pred": "The Switchboard conversational English corpus.",
    "llm_pred": "Switchboard conversational English corpus",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Their best average precision tops previous best result by 0.202",
    "final_answer": "Sizable improvements in average precision.",
    "slm_pred": "Sizable improvements in average precision.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "86a93a2d1c19cd0cd21ad1608f2a336240725700",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "interpretation of Frege's work are examples of holistic approaches to meaning",
    "final_answer": "It aligns with the distributional hypothesis, grounding meaning holistically in sentential/contextual use rather than isolated words.",
    "slm_pred": "Frege's sentence holism is analogous to the general distributional hypothesis, which states that the meaning of a word is given by the contexts in which it occurs.",
    "llm_pred": "It aligns with the distributional hypothesis, grounding meaning holistically in sentential/contextual use rather than isolated words.",
    "perf_chosen": 0.0740740741,
    "perf_slm": 0.2105263158,
    "perf_llm": 0.0740740741,
    "cost": 25.0
  },
  {
    "id": "6090d3187c41829613abe785f0f3665d9ecd90d9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Only in the context of a sentence does a word have a meaning.",
    "final_answer": "Meaning is defined as truth-value potential.",
    "slm_pred": "Meaning is defined as truth-value potential.",
    "llm_pred": "Meaning is something words have only in the context of sentences, and sentences have within a language.",
    "perf_chosen": 0.1052631579,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.5333333333,
    "cost": 5.0
  },
  {
    "id": "117aa7811ed60e84d40cd8f9cb3ca78781935a98",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "yes",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "c359ab8ebef6f60c5a38f5244e8c18d85e92761d",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",
    "final_answer": "10n",
    "slm_pred": "unanswerable",
    "llm_pred": "10n",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.0,
    "perf_llm": 0.11764705880000001,
    "cost": 25.0
  },
  {
    "id": "ad362365656b0b218ba324ae60701eb25fe664c1",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "syntactic information, semantic and topical information",
    "final_answer": "The latent variables are refined nonterminals that provide contextual information about each node in a given derivation.",
    "slm_pred": "The latent variables are refined nonterminals that provide contextual information about each node in a given derivation.",
    "llm_pred": "syntactic and semantic/topical latent states",
    "perf_chosen": 0.0869565217,
    "perf_slm": 0.0869565217,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "423bb905e404e88a168e7e807950e24ca166306c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation",
    "final_answer": "The baselines used are GraphParser without paraphrases and a monolingual machine translation based model for paraphrase generation.",
    "slm_pred": "The baselines used are GraphParser without paraphrases and a monolingual machine translation based model for paraphrase generation.",
    "llm_pred": "GraphParser without paraphrases and a monolingual MT-based paraphrase generator (Moses).",
    "perf_chosen": 0.7857142857,
    "perf_slm": 0.7857142857,
    "perf_llm": 0.4761904762,
    "cost": 5.0
  },
  {
    "id": "e5ae8ac51946db7475bb20b96e0a22083b366a6d",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes \n\nThey used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries.",
    "slm_pred": "yes \n\nThey used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries.",
    "llm_pred": "yes",
    "perf_chosen": 0.0952380952,
    "perf_slm": 0.0952380952,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "18288c7b0f8bd7839ae92f9c293e7fb85c7e146a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "weak correlation with p-value of 0.08",
    "final_answer": "unanswerable",
    "slm_pred": "The strongest correlation among the topics was determined to be between exercise and obesity, but there was also a notable correlation between exercise and diabetes.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.064516129,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b5e883b15e63029eb07d6ff42df703a64613a18a",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "using topic modeling model Latent Dirichlet Allocation (LDA)",
    "final_answer": "By applying LDA topic modeling (Mallet) to DDEO-related tweets, then filtering topics with LIWC’s health dictionary and labeling those containing DDEO terms.",
    "slm_pred": "Using a topic modeling approach with Latent Dirichlet Allocation (LDA) and fuzzy clustering of semantically related words.",
    "llm_pred": "By applying LDA topic modeling (Mallet) to DDEO-related tweets, then filtering topics with LIWC’s health dictionary and labeling those containing DDEO terms.",
    "perf_chosen": 0.2,
    "perf_slm": 0.56,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "c45a160d31ca8eddbfea79907ec8e59f543aab86",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Swissmetro dataset",
    "final_answer": "the Swissmetro dataset",
    "slm_pred": "The Swissmetro dataset is used for evaluation, and other datasets are also used but not specified.",
    "llm_pred": "the Swissmetro dataset",
    "perf_chosen": 0.8,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.8,
    "cost": 25.0
  },
  {
    "id": "7358a1ce2eae380af423d4feeaa67d2bd23ae9dd",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The embeddings are learned several times using the training set, then the average is taken.",
    "final_answer": "They train their embeddings using a neural network model with a softmax output layer, and the embeddings are learned simultaneously with the choice model, allowing the model to accommodate each other's effects.",
    "slm_pred": "They train their embeddings using a neural network model with a softmax output layer, and the embeddings are learned simultaneously with the choice model, allowing the model to accommodate each other's effects.",
    "llm_pred": "By jointly learning them in a supervised softmax model that predicts the target choice, training all embeddings simultaneously with regularization (including a reconstruction softmax regularizer) using train/dev/test splits.",
    "perf_chosen": 0.2978723404,
    "perf_slm": 0.2978723404,
    "perf_llm": 0.18604651160000002,
    "cost": 5.0
  },
  {
    "id": "1165fb0b400ec1c521c1aef7a4e590f76fee1279",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The data from collected travel surveys is used to model travel behavior.",
    "final_answer": "They model travel behavior using a novel method called Travel Behavior embeddings, which is inspired by natural language processing (NLP) concepts and aims to preserve semantic distance relative to a certain choice problem.",
    "slm_pred": "They model travel behavior using a novel method called Travel Behavior embeddings, which is inspired by natural language processing (NLP) concepts and aims to preserve semantic distance relative to a certain choice problem.",
    "llm_pred": "Using a multinomial logit choice model augmented with learned “travel behavior” embeddings for categorical variables.",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.0740740741,
    "cost": 5.0
  },
  {
    "id": "f2c5da398e601e53f9f545947f61de5f40ede1ee",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The coefficients are projected back to the dummy variable space.",
    "final_answer": "They interpret the coefficients by projecting them back into the original dummy variable space and following the rules of normal random variables to preserve statistical significance information.",
    "slm_pred": "They interpret the coefficients by projecting them back into the original dummy variable space and following the rules of normal random variables to preserve statistical significance information.",
    "llm_pred": "By projecting the embeddings/PCA back into the original dummy-variable space and interpreting the category coefficients there.",
    "perf_chosen": 0.4324324324,
    "perf_slm": 0.4324324324,
    "perf_llm": 0.3846153846,
    "cost": 5.0
  },
  {
    "id": "2d4d0735c50749aa8087d1502ab7499faa2f0dd8",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the text does provide that the authors' proposed model outperforms previous state-of-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN), in terms of weighted binary classification accuracy (Wt. Acc.).",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "43761478c26ad65bec4f0fd511ec3181a100681c",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "01866fe392d9196dda1d0b472290edbd48a99f66",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones",
    "final_answer": "The lexicon of trafficking flags is expanded through a data-driven approach by analyzing word embeddings learned from raw text data, particularly using a skip-gram model and t-SNE to visualize the vectors for emojis, and re-training the model periodically on new escort ads.",
    "slm_pred": "The lexicon of trafficking flags is expanded through a data-driven approach by analyzing word embeddings learned from raw text data, particularly using a skip-gram model and t-SNE to visualize the vectors for emojis, and re-training the model periodically on new escort ads.",
    "llm_pred": "By training Word2Vec on escort ads and using t-SNE to find emojis (and terms) used in similar contexts, then adding those similar items to the existing flags.",
    "perf_chosen": 0.3561643836,
    "perf_slm": 0.3561643836,
    "perf_llm": 0.27586206900000004,
    "cost": 5.0
  },
  {
    "id": "394cf73c0aac8ccb45ce1b133f4e765e8e175403",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "yes",
    "slm_pred": "yes",
    "llm_pred": "yes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2c4003f25e8d95a3768204f52a7a5f5e17cb2102",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "no",
    "slm_pred": "unanswerable",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "65e32f73357bb26a29a58596e1ac314f7e9c6c91",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between “Frustrate” and “Neutralize”, distinction between the classes “Troll” and “Engage”",
    "final_answer": "A comment that seems inoffensive on the surface, such as \"your comments fit well in Stormfront\", but may have a malicious intention due to the author's background.",
    "slm_pred": "A comment that seems inoffensive on the surface, such as \"your comments fit well in Stormfront\", but may have a malicious intention due to the author's background.",
    "llm_pred": "“your comments fit well in Stormfront”",
    "perf_chosen": 0.115942029,
    "perf_slm": 0.115942029,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "46f175e1322d648ab2c0258a9609fe6f43d3b44e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": " inclusion of longer parts of the conversation",
    "final_answer": "The potential solutions suggested include using additional knowledge from anthologies, modifying NELL, employing deeper semantics, jointly learning classifiers, and using the temporal sequence of response comments.",
    "slm_pred": "The potential solutions suggested include using additional knowledge from anthologies, modifying NELL, employing deeper semantics, jointly learning classifiers, and using the temporal sequence of response comments.",
    "llm_pred": "- Incorporate external knowledge (e.g., modify NELL) and sentiment/polarity cues\n- Use deeper semantic representations (logical forms) to infer intent\n- Jointly learn interpretation and response strategy\n- Exploit conversation temporality by using earlier responses/interpretations as features",
    "perf_chosen": 0.12121212120000001,
    "perf_slm": 0.12121212120000001,
    "perf_llm": 0.05,
    "cost": 5.0
  },
  {
    "id": "7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1000 conversations composed of 6833 sentences and 88047 tokens",
    "final_answer": "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens.",
    "slm_pred": "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens.",
    "llm_pred": "1000 conversations",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "3fa638e6167e1c7a931c8ee5c0e2e397ec1b6cda",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d2b3f2178a177183b1aeb88784e48ff7e3e5070c",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": " between 0.81 and 0.88",
    "final_answer": "A strong negative correlation (R² ≈ 0.81–0.88).",
    "slm_pred": "Strong.",
    "llm_pred": "A strong negative correlation (R² ≈ 0.81–0.88).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d5ff8fc4d3996db2c96cb8af5a6d215484991e62",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments",
    "final_answer": "The novel method (DBCA) achieves higher compound divergence at a similar atom divergence compared to other approaches.",
    "slm_pred": "The novel method (DBCA) achieves higher compound divergence at a similar atom divergence compared to other approaches.",
    "llm_pred": "They yield splits with significantly higher compound divergence at similar atom divergence than prior length/pattern splits, while still correlating with those aspects, and test accuracy strongly tracks compound divergence across models.",
    "perf_chosen": 0.6486486486,
    "perf_slm": 0.6486486486,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets",
    "final_answer": "The authors justify that the question answering dataset presented is realistic by stating that the questions are generated automatically using a rule-based system, and the generation rules are designed to have few and meaningful atoms, which yields a large variety of compounds, making the examples come from composing them, and by filtering out unnatural questions using semantic and structural rules.",
    "slm_pred": "The authors justify that the question answering dataset presented is realistic by stating that the questions are generated automatically using a rule-based system, and the generation rules are designed to have few and meaningful atoms, which yields a large variety of compounds, making the examples come from composing them, and by filtering out unnatural questions using semantic and structural rules.",
    "llm_pred": "Because all questions are natural-language queries grounded in Freebase—each paired with an executable SPARQL query and answerable using real Freebase data—after filtering to keep them natural.",
    "perf_chosen": 0.1686746988,
    "perf_slm": 0.1686746988,
    "perf_llm": 0.20408163270000002,
    "cost": 5.0
  },
  {
    "id": "0427ca83d6bf4ec113bc6fec484b2578714ae8ec",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "LSTM+attention, Transformer , Universal Transformer",
    "final_answer": "LSTM seq2seq with attention; Transformer; Universal Transformer",
    "slm_pred": "The three machine architectures analyzed are not explicitly mentioned in the provided snippet.",
    "llm_pred": "LSTM seq2seq with attention; Transformer; Universal Transformer",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.0,
    "perf_llm": 0.5454545455,
    "cost": 25.0
  },
  {
    "id": "f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "239,357 English question-answer pairs",
    "final_answer": "239,357 English question-answer pairs.",
    "slm_pred": "239,357 English question-answer pairs.",
    "llm_pred": "239,357 question–answer pairs",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "8db45a8217f6be30c31f9b9a3146bf267de68389",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "random , Output length, Input length, Output pattern, Input pattern",
    "final_answer": "unanswerable (the article does not provide a comprehensive list of other approaches to creating compositional generalization benchmarks).",
    "slm_pred": "unanswerable (the article does not provide a comprehensive list of other approaches to creating compositional generalization benchmarks).",
    "llm_pred": "Query-pattern splits for SQL datasets; the SCAN splits; CLEVR splits by factors like object counts or program depth; compositional VQA datasets with functional programs; greedy splits that keep question-type/answer pairs disjoint while maximizing concept coverage; and SQOOP’s object-pair generalization setup.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4e379d6d5f87554fabf6f7f7b6ed92d2025e7280",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "CSKS task",
    "final_answer": "Low training data imbalanced classification task in Continuous Speech Keyword Spotting.",
    "slm_pred": "Low training data imbalanced classification task in Continuous Speech Keyword Spotting.",
    "llm_pred": "a different audio task",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "518d0847b02b4f23a8f441faa38b935c9b892e1e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Honk, DeepSpeech-finetune",
    "final_answer": "Honk and DeepSpeech-finetune.",
    "slm_pred": "Honk and DeepSpeech-finetune.",
    "llm_pred": "Honk and DeepSpeech-finetune.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "8112d18681e266426cf7432ac4928b87f5ce8311",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English, Hindi",
    "final_answer": "English and Hindi.",
    "slm_pred": "English and Hindi.",
    "llm_pred": "English and Hindi",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article mentions that the model has \"almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear\" which implies that the model's training time might be different from state-of-the-art models.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 0.0487804878,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ba6422e22297c7eb0baa381225a2f146b9621791",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Difference is around 1 BLEU score lower on average than state of the art methods.",
    "final_answer": "The proposed method achieves competitive performance with slight degradation compared to state-of-the-art contemporaneous work on both WMT2014 and WMT2016 corpora.",
    "slm_pred": "The proposed method achieves competitive performance with slight degradation compared to state-of-the-art contemporaneous work on both WMT2014 and WMT2016 corpora.",
    "llm_pred": "slightly worse than state-of-the-art (competitive with only slight degradation)",
    "perf_chosen": 0.11428571430000001,
    "perf_slm": 0.11428571430000001,
    "perf_llm": 0.0833333333,
    "cost": 5.0
  },
  {
    "id": "65e72ad72a9cbfc379f126b10b0ce80cfe44579b",
    "dataset": "qasper_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM",
    "final_answer": "unanswerable",
    "slm_pred": "unanswerable \n\nHowever, the article does mention that the performance of a simple non-autoregressive model falls behind autoregressive models, and it compares FlowSeq with other non-autoregressive models, but it does not specify what those models are.",
    "llm_pred": "unanswerable",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WMT2014, WMT2016 and IWSLT-2014",
    "final_answer": "WMT2014 DE-EN, WMT2016 RO-EN, and IWSLT2014 DE-EN.",
    "slm_pred": "WMT2014 DE-EN, WMT2016 RO-EN, and IWSLT2014 DE-EN.",
    "llm_pred": "WMT2014 DE–EN, WMT2016 RO–EN, and IWSLT2014 DE–EN.",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "04aff4add28e6343634d342db92b3ac36aa8c255",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings",
    "final_answer": "The visual attention is very sparse, with just one source encoding being attended to.",
    "slm_pred": "The visual attention is very sparse, with just one source encoding being attended to.",
    "llm_pred": "Visual attention was extremely sparse—peaked on a single source token (max ≈0.99, std ≈0.015)—showing little variation and limited utility.",
    "perf_chosen": 0.4375,
    "perf_slm": 0.4375,
    "perf_llm": 0.2162162162,
    "cost": 5.0
  },
  {
    "id": "a8e4522ce2ce7336e731286654d6ad0931927a4e",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT",
    "final_answer": "The results of their Principal Component Analysis (PCA) show that the visual features in the How2 dataset are much more dominated by \"common\" dimensions, which makes them less discriminative among individual sentences and further aggravates the problem at the token level.",
    "slm_pred": "The results of their Principal Component Analysis (PCA) show that the visual features in the How2 dataset are much more dominated by \"common\" dimensions, which makes them less discriminative among individual sentences and further aggravates the problem at the token level.",
    "llm_pred": "Most of the variance is captured by a few principal components—visual features lie in a low-dimensional subspace, with How2 especially dominated by “common” directions, indicating low discriminativeness.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.1428571429,
    "cost": 5.0
  },
  {
    "id": "bd7039f81a5417474efa36f703ebddcf51835254",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Reasoner model, also implemented with the MatchLSTM architecture, Ranker model",
    "final_answer": "The two models' architectures in the proposed solution are the Ranker and the Reasoner, both implemented with the MatchLSTM architecture.",
    "slm_pred": "The two models' architectures in the proposed solution are the Ranker and the Reasoner, both implemented with the MatchLSTM architecture.",
    "llm_pred": "Both the Ranker and the Reasoner are MatchLSTM-based models (with 2‑layer bi-GRU encoders).",
    "perf_chosen": 0.4666666667,
    "perf_slm": 0.4666666667,
    "perf_llm": 0.347826087,
    "cost": 5.0
  },
  {
    "id": "022e5c996a72aeab890401a7fdb925ecd0570529",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards",
    "final_answer": "The two models, Ranker and Reasoner, cooperate in a cooperative game, where the Ranker selects passages and the Reasoner predicts the linking entity, with each model providing rewards to the other to improve the selection process.",
    "slm_pred": "The two models, Ranker and Reasoner, cooperate in a cooperative game, where the Ranker selects passages and the Reasoner predicts the linking entity, with each model providing rewards to the other to improve the selection process.",
    "llm_pred": "By playing a cooperative game: the Ranker selects passage chains, the Reasoner predicts the linking entities between adjacent passages, and when the Reasoner’s predictions are consistent, it passes extra rewards to the Ranker to guide selection toward confident chains.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "2a950ede24b26a45613169348d5db9176fda4f82",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Unanswerable",
    "final_answer": "unanswerable.",
    "slm_pred": "unanswerable.",
    "llm_pred": "unanswerable",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "34af2c512ec38483754e94e1ea814aa76552d60a",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples",
    "final_answer": "HotpotQA and MedHop datasets.",
    "slm_pred": "HotpotQA and MedHop datasets.",
    "llm_pred": "HotpotQA and DocRED.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c1429f7fed5a4dda11ac7d9643f97af87a83508b",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation",
    "final_answer": "They reference various empirical investigations, including BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF26, BIBREF27, BIBREF28, and BIBREF29, among others.",
    "slm_pred": "They reference various empirical investigations, including BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF26, BIBREF27, BIBREF28, and BIBREF29, among others.",
    "llm_pred": "the choice of raters, the use of linguistic context, and the creation/quality of reference translations",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "a93d4aa89ac3abbd08d725f3765c4f1bed35c889",
    "dataset": "qasper_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "English , Chinese ",
    "final_answer": "Chinese to English and English to Czech, but mainly Chinese to English.",
    "slm_pred": "Chinese to English and English to Czech, but mainly Chinese to English.",
    "llm_pred": "Chinese to English",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.8,
    "cost": 5.0
  }
]