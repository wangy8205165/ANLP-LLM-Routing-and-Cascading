[
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "white",
    "final_answer": "White.",
    "slm_pred": "White.",
    "llm_pred": "white",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in a barn",
    "final_answer": "Above the barn near a farm house.",
    "slm_pred": "Above the barn near a farm house.",
    "llm_pred": "in a barn near the farmhouse",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, she shared a hay bed with her mommy and 5 other sisters.",
    "slm_pred": "No, she shared a hay bed with her mommy and 5 other sisters.",
    "llm_pred": "No—she lived with her mother and five sisters.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "with her mommy and 5 sisters",
    "final_answer": "Her mommy and 5 other sisters.",
    "slm_pred": "Her mommy and 5 other sisters.",
    "llm_pred": "her mommy and five sisters",
    "perf_chosen": 0.8333333333,
    "perf_slm": 0.8333333333,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "orange and white",
    "final_answer": "Orange with white tiger stripes.",
    "slm_pred": "Orange with white tiger stripes.",
    "llm_pred": "orange with white tiger stripes",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Yes, she was, after realizing her uniqueness was special.",
    "slm_pred": "Yes, she was, after realizing her uniqueness was special.",
    "llm_pred": "Yes—by the end (though not at first).",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "she painted herself",
    "final_answer": "She painted herself with the old farmer's orange paint.",
    "slm_pred": "She painted herself with the old farmer's orange paint.",
    "llm_pred": "painted herself with the farmer’s orange paint",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the farmer",
    "final_answer": "the old farmer's.",
    "slm_pred": "the old farmer's.",
    "llm_pred": "the old farmer’s",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "they started laughing",
    "final_answer": "They started laughing.",
    "slm_pred": "They started laughing.",
    "llm_pred": "they started laughing",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a bucket of water",
    "final_answer": "A big bucket of water.",
    "slm_pred": "A big bucket of water.",
    "llm_pred": "a big bucket of water",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "licked her face",
    "final_answer": "they licked her face until Cotton's fur was all dry.",
    "slm_pred": "they licked her face until Cotton's fur was all dry.",
    "llm_pred": "licked her face until she was dry",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "e562679ab959501e27b94eafc2d395251153b1ab6ac76fe4d97d6f7018370067",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No, they liked her natural white fur.",
    "slm_pred": "No, they did not.",
    "llm_pred": "No, they liked her natural white fur.",
    "perf_chosen": 0.25,
    "perf_slm": 0.4,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Asta.",
    "final_answer": "Asta.",
    "slm_pred": "Asta.",
    "llm_pred": "Asta",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a bottle",
    "final_answer": "The bottle.",
    "slm_pred": "The bottle.",
    "llm_pred": "the bottle",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Asta.",
    "final_answer": "The answer is Asta’s papa.",
    "slm_pred": "Asta's papa",
    "llm_pred": "The answer is Asta’s papa.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "It is implied that Sharkie was a friend, as Asta and Sharkie referred to each other as friends and collaborated to open the bottle and read the note.",
    "slm_pred": "It is implied that Sharkie was a friend, as Asta and Sharkie referred to each other as friends and collaborated to open the bottle and read the note.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "yes.",
    "slm_pred": "Yes, they did find another bottle.",
    "llm_pred": "yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a note",
    "final_answer": "A note.",
    "slm_pred": "A note.",
    "llm_pred": "Answer: The answer is a note.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, a little girl wrote the note.",
    "slm_pred": "No, a little girl wrote the note.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Asta's papa",
    "final_answer": "Asta's papa.",
    "slm_pred": "Asta's papa.",
    "llm_pred": "Asta’s papa.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "took it to Asta’s papa",
    "slm_pred": "They took the note to Asta's papa.",
    "llm_pred": "took it to Asta’s papa",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "The answer is yes.",
    "slm_pred": "Yes, they did.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.5,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "a786de3586a68bda9c17973bce8f46e9f1baef113be99313aad4ad2614d10502",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "The answer is yes.",
    "slm_pred": "They were excited.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "An elderly Chinese lady and a little boy",
    "final_answer": "The elderly Chinese lady and the little boy.",
    "slm_pred": "The elderly Chinese lady and the little boy.",
    "llm_pred": "the elderly Chinese lady—Nicole’s mother, the boy’s grandmother",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "A paper carrier bag.",
    "slm_pred": "A paper carrier bag.",
    "llm_pred": "Yes, a paper carrier bag.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a paper carrier bag",
    "final_answer": "Thank you.",
    "slm_pred": "The elderly Chinese lady has brought food again.",
    "llm_pred": "Thank you.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "The elderly Chinese lady is the mother of your neighbor, Nicole, whom you have known since she moved in next door last October.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Nicole",
    "final_answer": "Nicole.",
    "slm_pred": "Nicole.",
    "llm_pred": "Nicole",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Shanghai",
    "final_answer": "Shanghai",
    "slm_pred": "Shanghai.",
    "llm_pred": "Shanghai",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "mother",
    "final_answer": "The boy's grandmother.",
    "slm_pred": "The boy's grandmother.",
    "llm_pred": "his grandmother",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "food",
    "final_answer": "a thermos with hot soup and a stainless-steel container with rice, vegetables, and either chicken, meat, or shrimp.",
    "slm_pred": "a thermos with hot soup and a stainless-steel container with rice, vegetables, and either chicken, meat, or shrimp.",
    "llm_pred": "a thermos of hot soup and a container of rice with vegetables and meat (sometimes a pancake)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, it is not her first visit.",
    "slm_pred": "Yes, it is not her first visit.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "I am having heart surgery soon, so her mother has decided I need more nutrients",
    "final_answer": "Because the grandmother’s continual, generous meals make mere thanks feel insufficient.",
    "slm_pred": "The answer is because she wants to show kindness and care.",
    "llm_pred": "Because the grandmother’s continual, generous meals make mere thanks feel insufficient.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "an iPad",
    "final_answer": "an iPad",
    "slm_pred": "The iPad.",
    "llm_pred": "an iPad",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "hot soup and a container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake",
    "final_answer": "a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake.",
    "slm_pred": "a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake.",
    "llm_pred": "Chinese dishes",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.9090909091,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "I am now working on some more Chinese words",
    "final_answer": "learn more Chinese words",
    "slm_pred": "Work on some more Chinese words.",
    "llm_pred": "learn more Chinese words",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.4615384615,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes—she keeps bringing it.",
    "slm_pred": "Most likely yes, as it has become a daily practice and she is trying to help with your upcoming surgery.",
    "llm_pred": "Yes—she keeps bringing it.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0952380952,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fde6258a9beb5bd79e9d658fe48ed7230b4f064d74a77d389e21f22d46247862",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Thank you\"",
    "final_answer": "\"Thank you\"",
    "slm_pred": "\"Thank you\"",
    "llm_pred": "Thank you",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "The answer is yes.",
    "slm_pred": "The answer is yes.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Dennis Farina",
    "final_answer": "Dennis Farina",
    "slm_pred": "The answer is Dennis Farina.",
    "llm_pred": "Dennis Farina",
    "perf_chosen": 1.0,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Actor",
    "final_answer": "died",
    "slm_pred": "He acted and worked as a police officer.",
    "llm_pred": "died",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, he is not.",
    "slm_pred": "No, he is not.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he was.",
    "slm_pred": "Yes, he was.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "Law & Order (2004)",
    "slm_pred": "The answer is no.",
    "llm_pred": "Law & Order (2004)",
    "perf_chosen": 0.0,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Farina was cast in a film",
    "final_answer": "Dennis Farina got into acting through director Michael Mann, who used him as a consultant and cast him in his 1981 movie, \"Thief.\"",
    "slm_pred": "Dennis Farina got into acting through director Michael Mann, who used him as a consultant and cast him in his 1981 movie, \"Thief.\"",
    "llm_pred": "He began acting in Michael Mann’s 1981 film “Thief.”",
    "perf_chosen": 0.27586206900000004,
    "perf_slm": 0.27586206900000004,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Michael Mann",
    "final_answer": "Director Michael Mann cast him.",
    "slm_pred": "Director Michael Mann cast him.",
    "llm_pred": "Michael Mann",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Thief\"",
    "final_answer": "There is no movie title mentioned in the story.",
    "slm_pred": "There is no movie title mentioned in the story.",
    "llm_pred": "The answer is Thief.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cops or gangsters",
    "final_answer": "Cops or gangsters.",
    "slm_pred": "Cops or gangsters.",
    "llm_pred": "cops or gangsters",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He joined a TV show cast.",
    "final_answer": "Dennis Farina joined the cast of the long-running \"Law & Order\".",
    "slm_pred": "Dennis Farina joined the cast of the long-running \"Law & Order\".",
    "llm_pred": "He joined the cast of Law & Order as Detective Joe Fontana.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.3529411765,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "\"Law & Order\"",
    "final_answer": "Ed Green",
    "slm_pred": "The answer is a CNN news article.",
    "llm_pred": "Ed Green",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Detective Joe Fontana",
    "final_answer": "Detective Joe Fontana.",
    "slm_pred": "Detective Joe Fontana.",
    "llm_pred": "Detective Joe Fontana",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No — he drove an expensive car.",
    "slm_pred": "No, he had an expensive car.",
    "llm_pred": "No — he drove an expensive car.",
    "perf_chosen": 0.25,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "An expensive car",
    "final_answer": "The answer is a mustache.",
    "slm_pred": "The answer is a mustache.",
    "llm_pred": "a long career as a police officer in Chicago",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No — they were flashy.",
    "slm_pred": "No, Lennie Briscoe's clothes were frumpy.",
    "llm_pred": "No — they were flashy.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Flashy",
    "final_answer": "flashy",
    "slm_pred": "They were warmhearted, professional, and passionate.",
    "llm_pred": "flashy",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No.",
    "llm_pred": "No, two years.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No—he was a Chicago police officer before acting.",
    "slm_pred": "No, he was a police officer first.",
    "llm_pred": "No—he was a Chicago police officer before acting.",
    "perf_chosen": 0.0,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "ef7bd8262d5999586c70f3285b3999f96becfae2c1ac055f805bd7939f4ad52a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A cop",
    "final_answer": "A police officer in Chicago.",
    "slm_pred": "A police officer in Chicago.",
    "llm_pred": "a Chicago police officer",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "school",
    "final_answer": "school.",
    "slm_pred": "school.",
    "llm_pred": "school",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No, Quinton lives closer to the bus stop.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "go to Quentin's house",
    "final_answer": "They walk home from the bus stop for cookies and milk.",
    "slm_pred": "They walk home from the bus stop for cookies and milk.",
    "llm_pred": "have cookies and milk",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No, he can’t eat cheese.",
    "slm_pred": "No.",
    "llm_pred": "No, he can’t eat cheese.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 1.0,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No, they play after their homework.",
    "slm_pred": "It is not explicitly stated.",
    "llm_pred": "No, they play after their homework.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.0,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "story time",
    "final_answer": "Story time which was right before bedtime.",
    "slm_pred": "Story time which was right before bedtime.",
    "llm_pred": "story time",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "right before bedtime",
    "final_answer": "right before bedtime",
    "slm_pred": "Tomorrow.",
    "llm_pred": "right before bedtime",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no one answered",
    "final_answer": "No one answered.",
    "slm_pred": "No one answered.",
    "llm_pred": "No one answered.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, it was not in the driveway.",
    "slm_pred": "Yes, it was not in the driveway.",
    "llm_pred": "The answer is: No; it wasn’t in the driveway.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The story doesn't explicitly state that she missed the bus, but rather that she hurried off to make it in time.",
    "slm_pred": "The story doesn't explicitly state that she missed the bus, but rather that she hurried off to make it in time.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "that she was upset",
    "final_answer": "Kendra being upset and Quinton not being with her that morning.",
    "slm_pred": "Kendra being upset and Quinton not being with her that morning.",
    "llm_pred": "that she was upset and Quinton wasn’t with her",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "She told him about Quinton not being at the bus stop.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "everything would be okay",
    "final_answer": "He said that he was sure that everything would be okay.",
    "slm_pred": "He said that he was sure that everything would be okay.",
    "llm_pred": "that everything would be okay",
    "perf_chosen": 0.5333333333,
    "perf_slm": 0.5333333333,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "her teacher",
    "final_answer": "Her teacher.",
    "slm_pred": "Her teacher.",
    "llm_pred": "her teacher",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No, he smiled.",
    "slm_pred": "No, the bus driver said he was sure that everything would be okay, and Kendra's teacher smiled when she told him what happened.",
    "llm_pred": "No, he smiled.",
    "perf_chosen": 0.5,
    "perf_slm": 0.0833333333,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Quinton's mother",
    "final_answer": "Quinton's mother.",
    "slm_pred": "Quinton's mother.",
    "llm_pred": "Quinton’s mother",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "to the dentist",
    "final_answer": "The dentist.",
    "slm_pred": "The dentist.",
    "llm_pred": "the dentist",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "2bb4c8622a4a9718ab0df549b74565f566bab46925b98f96fee6bb354f967b7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "after lunch",
    "slm_pred": "after lunch and the next day.",
    "llm_pred": "after lunch",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "five",
    "final_answer": "Five.",
    "slm_pred": "Five.",
    "llm_pred": "Five",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "New York City",
    "final_answer": "The answer is New York City.",
    "slm_pred": "The answer is New York City.",
    "llm_pred": "New York City",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "New York",
    "final_answer": "New York",
    "slm_pred": "The southernmost part of both the city and state of New York.",
    "llm_pred": "New York",
    "perf_chosen": 1.0,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "one of the five boroughs of New York City.",
    "slm_pred": "No.",
    "llm_pred": "one of the five boroughs of New York City.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "In the southwest of the city",
    "final_answer": "New York City, in the U.S. state of New York.",
    "slm_pred": "The answer is Staten Island.",
    "llm_pred": "New York City, in the U.S. state of New York.",
    "perf_chosen": 0.5,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Arthur Kill and the Kill Van Kull",
    "final_answer": "The Arthur Kill and the Kill Van Kull.",
    "slm_pred": "The Arthur Kill and the Kill Van Kull.",
    "llm_pred": "the Arthur Kill and the Kill Van Kull",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.9333333333,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "476,015",
    "final_answer": "476,015.",
    "slm_pred": "476,015.",
    "llm_pred": "476,015",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, it is the least populated of the boroughs.",
    "slm_pred": "No, it is the least populated of the boroughs.",
    "llm_pred": "No, it is the least populated.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "non-Hispanic White",
    "final_answer": "Non-Hispanic White.",
    "slm_pred": "Non-Hispanic White.",
    "llm_pred": "non-Hispanic White",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the forgotten borough",
    "final_answer": "The forgotten borough.",
    "slm_pred": "The forgotten borough.",
    "llm_pred": "the forgotten borough",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "because the inhabitants feel neglected by the city government",
    "final_answer": "because residents feel neglected by the city government",
    "slm_pred": "The answer is Staten Island feels neglected by the city government.",
    "llm_pred": "because residents feel neglected by the city government",
    "perf_chosen": 0.8235294118,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.8235294118,
    "cost": 25.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "North Shore",
    "final_answer": "The North Shore—especially the neighborhoods of St. George, Tompkinsville, Clifton, and Stapleton.",
    "slm_pred": "The North Shore—especially the neighborhoods of St. George, Tompkinsville, Clifton, and Stapleton.",
    "llm_pred": "The North Shore",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "b11c73dc8eca88bce2fd00e2ddeba682f99caef8f246280fc98931c872c92b16",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "St. George, Tompkinsville, Clifton, and Stapleton",
    "final_answer": "The answer is St. George, Tompkinsville, Clifton, and Stapleton.",
    "slm_pred": "The answer is St. George, Tompkinsville, Clifton, and Stapleton.",
    "llm_pred": "St. George, Tompkinsville, Clifton, and Stapleton.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Five in the morning",
    "final_answer": "Five in the morning.",
    "slm_pred": "Five in the morning.",
    "llm_pred": "at five in the morning",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Weather forecast",
    "final_answer": "The weather forecast.",
    "slm_pred": "The weather forecast.",
    "llm_pred": "the weather forecast",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, a violent storm was coming.",
    "slm_pred": "Yes, a violent storm was coming.",
    "llm_pred": "Yes, a violent storm was coming.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Firefighter",
    "final_answer": "A firefighter.",
    "slm_pred": "A firefighter.",
    "llm_pred": "The answer is firefighter.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, they did.",
    "slm_pred": "Yes, they did.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Flashlight",
    "final_answer": "A flashlight.",
    "slm_pred": "A flashlight.",
    "llm_pred": "a flashlight",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "R.J.",
    "final_answer": "R.J.",
    "slm_pred": "R.J.",
    "llm_pred": "R.J.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Joel",
    "final_answer": "Joel.",
    "slm_pred": "Joel.",
    "llm_pred": "Joel",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Glass, wood, plaster, and maybe the washing machine",
    "final_answer": "Glass, wood, and plaster.",
    "slm_pred": "Glass, wood, and plaster.",
    "llm_pred": "glass, wood, and plaster",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No — he was not badly hurt.",
    "slm_pred": "Not badly.",
    "llm_pred": "No — he was not badly hurt.",
    "perf_chosen": 0.25,
    "perf_slm": 0.0,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Eppes",
    "final_answer": "Reginald Eppes",
    "slm_pred": "Reginald Eppes.",
    "llm_pred": "Reginald Eppes",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "b03e8fc96ad437f35f50726d4593db562bf5a4d804ad12d4b22b2ba3ebc712a0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The flashlight",
    "final_answer": "The light of his father's flashlight.",
    "slm_pred": "The light of his father's flashlight.",
    "llm_pred": "the light of his father's flashlight",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Gary Giordano",
    "final_answer": "The Maryland home of Gary Giordano.",
    "slm_pred": "The Maryland home of Gary Giordano.",
    "llm_pred": "Gary Giordano’s",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Gaithersburg",
    "final_answer": "Gaithersburg.",
    "slm_pred": "Gaithersburg.",
    "llm_pred": "Gaithersburg",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Montgomery County",
    "final_answer": "Montgomery County",
    "slm_pred": "Montgomery County.",
    "llm_pred": "Montgomery County",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Maryland",
    "final_answer": "Maryland.",
    "slm_pred": "Maryland.",
    "llm_pred": "Maryland",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Aruban jail",
    "final_answer": "He is being held in an Aruban jail.",
    "slm_pred": "He is being held in an Aruban jail.",
    "llm_pred": "in an Aruban jail",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "suspect in the recent disappearance of an American woman",
    "final_answer": "The search is occurring in the Gaithersburg residence of Gary Giordano, who is currently being held in an Aruban jail, because he is a suspect in the recent disappearance of an American woman in Aruba.",
    "slm_pred": "The search is occurring in the Gaithersburg residence of Gary Giordano, who is currently being held in an Aruban jail, because he is a suspect in the recent disappearance of an American woman in Aruba.",
    "llm_pred": "the active investigation",
    "perf_chosen": 0.4090909091,
    "perf_slm": 0.4090909091,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "FBI",
    "final_answer": "The FBI.",
    "slm_pred": "The FBI.",
    "llm_pred": "the FBI",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "15",
    "final_answer": "About 15.",
    "slm_pred": "About 15.",
    "llm_pred": "about 15",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Aruban Solicitor General Taco Stein",
    "final_answer": "Aruban Solicitor General Taco Stein.",
    "slm_pred": "Aruban Solicitor General Taco Stein.",
    "llm_pred": "Taco Stein",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Monday",
    "final_answer": "Monday.",
    "slm_pred": "Monday.",
    "llm_pred": "Monday",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "at least eight more days",
    "final_answer": "At least eight more days.",
    "slm_pred": "At least eight more days.",
    "llm_pred": "at least eight more days",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Robyn Gardne",
    "final_answer": "Robyn Gardner.",
    "slm_pred": "Robyn Gardner.",
    "llm_pred": "Robyn Gardner",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ast seen near Baby Beach",
    "final_answer": "The answer is Gary Giordano's home in Gaithersburg, Maryland.",
    "slm_pred": "The answer is Gary Giordano's home in Gaithersburg, Maryland.",
    "llm_pred": "Gaithersburg, Maryland",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "snorkeling",
    "final_answer": "snorkeling",
    "slm_pred": "Snorkeling.",
    "llm_pred": "snorkeling",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Giordano",
    "final_answer": "Robyn Gardner",
    "slm_pred": "The answer is Gary Giordano.",
    "llm_pred": "Robyn Gardner",
    "perf_chosen": 0.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No, Gardner was nowhere to be found",
    "final_answer": "No",
    "slm_pred": "No.",
    "llm_pred": "No",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "locals say is not a popular snorkeling spot.",
    "final_answer": "It is not a popular snorkeling spot.",
    "slm_pred": "It is not a popular snorkeling spot.",
    "llm_pred": "No—it's not a popular snorkeling spot.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.7142857143,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "50",
    "final_answer": "50 years old.",
    "slm_pred": "50 years old.",
    "llm_pred": "50 years old",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "August 5",
    "final_answer": "August 5.",
    "slm_pred": "August 5.",
    "llm_pred": "August 5",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33b240b7e319629d6283ea10e8d1495994e36be96a4f246ef480891bf0303391",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "2, Giordano told authorities that he had been snorkeling with Gardner",
    "final_answer": "Two people, Gary Giordano and Robyn Gardner.",
    "slm_pred": "Two people, Gary Giordano and Robyn Gardner.",
    "llm_pred": "Two",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Great Britain",
    "final_answer": "Great Britain.",
    "slm_pred": "Great Britain.",
    "llm_pred": "Great Britain",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "India.",
    "final_answer": "India.",
    "slm_pred": "India.",
    "llm_pred": "India.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "may be 30 feet tall",
    "final_answer": "In the wild, tea plants may be 30 feet tall.",
    "slm_pred": "In the wild, tea plants may be 30 feet tall.",
    "llm_pred": "30 feet tall",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "prune it",
    "final_answer": "steamed.",
    "slm_pred": "steamed.",
    "llm_pred": "steamed it",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "may prevent heart disease.",
    "final_answer": "Prevent heart disease.",
    "slm_pred": "Prevent heart disease.",
    "llm_pred": "prevent heart disease",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by accident",
    "final_answer": "Leaves from a wild tea tree fell into a hot water pot.",
    "slm_pred": "Leaves from a wild tea tree fell into a hot water pot.",
    "llm_pred": "by accident—wild tea leaves fell into hot water",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Shen Nong",
    "final_answer": "Shen Nong.",
    "slm_pred": "Shen Nong.",
    "llm_pred": "Shen Nong",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "about 2737 B.C",
    "final_answer": "The next cup.",
    "slm_pred": "The next cup.",
    "llm_pred": "about 2737 B.C.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, he liked it.",
    "slm_pred": "Yes, he liked it.",
    "llm_pred": "Yes—he liked it.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d66da2f27102228d63a6e7bd8449085a3cfec9bcea9fdaa429a6bafab04d9c7b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "He liked it.",
    "slm_pred": "His body had good digestion.",
    "llm_pred": "He liked it.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Der Spiegel",
    "final_answer": "The German news outlet Der Spiegel.",
    "slm_pred": "The German news outlet Der Spiegel.",
    "llm_pred": "Der Spiegel",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Germany",
    "final_answer": "Afghanistan.",
    "slm_pred": "Afghanistan.",
    "llm_pred": "Germany",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "posing over the bodies of dead Afghans",
    "final_answer": "Posing over the bodies of dead Afghans.",
    "slm_pred": "Posing over the bodies of dead Afghans.",
    "llm_pred": "posing over the bodies of dead Afghans",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "bloody",
    "final_answer": "Bloody and sprawled over a patch of sand and grass.",
    "slm_pred": "Bloody and sprawled over a patch of sand and grass.",
    "llm_pred": "bloody",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "propped up, back to back",
    "final_answer": "What appears to be two bodies propped up, back to back, against a post in front of a military vehicle.",
    "slm_pred": "What appears to be two bodies propped up, back to back, against a post in front of a military vehicle.",
    "llm_pred": "two bodies propped up back-to-back against a post",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "military vehicle.",
    "final_answer": "A patch of sand and grass.",
    "slm_pred": "A patch of sand and grass.",
    "llm_pred": "a patch of sand and grass",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "taking or retaining individual souvenirs or trophies",
    "final_answer": "Taking or retaining individual souvenirs or trophies.",
    "slm_pred": "Taking or retaining individual souvenirs or trophies.",
    "llm_pred": "souvenirs or trophies",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jeremy Morlock",
    "final_answer": "Spc. Jeremy Morlock.",
    "slm_pred": "Spc. Jeremy Morlock.",
    "llm_pred": "Jeremy Morlock",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Pfc. Andrew Holmes",
    "final_answer": "Andrew Holmes",
    "slm_pred": "The other two U.S. soldiers involved in the incident.",
    "llm_pred": "Andrew Holmes",
    "perf_chosen": 0.8,
    "perf_slm": 0.0,
    "perf_llm": 0.8,
    "cost": 25.0
  },
  {
    "id": "16c44ceed2cb985089bf32ee222c589634bc3df42e24040af2388f1bd48102fd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Holmes is charged with the premeditated deaths of three civilians",
    "final_answer": "premeditated deaths of three civilians, possessing a dismembered human finger, wrongfully possessing photographs of human casualties, and smoking hashish.",
    "slm_pred": "premeditated deaths of three civilians, possessing a dismembered human finger, wrongfully possessing photographs of human casualties, and smoking hashish.",
    "llm_pred": "the premeditated deaths of three civilians",
    "perf_chosen": 0.34482758620000004,
    "perf_slm": 0.34482758620000004,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Floyd Mayweather and Manny Pacquiao",
    "final_answer": "Floyd Mayweather and Manny Pacquiao.",
    "slm_pred": "Floyd Mayweather and Manny Pacquiao.",
    "llm_pred": "Floyd Mayweather and Manny Pacquiao",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1 is the money man",
    "final_answer": "\"The Money Man\" or \"TBE\" (The Best Ever).",
    "slm_pred": "\"The Money Man\" or \"TBE\" (The Best Ever).",
    "llm_pred": "The Money Man",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "TBE",
    "final_answer": "The other boxer, Manny Pacquiao.",
    "slm_pred": "The other boxer, Manny Pacquiao.",
    "llm_pred": "his formidable opponent",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Best Ever",
    "final_answer": "TBE and The Money Man stand for \"The Best Ever\" and \"The Money Man\" respectively, and \"The Best Ever\" is also shortened to \"TBE\"",
    "slm_pred": "TBE and The Money Man stand for \"The Best Ever\" and \"The Money Man\" respectively, and \"The Best Ever\" is also shortened to \"TBE\"",
    "llm_pred": "The Best Ever",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Money Team,",
    "final_answer": "The Money Team.",
    "slm_pred": "The Money Team.",
    "llm_pred": "The Money Team",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a boxing promoter",
    "final_answer": "The CEO of Team Sauerland.",
    "slm_pred": "The CEO of Team Sauerland.",
    "llm_pred": "a boxing promoter (CEO of Team Sauerland)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "over 45 boxers.",
    "final_answer": "Over 45 boxers.",
    "slm_pred": "Over 45 boxers.",
    "llm_pred": "over 45",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "$300 million pending viewership numbers",
    "final_answer": "$300 million.",
    "slm_pred": "$300 million.",
    "llm_pred": "$300 million",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "38",
    "final_answer": "Thirty-eight years old.",
    "slm_pred": "Thirty-eight years old.",
    "llm_pred": "38 years old",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "a0938f69de2b6c6ff6f720acab396731f0eaa9de4c24ab2dac3083520a69bdd6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "just that it has bible references and shows him enjoying gos[e; singing with his wife",
    "final_answer": "He's the hope of a nation.",
    "slm_pred": "He's the hope of a nation.",
    "llm_pred": "the hope of a nation",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "OCLC",
    "final_answer": "The history of OCLC.",
    "slm_pred": "The history of OCLC.",
    "llm_pred": "OCLC (the Online Computer Library Center)",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Online Computer Library Center",
    "final_answer": "OCLC Online Computer Library Center.",
    "slm_pred": "OCLC Online Computer Library Center.",
    "llm_pred": "Online Computer Library Center",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "1967",
    "final_answer": "1967",
    "slm_pred": "July 5, 1967.",
    "llm_pred": "1967",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, the Ohio College Library Center was founded in 1967.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.1818181818,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Ohio",
    "final_answer": "on the campus of the Ohio State University",
    "slm_pred": "The campus of the Ohio State University",
    "llm_pred": "on the campus of the Ohio State University",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.25,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ohio State University",
    "final_answer": "The campus of the Ohio State University.",
    "slm_pred": "The campus of the Ohio State University.",
    "llm_pred": "the campus of Ohio State University",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Frederick G. Kilgour",
    "final_answer": "Frederick G. Kilgour",
    "slm_pred": "a former Yale University medical school librarian.",
    "llm_pred": "Frederick G. Kilgour",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He is not",
    "final_answer": "The answer is not specified in the story.",
    "slm_pred": "The answer is not specified in the story.",
    "llm_pred": "No",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "medical school librarian",
    "final_answer": "a former Yale University medical school librarian.",
    "slm_pred": "a former Yale University medical school librarian.",
    "llm_pred": "a medical school librarian",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WorldCat",
    "final_answer": "WorldCat, the largest online public access catalog.",
    "slm_pred": "WorldCat, the largest online public access catalog.",
    "llm_pred": "WorldCat",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "July 5, 1967",
    "final_answer": "July 5, 1967.",
    "slm_pred": "July 5, 1967.",
    "llm_pred": "July 5, 1967",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Alden Library",
    "final_answer": "The Alden Library at Ohio University.",
    "slm_pred": "The Alden Library at Ohio University.",
    "llm_pred": "Alden Library at Ohio University",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "online cataloging",
    "final_answer": "The site could merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library.",
    "slm_pred": "The site could merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library.",
    "llm_pred": "electronically merge Ohio libraries’ catalogs",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "August 26, 1971",
    "final_answer": "August 26, 1971",
    "slm_pred": "July 5, 1967",
    "llm_pred": "August 26, 1971",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6523db05cf900fafdac664b170486916e3242251a1773ffc17f9542c402a2374",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, it was the first occurrence of online cataloging by any library worldwide.",
    "slm_pred": "No, it was the first occurrence of online cataloging by any library worldwide.",
    "llm_pred": "No.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "Tom initially suggested not going inside, and the group seemed to agree with that decision.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "They bought flowers.",
    "final_answer": "the flowers",
    "slm_pred": "A bouquet of artificial flowers.",
    "llm_pred": "the flowers",
    "perf_chosen": 0.4,
    "perf_slm": 0.25,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "It's $15.",
    "final_answer": "$15",
    "slm_pred": "$15.",
    "llm_pred": "$15",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, they bought the flowers.",
    "slm_pred": "Yes, they bought the flowers.",
    "llm_pred": "Yes, the flowers.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "It doesn't look good.",
    "final_answer": "Because it will soon be summer.",
    "slm_pred": "The answer is Because they are not suitable for Mum's birthday.",
    "llm_pred": "Because it will soon be summer.",
    "perf_chosen": 0.2,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "summer",
    "final_answer": "The answer is summer.",
    "slm_pred": "The answer is summer.",
    "llm_pred": "summer",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "$15",
    "final_answer": "$15",
    "slm_pred": "$15.",
    "llm_pred": "$15",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The answer is no.",
    "slm_pred": "The answer is no.",
    "llm_pred": "No.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a pen",
    "final_answer": "The price of a pen.",
    "slm_pred": "The price of a pen.",
    "llm_pred": "a pen",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "She already has two blouses",
    "final_answer": "They decided a blouse would be more suitable for summer.",
    "slm_pred": "They decided a blouse would be more suitable for summer.",
    "llm_pred": "Because she already had two blouses.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Mother's birthday",
    "final_answer": "Mother's birthday.",
    "slm_pred": "Mother's birthday.",
    "llm_pred": "their mother’s birthday",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e2ba729314a3304ce5db8ba1d4e4d025123d712c4529f7dc38b3345213ab983c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "at least $500",
    "final_answer": "At least $500.",
    "slm_pred": "At least $500.",
    "llm_pred": "at least $500",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by a big lake by the woods",
    "final_answer": "by a big lake by the woods.",
    "slm_pred": "by a big lake by the woods.",
    "llm_pred": "by a big lake by the woods",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "mice",
    "final_answer": "the other side of the lake.",
    "slm_pred": "the other side of the lake.",
    "llm_pred": "two little mice looking at the water",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "toy boats",
    "final_answer": "toy boats.",
    "slm_pred": "toy boats.",
    "llm_pred": "plenty of toy boats",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Mary and Steve.",
    "slm_pred": "Mary and Steve.",
    "llm_pred": "Yes, Mary and Steve.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Mary and Steve",
    "final_answer": "very sad",
    "slm_pred": "The answer is Tommy, Mary, Steve, and the big dog.",
    "llm_pred": "very sad",
    "perf_chosen": 0.0,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "his house",
    "final_answer": "from his house",
    "slm_pred": "He had toy boats at home.",
    "llm_pred": "from his house",
    "perf_chosen": 0.8,
    "perf_slm": 0.0,
    "perf_llm": 0.8,
    "cost": 25.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "climbed on",
    "final_answer": "Sailed across the lake on Tommy’s toy boat.",
    "slm_pred": "They borrow a toy boat from Tommy to explore the other side of the lake.",
    "llm_pred": "Sailed across the lake on Tommy’s toy boat.",
    "perf_chosen": 0.2,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "swimming and splashing",
    "final_answer": "They were swimming and splashing in the water.",
    "slm_pred": "They were swimming and splashing in the water.",
    "llm_pred": "swimming and splashing in the water",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "threw a ball into the water",
    "final_answer": "He threw a ball into the water.",
    "slm_pred": "He threw a ball into the water.",
    "llm_pred": "threw a ball into the water.",
    "perf_chosen": 0.9230769231,
    "perf_slm": 0.9230769231,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7097b1dd4be4d114f6409c1dcedfeea0420eadc621bc553074c524695268c66b",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "got very wet",
    "final_answer": "splashed into the water to fetch the ball",
    "slm_pred": "The do refers to the big dog, so the big dog fetched the ball.",
    "llm_pred": "splashed into the water to fetch the ball",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The hospital had been bombed.",
    "final_answer": "It was bombed.",
    "slm_pred": "It was bombed.",
    "llm_pred": "It was bombed.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No.",
    "final_answer": "No, he was separated from his unit.",
    "slm_pred": "No, he was separated from his unit.",
    "llm_pred": "No—he had been separated from his unit.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Germany",
    "final_answer": "Western Germany",
    "slm_pred": "Western Germany",
    "llm_pred": "Germany",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Eastern Germany at the time of his hospital stay.",
    "final_answer": "Western Germany.",
    "slm_pred": "Western Germany.",
    "llm_pred": "the Western part",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Western Germany",
    "final_answer": "The hospital.",
    "slm_pred": "The hospital.",
    "llm_pred": "Western Germany",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No, he was a passenger in a taxi, or possibly he drove there, as he is described as a taxi driver.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0909090909,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes—towards the end of the war.",
    "slm_pred": "Yes, the war was nearly over by this time.",
    "llm_pred": "Yes—towards the end of the war.",
    "perf_chosen": 0.0,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "It had been bombed up.",
    "slm_pred": "It had been bombed up.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No, just guessed.",
    "final_answer": "No—he only assumed they’d been killed.",
    "slm_pred": "No, he didn't know.",
    "llm_pred": "No—he only assumed they’d been killed.",
    "perf_chosen": 0.0,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Hans settled down in a village fifty miles away.",
    "final_answer": "He settled down in a village fifty miles away.",
    "slm_pred": "He settled down in a village fifty miles away.",
    "llm_pred": "settle down in a village fifty miles away",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes, for twenty years",
    "final_answer": "Yes, he had remained ever since.",
    "slm_pred": "Yes, he had remained ever since.",
    "llm_pred": "Yes, for twenty years.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a workman",
    "final_answer": "a taxi driver",
    "slm_pred": "A workman.",
    "llm_pred": "a taxi driver",
    "perf_chosen": 0.4,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he does.",
    "slm_pred": "Yes, he does.",
    "llm_pred": "Yes—Hans Bussman.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Hans Bussman",
    "final_answer": "Franz Bussman.",
    "slm_pred": "Franz Bussman.",
    "llm_pred": "The answer is Hans Bussman.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes, Franz does.",
    "final_answer": "Yes, he is a taxi-driver.",
    "slm_pred": "Yes, he is a taxi-driver.",
    "llm_pred": "Yes, he is a taxi driver.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "He assumed Hans was dead.",
    "final_answer": "He thought his brother had been killed in action.",
    "slm_pred": "All records of him had been destroyed.",
    "llm_pred": "He thought his brother had been killed in action.",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.0,
    "perf_llm": 0.1428571429,
    "cost": 25.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Mrs. Bussman",
    "final_answer": "Mrs. Bussman.",
    "slm_pred": "Mrs. Bussman.",
    "llm_pred": "Mrs. Bussman",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Franz laughed at the idea",
    "final_answer": "He laughed at the idea.",
    "slm_pred": "He laughed at the idea.",
    "llm_pred": "He laughed at the idea.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "5350b17b1e71ad0da2f995bba0080972d8fa8abe0b79afc7dae6418be2b32060",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, she did not give up.",
    "slm_pred": "No, she did not give up.",
    "llm_pred": "No.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The _Ariel_",
    "final_answer": "The Ariel.",
    "slm_pred": "The Ariel.",
    "llm_pred": "the Ariel",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Lagoon",
    "final_answer": "A colour-riotous lagoon.",
    "slm_pred": "A colour-riotous lagoon.",
    "llm_pred": "the colour-riotous lagoon",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, they were narrow.",
    "slm_pred": "No, they were narrow.",
    "llm_pred": "No—they were narrow.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Winters",
    "final_answer": "Captain Winters",
    "slm_pred": "Captain Winters.",
    "llm_pred": "Captain Winters",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "There is no mention of his hair color.",
    "slm_pred": "There is no mention of his hair color.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, they were not in a hurry.",
    "slm_pred": "No, they were not in a hurry.",
    "llm_pred": "No—they were in no hurry.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Malaita",
    "final_answer": "The leeward coast of Malaita.",
    "slm_pred": "The leeward coast of Malaita.",
    "llm_pred": "the leeward coast of Malaita",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Harley Kennan",
    "final_answer": "Harley Kennan",
    "slm_pred": "Harley Kennan argued to Villa that he must have a name.",
    "llm_pred": "Harley Kennan",
    "perf_chosen": 1.0,
    "perf_slm": 0.3076923077,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Villa",
    "final_answer": "Villa.",
    "slm_pred": "Villa.",
    "llm_pred": "Villa Kennan",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Arangi",
    "final_answer": "The Arangi.",
    "slm_pred": "The Arangi.",
    "llm_pred": "the Arangi",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Until they get back to Tulagi",
    "final_answer": "until they return to Tulagi and learn his real name",
    "slm_pred": "Until they get back to Tulagi.",
    "llm_pred": "until they return to Tulagi and learn his real name",
    "perf_chosen": 0.5,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Harley Kennan",
    "final_answer": "Harley Kennan",
    "slm_pred": "Harley Kennan.",
    "llm_pred": "Harley Kennan",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, she did not mention an airplane crash.",
    "slm_pred": "No, she did not mention an airplane crash.",
    "llm_pred": "No.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "'Mrs. Riggs'",
    "final_answer": "\"Benedict Arnold\"",
    "slm_pred": "\"Benedict Arnold\"",
    "llm_pred": "Mrs. Riggs",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "'Topsy'",
    "final_answer": "Mister Dog",
    "slm_pred": "Haman.",
    "llm_pred": "Mister Dog",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "'Mademoiselle de Maupin'",
    "final_answer": "Haman",
    "slm_pred": "The answer is \"Laddie.\"",
    "llm_pred": "Haman",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No, that's not mentioned.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The lagoon was \"coral-patched\".",
    "slm_pred": "The lagoon was \"coral-patched\".",
    "llm_pred": "Yes—coral‑patched throughout.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Every walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life.",
    "final_answer": "Every walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life.",
    "slm_pred": "Every walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life.",
    "llm_pred": "every walled inlet of the outer reef and every mangrove swamp of the mainland",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "d2f410d23a2c0beb7a11e24505ee3aa397af3259f13986784011e13202e49ff6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "Yes.",
    "slm_pred": "The text suggests that there \"looked promising of cannibal life\" but it does not confirm the presence of actual cannibals.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Brownie and Spotty",
    "final_answer": "Brownie and Spotty.",
    "slm_pred": "Brownie and Spotty.",
    "llm_pred": "Brownie and Spotty.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "every day",
    "final_answer": "every day.",
    "slm_pred": "every day.",
    "llm_pred": "every day",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "loved each other",
    "final_answer": "They loved each other.",
    "slm_pred": "They loved each other deeply.",
    "llm_pred": "They loved each other.",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.75,
    "perf_llm": 0.8571428571,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "worn a path through the grass of the field",
    "final_answer": "in the field between their houses",
    "slm_pred": "The field between their houses.",
    "llm_pred": "in the field between their houses",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.2666666667,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ted,",
    "final_answer": "Ted.",
    "slm_pred": "Ted.",
    "llm_pred": "Ted",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Brownie",
    "final_answer": "Brownie.",
    "slm_pred": "Brownie.",
    "llm_pred": "Brownie",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Not explicitly stated.",
    "slm_pred": "Not explicitly stated.",
    "llm_pred": "No, Spotty did not.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, initially.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a spot half a mile from the house",
    "final_answer": "A deserted spot half a mile from the house.",
    "slm_pred": "A deserted spot half a mile from the house.",
    "llm_pred": "a deserted spot half a mile from the house",
    "perf_chosen": 0.9411764706,
    "perf_slm": 0.9411764706,
    "perf_llm": 0.9411764706,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "brought him food",
    "final_answer": "brought him food",
    "slm_pred": "Spotty kept Brownie fed.",
    "llm_pred": "brought him food",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "protect him from other dangers",
    "final_answer": "He stayed with Brownie to protect him from hunger and other dangers.",
    "slm_pred": "He stayed with Brownie to protect him from hunger and other dangers.",
    "llm_pred": "brought his own food to feed Brownie",
    "perf_chosen": 0.5882352941,
    "perf_slm": 0.5882352941,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "keep his spirits up.",
    "final_answer": "No, there isn't one mentioned in the story.",
    "slm_pred": "No, there isn't one mentioned in the story.",
    "llm_pred": "Spotty",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, he wished he'd taken Spotty's earlier appeals seriously.",
    "slm_pred": "Yes, he wished he'd taken Spotty's earlier appeals seriously.",
    "llm_pred": "Yes—he wished he’d taken Spotty’s earlier appeals seriously.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": ". Spotty followed Ted about, barking insistently,",
    "final_answer": "followed Ted around, barking insistently",
    "slm_pred": "barking insistently and rushing toward an empty lot.",
    "llm_pred": "followed Ted around, barking insistently",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.7272727273,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, with veterinary treatment, he recovered.",
    "slm_pred": "Yes, with veterinary treatment, he recovered.",
    "llm_pred": "Yes—he was treated and recovered.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes—after Brownie recovered, they played together for many years.",
    "slm_pred": "Yes, they were.",
    "llm_pred": "Yes—after Brownie recovered, they played together for many years.",
    "perf_chosen": 0.0,
    "perf_slm": 0.5,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No — he was noticed missing in the evening.",
    "slm_pred": "The story does not explicitly state that Brownie disappeared at night.",
    "llm_pred": "No — he was noticed missing in the evening.",
    "perf_chosen": 0.2,
    "perf_slm": 0.0,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes, They went looking for him with no success",
    "final_answer": "Yes.",
    "slm_pred": "No, they did not hunt him.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.2,
    "perf_slm": 0.4,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No.",
    "slm_pred": "No, he wasn't.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They were busy with their own lives,",
    "final_answer": "They were busy with their own lives.",
    "slm_pred": "They were busy with their own lives.",
    "llm_pred": "Because they were busy with their own lives.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.9333333333,
    "cost": 5.0
  },
  {
    "id": "3eabb23116fde995461ad05d426881539ec5c32ef1b5dd2e439afa587ec880ad",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Follow me! It's urgent!\"",
    "final_answer": "\"Follow me, it's urgent.\"",
    "slm_pred": "\"Follow me, it's urgent.\"",
    "llm_pred": "Follow me — it’s urgent!",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A girl and a dog.",
    "final_answer": "A young girl's unexpected encounter with a friendly bear in the woods.",
    "slm_pred": "A young girl's unexpected encounter with a friendly bear in the woods.",
    "llm_pred": "a young girl and her dog meeting a friendly bear in the woods",
    "perf_chosen": 0.23529411760000002,
    "perf_slm": 0.23529411760000002,
    "perf_llm": 0.5555555556,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Set on on a trip",
    "final_answer": "Taking a trip into the woods.",
    "slm_pred": "Taking a trip into the woods.",
    "llm_pred": "taking a trip into the woods",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the woods",
    "final_answer": "The woods.",
    "slm_pred": "The woods.",
    "llm_pred": "The answer is in the woods.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "scared",
    "final_answer": "The girl was a little scared.",
    "slm_pred": "The girl was a little scared.",
    "llm_pred": "a little scared",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "He wasn't",
    "final_answer": "acting very interested in the bushes ahead",
    "slm_pred": "The dog remained steadfast and curious throughout the encounter.",
    "llm_pred": "acting very interested in the bushes ahead",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "He was interested",
    "final_answer": "friendly",
    "slm_pred": "The bear seemed nonchalant and friendly.",
    "llm_pred": "friendly",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "what was in the bushes",
    "final_answer": "The woods.",
    "slm_pred": "The woods.",
    "llm_pred": "The answer is the bushes.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a bear",
    "final_answer": "a small brown bear",
    "slm_pred": "The bear was sleeping.",
    "llm_pred": "a small brown bear",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "rested in the bushes",
    "final_answer": "went back to sleep",
    "slm_pred": "The bear rested its head on its bear paws and went back to sleep.",
    "llm_pred": "went back to sleep",
    "perf_chosen": 0.0,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Not really",
    "final_answer": "The bear was not surprised.",
    "slm_pred": "The bear was not surprised.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "surprised",
    "final_answer": "the girl was scared, the dog curious",
    "slm_pred": "Initially scared, but later relieved.",
    "llm_pred": "the girl was scared, the dog curious",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "not surprised",
    "final_answer": "The bear was not surprised and seemed friendly.",
    "slm_pred": "The bear was not surprised and seemed friendly.",
    "llm_pred": "calmly looked up and went back to sleep",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Looked at the girl",
    "final_answer": "went back to sleep",
    "slm_pred": "He rested his head on his bear paws and went back to sleep.",
    "llm_pred": "went back to sleep",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He smiled",
    "final_answer": "No, he seemed friendly.",
    "slm_pred": "No, he seemed friendly.",
    "llm_pred": "The answer is: No, he was friendly.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no one",
    "final_answer": "No one.",
    "slm_pred": "No one.",
    "llm_pred": "The answer is no one.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, the woods were dark and cold.",
    "slm_pred": "No, the woods were dark and cold.",
    "llm_pred": "no; it was dark and cold.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "dark and cold",
    "final_answer": "dark and cold",
    "slm_pred": "Dark and cold.",
    "llm_pred": "dark and cold",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she was a little scared and was thinking of turning back.",
    "slm_pred": "Yes, she was a little scared and was thinking of turning back.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e8692eed2fc5c837326ae742f2e2cfac2abc009700c388bf3f44f50671dc40f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "The answer is: No, she didn’t.",
    "slm_pred": "make it out of the woods.",
    "llm_pred": "The answer is: No, she didn’t.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.0,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "10-year-old boy fatally shot his father",
    "final_answer": "a 10-year-old boy fatally shot his father",
    "slm_pred": "A 10-year-old boy fatally shot his father in the front seat of a SUV.",
    "llm_pred": "a 10-year-old boy fatally shot his father",
    "perf_chosen": 0.9230769231,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.9230769231,
    "cost": 25.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "in the front seat of a SUV",
    "final_answer": "walking in the neighborhood",
    "slm_pred": "The front seat of a SUV.",
    "llm_pred": "walking in the neighborhood",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.9230769231,
    "perf_llm": 0.3636363636,
    "cost": 25.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "The shooting took place outside his mom's house.",
    "slm_pred": "The shooting took place outside his mom's house.",
    "llm_pred": "At his mother's house.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "outside the home of Lohstroh's ex-wife",
    "final_answer": "The answer is a cul-de-sac in the 1700 block of Cedar Cove Court in Houston.",
    "slm_pred": "The answer is a cul-de-sac in the 1700 block of Cedar Cove Court in Houston.",
    "llm_pred": "on a cul-de-sac in the 1700 block of Cedar Cove Court",
    "perf_chosen": 0.1904761905,
    "perf_slm": 0.1904761905,
    "perf_llm": 0.23529411760000002,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Friday",
    "final_answer": "about 3 p.m. Friday",
    "slm_pred": "The incident happened about 3pm on a cul-de-sac in the 1700 block of Cedar Cove Court.",
    "llm_pred": "about 3 p.m. Friday",
    "perf_chosen": 0.4,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3pm",
    "final_answer": "About 3 pm.",
    "slm_pred": "About 3 pm.",
    "llm_pred": "about 3 p.m.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "belonged to the boy's mother",
    "final_answer": "The boy's mother's gun.",
    "slm_pred": "The boy's mother's gun.",
    "llm_pred": "the boy’s mother",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He exited the back of the vehicle and continued to fire at the car",
    "final_answer": "He exited the back of the vehicle and continued to fire at the car.",
    "slm_pred": "He exited the back of the vehicle and continued to fire at the car.",
    "llm_pred": "got out of the vehicle and kept shooting at the car",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.56,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No.",
    "llm_pred": "No—he died on the way to the hospital.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "at the University of Texas Medical Branch",
    "final_answer": "The University of Texas Medical Branch.",
    "slm_pred": "The University of Texas Medical Branch.",
    "llm_pred": "the University of Texas Medical Branch",
    "perf_chosen": 0.9230769231,
    "perf_slm": 0.9230769231,
    "perf_llm": 0.9230769231,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "inside the house",
    "final_answer": "inside the house with her 7-year-old son.",
    "slm_pred": "inside the house with her 7-year-old son.",
    "llm_pred": "inside the house",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "The 7-year-old brother.",
    "slm_pred": "The 7-year-old brother.",
    "llm_pred": "her 7-year-old son",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "81c56aa507af46cb92ecb2e747d94e2bf8bd9cf2ae6d159aae271924d8627d35",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the 7-year-old",
    "final_answer": "The answer is a 10-year-old boy.",
    "slm_pred": "The answer is a 10-year-old boy.",
    "llm_pred": "Rick James Lohstroh",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Fra Girolamo",
    "final_answer": "Savonarola.",
    "slm_pred": "Savonarola.",
    "llm_pred": "Savonarola",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Not in any heretical belief.",
    "slm_pred": "Not in any heretical belief.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "Yes.",
    "slm_pred": "No, his distinction from the clergy lay in his Christian life.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "for defying the pope by continuing reformist preaching against corruption despite orders to stop",
    "slm_pred": "For some weeks arrived from Rome, implying it was a decision made in Rome, but the reason for the excommunication is not explicitly stated in this passage.",
    "llm_pred": "for defying the pope by continuing reformist preaching against corruption despite orders to stop",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Romola",
    "final_answer": "Romola",
    "slm_pred": "Romola, as well as the crowd in the Duomo.",
    "llm_pred": "Romola",
    "perf_chosen": 1.0,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in the Duomo",
    "final_answer": "The Duomo.",
    "slm_pred": "The Duomo.",
    "llm_pred": "in the Duomo",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "June",
    "final_answer": "Far on in the hot days of June.",
    "slm_pred": "Far on in the hot days of June.",
    "llm_pred": "Far on in the hot days of June",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "for some weeks",
    "final_answer": "for some weeks",
    "slm_pred": "It was published for some weeks.",
    "llm_pred": "for some weeks",
    "perf_chosen": 1.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a sign from Baldassarre",
    "final_answer": "a sign from Baldassarre",
    "slm_pred": "either what she most desired or what she most dreaded, but it is not explicitly stated in the passage.",
    "llm_pred": "a sign from Baldassarre",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "sympathy with Savonarola",
    "final_answer": "a revelation of the suspected conspiracy",
    "slm_pred": "A sign from Baldassarre.",
    "llm_pred": "a revelation of the suspected conspiracy",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Plague",
    "final_answer": "the spreading Plague and Savonarola’s excommunication",
    "slm_pred": "Neither what Romola desired nor what she dreaded, but rather the spreading Plague and the Excommunication of Savonarola.",
    "llm_pred": "the spreading Plague and Savonarola’s excommunication",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.1052631579,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the Frate",
    "final_answer": "the Frate (Savonarola)",
    "slm_pred": "Savonarola.",
    "llm_pred": "the Frate (Savonarola)",
    "perf_chosen": 0.8,
    "perf_slm": 0.0,
    "perf_llm": 0.8,
    "cost": 25.0
  },
  {
    "id": "33ed18dd969df4735400f07054020ded7873161c38bb07d2c33a967baeb7231e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "Yes—she aided the plague‑stricken.",
    "slm_pred": "Not explicitly stated.",
    "llm_pred": "Yes—she aided the plague‑stricken.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she did.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "Yes, she helped her mom pack boxes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to Utah",
    "final_answer": "Utah",
    "slm_pred": "Utah.",
    "llm_pred": "Utah",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in Seattle",
    "final_answer": "On the road to Utah.",
    "slm_pred": "On the road to Utah.",
    "llm_pred": "a small apartment in Seattle",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "a small apartment in Seattle",
    "slm_pred": "The answer is no.",
    "llm_pred": "a small apartment in Seattle",
    "perf_chosen": 0.0,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in a small apartment",
    "final_answer": "a small apartment in Seattle.",
    "slm_pred": "a small apartment in Seattle.",
    "llm_pred": "a small apartment",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "Yes.",
    "slm_pred": "Initially sad, but eventually happy.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "her friends",
    "final_answer": "her friends.",
    "slm_pred": "her friends.",
    "llm_pred": "her friends",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, she did, particularly with her toys.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Two",
    "final_answer": "Two vehicles: a car and a large truck.",
    "slm_pred": "Two vehicles: a car and a large truck.",
    "llm_pred": "two",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the large truck",
    "final_answer": "A large truck.",
    "slm_pred": "A large truck.",
    "llm_pred": "the truck",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Jenny's mom",
    "final_answer": "Jenny’s mom.",
    "slm_pred": "Jenny's mom.",
    "llm_pred": "Jenny’s mom.",
    "perf_chosen": 0.5,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, they ate fast food on the way.",
    "slm_pred": "Yes, they ate fast food on the way.",
    "llm_pred": "Yes—lots of fast food.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yummy fast food",
    "final_answer": "Fast food.",
    "slm_pred": "Fast food.",
    "llm_pred": "fast food",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "she loved it",
    "final_answer": "She loved her new bedroom.",
    "slm_pred": "She loved her new bedroom.",
    "llm_pred": "Yes, she loved it.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a knock at the door",
    "final_answer": "a knock at the door",
    "slm_pred": "A knock at the door.",
    "llm_pred": "a knock at the door",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a little girl",
    "final_answer": "a little girl.",
    "slm_pred": "a little girl.",
    "llm_pred": "a little girl",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to play with Jenny",
    "final_answer": "to play with Jenny",
    "slm_pred": "to play.",
    "llm_pred": "to play with Jenny",
    "perf_chosen": 1.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f0efdcd9e2f9dd8ca6f515562fcd03b1470b8923ab18768b61d9952fa0a1ee2e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "she liked it",
    "final_answer": "she liked it",
    "slm_pred": "She started to like it.",
    "llm_pred": "she liked it",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "She  fell on a beginners' slope",
    "final_answer": "She died after suffering injuries in a ski accident.",
    "slm_pred": "She died after suffering injuries in a ski accident.",
    "llm_pred": "from injuries sustained in a skiing accident",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.1538461538,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Skiing.",
    "final_answer": "The answer is skiing.",
    "slm_pred": "The answer is skiing.",
    "llm_pred": "skiing",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Canada.",
    "final_answer": "Canada.",
    "slm_pred": "Canada.",
    "llm_pred": "Canada",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, the resort followed strict procedures.",
    "slm_pred": "Yes, the resort followed strict procedures.",
    "llm_pred": "Yes, they followed strict procedures.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "She did not.",
    "final_answer": "No.",
    "slm_pred": "No, initially she didn't show any visible sign of injury.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "About an hour.",
    "final_answer": "about an hour",
    "slm_pred": "About an hour after the fall.",
    "llm_pred": "about an hour",
    "perf_chosen": 1.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "She did not show signs.",
    "final_answer": "She did not show any visible signs of injury.",
    "slm_pred": "She did not show any visible signs of injury.",
    "llm_pred": "No—she showed no visible signs of injury.",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.1666666667,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A local hospital",
    "final_answer": "a local hospital.",
    "slm_pred": "a local hospital.",
    "llm_pred": "a local hospital",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Hopital du Sacre-Coeur",
    "final_answer": "Lenox Hill Hospital in New York City",
    "slm_pred": "The answer is Lenox Hill Hospital in New York City.",
    "llm_pred": "Lenox Hill Hospital in New York City",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "New York City.",
    "final_answer": "Lenox Hill Hospital in New York City.",
    "slm_pred": "Lenox Hill Hospital in New York City.",
    "llm_pred": "Lenox Hill Hospital in New York City",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "She was 45",
    "final_answer": "She was 45.",
    "slm_pred": "She was 45.",
    "llm_pred": "45",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A film star",
    "final_answer": "A film star and Tony-winning stage actress.",
    "slm_pred": "A film star and Tony-winning stage actress.",
    "llm_pred": "actress",
    "perf_chosen": 0.6000000000000001,
    "perf_slm": 0.6000000000000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, he was an actor.",
    "slm_pred": "Yes, he was an actor.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Liam Neeson",
    "final_answer": "Liam Neeson.",
    "slm_pred": "Liam Neeson.",
    "llm_pred": "The answer is Liam Neeson.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes.",
    "final_answer": "Yes — two sons.",
    "slm_pred": "She was the wife of actor Liam Neeson and mother of two sons.",
    "llm_pred": "Yes — two sons.",
    "perf_chosen": 0.4,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Sons",
    "final_answer": "boys",
    "slm_pred": "The answer is boys.",
    "llm_pred": "boys",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, a Tony award.",
    "slm_pred": "Yes, a Tony award.",
    "llm_pred": "Yes — a Tony Award.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Tony",
    "final_answer": "Mont Tremblant Ski Resort",
    "slm_pred": "The answer is a news article or a short obituary.",
    "llm_pred": "Mont Tremblant Ski Resort",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "The Redgrave acting family.",
    "slm_pred": "The Redgrave acting family.",
    "llm_pred": "Yes—the famed Redgrave acting family.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "30c127ae972228296fbee690f16a2455a1956e3e2a49cda9915e46efa04808bf",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Acting.",
    "final_answer": "privacy during this very difficult time",
    "slm_pred": "The answer is a doctor.",
    "llm_pred": "privacy during this very difficult time",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "bark",
    "final_answer": "his bark.",
    "slm_pred": "his bark.",
    "llm_pred": "his bark",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "three months",
    "final_answer": "Three months.",
    "slm_pred": "Three months.",
    "llm_pred": "The answer is three months.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "The story doesn't mention Peter's size.",
    "slm_pred": "The story doesn't mention Peter's size.",
    "llm_pred": "No, he wasn’t the smallest.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Sammie",
    "final_answer": "Sammie",
    "slm_pred": "Sammie and many other people.",
    "llm_pred": "Sammie",
    "perf_chosen": 1.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "golden puppy",
    "final_answer": "to snuggle with",
    "slm_pred": "The answer is snuggling.",
    "llm_pred": "to snuggle with",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, he was very sad and tired that day.",
    "slm_pred": "No, he was very sad and tired that day.",
    "llm_pred": "No—he was sad and tired.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "tired",
    "final_answer": "because they all picked smaller puppies",
    "slm_pred": "The answer is because Sammie was looking for a golden puppy, and Peter fit her criteria.",
    "llm_pred": "because they all picked smaller puppies",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, there is no indication of Sammie taking another puppy home.",
    "slm_pred": "No, there is no indication of Sammie taking another puppy home.",
    "llm_pred": "No.",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Peter",
    "final_answer": "The answer is Peter.",
    "slm_pred": "The answer is Peter.",
    "llm_pred": "Peter",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "find a person",
    "final_answer": "They found people to take home.",
    "slm_pred": "They found people to take home.",
    "llm_pred": "find a person",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Not mentioned.",
    "slm_pred": "Not mentioned.",
    "llm_pred": "Not stated",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "4e64e5b443ec0c4c19d03f3220f468c9fb7148eae8adb52f3d777efe5c9ee1a6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "sleep",
    "final_answer": "fell asleep in her arms",
    "slm_pred": "He fell right to sleep.",
    "llm_pred": "fell asleep in her arms",
    "perf_chosen": 0.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "an alien dog",
    "final_answer": "A cute alien dog.",
    "slm_pred": "A cute alien dog.",
    "llm_pred": "an alien dog",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "false",
    "final_answer": "False. The dog is cute with big eyes and green hair.",
    "slm_pred": "False. The dog is cute with big eyes and green hair.",
    "llm_pred": "The answer is false.",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "three",
    "final_answer": "3",
    "slm_pred": "Three: Dicky, Ti, and CJ7.",
    "llm_pred": "3",
    "perf_chosen": 0.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ti, Dicky and CJ7",
    "final_answer": "Ti, Dicky, and CJ7.",
    "slm_pred": "Ti, Dicky, and CJ7.",
    "llm_pred": "Ti and Dicky.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, they are movie characters.",
    "slm_pred": "No, they are movie characters.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a movie",
    "final_answer": "in the movie CJ7",
    "slm_pred": "In a poor life with his father in Hong Kong.",
    "llm_pred": "in the movie CJ7",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, the movie is Chinese.",
    "slm_pred": "No, the movie is Chinese.",
    "llm_pred": "The answer is: No, it’s Chinese.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "China.",
    "final_answer": "The answer is China.",
    "slm_pred": "The answer is China.",
    "llm_pred": "The answer is China.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ti and his son",
    "final_answer": "Ti and his son Dicky.",
    "slm_pred": "Ti and his son Dicky.",
    "llm_pred": "Ti and his son Dicky.",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, it lost its powers and became a doll.",
    "slm_pred": "No, it lost its powers and became a doll.",
    "llm_pred": "No, it turned into a doll.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a doll.",
    "final_answer": "a doll",
    "slm_pred": "A doll.",
    "llm_pred": "a doll",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "False",
    "final_answer": "False: the boy still wears the doll around his neck.",
    "slm_pred": "False: the boy still wears the doll around his neck.",
    "llm_pred": "False",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "around his neck",
    "final_answer": "around his neck",
    "slm_pred": "Around his neck.",
    "llm_pred": "around his neck",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, they are poor.",
    "slm_pred": "No, they are poor.",
    "llm_pred": "No, they are poor.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "It can talk and do magic .",
    "final_answer": "It can talk and do magic.",
    "slm_pred": "It can talk and do magic.",
    "llm_pred": "talk and do magic",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "The father found the ball in the trash.",
    "slm_pred": "The father found the ball in the trash.",
    "llm_pred": "No—he found it in the trash.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "he found it in the trash",
    "final_answer": "He found it in the trash.",
    "slm_pred": "He found it in the trash.",
    "llm_pred": "by finding it in the trash",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "January",
    "final_answer": "January.",
    "slm_pred": "January.",
    "llm_pred": "January",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "446950d4af23a910aafe0f2f31aaeb4d5ecfb7d46de83ccd46648cec3930a989",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "2008.",
    "final_answer": "The year is 2008.",
    "slm_pred": "The year is 2008.",
    "llm_pred": "2008",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "arts-and-crafts",
    "final_answer": "arts-and-crafts projects and recipes.",
    "slm_pred": "arts-and-crafts projects and recipes.",
    "llm_pred": "arts-and-crafts projects and recipes",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "recipes",
    "final_answer": "a shared color board for redecorating her daughter's bedroom.",
    "slm_pred": "a shared color board for redecorating her daughter's bedroom.",
    "llm_pred": "re-pin another person’s images",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Heather Neroy",
    "final_answer": "Heather Neroy.",
    "slm_pred": "Heather Neroy.",
    "llm_pred": "Heather Neroy",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Southern California",
    "final_answer": "Southern California.",
    "slm_pred": "Southern California.",
    "llm_pred": "Southern California",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "she's a stay-at-home mom",
    "final_answer": "a stay-at-home mom",
    "slm_pred": "She browses the internet for arts-and-crafts projects and recipes, and saves them using Pinterest.",
    "llm_pred": "a stay-at-home mom",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.0,
    "perf_llm": 0.8571428571,
    "cost": 25.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by copying the link",
    "final_answer": "By copying links and emailing them to herself.",
    "slm_pred": "By copying links and emailing them to herself.",
    "llm_pred": "by e‑mailing the links to herself",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "emailing it to herself.",
    "final_answer": "She began re-pinning others’ images and following users with similar tastes.",
    "slm_pred": "She soon became hooked on Pinterest.",
    "llm_pred": "She began re-pinning others’ images and following users with similar tastes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Not exactly the most efficient system.",
    "slm_pred": "Not exactly the most efficient system.",
    "llm_pred": "No, it wasn’t efficient.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Pinterest",
    "final_answer": "Pinterest.",
    "slm_pred": "Pinterest.",
    "llm_pred": "Pinterest",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Not at first.",
    "slm_pred": "Not at first.",
    "llm_pred": "No—not at first.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she quickly changed her mind.",
    "slm_pred": "Yes, she quickly changed her mind.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the filing system",
    "final_answer": "An online filing system.",
    "slm_pred": "An online filing system.",
    "llm_pred": "its online filing system for organizing ideas/images",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a Halloween board",
    "final_answer": "A Halloween board with costume ideas.",
    "slm_pred": "A Halloween board with costume ideas.",
    "llm_pred": "a Halloween board",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a shared color board",
    "final_answer": "file the e-mail in a folder for future reference",
    "slm_pred": "she read a blog post that gushed about a new website called Pinterest.",
    "llm_pred": "file the e-mail in a folder for future reference",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.1538461538,
    "cost": 25.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "redecorating her daughter's bedroom",
    "final_answer": "To organize her internet finds.",
    "slm_pred": "To organize her internet finds.",
    "llm_pred": "pinning costume ideas",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "follow other's boards",
    "final_answer": "follow each other's boards and \"re-pin\" another person's images.",
    "slm_pred": "follow each other's boards and \"re-pin\" another person's images.",
    "llm_pred": "follow each other’s boards and re-pin others’ images",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.3636363636,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "\"re-pin\" another person's images",
    "final_answer": "No.",
    "slm_pred": "No.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "She saved links by copying and pasting into an email, then filed them in a folder.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "as neat",
    "final_answer": "\"It's been really neat to see what other moms are pinning.\"",
    "slm_pred": "\"It's been really neat to see what other moms are pinning.\"",
    "llm_pred": "really neat",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "dac0b01c4518cdcb463c9d33917dfd0389cc22ccc645f629962180fb543895f6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "\"It's really a community sharing all these different ideas.\"",
    "slm_pred": "\"It's really a community sharing all these different ideas.\"",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Sir Earl",
    "final_answer": "the Earl of Pembroke",
    "slm_pred": "Sir Earl Pembroke.",
    "llm_pred": "the Earl of Pembroke",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.8,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Archie's traces.",
    "final_answer": "my traces",
    "slm_pred": "Archie's traces.",
    "llm_pred": "my traces",
    "perf_chosen": 0.5,
    "perf_slm": 1.0,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Makes him impatient to go forward",
    "final_answer": "get forward.",
    "slm_pred": "get forward.",
    "llm_pred": "to get forward",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he is.",
    "slm_pred": "Yes, he is.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Orders",
    "final_answer": "Orders to prepare to start with the hound in search of Bruce.",
    "slm_pred": "Orders to prepare to start with the hound in search of Bruce.",
    "llm_pred": "orders to prepare to start with the hound",
    "perf_chosen": 0.1538461538,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "3 days",
    "final_answer": "The third day.",
    "slm_pred": "The third day.",
    "llm_pred": "three days",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Search for Bruce",
    "final_answer": "Prepare to start with the hound in search of Bruce.",
    "slm_pred": "Prepare to start with the hound in search of Bruce.",
    "llm_pred": "to start with the hound in search of Bruce",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "with the hound, with the earl and a large party of men-at-arms",
    "final_answer": "The chain.",
    "slm_pred": "The chain.",
    "llm_pred": "a loud deep bay",
    "perf_chosen": 0.1428571429,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A traitor",
    "final_answer": "A traitor.",
    "slm_pred": "A traitor.",
    "llm_pred": "a traitor",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Where Bruce slept",
    "final_answer": "A traitor had just come in and told them where Bruce had slept the night before.",
    "slm_pred": "A traitor had just come in and told them where Bruce had slept the night before.",
    "llm_pred": "where Bruce had slept the night before",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Reluctant",
    "final_answer": "reluctant",
    "slm_pred": "Reluctant.",
    "llm_pred": "reluctant",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Hector",
    "final_answer": "Hector.",
    "slm_pred": "Hector.",
    "llm_pred": "Hector",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he did.",
    "slm_pred": "Yes, he did.",
    "llm_pred": "Yes—the dog.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, he would try to escape by fleetness of foot.",
    "slm_pred": "No, he would try to escape by fleetness of foot.",
    "llm_pred": "No—on foot.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "by foot",
    "final_answer": "By fleetness of foot.",
    "slm_pred": "By fleetness of foot.",
    "llm_pred": "by fleetness of foot",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "He has but slight hope.",
    "slm_pred": "He has but slight hope.",
    "llm_pred": "No—only slight hope.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Too many mounted men",
    "final_answer": "The sound of the horse's hoofs and the jingling of the equipments make him all the more impatient to get forward.",
    "slm_pred": "The sound of the horse's hoofs and the jingling of the equipments make him all the more impatient to get forward.",
    "llm_pred": "Because the horses’ noise makes the hound impatient and hard to restrain.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "The hut",
    "final_answer": "the hut",
    "slm_pred": "the hut where Bruce had slept the night before.",
    "llm_pred": "the hut",
    "perf_chosen": 1.0,
    "perf_slm": 0.3636363636,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Loud deep bay",
    "final_answer": "a loud deep bay.",
    "slm_pred": "a loud deep bay.",
    "llm_pred": "a loud deep bay",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "7ee058c60b529c381d91e2398587256ed4c88fdafda0adc1da7e67d6aa46c99d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Pembroke",
    "final_answer": "Pembroke's knights.",
    "slm_pred": "Pembroke's knights.",
    "llm_pred": "Pembroke’s",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the armed forces",
    "final_answer": "The armed forces.",
    "slm_pred": "The armed forces.",
    "llm_pred": "the armed forces",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Paraguay",
    "final_answer": "Paraguay.",
    "slm_pred": "Paraguay.",
    "llm_pred": "Paraguay",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Paraguay's history is filled with unstable transitions of power.",
    "slm_pred": "Paraguay's history is filled with unstable transitions of power.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the President",
    "final_answer": "President Fernando Lugo.",
    "slm_pred": "President Fernando Lugo.",
    "llm_pred": "President Fernando Lugo",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "military commanders",
    "final_answer": "The top military commanders of the army, air force, and navy.",
    "slm_pred": "The top military commanders of the army, air force, and navy.",
    "llm_pred": "the country’s army, air force, and navy leadership",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Brig. Gen. Bartolome Ramon Pineda Ortiz",
    "final_answer": "Brig. Gen. Bartolome Ramon Pineda Ortiz.",
    "slm_pred": "Brig. Gen. Bartolome Ramon Pineda Ortiz.",
    "llm_pred": "Brig. Gen. Bartolome Ramon Pineda Ortiz",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Cibar Benitez, commander of the armed forces.",
    "slm_pred": "Cibar Benitez, commander of the armed forces.",
    "llm_pred": "Yes—Cibar Benitez.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Cibar Benitez",
    "final_answer": "President Fernando Lugo",
    "slm_pred": "The answer is Fernando Lugo.",
    "llm_pred": "President Fernando Lugo",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Rear Adm. Egberto Emerito Orie Benegas",
    "final_answer": "Rear Adm. Egberto Emerito Orie Benegas.",
    "slm_pred": "Rear Adm. Egberto Emerito Orie Benegas.",
    "llm_pred": "Rear Adm. Egberto Emerito Orie Benegas",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, other changes would be forthcoming in the lower ranks.",
    "slm_pred": "Yes, other changes would be forthcoming in the lower ranks.",
    "llm_pred": "Yes, in the lower ranks.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Benitez",
    "final_answer": "Cibar Benitez",
    "slm_pred": "Cibar Benitez.",
    "llm_pred": "Cibar Benitez",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "1989",
    "final_answer": "1989",
    "slm_pred": "There hasn't been a coup since 1989.",
    "llm_pred": "1989",
    "perf_chosen": 1.0,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "yes, there were attempted coups in 1996 and 2000.",
    "slm_pred": "yes, there were attempted coups in 1996 and 2000.",
    "llm_pred": "Yes—attempted coups in 1996 and 2000.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in 1996 and 2000",
    "final_answer": "The answer is Thursday morning.",
    "slm_pred": "The answer is Thursday morning.",
    "llm_pred": "Thursday morning",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "three",
    "final_answer": "The third.",
    "slm_pred": "The third.",
    "llm_pred": "three",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Catholic bishop",
    "final_answer": "A former Catholic bishop.",
    "slm_pred": "A former Catholic bishop.",
    "llm_pred": "a Catholic bishop",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, he has at least one child.",
    "slm_pred": "Yes, he has at least one child.",
    "llm_pred": "Yes—he has at least one child.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "while he was still a priest.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "shocking",
    "final_answer": "The revelation about President Lugo fathering a child while still a priest was considered shocking.",
    "slm_pred": "The revelation about President Lugo fathering a child while still a priest was considered shocking.",
    "llm_pred": "shocking",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c88168b1ce62fae65cfde5653b16b05abbb705e1d51ba128ebc9b3533e1fab2",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "struggled",
    "final_answer": "he has struggled to push reforms through.",
    "slm_pred": "he has struggled to push reforms through.",
    "llm_pred": "struggled to push reforms through",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a dress",
    "final_answer": "A dress to wear to her party.",
    "slm_pred": "A dress to wear to her party.",
    "llm_pred": "a dress",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "her party",
    "final_answer": "The answer is her party.",
    "slm_pred": "The answer is the bug.",
    "llm_pred": "The answer is her party.",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.0,
    "perf_llm": 0.5714285714,
    "cost": 25.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, he was busy.",
    "slm_pred": "No, he was busy.",
    "llm_pred": "No.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Her mom",
    "final_answer": "Nobody",
    "slm_pred": "The woman.",
    "llm_pred": "Nobody",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she did.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "yes",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a bug",
    "final_answer": "A bug.",
    "slm_pred": "A bug.",
    "llm_pred": "a bug",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "nine in the morning",
    "final_answer": "nine in the morning",
    "slm_pred": "Nine in the morning.",
    "llm_pred": "nine in the morning",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "because it was opposite day",
    "final_answer": "it was opposite day.",
    "slm_pred": "it was opposite day.",
    "llm_pred": "because it was opposite day",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, they bought steaks.",
    "slm_pred": "Yes, they bought steaks.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "e5ee001d2c528159a3bbdc465c09a9c85731032c4796007c9fb00b0d1679c4ba",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "went to more stores to shop",
    "final_answer": "went to more stores to shop",
    "slm_pred": "They went to more stores to shop.",
    "llm_pred": "went to more stores to shop",
    "perf_chosen": 1.0,
    "perf_slm": 0.9230769231,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Buckinghamshire",
    "final_answer": "Buckinghamshire.",
    "slm_pred": "Buckinghamshire.",
    "llm_pred": "Buckinghamshire",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "South East England",
    "final_answer": "In South East England.",
    "slm_pred": "In South East England.",
    "llm_pred": "South East England",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Greater London",
    "final_answer": "Greater London.",
    "slm_pred": "Greater London.",
    "llm_pred": "Greater London",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Berkshire",
    "final_answer": "Berkshire.",
    "slm_pred": "Berkshire.",
    "llm_pred": "Berkshire",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Oxfordshire",
    "final_answer": "The answer is Oxfordshire.",
    "slm_pred": "The answer is Oxfordshire.",
    "llm_pred": "The answer is Oxfordshire.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Northamptonshire",
    "final_answer": "Northamptonshire and Bedfordshire.",
    "slm_pred": "Northamptonshire and Bedfordshire.",
    "llm_pred": "Northamptonshire.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Hertfordshire",
    "final_answer": "The answer is Hertfordshire.",
    "slm_pred": "The answer is Hertfordshire.",
    "llm_pred": "Hertfordshire",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "High Wycombe, Amersham, Chesham and the Chalfonts",
    "final_answer": "The London commuter belt and towns such as High Wycombe, Amersham, Chesham, and the Chalfonts.",
    "slm_pred": "The London commuter belt and towns such as High Wycombe, Amersham, Chesham, and the Chalfonts.",
    "llm_pred": "the towns of High Wycombe, Amersham, Chesham, and the Chalfonts",
    "perf_chosen": 0.6363636364,
    "perf_slm": 0.6363636364,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "London commuter belt",
    "final_answer": "The answer is a county.",
    "slm_pred": "The answer is a county.",
    "llm_pred": "a county",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, it does.",
    "slm_pred": "Yes, it does.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Development",
    "final_answer": "The Metropolitan Green Belt.",
    "slm_pred": "The Metropolitan Green Belt.",
    "llm_pred": "Development",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the Metropolitan Green Belt",
    "final_answer": "the Metropolitan Green Belt.",
    "slm_pred": "The answer is restricted by the Metropolitan Green Belt.",
    "llm_pred": "the Metropolitan Green Belt.",
    "perf_chosen": 1.0,
    "perf_slm": 0.6153846154,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, they are.",
    "slm_pred": "Yes, they are.",
    "llm_pred": "Yes, much less populous.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Milton Keynes",
    "final_answer": "Milton Keynes.",
    "slm_pred": "Milton Keynes.",
    "llm_pred": "Milton Keynes",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "ef4a07e69216818c7b3fc30ae31c3a72d8b1a4bbf1685336ead3abd3f448e9a7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Conservative Party",
    "final_answer": "The Conservative Party.",
    "slm_pred": "The Conservative Party.",
    "llm_pred": "The Conservative Party",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Philip",
    "final_answer": "Philip.",
    "slm_pred": "Philip.",
    "llm_pred": "Philip",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "No, Pierre was still with Philip.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to look after the horses,",
    "final_answer": "Because the landlady warned that hostile men were seeking him.",
    "slm_pred": "The answer is to prevent a potential attack.",
    "llm_pred": "Because the landlady warned that hostile men were seeking him.",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.3076923077,
    "perf_llm": 0.1333333333,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "twelve miles away",
    "final_answer": "Twelve miles.",
    "slm_pred": "Twelve miles.",
    "llm_pred": "twelve miles away",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "four hours, at least",
    "final_answer": "At least four hours.",
    "slm_pred": "At least four hours.",
    "llm_pred": "at least four hours",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "There could be no hurry.",
    "slm_pred": "There could be no hurry.",
    "llm_pred": "No.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the landlady",
    "final_answer": "The landlady herself.",
    "slm_pred": "The landlady herself.",
    "llm_pred": "the landlady",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she did.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes—initially.",
    "slm_pred": "No, he intended to leave.",
    "llm_pred": "Yes—initially.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "very poor",
    "final_answer": "The accommodation was very poor.",
    "slm_pred": "The accommodation was very poor.",
    "llm_pred": "very poor",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, she warned him to think otherwise.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": ".yes",
    "final_answer": "Yes.",
    "slm_pred": "She thought some people were looking for them.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, they offered a reward.",
    "slm_pred": "Yes, they offered a reward.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "for carrying  the news to them",
    "final_answer": "to carry the news to them",
    "slm_pred": "The accommodation.",
    "llm_pred": "to carry the news to them",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.25,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Their intentions were not friendly.",
    "slm_pred": "Their intentions were not friendly.",
    "llm_pred": "No— their intentions were not friendly.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "No, he decided to continue riding.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, she asked him not to let her husband know she had spoken to him.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.125,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "that she has spoken to him",
    "final_answer": "his bill",
    "slm_pred": "The answer is a party like Philip's.",
    "llm_pred": "his bill",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "honest but timid",
    "final_answer": "The guy is Philip.",
    "slm_pred": "The guy is Philip.",
    "llm_pred": "honest but timid",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a9f5560a16f16c84fc5a93e27d581cff795630afb1c79d49ca3ed6d0accdab90",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the landlord",
    "final_answer": "The landlord.",
    "slm_pred": "The landlord.",
    "llm_pred": "the landlord",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "its high downland and wide valleys",
    "final_answer": "Its high downland and wide valleys.",
    "slm_pred": "Its high downland and wide valleys.",
    "llm_pred": "its high downland and wide valleys",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "South West England",
    "final_answer": "South West England.",
    "slm_pred": "South West England.",
    "llm_pred": "South West England",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire and Berkshire",
    "final_answer": "The counties of Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire, and Berkshire.",
    "slm_pred": "The counties of Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire, and Berkshire.",
    "llm_pred": "the counties of Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire, and Berkshire",
    "perf_chosen": 0.8235294118,
    "perf_slm": 0.8235294118,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wilton",
    "final_answer": "\"Wiltunscir\".",
    "slm_pred": "\"Wiltunscir\".",
    "llm_pred": "Wiltunscir",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Trowbridge.",
    "final_answer": "The answer is Trowbridge.",
    "slm_pred": "The answer is Trowbridge.",
    "llm_pred": "Trowbridge",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "stone circles",
    "final_answer": "its mediaeval cathedral.",
    "slm_pred": "its mediaeval cathedral.",
    "llm_pred": "its medieval cathedral",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, they are open to the public.",
    "slm_pred": "Yes, they are open to the public.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Longleat",
    "final_answer": "Dorset",
    "slm_pred": "The answer is Stonehenge.",
    "llm_pred": "Dorset",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Warminster,",
    "final_answer": "Mere",
    "slm_pred": "The answer is Salisbury Plain.",
    "llm_pred": "Mere",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "National Trust's Stourhead",
    "final_answer": "Wiltonshire",
    "slm_pred": "The answer is county town.",
    "llm_pred": "Wiltonshire",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Mere.",
    "final_answer": "The answer is near Mere.",
    "slm_pred": "The answer is near Mere.",
    "llm_pred": "Mere",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wiltunscir",
    "final_answer": "\"Wiltunscir\".",
    "slm_pred": "\"Wiltunscir\".",
    "llm_pred": "Wiltunscir",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "pre-Roman",
    "final_answer": "Pre-Roman archaeology.",
    "slm_pred": "Pre-Roman archaeology.",
    "llm_pred": "pre-Roman archaeology",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Stonehenge and Avebury",
    "final_answer": "Stonehenge and Avebury.",
    "slm_pred": "Stonehenge and Avebury.",
    "llm_pred": "Stonehenge and Avebury",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Battle of Bedwyn",
    "final_answer": "The Battle of Bedwyn.",
    "slm_pred": "The Battle of Bedwyn.",
    "llm_pred": "The Battle of Bedwyn",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Escuin and King Wulfhere",
    "final_answer": "Escuin and King Wulfhere of Mercia.",
    "slm_pred": "Escuin and King Wulfhere of Mercia.",
    "llm_pred": "Escuin and King Wulfhere of Mercia",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a West Saxon nobleman",
    "final_answer": "A West Saxon nobleman who had seized the throne of Queen Saxburga.",
    "slm_pred": "A West Saxon nobleman who had seized the throne of Queen Saxburga.",
    "llm_pred": "a West Saxon nobleman who seized Queen Saxburga’s throne",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.6153846154,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Danes invaded the county",
    "final_answer": "The Danes invaded the county.",
    "slm_pred": "The Danes invaded the county.",
    "llm_pred": "The Danes invaded Wiltshire.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7bd201e1f491911d1a0f709c4d2fd67b00099dcb99db5eb4d35c94a405527832",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "large areas of the country came into the possession o f the crown and church",
    "final_answer": "Large areas of the country came into the possession of the crown and the church.",
    "slm_pred": "Large areas of the country came into the possession of the crown and the church.",
    "llm_pred": "large areas of the country came into the possession of the crown and the church",
    "perf_chosen": 0.8666666667,
    "perf_slm": 0.8666666667,
    "perf_llm": 0.8666666667,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Male.",
    "final_answer": "The story refers to Joey as a male.",
    "slm_pred": "The story refers to Joey as a male.",
    "llm_pred": "male",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A piece of spaghetti.",
    "final_answer": "The answer is Joey, the noodle friend.",
    "slm_pred": "The answer is Joey, the noodle friend.",
    "llm_pred": "Joey.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Marsha",
    "final_answer": "Marsha.",
    "slm_pred": "Marsha.",
    "llm_pred": "Marsha.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Her mom.",
    "final_answer": "Marsha's mom.",
    "slm_pred": "Marsha's mom.",
    "llm_pred": "Marsha’s mom",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "It was spaghetti night.",
    "final_answer": "because it was spaghetti night",
    "slm_pred": "It was spaghetti night.",
    "llm_pred": "because it was spaghetti night",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 1.0,
    "perf_llm": 0.8888888889000001,
    "cost": 25.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Tuesday night.",
    "final_answer": "The night Marsha met her noodle friend.",
    "slm_pred": "The night Marsha met her noodle friend.",
    "llm_pred": "Tuesday night",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A plastic bag.",
    "final_answer": "In Marsha's pocket or under her pillow.",
    "slm_pred": "In Marsha's pocket or under her pillow.",
    "llm_pred": "in a plastic bag in her pocket",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "He gets soaked in water every few days.",
    "slm_pred": "He gets soaked in water every few days.",
    "llm_pred": "Yes—she soaks him in water every few days.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Marsha's mom told her to soak him in water every few days.",
    "final_answer": "by keeping him in a plastic bag by day and under her pillow at night",
    "slm_pred": "The answer is Marsha keeps Joey safe by carrying him in a plastic bag in her pocket and putting him under her pillow at night.",
    "llm_pred": "by keeping him in a plastic bag by day and under her pillow at night",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.16216216220000002,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "74d0f4b277837bfe1d392c2189de1bdb175de158fcbc5309d80290a4e79b2610",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "es",
    "final_answer": "Yes, in a plastic bag in her pocket.",
    "slm_pred": "Yes, in a plastic bag in her pocket.",
    "llm_pred": "Yes—she carries him in a plastic bag in her pocket.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "rockers",
    "final_answer": "Jane Humphreys, Hester Bridgeman, and Pauline Dunord.",
    "slm_pred": "Jane Humphreys, Hester Bridgeman, and Pauline Dunord.",
    "llm_pred": "Jane Humphreys",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "an odd volume of Parthenissa",
    "final_answer": "an odd volume of Parthenissa.",
    "slm_pred": "an odd volume of Parthenissa.",
    "llm_pred": "an odd volume of Parthenissa",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "she had put herself into an inferior position",
    "final_answer": "Disappointment.",
    "slm_pred": "Disappointment.",
    "llm_pred": "disappointment",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "t Portchester",
    "final_answer": "Portchester.",
    "slm_pred": "Portchester.",
    "llm_pred": "Portchester",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No—she thought them inferior.",
    "slm_pred": "Jane Humphreys was considered inferior.",
    "llm_pred": "No—she thought them inferior.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the constant thought of the grief and horror",
    "final_answer": "the grief and horror.",
    "slm_pred": "the grief and horror.",
    "llm_pred": "the grief and horror",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "separate",
    "final_answer": "The nursery was a separate establishment.",
    "slm_pred": "The nursery was a separate establishment.",
    "llm_pred": "separate",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "her Church",
    "final_answer": "her devotions and her Church.",
    "slm_pred": "her devotions and her Church.",
    "llm_pred": "her devotions",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jane Humphreys",
    "final_answer": "Jane Humphreys.",
    "slm_pred": "Jane Humphreys.",
    "llm_pred": "Jane Humphreys",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "no",
    "slm_pred": "No, she was introspective.",
    "llm_pred": "no",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "silly",
    "final_answer": "Jane Humphreys was described as \"harmless but silly\".",
    "slm_pred": "Jane Humphreys was described as \"harmless but silly\".",
    "llm_pred": "silly",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "four",
    "final_answer": "four",
    "slm_pred": "four, including Anne Woodford.",
    "llm_pred": "four",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": ".yes",
    "final_answer": "Yes.",
    "slm_pred": "She was one of them.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "an attorney in the city,",
    "final_answer": "an attorney in the city.",
    "slm_pred": "an attorney in the city.",
    "llm_pred": "an attorney in the city",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "only ones that were to her advantage",
    "final_answer": "only her own advantage",
    "slm_pred": "No, other than devotions and her church.",
    "llm_pred": "only her own advantage",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.1428571429,
    "perf_llm": 0.5454545455,
    "cost": 25.0
  },
  {
    "id": "9e4f3cac92131e9bde746b67cdb7ff63dd609e2b8ebd8cfb3b9c54b0a54ddbcd",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "MACHINATIONS",
    "final_answer": "CHAPTER XVII: MACHINATIONS.",
    "slm_pred": "CHAPTER XVII: MACHINATIONS.",
    "llm_pred": "MACHINATIONS",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Tom",
    "final_answer": "Tom.",
    "slm_pred": "Tom.",
    "llm_pred": "Tom Rover",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "William",
    "final_answer": "William Philander",
    "slm_pred": "The poster.",
    "llm_pred": "William Philander",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.0,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Sam",
    "final_answer": "Sam.",
    "slm_pred": "Sam.",
    "llm_pred": "Sam",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "What did you do to him?",
    "final_answer": "\"What did you do to him?\"",
    "slm_pred": "\"What did you do to him?\"",
    "llm_pred": "\"What did you do to him?\"",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Put an advertisement of pills on his back",
    "final_answer": "He put advertisements on William Philander's back and in his textbooks.",
    "slm_pred": "He put advertisements on William Philander's back and in his textbooks.",
    "llm_pred": "put an ad for Gumley’s Red Pills on William Philander’s back",
    "perf_chosen": 0.4210526316,
    "perf_slm": 0.4210526316,
    "perf_llm": 0.5263157895,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "and some other ads in his text books",
    "final_answer": "some other ads in his textbooks",
    "slm_pred": "The answer is some other ads. in his text books.",
    "llm_pred": "some other ads in his textbooks",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7777777778,
    "perf_llm": 0.7142857143,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "yes he did, he noticed the poster on his back.",
    "slm_pred": "yes he did, he noticed the poster on his back.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "He was shaking his fist wrathfully.",
    "slm_pred": "He was shaking his fist wrathfully.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "approaching with long strides",
    "final_answer": "approaching with long strides",
    "slm_pred": "Approaching with long strides.",
    "llm_pred": "approaching with long strides",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "he was shaking his other fist wrathfully",
    "final_answer": "some other ads in his textbooks",
    "slm_pred": "The answer is some other ads. in his text books.",
    "llm_pred": "some other ads in his textbooks",
    "perf_chosen": 0.3076923077,
    "perf_slm": 0.23529411760000002,
    "perf_llm": 0.3076923077,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes—he knew Tom did it.",
    "slm_pred": "The class knows it was Tom.",
    "llm_pred": "Yes—he knew Tom did it.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "he was so full of wrath he could not speak",
    "final_answer": "He was too full of wrath.",
    "slm_pred": "He was too full of wrath.",
    "llm_pred": "because he was so full of wrath",
    "perf_chosen": 0.625,
    "perf_slm": 0.625,
    "perf_llm": 0.7058823529,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Tom Rover, you've--er--insulted me!",
    "final_answer": "“Gumley’s Red Pills for Red-Blooded People.”",
    "slm_pred": "\"Take a cough drop and clear your throat Billy,\"",
    "llm_pred": "“Gumley’s Red Pills for Red-Blooded People.”",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Take a cough drop and clear your throat Billy,",
    "final_answer": "\"Take a cough drop and clear your throat, Billy.\"",
    "slm_pred": "\"Take a cough drop and clear your throat Billy,\"",
    "llm_pred": "\"Take a cough drop and clear your throat, Billy.\"",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Gumley's Red Pills",
    "final_answer": "Gumley's Red Pills for Red-Blooded People.",
    "slm_pred": "Gumley's Red Pills for Red-Blooded People.",
    "llm_pred": "Gumley’s Red Pills",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"'Gumley's Red Pills for Red-Blooded People,'\"",
    "final_answer": "\"Gumley's Red Pills for Red-Blooded People.\"",
    "slm_pred": "\"Gumley's Red Pills for Red-Blooded People.\"",
    "llm_pred": "“for Red-Blooded People.”",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "he was flat on his back with half a dozen fatal diseases",
    "final_answer": "He got up and went back to work.",
    "slm_pred": "he got up and went back to work.",
    "llm_pred": "He got up and went back to work.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.2,
    "cost": 25.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "three days,",
    "final_answer": "Three days.",
    "slm_pred": "Three days.",
    "llm_pred": "three days",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "6d5af7040be13fa5cdecfddd92798bf878fc4bdf29406975b20416249e51bbbc",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "yes, they did.",
    "slm_pred": "yes, they did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cats",
    "final_answer": "The cats.",
    "slm_pred": "The cats.",
    "llm_pred": "his cats",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Eight",
    "final_answer": "Eight.",
    "slm_pred": "Eight.",
    "llm_pred": "8",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "females",
    "final_answer": "There are more females.",
    "slm_pred": "There are more females.",
    "llm_pred": "females",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "7 girl cats and only 1 boy cat",
    "final_answer": "The answer is 8.",
    "slm_pred": "The answer is 8.",
    "llm_pred": "8",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cat's hair",
    "final_answer": "The cats' hair.",
    "slm_pred": "The cats' hair.",
    "llm_pred": "the cats' hair",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "treats",
    "final_answer": "Regular food, and special treats.",
    "slm_pred": "Regular food, and special treats.",
    "llm_pred": "special treats",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "because he loves them",
    "final_answer": "because he loves them",
    "slm_pred": "The answer is because Brendan loves them.",
    "llm_pred": "because he loves them",
    "perf_chosen": 1.0,
    "perf_slm": 0.5454545455,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "chips and cake and candy",
    "final_answer": "chips, cake, and candy.",
    "slm_pred": "chips, cake, and candy.",
    "llm_pred": "chips, cake, and candy",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "balls of paper",
    "final_answer": "balls of paper.",
    "slm_pred": "balls of paper.",
    "llm_pred": "balls of paper",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Brendan",
    "final_answer": "Brendan creates the balls of paper for the cats to chase.",
    "slm_pred": "Brendan creates the balls of paper for the cats to chase.",
    "llm_pred": "Brendan",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "orange, black, spotted, and white",
    "final_answer": "orange, black, white, and spotted.",
    "slm_pred": "orange, black, white, and spotted.",
    "llm_pred": "orange, black, spotted, and white",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The white cat",
    "final_answer": "The white cat Snowball.",
    "slm_pred": "The white cat Snowball.",
    "llm_pred": "The answer is Snowball.",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the white cat",
    "final_answer": "The white cat Snowball.",
    "slm_pred": "The white cat Snowball.",
    "llm_pred": "Yes, the white cat Snowball.",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "female",
    "final_answer": "female",
    "slm_pred": "The answer is boy.",
    "llm_pred": "female",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Snowball",
    "final_answer": "Snowball.",
    "slm_pred": "A ball of paper.",
    "llm_pred": "Snowball.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, there is no cat named Binky.",
    "slm_pred": "No, there is no cat named Binky.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "d256e783d092b330aadefad663110a0bd244ffd755c32b119290b302263fd002",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "There is no mention of a cat named Scruff.",
    "slm_pred": "There is no mention of a cat named Scruff.",
    "llm_pred": "The answer is not mentioned.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A manuscript",
    "final_answer": "A Hawaiian story titled 'Wiki-wiki'.",
    "slm_pred": "A Hawaiian story titled 'Wiki-wiki'.",
    "llm_pred": "a manuscript",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wiki-wiki",
    "final_answer": "\"Wiki-wiki\".",
    "slm_pred": "\"Wiki-wiki\".",
    "llm_pred": "Wiki-wiki",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The answer is no.",
    "slm_pred": "The answer is no.",
    "llm_pred": "No—Hawaii.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "In Hawai'i",
    "final_answer": "at the gate",
    "slm_pred": "The answer is Maria's front steps.",
    "llm_pred": "at the gate",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "None",
    "final_answer": "None",
    "slm_pred": "It is not explicitly stated in the passage, but it can be inferred that \"Wiki-wiki\" is different from anything he has done before.",
    "llm_pred": "None",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "He's almost afraid of it.",
    "slm_pred": "He's almost afraid of it.",
    "llm_pred": "No—he thinks it’s good.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He has a sneaking idea",
    "final_answer": "He is almost afraid of it.",
    "slm_pred": "He is almost afraid of it.",
    "llm_pred": "Only a sneaking idea.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "To Ruth",
    "final_answer": "Ruth",
    "slm_pred": "Ruth.",
    "llm_pred": "Ruth",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "To see whether he is coming for Thanksgiving",
    "final_answer": "To invite Martin to Thanksgiving dinner.",
    "slm_pred": "To invite Martin to Thanksgiving dinner.",
    "llm_pred": "to confirm whether he’d attend their Thanksgiving dinner",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.375,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "Yes.",
    "slm_pred": "Yes, he did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "Yes.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "The answer is: Yes, Arthur.",
    "slm_pred": "Arthur remained at the gate.",
    "llm_pred": "The answer is: Yes, Arthur.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "WIth Arthur",
    "final_answer": "The answer is Martin.",
    "slm_pred": "The answer is Martin.",
    "llm_pred": "Arthur",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "At the gate",
    "final_answer": "at the gate",
    "slm_pred": "The gate.",
    "llm_pred": "at the gate",
    "perf_chosen": 1.0,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Maria",
    "final_answer": "The answer is Maria.",
    "slm_pred": "The answer is Maria.",
    "llm_pred": "his landlady, Maria",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Not explicitly stated in the given text.",
    "slm_pred": "Not explicitly stated in the given text.",
    "llm_pred": "No—Martin reads it aloud to her.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Martin",
    "final_answer": "Martin reads it.",
    "slm_pred": "Martin reads it.",
    "llm_pred": "Martin",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, he does.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "She does not seem to love it.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "09fd25834237afebab8ecee7e770e89736459456998eb600533f2781496d27e0",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "It's too strong for the magazines",
    "final_answer": "It's too strong for the magazines.",
    "slm_pred": "It's too strong for the magazines.",
    "llm_pred": "Because it’s too strong for the magazines.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.7692307692,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "home",
    "final_answer": "His home in Dubai.",
    "slm_pred": "His home in Dubai.",
    "llm_pred": "his home in Dubai",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "injured in a traing crash",
    "final_answer": "The accident was caused by the unpredictably gusty winds at that part of the circuit at that time.",
    "slm_pred": "The accident was caused by the unpredictably gusty winds at that part of the circuit at that time.",
    "llm_pred": "because of engine problems on his way to the starting grid",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "gusty winds on the track",
    "final_answer": "unpredictably gusty winds.",
    "slm_pred": "unpredictably gusty winds.",
    "llm_pred": "unpredictably gusty winds",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "February 22.",
    "final_answer": "February 22.",
    "slm_pred": "February 22.",
    "llm_pred": "February 22",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Barcelona",
    "final_answer": "The answer is Woking, England.",
    "slm_pred": "The answer is Woking, England.",
    "llm_pred": "at McLaren’s headquarters in Woking, England",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "McLaren's simulator",
    "final_answer": "The McLaren simulator.",
    "slm_pred": "The McLaren simulator.",
    "llm_pred": "the McLaren simulator",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "comeback at Malaysian GP",
    "final_answer": "a comeback at the Malaysian Grand Prix.",
    "slm_pred": "a comeback at the Malaysian Grand Prix.",
    "llm_pred": "his comeback at the Malaysian Grand Prix",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Lewis Hamilton",
    "final_answer": "Lewis Hamilton",
    "slm_pred": "Lewis Hamilton.",
    "llm_pred": "Lewis Hamilton",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "double world champion",
    "final_answer": "The Spanish double world champion.",
    "slm_pred": "The Spanish double world champion.",
    "llm_pred": "double world champion",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "32ff4e87bf04ad231f3c40027f61db62a397c4fe63eacc495aa1c8c2a1354bf8",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Woking, England",
    "final_answer": "Woking, England",
    "slm_pred": "McLaren's headquarters in Woking, England.",
    "llm_pred": "Woking, England",
    "perf_chosen": 1.0,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Ben, Johnny, and Paul",
    "final_answer": "Ben, Johnny, and Paul",
    "slm_pred": "Ben, Johnny, and Paul.",
    "llm_pred": "Ben, Johnny, and Paul",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "his clothes and his ignorance of the slang",
    "final_answer": "His clothes and entire ignorance of the slang of the street.",
    "slm_pred": "His clothes and entire ignorance of the slang of the street.",
    "llm_pred": "his clothes and his total ignorance of street slang",
    "perf_chosen": 0.7368421053,
    "perf_slm": 0.7368421053,
    "perf_llm": 0.8235294118,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "slang of the street",
    "final_answer": "street slang.",
    "slm_pred": "street slang.",
    "llm_pred": "street slang",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "they had engaged rooms at a regular boarding-house",
    "final_answer": "Engaging rooms at a regular boarding-house.",
    "slm_pred": "Engaging rooms at a regular boarding-house.",
    "llm_pred": "taking rooms at a regular boarding-house",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.7142857143,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "they were to pay for three square meals each day",
    "final_answer": "h",
    "slm_pred": "The answer is none.",
    "llm_pred": "h",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "five dollars per week",
    "final_answer": "Five dollars per week.",
    "slm_pred": "Five dollars per week.",
    "llm_pred": "five dollars per week",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "ten dollars per week",
    "final_answer": "Ten dollars per week.",
    "slm_pred": "Ten dollars per week.",
    "llm_pred": "ten dollars per week",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Master Spry's misfortune and Tim Dooley's perfidy",
    "final_answer": "Master Spry's misfortune and Tim Dooley's perfidy.",
    "slm_pred": "Master Spry's misfortune and Tim Dooley's perfidy.",
    "llm_pred": "Master Spry’s misfortune and Tim Dooley’s perfidy.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.7142857143,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "The news was spreading and fueling gossip.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "That it was wonderful news.",
    "final_answer": "a reckless disregard of money",
    "slm_pred": "They thought it was reckless and would lead to financial ruin.",
    "llm_pred": "a reckless disregard of money",
    "perf_chosen": 0.0,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "whispered from one to the other",
    "final_answer": "by whispering from one to another",
    "slm_pred": "They communicated with each other through whispers and conversations from one to another.",
    "llm_pred": "by whispering from one to another",
    "perf_chosen": 0.5,
    "perf_slm": 0.4210526316,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "that they were to start a regular theatre",
    "final_answer": "They were to start a regular theatre.",
    "slm_pred": "They were to start a regular theatre.",
    "llm_pred": "starting a regular theater and that they’d already engaged a hall",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.9333333333,
    "perf_llm": 0.3157894737,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Most people believed Jimmy.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Financially reckless and potentially disastrous.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "he had already engaged a hall",
    "final_answer": "He had already engaged a hall.",
    "slm_pred": "He had already engaged a hall.",
    "llm_pred": "already engaged a hall",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, it was to be converted.",
    "slm_pred": "No, it was to be converted.",
    "llm_pred": "No—converted into a first-class theatre.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "it would be converted into a first-class place of amusement",
    "final_answer": "The hall was going to be converted into a first-class theatre.",
    "slm_pred": "The hall was going to be converted into a first-class theatre.",
    "llm_pred": "be converted into a first-class place of amusement",
    "perf_chosen": 0.4761904762,
    "perf_slm": 0.4761904762,
    "perf_llm": 0.8888888889000001,
    "cost": 5.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "as soon as possible",
    "final_answer": "the day after they took up their abode at Mrs. Green’s",
    "slm_pred": "The day after Ben and Johnny took up their abode at Mrs. Green's.",
    "llm_pred": "the day after they took up their abode at Mrs. Green’s",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "It was not thought to be so.",
    "final_answer": "The answer is no.",
    "slm_pred": "It was supported by several others and came directly from one of the parties concerned.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.0,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "fefb2a1707b7082122900b51f0362d237e9656e65dd77b743ad06a20ff8db3ed",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Because it came directly from one of the parties concerned.",
    "final_answer": "because it came directly from one of the parties concerned",
    "slm_pred": "The news would not have been regarded as a rumor started for the purpose of injuring the credit of these young gentlemen, had it not come directly from one of the parties concerned.",
    "llm_pred": "because it came directly from one of the parties concerned",
    "perf_chosen": 1.0,
    "perf_slm": 0.3720930233,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "basketball",
    "final_answer": "Basketball.",
    "slm_pred": "Basketball.",
    "llm_pred": "basketball",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "NBA players",
    "final_answer": "The NBA players they watch on TV.",
    "slm_pred": "The NBA players they watch on TV.",
    "llm_pred": "the NBA players they watch on TV",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.4444444444,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jared's",
    "final_answer": "Jared's house.",
    "slm_pred": "Jared's house.",
    "llm_pred": "Jared's house.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jared",
    "final_answer": "Jared.",
    "slm_pred": "Jared.",
    "llm_pred": "Jared",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "all get to play on a basketball team",
    "final_answer": "They get to play on a basketball team at their school.",
    "slm_pred": "They get to play on a basketball team at their school.",
    "llm_pred": "they all get to play on a basketball team at their school",
    "perf_chosen": 0.7368421053,
    "perf_slm": 0.7368421053,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "at their school",
    "final_answer": "at their school",
    "slm_pred": "The answer is Jared's house.",
    "llm_pred": "at their school",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "practicing",
    "final_answer": "Practicing a lot.",
    "slm_pred": "Practicing a lot.",
    "llm_pred": "practicing a lot",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "every day they can",
    "final_answer": "They play every day they can.",
    "slm_pred": "They play every day they can.",
    "llm_pred": "every day they can",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ryan",
    "final_answer": "Ryan got a football for his birthday.",
    "slm_pred": "Ryan got a football for his birthday.",
    "llm_pred": "Ryan",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "birthday",
    "final_answer": "his ninth birthday.",
    "slm_pred": "The basketball hoop.",
    "llm_pred": "his ninth birthday.",
    "perf_chosen": 0.5,
    "perf_slm": 0.0,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "4baf9ace3801abd645c26779270b4433ea482fe3ce820348ebe53a5d7253f574",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Josh, Ty, and Max",
    "final_answer": "Josh, Ty, and Max.",
    "slm_pred": "Ty, Josh, and Max.",
    "llm_pred": "Josh, Ty, and Max.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No.",
    "final_answer": "He didn't like golden coins, they were too yellow.",
    "slm_pred": "He didn't like golden coins, they were too yellow.",
    "llm_pred": "no",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They were too yellow.",
    "final_answer": "The other dragons were worried about Sean getting hurt from sleeping around a lot of plants.",
    "slm_pred": "The other dragons were worried about Sean getting hurt from sleeping around a lot of plants.",
    "llm_pred": "The answer is: Because Sean traded for green stones that made a safe, nonflammable green bed.",
    "perf_chosen": 0.1,
    "perf_slm": 0.1,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Green.",
    "final_answer": "Grass, clovers, leaves, and vines.",
    "slm_pred": "Grass, clovers, leaves, and vines.",
    "llm_pred": "green",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No.",
    "final_answer": "Yes, he covered his cave.",
    "slm_pred": "Yes, he covered his cave.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, he did.",
    "slm_pred": "Yes, he did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Grass, clovers, leaves, and vines.",
    "final_answer": "grass, clovers, leaves, and vines",
    "slm_pred": "The green stone.",
    "llm_pred": "grass, clovers, leaves, and vines",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The other dragons.",
    "final_answer": "The other dragons.",
    "slm_pred": "The other dragons.",
    "llm_pred": "the other dragons",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, they were.",
    "slm_pred": "Yes, they were.",
    "llm_pred": "yes",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No.",
    "final_answer": "yes he did.",
    "slm_pred": "yes he did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Zarah.",
    "final_answer": "Zarah.",
    "slm_pred": "Zarah.",
    "llm_pred": "Zarah",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0c35978d4ef8f75ffc9bcce71c085460fd58a603df89434e6a8361e227d0cddb",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A piece of green in a golden necklace.",
    "final_answer": "A piece of green in a golden necklace.",
    "slm_pred": "A piece of green in a golden necklace.",
    "llm_pred": "a green stone in a golden necklace",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Miss Baldwin",
    "final_answer": "Miss Baldwin (Sarah).",
    "slm_pred": "Miss Baldwin (Sarah).",
    "llm_pred": "Sarah Baldwin",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Sarah",
    "final_answer": "Sarah.",
    "slm_pred": "Sarah.",
    "llm_pred": "Sarah",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, she smokes cigarettes.",
    "slm_pred": "Yes, she smokes cigarettes.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "soon after eleven o'clock",
    "final_answer": "the following evening, soon after eleven o'clock.",
    "slm_pred": "the following evening, soon after eleven o'clock.",
    "llm_pred": "soon after eleven o’clock the following evening",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "The following evening.",
    "slm_pred": "soon after eleven o'clock.",
    "llm_pred": "The following evening.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No—she’s young.",
    "slm_pred": "Not explicitly stated.",
    "llm_pred": "No—she’s young.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Josephine",
    "final_answer": "Josephine.",
    "slm_pred": "Josephine.",
    "llm_pred": "Josephine",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wingate's easy-chair",
    "final_answer": "An easy-chair.",
    "slm_pred": "An easy-chair.",
    "llm_pred": "an easy-chair",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "domestic ructions with her husband",
    "slm_pred": "domestic ructions.",
    "llm_pred": "domestic ructions with her husband",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "brief pause",
    "final_answer": "During a brief pause.",
    "slm_pred": "During a brief pause.",
    "llm_pred": "during a brief pause",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "She hates him.",
    "slm_pred": "She hates him.",
    "llm_pred": "No—she hates him.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wilshaw",
    "final_answer": "Wilshaw.",
    "slm_pred": "Wilshaw.",
    "llm_pred": "Wilshaw",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "at least half a sovereign",
    "final_answer": "at least half a sovereign",
    "slm_pred": "At least half a sovereign every time she leaves the cab.",
    "llm_pred": "at least half a sovereign",
    "perf_chosen": 1.0,
    "perf_slm": 0.625,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "every time I leave the cab",
    "final_answer": "Every time she leaves the cab.",
    "slm_pred": "When Sarah is fined for everything except disorderly driving.",
    "llm_pred": "Every time she leaves the cab.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.0,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes—he asks how much she makes a week.",
    "slm_pred": "Yes, he does, specifically her driving income.",
    "llm_pred": "Yes—he asks how much she makes a week.",
    "perf_chosen": 0.0,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Jimmy",
    "final_answer": "Jimmy",
    "slm_pred": "Jimmy and her husband.",
    "llm_pred": "Jimmy",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "anywhere",
    "final_answer": "Anywhere in Wingate's rooms.",
    "slm_pred": "Anywhere in Wingate's rooms.",
    "llm_pred": "anywhere",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "0f82ff5537d6a0abab4fedde42424350e678959776fc091f12a596ef4257e24a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "-ham and pâté-de-foie-gras",
    "final_answer": "ham and pâté-de-foie-gras.",
    "slm_pred": "ham and pâté-de-foie-gras.",
    "llm_pred": "ham and pâté-de-foie-gras",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in 2013",
    "final_answer": "In 2013.",
    "slm_pred": "In 2013.",
    "llm_pred": "2013",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Pope",
    "final_answer": "Pope Emeritus.",
    "slm_pred": "Pope Emeritus.",
    "llm_pred": "Pope Emeritus",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the 16th",
    "final_answer": "16th.",
    "slm_pred": "16th.",
    "llm_pred": "the 16th",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Aloisius",
    "final_answer": "Aloisius.",
    "slm_pred": "Aloisius.",
    "llm_pred": "Aloisius",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "John Paul II",
    "final_answer": "Pope John Paul II.",
    "slm_pred": "Pope John Paul II.",
    "llm_pred": "Pope John Paul II",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Archbishop of Munich and Freising and Cardinal",
    "final_answer": "Archbishop of Munich and Freising",
    "slm_pred": "Archbishop of Munich and Freising.",
    "llm_pred": "Archbishop of Munich and Freising",
    "perf_chosen": 0.8333333333,
    "perf_slm": 0.8333333333,
    "perf_llm": 0.8333333333,
    "cost": 25.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "an unusual lack of experience.",
    "slm_pred": "an unusual lack of experience.",
    "llm_pred": "No—he had little pastoral experience.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "theologian",
    "final_answer": "Dean of the College of Cardinals",
    "slm_pred": "Dean of the College of Cardinals.",
    "llm_pred": "Dean of the College of Cardinals",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Eight",
    "final_answer": "8 years",
    "slm_pred": "Eight years.",
    "llm_pred": "8 years",
    "perf_chosen": 0.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bavaria",
    "final_answer": "The answer is Bavaria.",
    "slm_pred": "The answer is Bavaria.",
    "llm_pred": "Bavaria",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Pope John Paul II",
    "final_answer": "Pope John Paul II",
    "slm_pred": "Pope John Paul II.",
    "llm_pred": "Pope John Paul II",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1927",
    "final_answer": "1927.",
    "slm_pred": "1927.",
    "llm_pred": "1927.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "e926f4f9dca5b8bc8abbe55d81456bfea1b6093e546f172c309e66bb2bf9376f",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ratzinger",
    "final_answer": "Ratzinger",
    "slm_pred": "Ratzinger",
    "llm_pred": "The answer is Ratzinger.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "at Aaron Poole's home",
    "final_answer": "Aaron Poole's home.",
    "slm_pred": "Aaron Poole's home.",
    "llm_pred": "at Aaron Poole’s home",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "he was in his right mind",
    "final_answer": "in his right mind",
    "slm_pred": "in his right mind then.",
    "llm_pred": "in his right mind",
    "perf_chosen": 0.8,
    "perf_slm": 0.7272727273,
    "perf_llm": 0.8,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Three",
    "final_answer": "Three different Poole family members are mentioned: Wilbur Poole, Nat Poole, and Aaron Poole.",
    "slm_pred": "Three different Poole family members are mentioned: Wilbur Poole, Nat Poole, and Aaron Poole.",
    "llm_pred": "three",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Aaron, Wilbur and Nat",
    "final_answer": "Dave, Ben, Roger, and Phil",
    "slm_pred": "The names mentioned are Ben, Dave, Laura, Roger, Phil, and Mr. Basswood.",
    "llm_pred": "Dave, Ben, Roger, and Phil",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.125,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "He doesn't think so.",
    "slm_pred": "He doesn't think so.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No.",
    "slm_pred": "No, he says it won't hurt to take a look around.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.1666666667,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a telegram",
    "final_answer": "A telegram to Aaron Poole.",
    "slm_pred": "A telegram to Aaron Poole.",
    "llm_pred": "a telegram to Aaron Poole",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Aaron Poole",
    "final_answer": "Aaron Poole",
    "slm_pred": "The answer is Aaron Poole.",
    "llm_pred": "Aaron Poole",
    "perf_chosen": 1.0,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "set off on a hunt",
    "final_answer": "They go on a hunt for Wilbur Poole.",
    "slm_pred": "They go on a hunt for Wilbur Poole.",
    "llm_pred": "hunt for the wild man",
    "perf_chosen": 0.4615384615,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.2,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the wild man",
    "final_answer": "The answer is for Nat Poole and his folks.",
    "slm_pred": "The answer is for Nat Poole and his folks.",
    "llm_pred": "Nat Poole and his folks.",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No.",
    "slm_pred": "No, they don't.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes—three locals said they’d talked with him.",
    "slm_pred": "They met three people who had talked to the strange individual.",
    "llm_pred": "Yes—three locals said they’d talked with him.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "three people who had talked to him",
    "final_answer": "X",
    "slm_pred": "The answer is \"the storm\"",
    "llm_pred": "X",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "when it was getting late",
    "final_answer": "when it was getting late",
    "slm_pred": "When it's getting late and they have a hard ride ahead of them the next day.",
    "llm_pred": "when it was getting late",
    "perf_chosen": 1.0,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Phil",
    "final_answer": "Roger’s",
    "slm_pred": "Roger's idea was that.",
    "llm_pred": "Roger’s",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "he's tired",
    "final_answer": "because he’s tired out",
    "slm_pred": "He is tired out.",
    "llm_pred": "because he’s tired out",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "They have a hard ride ahead to reach Carpen Falls in time.",
    "slm_pred": "They have a hard ride ahead to reach Carpen Falls in time.",
    "llm_pred": "a hard ride to Carpen Falls to hit the trail for Bear Camp before dark",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "ride to Carpen Falls",
    "final_answer": "start a hunt for him",
    "slm_pred": "They will give up the hunt.",
    "llm_pred": "start a hunt for him",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No—it's a hard ride.",
    "slm_pred": "The answer is unlikely.",
    "llm_pred": "No—it's a hard ride.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "613497d847dddc6aa15d388354eb2a077dd55e25beede208257701de1402607a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "hit the trail for Bear Camp",
    "final_answer": "hit the trail for Bear Camp before it gets dark.",
    "slm_pred": "hit the trail for Bear Camp before it gets dark.",
    "llm_pred": "hit the trail for Bear Camp before dark",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "adolescence",
    "final_answer": "adolescence.",
    "slm_pred": "adolescence.",
    "llm_pred": "Adolescence",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "The answer is transitional.",
    "slm_pred": "Development.",
    "llm_pred": "The answer is transitional.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "cultural",
    "final_answer": "The preparation of children for adult roles.",
    "slm_pred": "The preparation of children for adult roles.",
    "llm_pred": "preparation of children for adult roles",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "puberty",
    "final_answer": "Puberty.",
    "slm_pred": "Puberty.",
    "llm_pred": "puberty",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "secondary sex characeristics",
    "final_answer": "Secondary sex characteristics.",
    "slm_pred": "Secondary sex characteristics.",
    "llm_pred": "secondary sex characteristics",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a deeper voice in boys",
    "final_answer": "A deeper voice and larger Adam's apple in boys.",
    "slm_pred": "A deeper voice and larger Adam's apple in boys.",
    "llm_pred": "a deeper voice",
    "perf_chosen": 0.7142857143,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the pituitary gland",
    "final_answer": "A surge in hormone production.",
    "slm_pred": "A surge in hormone production.",
    "llm_pred": "a surge in hormone production",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the male and female gonads",
    "final_answer": "The hormonal balance towards an adult state.",
    "slm_pred": "The hormonal balance towards an adult state.",
    "llm_pred": "the hormonal balance",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.25,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "hormonal agents",
    "final_answer": "The pituitary gland.",
    "slm_pred": "The pituitary gland.",
    "llm_pred": "hormones",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "gynecomastia",
    "final_answer": "gynecomastia (breast development in boys)",
    "slm_pred": "Developmental abnormalities and health issues.",
    "llm_pred": "gynecomastia (breast development in boys)",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.0,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "tissue responsiveness or obesity",
    "final_answer": "obesity",
    "slm_pred": "The answer is an imbalance of sex hormones, tissue responsiveness, or obesity.",
    "llm_pred": "obesity",
    "perf_chosen": 0.4,
    "perf_slm": 0.5,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "a852dad57f00fdc25948b57ec3c6077c1c4e1da68b7d6e338fdcd533dc7b32f7",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the testes and the ovaries",
    "final_answer": "The blood stream.",
    "slm_pred": "The blood stream.",
    "llm_pred": "the bloodstream",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jeff Jones",
    "final_answer": "Jeff Jones.",
    "slm_pred": "Jeff Jones.",
    "llm_pred": "Jeff Jones",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Stealing horses",
    "final_answer": "biting his lip in vexation",
    "slm_pred": "He was speaking out about what he knew.",
    "llm_pred": "biting his lip in vexation",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "two",
    "final_answer": "Two people, Paul and Chet Winthrop.",
    "slm_pred": "Two people, Paul and Chet Winthrop.",
    "llm_pred": "Two",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Paul and Chet Winthrop",
    "final_answer": "Jeff Jones",
    "slm_pred": "The answer is Jeff Jones.",
    "llm_pred": "Jeff Jones",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "interested in him",
    "final_answer": "stealing horses",
    "slm_pred": "Questioning a prisoner.",
    "llm_pred": "stealing horses",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "he knew Captain Grady",
    "final_answer": "The answer is because he knew he would be lynched if taken to the next camp.",
    "slm_pred": "The answer is because he knew he would be lynched if taken to the next camp.",
    "llm_pred": "To avoid being lynched at the Fork for horse stealing.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "Yes—reluctantly.",
    "slm_pred": "Eventually, yes.",
    "llm_pred": "Yes—reluctantly.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "wnated liberty",
    "final_answer": "Because Mr. Dottery hasn’t returned yet.",
    "slm_pred": "The Fork is a place where they would likely lynch him.",
    "llm_pred": "Because Mr. Dottery hasn’t returned yet.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "No",
    "slm_pred": "Yes, he finally agreed to talk.",
    "llm_pred": "No",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Paul threatened to have him lynched if he didn't speak.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "tell the men that he is a downright horse thief",
    "final_answer": "severely",
    "slm_pred": "The answer is severely.",
    "llm_pred": "severely",
    "perf_chosen": 0.0,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "It effectively broke Jeff Jones's resistance.",
    "slm_pred": "It effectively broke Jeff Jones's resistance.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "told all he knew",
    "final_answer": "all he knew",
    "slm_pred": "The little he knew about Captain Grady and their uncle, Barnaby Winthrop.",
    "llm_pred": "all he knew",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.25,
    "perf_llm": 0.8571428571,
    "cost": 25.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "He asked them to do their best for him in exchange for his cooperation.",
    "slm_pred": "He asked them to do their best for him in exchange for his cooperation.",
    "llm_pred": "Yes—only if they promised to help secure his release.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5b046bfda74b47b5c01beb7fc2a178801f70eaa3d259077aeeb87e2dc826279a",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "best they can do for him",
    "final_answer": "The Fork.",
    "slm_pred": "The Fork.",
    "llm_pred": "the next camp (the Fork)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Federal City of Bonn",
    "final_answer": "The Federal City of Bonn.",
    "slm_pred": "The Federal City of Bonn.",
    "llm_pred": "Bonn",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "on the banks of the Rhine",
    "final_answer": "The banks of the Rhine in the German state of North Rhine-Westphalia.",
    "slm_pred": "The banks of the Rhine in the German state of North Rhine-Westphalia.",
    "llm_pred": "on the banks of the Rhine in North Rhine-Westphalia, Germany",
    "perf_chosen": 0.5555555556,
    "perf_slm": 0.5555555556,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the German state of North Rhine-Westphalia",
    "final_answer": "North Rhine-Westphalia",
    "slm_pred": "The temperate zone.",
    "llm_pred": "North Rhine-Westphalia",
    "perf_chosen": 0.5,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, it's in the southernmost part of the Rhine-Ruhr region.",
    "slm_pred": "No, it's in the southernmost part of the Rhine-Ruhr region.",
    "llm_pred": "No.",
    "perf_chosen": 0.1818181818,
    "perf_slm": 0.1818181818,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "southernmost",
    "final_answer": "the de facto joint seat of government of Germany",
    "slm_pred": "The story about the city of Bonn.",
    "llm_pred": "the de facto joint seat of government of Germany",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "Yes.",
    "slm_pred": "No, over 11 million inhabitants.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Bonn is a city. Do you mean in Germany?",
    "final_answer": "Cologne",
    "slm_pred": "Bonn.",
    "llm_pred": "Cologne",
    "perf_chosen": 0.0,
    "perf_slm": 0.2,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, it is.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in the 1st century",
    "final_answer": "In the 1st century BC as a Roman settlement.",
    "slm_pred": "In the 1st century BC as a Roman settlement.",
    "llm_pred": "the 1st century BC",
    "perf_chosen": 0.6153846154,
    "perf_slm": 0.6153846154,
    "perf_llm": 0.75,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "as a Roman settlement",
    "final_answer": "The Federal City of Bonn.",
    "slm_pred": "The Federal City of Bonn.",
    "llm_pred": "The answer is a Roman settlement.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.6000000000000001,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No—it's one of Germany’s oldest cities (founded in the 1st century BC).",
    "slm_pred": "No.",
    "llm_pred": "No—it's one of Germany’s oldest cities (founded in the 1st century BC).",
    "perf_chosen": 0.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Composer Ludwig van Beethoven.",
    "slm_pred": "Composer Ludwig van Beethoven.",
    "llm_pred": "Yes—Ludwig van Beethoven.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Ludwig van Beethoven",
    "final_answer": "Ludwig van Beethoven.",
    "slm_pred": "The answer is Ludwig van Beethoven.",
    "llm_pred": "Ludwig van Beethoven.",
    "perf_chosen": 1.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "in 1770",
    "final_answer": "in the 1st century BC",
    "slm_pred": "1949.",
    "llm_pred": "in the 1st century BC",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.0,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Basic Law, was declared",
    "final_answer": "Germany's present constitution, the Basic Law, was declared in Bonn.",
    "slm_pred": "Germany's present constitution, the Basic Law, was declared in Bonn.",
    "llm_pred": "Germany’s Basic Law was declared in Bonn.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bonn served as the seat of government, but not the capitol",
    "final_answer": "Bonn served as the seat of government of reunited Germany.",
    "slm_pred": "Bonn served as the seat of government of reunited Germany.",
    "llm_pred": "Bonn served as the seat of government (but not the capital) of reunited Germany.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "primary seat of six federal government ministries",
    "final_answer": "six federal government ministries and twenty federal authorities",
    "slm_pred": "six federal government ministries and twenty federal authorities.",
    "llm_pred": "six federal government ministries and twenty federal authorities",
    "perf_chosen": 0.5333333333,
    "perf_slm": 0.5333333333,
    "perf_llm": 0.5333333333,
    "cost": 25.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "reflects its important political status within Germany",
    "final_answer": "It reflects Bonn's important political status within Germany.",
    "slm_pred": "It reflects Bonn's important political status within Germany.",
    "llm_pred": "Yes—an official title reflecting Bonn’s role as a federal government seat.",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d802048d1aab8eba58f6c80e20834c4a83a2e32519be1a4e678838ea58447159",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No",
    "slm_pred": "No, the Rhine-Ruhr region is Germany's largest metropolitan area.",
    "llm_pred": "No",
    "perf_chosen": 1.0,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Aquila",
    "final_answer": "Aquila",
    "slm_pred": "Aquila rose to his feet.",
    "llm_pred": "Aquila",
    "perf_chosen": 1.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Masuccio Torri.\"",
    "final_answer": "Masuccio Torri.",
    "slm_pred": "Masuccio Torri.",
    "llm_pred": "Masuccio Torri",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "50.",
    "final_answer": "fifty",
    "slm_pred": "Fifty mercenaries.",
    "llm_pred": "fifty",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Seven.",
    "final_answer": "seven",
    "slm_pred": "Seven.",
    "llm_pred": "seven",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Armed men, my lords!",
    "final_answer": "\"Armed men, my lords! We are betrayed!\"",
    "slm_pred": "\"Armed men, my lords! We are betrayed!\"",
    "llm_pred": "\"Armed men, my lords!\"",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "\"We are betrayed!\"",
    "final_answer": "\"We are betrayed!\"",
    "slm_pred": "\"Masuccio Torri.\"",
    "llm_pred": "\"We are betrayed!\"",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Lodi.",
    "final_answer": "Lodi.",
    "slm_pred": "Lodi.",
    "llm_pred": "Lodi",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "His sword in its scabbard.",
    "final_answer": "his sword in its scabbard",
    "slm_pred": "His sword in its scabbard.",
    "llm_pred": "his sword in its scabbard",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No.",
    "final_answer": "No—he urged him to leave.",
    "slm_pred": "No, he wanted the Count to leave.",
    "llm_pred": "No—he urged him to leave.",
    "perf_chosen": 0.0,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "He was their only hope.",
    "final_answer": "Because he’s their only hope, unknown to the enemy, and must escape.",
    "slm_pred": "The answer is Aquila was not willing to leave his friends behind.",
    "llm_pred": "Because he’s their only hope, unknown to the enemy, and must escape.",
    "perf_chosen": 0.3529411765,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.3529411765,
    "cost": 25.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Babbiano.",
    "final_answer": "Babbiano.",
    "slm_pred": "Babbiano.",
    "llm_pred": "Babbiano",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The throne of Gian Maria.",
    "final_answer": "the throne of Gian Maria.",
    "slm_pred": "the throne of Gian Maria.",
    "llm_pred": "the throne of Gian Maria",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "347dee8f2ccd3a481f6f9c4652acd78fbc5e1fcaedcdca6f352acbed5f2a348d",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Lodi.",
    "final_answer": "Lodi",
    "slm_pred": "Lodi.",
    "llm_pred": "Lodi",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a grey dimness",
    "final_answer": "the Selache (the schooner)",
    "slm_pred": "The schooner _Selache_.",
    "llm_pred": "the Selache (the schooner)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a bitter breeze",
    "final_answer": "A little bitter breeze.",
    "slm_pred": "A little bitter breeze.",
    "llm_pred": "a little bitter breeze",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Dampier",
    "final_answer": "Dampier",
    "slm_pred": "Dampier.",
    "llm_pred": "Dampier",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "get the mainsail on",
    "final_answer": "He wanted the cover off the second boat.",
    "slm_pred": "He wanted the cover off the second boat.",
    "llm_pred": "the cover taken off the second boat, ready to hoist it out",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "He didn't like the look of the weather.",
    "slm_pred": "He didn't like the look of the weather.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "that boat",
    "final_answer": "the boat.",
    "slm_pred": "the boat.",
    "llm_pred": "the boat",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the mainsail",
    "final_answer": "The mainsail.",
    "slm_pred": "The mainsail.",
    "llm_pred": "the mainsail",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a little warm",
    "final_answer": "It put a little warmth into them.",
    "slm_pred": "It put a little warmth into them.",
    "llm_pred": "relieved",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "white caps",
    "final_answer": "white caps.",
    "slm_pred": "white caps.",
    "llm_pred": "white caps",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "for the ice",
    "final_answer": "for the ice",
    "slm_pred": "The ice.",
    "llm_pred": "for the ice",
    "perf_chosen": 1.0,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Selache",
    "final_answer": "The schooner (later referred to as the \"_Selache_\").",
    "slm_pred": "The schooner (later referred to as the \"_Selache_\").",
    "llm_pred": "the Selache",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "It had changed",
    "final_answer": "The ice had broken up and masses were moving out into the open water.",
    "slm_pred": "The ice had broken up and masses were moving out into the open water.",
    "llm_pred": "Large masses had broken off and drifted into open water, narrowing the opening.",
    "perf_chosen": 0.11764705880000001,
    "perf_slm": 0.11764705880000001,
    "perf_llm": 0.125,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "big masses became detached",
    "final_answer": "It had become more fragmented and narrower.",
    "slm_pred": "It had become more fragmented and narrower.",
    "llm_pred": "Big masses had broken off and the opening had grown narrower.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "A grey dimness was creeping in upon the schooner.",
    "slm_pred": "A grey dimness was creeping in upon the schooner.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wyllard",
    "final_answer": "Wyllard took the wheel.",
    "slm_pred": "Wyllard took the wheel.",
    "llm_pred": "Wyllard",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "sent the man forward",
    "final_answer": "sent a man up into the foremast shrouds",
    "slm_pred": "He sent a man up into the foremast shrouds.",
    "llm_pred": "sent a man up into the foremast shrouds",
    "perf_chosen": 0.5,
    "perf_slm": 0.4615384615,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the second boat",
    "final_answer": "everything clear for hoisting out.",
    "slm_pred": "everything clear for hoisting out.",
    "llm_pred": "the second boat",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Wyllard",
    "final_answer": "Wyllard.",
    "slm_pred": "Wyllard.",
    "llm_pred": "the man he’d sent forward (the former helmsman)",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "f9fa1f5671ad9552f292536444fd8f2837bbf7b1d178f72d2ed93903e8affdad",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "heave the boat around",
    "final_answer": "He ordered him to get the cover off the second boat.",
    "slm_pred": "He ordered him to get the cover off the second boat.",
    "llm_pred": "Take the cover off the second boat and get it ready to hoist out.",
    "perf_chosen": 0.2666666667,
    "perf_slm": 0.2666666667,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Multimedia",
    "final_answer": "Multimedia content.",
    "slm_pred": "Multimedia content.",
    "llm_pred": "multimedia",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "interacted with",
    "final_answer": "part of a live performance.",
    "slm_pred": "part of a live performance.",
    "llm_pred": "The answer is part of a live performance.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "accessed by information content processing devices",
    "final_answer": "No.",
    "slm_pred": "The answer is No.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bob Goldstein",
    "final_answer": "Bob Goldstein.",
    "slm_pred": "Bob Goldstein.",
    "llm_pred": "Bob Goldstein",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "July 1966",
    "final_answer": "July 1966",
    "slm_pred": "August 10, 1966",
    "llm_pred": "July 1966",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a singer and artist",
    "final_answer": "The answer is Bob Goldstein (later 'Bobb Goldsteinn').",
    "slm_pred": "The answer is Bob Goldstein (later 'Bobb Goldsteinn').",
    "llm_pred": "David Sawyer",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Southampton, Long Island",
    "final_answer": "At Southampton, Long Island.",
    "slm_pred": "At Southampton, Long Island.",
    "llm_pred": "Southampton, Long Island",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"LightWorks at L'Oursin\"",
    "final_answer": "\"LightWorks at L'Oursin\".",
    "slm_pred": "\"LightWorks at L'Oursin\".",
    "llm_pred": "LightWorks at L'Oursin",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "content that uses a combination of different content forms",
    "final_answer": "A combination of different content forms such as text, audio, images, animations, video, and interactive content.",
    "slm_pred": "A combination of different content forms such as text, audio, images, animations, video, and interactive content.",
    "llm_pred": "content that combines multiple media forms (text, audio, images, video, animation, interactivity)",
    "perf_chosen": 0.56,
    "perf_slm": 0.56,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "text, audio, images, animations, video and interactive content.",
    "final_answer": "The answer is text, audio, images, animations, video, and interactive content.",
    "slm_pred": "The answer is text, audio, images, animations, video, and interactive content.",
    "llm_pred": "text, audio, images, animations, video, and interactive content",
    "perf_chosen": 0.8421052632,
    "perf_slm": 0.8421052632,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "media that use only rudimentary computer displays",
    "final_answer": "Text-only or traditional forms of printed or hand-produced material.",
    "slm_pred": "Text-only or traditional forms of printed or hand-produced material.",
    "llm_pred": "media that use only rudimentary computer displays (e.g., text-only or traditional printed/hand-produced material)",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.7000000000000001,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Multimedia devices",
    "final_answer": "Electronic devices such as computers.",
    "slm_pred": "Electronic devices such as computers.",
    "llm_pred": "multimedia devices",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"rich media\"",
    "final_answer": "\"Rich media\".",
    "slm_pred": "\"Rich media\".",
    "llm_pred": "rich media",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Dick Higgins",
    "final_answer": "An American artist named Dick Higgins.",
    "slm_pred": "An American artist named Dick Higgins.",
    "llm_pred": "Dick Higgins",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"intermedia\".",
    "final_answer": "\"intermedia\".",
    "slm_pred": "\"intermedia\".",
    "llm_pred": "intermedia",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "The answer is Yes.",
    "slm_pred": "The answer is yes, he was American.",
    "llm_pred": "The answer is Yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.25,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Richard Albarino",
    "final_answer": "Richard Albarino of \"Variety\".",
    "slm_pred": "Richard Albarino of \"Variety\".",
    "llm_pred": "Richard Albarino",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"Variety\"",
    "final_answer": "\"Variety\".",
    "slm_pred": "\"Variety\".",
    "llm_pred": "Variety",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "August 10, 1966,",
    "final_answer": "August 10, 1966",
    "slm_pred": "On August 10, 1966.",
    "llm_pred": "August 10, 1966",
    "perf_chosen": 1.0,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "aefd5885789816c0f1a46d33cf74584362b7b50a569047b38951128fef64c008",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Iris Sawyer",
    "final_answer": "One of Bob's producers was Iris Sawyer.",
    "slm_pred": "One of Bob's producers was Iris Sawyer.",
    "llm_pred": "Iris Sawyer",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the Crooked Creek Company",
    "final_answer": "The Crooked Creek Company.",
    "slm_pred": "The Crooked Creek Company.",
    "llm_pred": "The Crooked Creek Company",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "George Purvis.",
    "final_answer": "George Purvis.",
    "slm_pred": "George Purvis.",
    "llm_pred": "George Purvis",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes.",
    "final_answer": "The answer is yes.",
    "slm_pred": "yes, they had a fight.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Harry",
    "final_answer": "Harry said \"hello\" first.",
    "slm_pred": "Harry said \"hello\" first.",
    "llm_pred": "Harry",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "He felt very much embarrassed.",
    "slm_pred": "He felt very much embarrassed.",
    "llm_pred": "Yes—very much.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Hetertown",
    "final_answer": "Hetertown",
    "slm_pred": "Hetertown.",
    "llm_pred": "Hetertown",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "as slowly as their horses would consent to go",
    "final_answer": "as slowly as their horses would consent to go.",
    "slm_pred": "as slowly as their horses would consent to go.",
    "llm_pred": "as slowly as their horses would consent to go",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "About a week",
    "final_answer": "about a week",
    "slm_pred": "about a week.",
    "llm_pred": "about a week",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "harry",
    "final_answer": "Harry",
    "slm_pred": "Harry.",
    "llm_pred": "Harry",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Thus the quarrel came to an end.",
    "slm_pred": "Thus the quarrel came to an end.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No.",
    "final_answer": "No, Harry sold the telegraph.",
    "slm_pred": "No, Harry sold the telegraph.",
    "llm_pred": "No—Harry did.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes.",
    "final_answer": "Yes, he thinks they did.",
    "slm_pred": "Yes, he thinks they did.",
    "llm_pred": "Yes—“a pretty good bargain.”",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "relatives in Richmond.",
    "final_answer": "some relatives in Richmond.",
    "slm_pred": "some relatives in Richmond.",
    "llm_pred": "his relatives in Richmond",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No.",
    "final_answer": "No—he didn’t know how George felt.",
    "slm_pred": "He was not.",
    "llm_pred": "No—he didn’t know how George felt.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No.",
    "final_answer": "The answer is no.",
    "slm_pred": "He thought of stopping at the mill.",
    "llm_pred": "The answer is no.",
    "perf_chosen": 0.4,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "George",
    "final_answer": "George Purvis",
    "slm_pred": "Harry stopped first.",
    "llm_pred": "George Purvis",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.0,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Harry",
    "final_answer": "Harry",
    "slm_pred": "Harry was annoyed.",
    "llm_pred": "Harry",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "unknown",
    "final_answer": "No",
    "slm_pred": "He felt very much embarrassed and very much annoyed.",
    "llm_pred": "No",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "28c797bbd1dcd55be3e979f210d152ea719160aa69b63c9d9fcda64c0290f1b5",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "horses",
    "final_answer": "horses.",
    "slm_pred": "horses.",
    "llm_pred": "horses",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "doctor",
    "final_answer": "a doctor at the Royal Liverpool Hospital",
    "slm_pred": "A doctor at the Royal Liverpool hospital.",
    "llm_pred": "a doctor at the Royal Liverpool Hospital",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.25,
    "cost": 25.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "at the Royal Liverpool hospital",
    "final_answer": "the Royal Liverpool Hospital",
    "slm_pred": "The Royal Liverpool hospital.",
    "llm_pred": "the Royal Liverpool Hospital",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 0.8888888889000001,
    "cost": 25.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Palestine",
    "final_answer": "Palestinian.",
    "slm_pred": "Palestinian.",
    "llm_pred": "Palestine",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "They carried out Gaza's first organ transplants",
    "final_answer": "They carried out Gaza's first organ transplants.",
    "slm_pred": "They carried out Gaza's first organ transplants.",
    "llm_pred": "perform organ transplants in Gaza",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Two",
    "final_answer": "Two patients.",
    "slm_pred": "Two patients.",
    "llm_pred": "Two",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "kidneys",
    "final_answer": "Kidneys.",
    "slm_pred": "Kidneys.",
    "llm_pred": "kidneys",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "family",
    "final_answer": "family members",
    "slm_pred": "They were donated by family.",
    "llm_pred": "family members",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Mohammed Duhair and Ziad Matouk",
    "final_answer": "Ziad Matouk and Mohammed Duhair.",
    "slm_pred": "Ziad Matouk and Mohammed Duhair.",
    "llm_pred": "Mohammed Duhair and Ziad Matouk",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "six hours",
    "final_answer": "Six hours.",
    "slm_pred": "Six hours.",
    "llm_pred": "six hours",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "The operations were very successful.",
    "slm_pred": "The operations were very successful.",
    "llm_pred": "Yes—the transplants were successful.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "that Gaza medical teams do independent kidney transplants, and possibly other organs",
    "final_answer": "Gaza medical teams will eventually carry out kidney transplants independently.",
    "slm_pred": "Gaza medical teams will eventually carry out kidney transplants independently.",
    "llm_pred": "that Gaza medical teams will eventually carry out kidney transplants independently",
    "perf_chosen": 0.4545454545,
    "perf_slm": 0.4545454545,
    "perf_llm": 0.5217391304,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Funding",
    "final_answer": "Funding is a problem.",
    "slm_pred": "The answer is Funding.",
    "llm_pred": "Funding is a problem.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "They will go back as volunteers to Gaza to do more transplants",
    "final_answer": "go back to Gaza as volunteers to do more transplants",
    "slm_pred": "They will go back as volunteers to Gaza for the next couple of years to do more transplants.",
    "llm_pred": "go back to Gaza as volunteers to do more transplants",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.8,
    "perf_llm": 0.9090909091,
    "cost": 25.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "May",
    "final_answer": "The Liverpool team's next visit is scheduled for May.",
    "slm_pred": "The Liverpool team's next visit is scheduled for May.",
    "llm_pred": "May",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a fortnight ago",
    "final_answer": "A fortnight ago.",
    "slm_pred": "A fortnight ago.",
    "llm_pred": "a fortnight ago",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "less than six months",
    "final_answer": "six months.",
    "slm_pred": "six months.",
    "llm_pred": "within six months",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "42",
    "final_answer": "42 years old.",
    "slm_pred": "42 years old.",
    "llm_pred": "42",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "42",
    "final_answer": "Mohammed Duhair, 42, received a kidney donated by his younger brother.",
    "slm_pred": "Mohammed Duhair, 42, received a kidney donated by his younger brother.",
    "llm_pred": "He received a kidney from his younger brother.",
    "perf_chosen": 0.1666666667,
    "perf_slm": 0.1666666667,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "36",
    "final_answer": "36 years old.",
    "slm_pred": "36 years old.",
    "llm_pred": "36 years old",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "fa5b3921fc3cfac7d98054a08a0baf46983e5079dd6053474734adff894e5546",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, Nadia Matouk was a good match.",
    "slm_pred": "Yes, Nadia Matouk was a good match.",
    "llm_pred": "Yes, she was a good match.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Robinson",
    "final_answer": "Robinson Crusoe.",
    "slm_pred": "Robinson Crusoe.",
    "llm_pred": "Robinson Crusoe.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A storm",
    "final_answer": "A great storm.",
    "slm_pred": "A great storm.",
    "llm_pred": "a great storm",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "Only Robinson.",
    "slm_pred": "The wild man, later named \"Friday\".",
    "llm_pred": "Only Robinson.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "On an island",
    "final_answer": "Alone on an island.",
    "slm_pred": "Alone on an island.",
    "llm_pred": "on an island",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No gun.",
    "slm_pred": "No gun.",
    "llm_pred": "No—he had no gun.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Worked hard",
    "final_answer": "made a cave, built a house, grew corn, and kept goats",
    "slm_pred": "He used his hands to work hard.",
    "llm_pred": "made a cave, built a house, grew corn, and kept goats",
    "perf_chosen": 0.0,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, eventually he was.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "A wild man",
    "final_answer": "A wild man, and he named him \"Friday\".",
    "slm_pred": "A wild man, and he named him \"Friday\".",
    "llm_pred": "Friday.",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Friday",
    "final_answer": "Friday",
    "slm_pred": "Friday",
    "llm_pred": "The answer is Friday.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a servant",
    "final_answer": "He took on the job of a seaman.",
    "slm_pred": "He took on the job of a seaman.",
    "llm_pred": "a seaman (sailor)",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Robinson",
    "final_answer": "The answer is Robinson Crusoe.",
    "slm_pred": "The answer is Robinson Crusoe.",
    "llm_pred": "himself",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, they escaped with the help of a boat they built and a wild man named \"Friday\".",
    "slm_pred": "Yes, they escaped with the help of a boat they built and a wild man named \"Friday\".",
    "llm_pred": "Yes—they escaped by boat.",
    "perf_chosen": 0.1111111111,
    "perf_slm": 0.1111111111,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "With a boat",
    "final_answer": "The answer is: by using our own hands to work hard with confidence.",
    "slm_pred": "The answer is by using his own hands to work hard with confidence.",
    "llm_pred": "The answer is: by using our own hands to work hard with confidence.",
    "perf_chosen": 0.125,
    "perf_slm": 0.125,
    "perf_llm": 0.125,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Use your own hands to work hard",
    "final_answer": "use our own hands to work hard with confidence (self-reliance)",
    "slm_pred": "Use our own hands to work hard with confidence.",
    "llm_pred": "use our own hands to work hard with confidence (self-reliance)",
    "perf_chosen": 0.7058823529,
    "perf_slm": 0.75,
    "perf_llm": 0.7058823529,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Cry",
    "final_answer": "Cry.",
    "slm_pred": "Cry or escape.",
    "llm_pred": "Cry.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He wanted to be a seaman",
    "final_answer": "He wanted to be a seaman.",
    "slm_pred": "He wanted to be a seaman.",
    "llm_pred": "be a seaman",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Travel around the world.",
    "final_answer": "use our own hands to work hard with confidence",
    "slm_pred": "Use your own hands to work hard with confidence.",
    "llm_pred": "use our own hands to work hard with confidence",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Rome",
    "final_answer": "The answer is Rome.",
    "slm_pred": "Rome.",
    "llm_pred": "The answer is Rome.",
    "perf_chosen": 0.4,
    "perf_slm": 1.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in the saying \"all roads lead to Rome!\"",
    "final_answer": "He used his hands to work hard.",
    "slm_pred": "He used his hands to work hard.",
    "llm_pred": "to escape from the island",
    "perf_chosen": 0.1333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.3076923077,
    "cost": 5.0
  },
  {
    "id": "c203fd53718f1efac12bdca68b6ce6126175f595bec849a95b7a780354fb0e6a",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes—goats.",
    "slm_pred": "Yes, goats.",
    "llm_pred": "Yes—goats.",
    "perf_chosen": 0.0,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Chapter 6.",
    "final_answer": "CHAPTER VI.",
    "slm_pred": "CHAPTER VI.",
    "llm_pred": "Chapter VI",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "An Indian club",
    "final_answer": "The Indian club.",
    "slm_pred": "The Indian club.",
    "llm_pred": "an Indian club",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Leo",
    "final_answer": "Leo.",
    "slm_pred": "Leo.",
    "llm_pred": "Leo",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a savage blow",
    "final_answer": "a stinging slap",
    "slm_pred": "Had the stick struck Leo, his head would have sustained a severe injury.",
    "llm_pred": "a stinging slap",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.125,
    "perf_llm": 0.33333333330000003,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the boy’s head would have sustained a severe injury.",
    "final_answer": "Leo's head would have sustained a severe injury.",
    "slm_pred": "Leo's head would have sustained a severe injury.",
    "llm_pred": "a severe head injury to Leo",
    "perf_chosen": 0.8235294118,
    "perf_slm": 0.8235294118,
    "perf_llm": 0.5333333333,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a crowd of performers",
    "final_answer": "A crowd of performers.",
    "slm_pred": "A crowd of performers.",
    "llm_pred": "a crowd of performers",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Leo",
    "final_answer": "Very few of them liked Jack Snipper.",
    "slm_pred": "Very few of them liked Jack Snipper.",
    "llm_pred": "Leo",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Snipper",
    "final_answer": "Snipper.",
    "slm_pred": "Snipper.",
    "llm_pred": "Snipper",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He dodged",
    "final_answer": "He dodged it.",
    "slm_pred": "He dodged it.",
    "llm_pred": "by dodging the blow",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "as quick as a flash",
    "slm_pred": "The answer is no.",
    "llm_pred": "as quick as a flash",
    "perf_chosen": 0.0,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "The Indian club merely circled through the empty air.",
    "slm_pred": "The Indian club merely circled through the empty air.",
    "llm_pred": "no—it swung through empty air",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "At least 2",
    "final_answer": "two",
    "slm_pred": "\"That's right, boy, stand up for your rights!\"",
    "llm_pred": "two",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "“Do you want to kill the boy?\"",
    "final_answer": "Nothing else.",
    "slm_pred": "No.",
    "llm_pred": "Nothing else.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "“Mind your own affairs!”",
    "final_answer": "\"I'll teach you a lesson!\"",
    "slm_pred": "\"I'll teach you a lesson!\"",
    "llm_pred": "I'll teach you a lesson!",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "Yes.",
    "slm_pred": "He replied calmly.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "maddened",
    "final_answer": "calm",
    "slm_pred": "The answer is \"maddened.\"",
    "llm_pred": "calm",
    "perf_chosen": 0.0,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "teach the boy a lesson",
    "final_answer": "teach the boy a lesson.",
    "slm_pred": "teach the boy a lesson.",
    "llm_pred": "teach the boy a lesson",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "in the face",
    "final_answer": "full in the face.",
    "slm_pred": "full in the face.",
    "llm_pred": "in the face",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "A stinging slap",
    "final_answer": "a club",
    "slm_pred": "the Indian club.",
    "llm_pred": "a club",
    "perf_chosen": 0.4,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "25a407d3ebc07e247047f874a3691e94f7e6cd7951c17e28d6853faa55b43f05",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "all the performers walked away",
    "final_answer": "all the performers walked away",
    "slm_pred": "The performers walked away.",
    "llm_pred": "all the performers walked away",
    "perf_chosen": 1.0,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Zink Media, Inc",
    "final_answer": "Zink Media, Inc.",
    "slm_pred": "Zink Media, Inc.",
    "llm_pred": "Zink Media, Inc.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Portland,",
    "final_answer": "The Discogs servers are located in Portland, Oregon, US.",
    "slm_pred": "The Discogs servers are located in Portland, Oregon, US.",
    "llm_pred": "Portland, Oregon, US.",
    "perf_chosen": 0.2,
    "perf_slm": 0.2,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "30 August 2000",
    "final_answer": "30 August 2000.",
    "slm_pred": "30 August 2000.",
    "llm_pred": "30 August 2000",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "8 million releases",
    "final_answer": "Over 8 million releases.",
    "slm_pred": "Over 8 million releases.",
    "llm_pred": "over 8 million releases",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "4.9 million",
    "final_answer": "The answer is nearly 4.9 million artists.",
    "slm_pred": "The answer is nearly 4.9 million artists.",
    "llm_pred": "nearly 4.9 million artists",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "1 million",
    "final_answer": "over 1 million labels.",
    "slm_pred": "over 1 million labels.",
    "llm_pred": "over 1 million labels",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, with constantly growing figures.",
    "slm_pred": "Yes, with constantly growing figures.",
    "llm_pred": "Yes, continuously.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Kevin Lewandowski",
    "final_answer": "Kevin Lewandowski.",
    "slm_pred": "Kevin Lewandowski.",
    "llm_pred": "Kevin Lewandowski",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "community-built sites",
    "final_answer": "community-built sites such as Slashdot, eBay, and Open Directory Project.",
    "slm_pred": "community-built sites such as Slashdot, eBay, and Open Directory Project.",
    "llm_pred": "the success of community-built sites like Slashdot, eBay, and the Open Directory Project",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.2666666667,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "electronic music",
    "final_answer": "Electronic music.",
    "slm_pred": "Electronic music.",
    "llm_pred": "electronic music",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes—now includes virtually all music genres.",
    "slm_pred": "Yes, it has expanded to support virtually every type of audio recording.",
    "llm_pred": "Yes—now includes virtually all music genres.",
    "perf_chosen": 0.0,
    "perf_slm": 0.1538461538,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "January 2004",
    "final_answer": "January 2004",
    "slm_pred": "In 2004.",
    "llm_pred": "January 2004",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to build the most comprehensive database of music",
    "final_answer": "to build the most comprehensive database of electronic music",
    "slm_pred": "To build the most comprehensive database of music releases.",
    "llm_pred": "to build the most comprehensive database of electronic music",
    "perf_chosen": 0.9411764706,
    "perf_slm": 0.9411764706,
    "perf_llm": 0.9411764706,
    "cost": 25.0
  },
  {
    "id": "7a06ee149134016d73c404060a134f4efe27254b278ce1d335c5e47d5d1f166c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, it is crowd sourced.",
    "slm_pred": "Yes, it is crowd sourced.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Buddhism",
    "final_answer": "Buddhism",
    "slm_pred": "Buddhism.",
    "llm_pred": "Buddhism",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Mahayana Buddhism",
    "final_answer": "Mahayana Buddhism.",
    "slm_pred": "Mahayana Buddhism.",
    "llm_pred": "Mahayana Buddhism",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "China",
    "final_answer": "China during the Tang dynasty.",
    "slm_pred": "China during the Tang dynasty.",
    "llm_pred": "China",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "During the Tang dynasty",
    "final_answer": "During the Tang dynasty.",
    "slm_pred": "The Tang dynasty.",
    "llm_pred": "During the Tang dynasty.",
    "perf_chosen": 1.0,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Taoism",
    "final_answer": "Taoism and Mahayana Buddhism.",
    "slm_pred": "Taoism and Mahayana Buddhism.",
    "llm_pred": "Taoism",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Japan",
    "final_answer": "Vietnam.",
    "slm_pred": "Vietnam.",
    "llm_pred": "The answer is Japan.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Meditation-practice",
    "final_answer": "Buddha-nature",
    "slm_pred": "Rigorous self-control and meditation-practice.",
    "llm_pred": "Buddha-nature",
    "perf_chosen": 0.0,
    "perf_slm": 0.4,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Rigorous self-control",
    "final_answer": "the Sanskrit word “dhyāna”",
    "slm_pred": "The answer is Dhyana (meditation).",
    "llm_pred": "the Sanskrit word “dhyāna”",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Insight into Buddha's nature",
    "final_answer": "meditative state",
    "slm_pred": "The answer is in the text.",
    "llm_pred": "meditative state",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "All the time, especially for the benefit of others.",
    "final_answer": "especially for the benefit of others.",
    "slm_pred": "especially for the benefit of others.",
    "llm_pred": "in daily life",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, it de-emphasizes mere knowledge of doctrine.",
    "slm_pred": "No, it de-emphasizes mere knowledge of doctrine.",
    "llm_pred": "No.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Direct understanding",
    "final_answer": "Direct understanding through zazen and interaction with an accomplished teacher.",
    "slm_pred": "Direct understanding through zazen and interaction with an accomplished teacher.",
    "llm_pred": "direct understanding through zazen and interaction with an accomplished teacher",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Through \"zazen\" and interaction with an accomplished teacher.",
    "final_answer": "zazen and interaction with an accomplished teacher",
    "slm_pred": "The answer is rigorous self-control and meditation-practice.",
    "llm_pred": "zazen and interaction with an accomplished teacher",
    "perf_chosen": 0.9333333333,
    "perf_slm": 0.1333333333,
    "perf_llm": 0.9333333333,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "禪",
    "final_answer": "The Middle Chinese word 禪 (Chan).",
    "slm_pred": "The Middle Chinese word 禪 (Chan).",
    "llm_pred": "Chán (禪)",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "ध्यान",
    "final_answer": "dhyāna",
    "slm_pred": "The Sanskrit word \"dhyāna\" (ध्यान ).",
    "llm_pred": "dhyāna",
    "perf_chosen": 0.0,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "\"absorption\" or \"meditative state\"",
    "final_answer": "The answer is \"absorption\" or \"meditative state\".",
    "slm_pred": "The answer is \"absorption\" or \"meditative state\".",
    "llm_pred": "meditative state",
    "perf_chosen": 0.7272727273,
    "perf_slm": 0.7272727273,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Indians",
    "final_answer": "Indians",
    "slm_pred": "The Indian practitioners of Dhyana.",
    "llm_pred": "Indians",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, it was influenced by Chan Buddhism.",
    "slm_pred": "Yes, it was influenced by Chan Buddhism.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "unknown",
    "final_answer": "Not mentioned.",
    "slm_pred": "Not mentioned.",
    "llm_pred": "Not mentioned.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "054ad98985a6dd328387f083de3eb77015b865edc8df3597572e518cf1f37217",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Japanese Zen",
    "final_answer": "Japanese Zen.",
    "slm_pred": "Japanese Zen.",
    "llm_pred": "Zen",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Charles VIII",
    "final_answer": "Charles VIII.",
    "slm_pred": "Charles VIII.",
    "llm_pred": "Charles VIII",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Naples",
    "final_answer": "Naples.",
    "slm_pred": "Naples.",
    "llm_pred": "Naples",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ferdinand II",
    "final_answer": "Ferdinand II.",
    "slm_pred": "Ferdinand II.",
    "llm_pred": "Ferdinand II",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "July 7",
    "final_answer": "July 7",
    "slm_pred": "July 7",
    "llm_pred": "by July 7",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, Ferdinand II reascended his throne.",
    "slm_pred": "Yes, Ferdinand II reascended his throne.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "D'Aubigny",
    "final_answer": "D'Aubigny, the French general.",
    "slm_pred": "D'Aubigny, the French general.",
    "llm_pred": "d'Aubigny",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the French general",
    "final_answer": "The Great Captain.",
    "slm_pred": "The Great Captain.",
    "llm_pred": "the Great Captain",
    "perf_chosen": 0.33333333330000003,
    "perf_slm": 0.33333333330000003,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the following year",
    "final_answer": "in the following year.",
    "slm_pred": "in the following year.",
    "llm_pred": "the following year",
    "perf_chosen": 0.8571428571,
    "perf_slm": 0.8571428571,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Pozzuoli",
    "final_answer": "Pozzuoli",
    "slm_pred": "The answer is Calabria.",
    "llm_pred": "Pozzuoli",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Gonzalo de Cordoba",
    "final_answer": "Gonzalo de Cordoba.",
    "slm_pred": "Gonzalo de Cordoba.",
    "llm_pred": "Gonzalo de Cordoba",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "an army",
    "final_answer": "an army",
    "slm_pred": "an army.",
    "llm_pred": "an army",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Ferdinand's and Isabella's",
    "final_answer": "Ferdinand and Isabella’s (the Spanish) army.",
    "slm_pred": "The Spanish army under the command of Gonzalo de Cordoba.",
    "llm_pred": "Ferdinand and Isabella’s (the Spanish) army.",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.0,
    "perf_llm": 0.2222222222,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Doña Sancia",
    "final_answer": "Doña Sancia of Aragon.",
    "slm_pred": "Doña Sancia of Aragon.",
    "llm_pred": "Doña Sancia of Aragon",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the habits acquired in the most licentious Court",
    "final_answer": "The habits acquired in the most licentious Court.",
    "slm_pred": "The habits acquired in the most licentious Court.",
    "llm_pred": "licentious court habits",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Lucrezia Borgia was married.",
    "slm_pred": "Lucrezia Borgia was married.",
    "llm_pred": "Yes—married to Giuffredo Borgia.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Giuffredo Borgia",
    "final_answer": "France",
    "slm_pred": "The answer is to Ferdinand and Isabella.",
    "llm_pred": "France",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Prince of Squillace",
    "final_answer": "Prince of Squillace",
    "slm_pred": "The Great Captain.",
    "llm_pred": "Prince of Squillace",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Calabria",
    "final_answer": "Calabria.",
    "slm_pred": "Calabria.",
    "llm_pred": "Calabria",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "1496",
    "final_answer": "July 7, 1496",
    "slm_pred": "The answer is in the spring of 1496.",
    "llm_pred": "July 7, 1496",
    "perf_chosen": 0.5,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "3dd812945c18a89fc1f9bda6020e4c502e3911c5da7ce8f1c4fcab5fa84a660c",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Marquis Gonzaga",
    "final_answer": "The Marquis Gonzaga of Mantua.",
    "slm_pred": "The Marquis Gonzaga of Mantua.",
    "llm_pred": "the Marquis Gonzaga of Mantua",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 0.5714285714,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "at Park House",
    "final_answer": "At Park House, a country estate leased from the royal family's Sandringham estate in Norfolk, England.",
    "slm_pred": "At Park House, a country estate leased from the royal family's Sandringham estate in Norfolk, England.",
    "llm_pred": "Park House on the Sandringham estate in Norfolk, England.",
    "perf_chosen": 0.3157894737,
    "perf_slm": 0.3157894737,
    "perf_llm": 0.33333333330000003,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "in Norfolk",
    "final_answer": "Norfolk, England",
    "slm_pred": "Norfolk, England.",
    "llm_pred": "Norfolk, England",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "The royal family leased it.",
    "slm_pred": "The royal family leased it.",
    "llm_pred": "No—her family leased it from the royal family.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the royal family's",
    "final_answer": "the royal family’s",
    "slm_pred": "The royal family's Sandringham estate, leased to Diana's family.",
    "llm_pred": "the royal family’s",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.5,
    "perf_llm": 0.6666666667000001,
    "cost": 25.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Johnnie and Frances Spencer, were well-known members of the English aristocracy",
    "final_answer": "Johnnie and Frances Spencer.",
    "slm_pred": "Johnnie and Frances Spencer.",
    "llm_pred": "Johnnie and Frances Spencer",
    "perf_chosen": 0.5333333333,
    "perf_slm": 0.5333333333,
    "perf_llm": 0.5333333333,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she did.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "Yes",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Three",
    "final_answer": "9",
    "slm_pred": "9.",
    "llm_pred": "9",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a nanny",
    "final_answer": "Inge Crane and Mary Clarke",
    "slm_pred": "Inge Crane, her nanny Mary Clarke.",
    "llm_pred": "Inge Crane and Mary Clarke",
    "perf_chosen": 0.0,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "about a mile away",
    "final_answer": "about a mile away.",
    "slm_pred": "about a mile away.",
    "llm_pred": "about a mile away",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Princes Andrew and Edward",
    "final_answer": "The young Princes Andrew and Edward.",
    "slm_pred": "The young Princes Andrew and Edward.",
    "llm_pred": "the young Princes Andrew and Edward",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to swim in the pool",
    "final_answer": "to swim in the pool",
    "slm_pred": "The answer is describe an \"ordinary\" childhood for Diana.",
    "llm_pred": "to swim in the pool",
    "perf_chosen": 1.0,
    "perf_slm": 0.1428571429,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "going outdoors, climbing trees and playing with animals",
    "final_answer": "She loved going outdoors, climbing trees, and playing with animals.",
    "slm_pred": "She loved going outdoors, climbing trees, and playing with animals.",
    "llm_pred": "going outdoors, climbing trees, and playing with animals",
    "perf_chosen": 0.8888888889000001,
    "perf_slm": 0.8888888889000001,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "5b24bb9f1e1127f67aa7cee4046e366eccf763e4fa3dd40580e59db7d048f35e",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "a tomboy.",
    "slm_pred": "a tomboy.",
    "llm_pred": "The answer is yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the barn",
    "final_answer": "The barn.",
    "slm_pred": "The barn.",
    "llm_pred": "the barn",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the chicken pen",
    "final_answer": "Gather eggs for breakfast.",
    "slm_pred": "Gather eggs for breakfast.",
    "llm_pred": "the house",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "so her father could make scrambled eggs",
    "final_answer": "For her father to make scrambled eggs for breakfast.",
    "slm_pred": "For her father to make scrambled eggs for breakfast.",
    "llm_pred": "so her father could make scrambled eggs for breakfast",
    "perf_chosen": 0.625,
    "perf_slm": 0.625,
    "perf_llm": 0.875,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Five",
    "final_answer": "At least 5 eggs for breakfast, with 2 for Janet and 3 for her father.",
    "slm_pred": "At least 5 eggs for breakfast, with 2 for Janet and 3 for her father.",
    "llm_pred": "5 eggs",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "four",
    "final_answer": "The answer is 5.",
    "slm_pred": "The answer is 5.",
    "llm_pred": "four",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a quacking sound",
    "final_answer": "a quacking sound.",
    "slm_pred": "a quacking sound.",
    "llm_pred": "a quacking sound",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a nest",
    "final_answer": "A nest near the pond with large brown eggs.",
    "slm_pred": "A nest near the pond with large brown eggs.",
    "llm_pred": "a nest with large brown eggs",
    "perf_chosen": 0.3636363636,
    "perf_slm": 0.3636363636,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "eggs",
    "final_answer": "large brown eggs",
    "slm_pred": "large brown eggs.",
    "llm_pred": "large brown eggs",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "brown",
    "final_answer": "The answer is brown.",
    "slm_pred": "The answer is brown.",
    "llm_pred": "The answer is brown.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No, they were large.",
    "slm_pred": "The eggs were large.",
    "llm_pred": "No, they were large.",
    "perf_chosen": 0.4,
    "perf_slm": 0.0,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "an egg",
    "final_answer": "Yes, she took an egg.",
    "slm_pred": "Yes, she took an egg.",
    "llm_pred": "an egg",
    "perf_chosen": 0.5714285714,
    "perf_slm": 0.5714285714,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "b12022c7907ecead72d03d4e81c1c97c9bd799b5479c440ee21570f4e351b745",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "No, initially she was not.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Donovan",
    "final_answer": "Donovan.",
    "slm_pred": "Donovan.",
    "llm_pred": "Donovan",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No.",
    "slm_pred": "He had no hesitation.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "very severe names",
    "final_answer": "very severe names.",
    "slm_pred": "very severe names.",
    "llm_pred": "very severe names",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Being stupid",
    "final_answer": "Because he’ll owe a pretty penny for legal fees.",
    "slm_pred": "The answer is because he is owed a pretty penny for law.",
    "llm_pred": "Because he’ll owe a pretty penny for legal fees.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "His honesty",
    "final_answer": "Sam's honesty.",
    "slm_pred": "Sam's honesty.",
    "llm_pred": "his honesty",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "call at Mrs. Byram's",
    "final_answer": "Call at Mrs. Byram's and congratulate Sam on his good proof of innocence.",
    "slm_pred": "Call at Mrs. Byram's and congratulate Sam on his good proof of innocence.",
    "llm_pred": "to call at Mrs. Byram’s and congratulate him",
    "perf_chosen": 0.4705882353,
    "perf_slm": 0.4705882353,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No; he berated himself.",
    "slm_pred": "No, they congratulated him.",
    "llm_pred": "No; he berated himself.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "congratulate him",
    "final_answer": "They celebrated their victory and made plans for a stock company.",
    "slm_pred": "They celebrated their victory and made plans for a stock company.",
    "llm_pred": "Had a long talk.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Bill",
    "final_answer": "Bill.",
    "slm_pred": "Bill.",
    "llm_pred": "Bill",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "the miners",
    "final_answer": "The miners.",
    "slm_pred": "The miners.",
    "llm_pred": "the miners",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he did.",
    "slm_pred": "Yes, he did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "making a stock company of the new mine",
    "final_answer": "making a stock company of the new mine.",
    "slm_pred": "making a stock company of the new mine.",
    "llm_pred": "making a stock company of the new mine",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Before returning home",
    "final_answer": "Before returning home.",
    "slm_pred": "Before returning home.",
    "llm_pred": "In the evening.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes.",
    "slm_pred": "Yes, he left.",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "to do whatever he believed their interests demanded",
    "final_answer": "full authority",
    "slm_pred": "The answer is with full authority.",
    "llm_pred": "full authority",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The cashier",
    "final_answer": "The cashier.",
    "slm_pred": "The cashier.",
    "llm_pred": "the cashier",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Donovan",
    "final_answer": "Donovan",
    "slm_pred": "The cashier.",
    "llm_pred": "Donovan",
    "perf_chosen": 1.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "a fish",
    "final_answer": "a fish",
    "slm_pred": "a fish.",
    "llm_pred": "a fish",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "No",
    "final_answer": "No.",
    "slm_pred": "No, Donovan had not seen Mr. Wright.",
    "llm_pred": "No.",
    "perf_chosen": 1.0,
    "perf_slm": 0.25,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "72167ee4b9986935f6f9c92707f7b5658c3d26b536db17dfd55710546c5da5d3",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Yes—he left with full authority.",
    "slm_pred": "Yes, he did.",
    "llm_pred": "Yes—he left with full authority.",
    "perf_chosen": 0.0,
    "perf_slm": 0.5,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He wanted to help the poor students",
    "final_answer": "He wanted to help poor students keep sports alive.",
    "slm_pred": "He wanted to help poor students keep sports alive.",
    "llm_pred": "to help poor students keep playing sports (covering their fees)",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.4705882353,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Really, she couldn't believe Michael Jordan did that for them.",
    "slm_pred": "Really, she couldn't believe Michael Jordan did that for them.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "yes",
    "final_answer": "Yes.",
    "slm_pred": "yes",
    "llm_pred": "Yes.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Grade 8",
    "final_answer": "Grade 8.",
    "slm_pred": "Grade 8.",
    "llm_pred": "Grade 8",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ridge Road Middle School",
    "final_answer": "Ridge Road Middle School.",
    "slm_pred": "Ridge Road Middle School.",
    "llm_pred": "The answer is Ridge Road Middle School.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "North Carolina",
    "final_answer": "North Carolina.",
    "slm_pred": "North Carolina.",
    "llm_pred": "North Carolina",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, basketball.",
    "slm_pred": "Yes, basketball.",
    "llm_pred": "Yes—basketball.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "basketball",
    "final_answer": "The answer is basketball.",
    "slm_pred": "The answer is basketball.",
    "llm_pred": "basketball",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "Yes, she did.",
    "slm_pred": "Yes, she did.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "$ 50",
    "final_answer": "The answer is $50.",
    "slm_pred": "The answer is $50.",
    "llm_pred": "$50",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "Initially, no.",
    "slm_pred": "Initially, no.",
    "llm_pred": "No, not at first.",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "d3d113a14c0e872010ed3a0697d627265d102c3c14f139457e084e176e68f293",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, she wasn't the only one.",
    "slm_pred": "No, she wasn't the only one.",
    "llm_pred": "No—other middle school players had the same problem.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The Proposal",
    "final_answer": "The Proposal.",
    "slm_pred": "The Proposal.",
    "llm_pred": "The Proposal",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "romantic comedy",
    "final_answer": "A traditional romantic comedy.",
    "slm_pred": "A traditional romantic comedy.",
    "llm_pred": "a romantic comedy",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.8,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "How the elements of the comedy are like the checklist of a charming, traditional bride?",
    "final_answer": "The elements of \"The Proposal\" are compared to a checklist of a charming, traditional bride.",
    "slm_pred": "The elements of \"The Proposal\" are compared to a checklist of a charming, traditional bride.",
    "llm_pred": "the film’s elements and a bride’s “something old, something new” wedding checklist",
    "perf_chosen": 0.7333333333000001,
    "perf_slm": 0.7333333333000001,
    "perf_llm": 0.2962962963,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Ryan Reynolds and Sandra Bullock",
    "final_answer": "Sandra Bullock and Ryan Reynolds.",
    "slm_pred": "Sandra Bullock and Ryan Reynolds.",
    "llm_pred": "Sandra Bullock and Ryan Reynolds.",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "Yes, he is her executive assistant.",
    "slm_pred": "Yes, he is her executive assistant.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "no",
    "final_answer": "No—they dislike each other.",
    "slm_pred": "They start out disliking each other.",
    "llm_pred": "No—they dislike each other.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Margaret,",
    "final_answer": "Margaret, a pitiless, high-powered New York book editor.",
    "slm_pred": "Margaret, a pitiless, high-powered New York book editor.",
    "llm_pred": "Margaret",
    "perf_chosen": 0.2222222222,
    "perf_slm": 0.2222222222,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, his character's name is Andrew.",
    "slm_pred": "No, his character's name is Andrew.",
    "llm_pred": "No — his name is Andrew.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "executive assistant",
    "final_answer": "executive assistant",
    "slm_pred": "Her executive assistant.",
    "llm_pred": "executive assistant",
    "perf_chosen": 1.0,
    "perf_slm": 0.8,
    "perf_llm": 1.0,
    "cost": 25.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "book editor",
    "final_answer": "her mid-40s",
    "slm_pred": "The fatal flaw - she's Canadian.",
    "llm_pred": "her mid-40s",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "no",
    "final_answer": "No, they are not.",
    "slm_pred": "No, they are not.",
    "llm_pred": "The answer is No.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "yes",
    "final_answer": "The plot has potential, driven by witty banter and a charming on-screen chemistry between the leads.",
    "slm_pred": "The plot has potential, driven by witty banter and a charming on-screen chemistry between the leads.",
    "llm_pred": "No",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "marrying Andrew",
    "final_answer": "they plan to marry.",
    "slm_pred": "they plan to marry.",
    "llm_pred": "by a sham green-card marriage to Andrew",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.2222222222,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "a green-card",
    "final_answer": "A green card.",
    "slm_pred": "A green card.",
    "llm_pred": "a green card",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 0.4,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Alaska",
    "final_answer": "Andrew's family in Alaska.",
    "slm_pred": "Andrew's family in Alaska.",
    "llm_pred": "Alaska",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "No",
    "slm_pred": "No, they hate each other.",
    "llm_pred": "No",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "the groom-to-be's family",
    "final_answer": "Ryan Reynolds",
    "slm_pred": "The answer is Sandra Bullock and Ryan Reynolds.",
    "llm_pred": "Ryan Reynolds",
    "perf_chosen": 0.0,
    "perf_slm": 0.1818181818,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "No",
    "final_answer": "No, they realize they love each other.",
    "slm_pred": "No, they realize they love each other.",
    "llm_pred": "No—they fall in love.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "8d0cb99e7d77b13be933c031359b18914f1e5bad853c08b92af347c0678129e8",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Shakespeare's 1590s",
    "final_answer": "The 1940s and '50s, or Shakespeare's 1590s.",
    "slm_pred": "The 1940s and '50s, or Shakespeare's 1590s.",
    "llm_pred": "the 1940s and ’50s",
    "perf_chosen": 0.4444444444,
    "perf_slm": 0.4444444444,
    "perf_llm": 0.0,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "It stands for Height about average terrain",
    "final_answer": "Height Above Average Terrain.",
    "slm_pred": "Height Above Average Terrain.",
    "llm_pred": "Height Above Average Terrain",
    "perf_chosen": 0.5454545455,
    "perf_slm": 0.5454545455,
    "perf_llm": 0.5454545455,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "The less popular version, EHAAT, Effective Height Above Average Terrain",
    "final_answer": "EHAAT (Effective Height Above Average Terrain)",
    "slm_pred": "EHAAT (Effective Height Above Average Terrain)",
    "llm_pred": "EHAAT",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.1818181818,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, it is possible.",
    "slm_pred": "Yes, it is possible.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.4,
    "perf_slm": 0.4,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "More commonly, FM radio and television",
    "final_answer": "FM radio and television.",
    "slm_pred": "FM radio and television.",
    "llm_pred": "FM radio and television broadcasting",
    "perf_chosen": 0.8,
    "perf_slm": 0.8,
    "perf_llm": 0.7272727273,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Usually VHF and UHF",
    "final_answer": "VHF and UHF transmissions.",
    "slm_pred": "VHF and UHF transmissions.",
    "llm_pred": "VHF and UHF",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "NO",
    "final_answer": "No, it is officially measured in meters.",
    "slm_pred": "No, it is officially measured in meters.",
    "llm_pred": "The answer is: No, in meters.",
    "perf_chosen": 0.25,
    "perf_slm": 0.25,
    "perf_llm": 0.2857142857,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Meters",
    "final_answer": "Meters.",
    "slm_pred": "Meters.",
    "llm_pred": "meters",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "1d0b6576ff62eda79b6426d863d41303e5bda0d3909d002496931cb021ce4e64",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "average altitude",
    "final_answer": "the average altitude of the specified points from the antenna site’s altitude",
    "slm_pred": "The average altitude of the specified points is subtracted from the altitude of the antenna site.",
    "llm_pred": "the average altitude of the specified points from the antenna site’s altitude",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2222222222,
    "perf_llm": 0.2857142857,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Michael Brewer",
    "final_answer": "A teenage boy who was set on fire by a group of teenagers and his family's experience with his recovery.",
    "slm_pred": "A teenage boy who was set on fire by a group of teenagers and his family's experience with his recovery.",
    "llm_pred": "Michael Brewer",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "He was burned",
    "final_answer": "He was set on fire.",
    "slm_pred": "He was burned over 65 percent of his body by a group of teenagers.",
    "llm_pred": "He was set on fire.",
    "perf_chosen": 0.5,
    "perf_slm": 0.3529411765,
    "perf_llm": 0.5,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "over 65% of his body",
    "final_answer": "over 65 percent of his body",
    "slm_pred": "He was burned over 65 percent of his body.",
    "llm_pred": "over 65 percent of his body",
    "perf_chosen": 0.9090909091,
    "perf_slm": 0.7142857143,
    "perf_llm": 0.9090909091,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.25,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "Allegedly a group of teenagers.",
    "slm_pred": "A 13-year-old boy's older brother, who has been charged.",
    "llm_pred": "Allegedly a group of teenagers.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Jeremy Jarvis' older brother.",
    "final_answer": "Jeremy Jarvis.",
    "slm_pred": "Jeremy Jarvis.",
    "llm_pred": "Jeremy Jarvis",
    "perf_chosen": 0.6666666667000001,
    "perf_slm": 0.6666666667000001,
    "perf_llm": 0.6666666667000001,
    "cost": 5.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Yes",
    "final_answer": "Yes, he remains in the hospital.",
    "slm_pred": "Yes, he remains in the hospital.",
    "llm_pred": "Yes.",
    "perf_chosen": 0.2857142857,
    "perf_slm": 0.2857142857,
    "perf_llm": 1.0,
    "cost": 5.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.5,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "He has a lifelong recoery.",
    "final_answer": "A lifelong recovery.",
    "slm_pred": "A lifelong recovery.",
    "llm_pred": "a lifelong recovery",
    "perf_chosen": 0.5,
    "perf_slm": 0.5,
    "perf_llm": 0.5,
    "cost": 5.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.75,
    "threshold": 0.5,
    "action": "keep",
    "model_used": "lm1",
    "gold_answer": "Valerie Brewer and her husband Michael Brewer, Sr.",
    "final_answer": "Michael Brewer, Sr. and his wife Valerie Brewer.",
    "slm_pred": "Michael Brewer, Sr. and his wife Valerie Brewer.",
    "llm_pred": "Valerie Brewer and Michael Brewer Sr.",
    "perf_chosen": 0.75,
    "perf_slm": 0.75,
    "perf_llm": 0.8571428571,
    "cost": 5.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "Yes",
    "final_answer": "No.",
    "slm_pred": "No, they are not related to the victim.",
    "llm_pred": "No.",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "His parents.",
    "final_answer": "after being set on fire by a group of teenagers",
    "slm_pred": "The answer is \"it enlightens at the same time to know my son is strong enough to make it through on a daily basis.\"",
    "llm_pred": "after being set on fire by a group of teenagers",
    "perf_chosen": 0.0,
    "perf_slm": 0.0,
    "perf_llm": 0.0,
    "cost": 25.0
  },
  {
    "id": "468e3e42e3fb9c5357515ab00c1868a8c176d0f03901685c1fcfd96960e5b5d6",
    "dataset": "coqa_short",
    "slm_confidence": 0.0,
    "threshold": 0.5,
    "action": "route",
    "model_used": "lm2",
    "gold_answer": "CNN",
    "final_answer": "CNN",
    "slm_pred": "CNN.",
    "llm_pred": "CNN",
    "perf_chosen": 1.0,
    "perf_slm": 1.0,
    "perf_llm": 1.0,
    "cost": 25.0
  }
]