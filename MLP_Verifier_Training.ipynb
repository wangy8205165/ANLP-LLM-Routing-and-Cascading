{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP Verifier Training for Router\n",
        "\n",
        "This notebook trains small MLP models to predict whether LLM or SLM should be used based on confidence scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Required libraries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Dict, Tuple\n",
        "from metric_compute import f1_score, normalize_cnli_label\n",
        "from prepare_router_data import compute_perf\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost Parameters:\n",
            "  C_SLM: 1.0\n",
            "  C_LLM: 20.0\n",
            "  C_ver: 4.0\n",
            "  COST_LM1: 5.0\n",
            "  COST_LM2: 25.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATASETS = [\"cnli_short\", \"coqa_short\", \"narrative_qa_short\", \"qasper_short\"]\n",
        "ROUTER_DATA_DIR = \"router_data\"\n",
        "MODEL_SAVE_DIR = \"mlp_verifier_models\"\n",
        "RESULTS_DIR = \"mlp_verifier_results\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Cost parameters\n",
        "C_SLM = 1.0\n",
        "C_LLM = 20.0\n",
        "C_ver = 4 * C_SLM\n",
        "COST_LM1 = C_SLM + C_ver  # 5.0\n",
        "COST_LM2 = C_SLM + C_ver + C_LLM  # 25.0\n",
        "\n",
        "# Training parameters\n",
        "TRAIN_RATIO = 0.8\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Logits thresholds to test\n",
        "LOGITS_THRESHOLDS = np.arange(0.1, 1.0, 0.1)\n",
        "\n",
        "print(f\"Cost Parameters:\")\n",
        "print(f\"  C_SLM: {C_SLM}\")\n",
        "print(f\"  C_LLM: {C_LLM}\")\n",
        "print(f\"  C_ver: {C_ver}\")\n",
        "print(f\"  COST_LM1: {COST_LM1}\")\n",
        "print(f\"  COST_LM2: {COST_LM2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define MLP model with structure: 32-64-64-64-32\n",
        "class MLPVerifier(nn.Module):\n",
        "    def __init__(self, input_dim=1):\n",
        "        super(MLPVerifier, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Dataset class\n",
        "class ConfidenceDataset(Dataset):\n",
        "    def __init__(self, confidences, labels):\n",
        "        self.confidences = torch.FloatTensor(confidences).unsqueeze(1)\n",
        "        self.labels = torch.FloatTensor(labels).unsqueeze(1)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.confidences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.confidences[idx], self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mlp_verifier(dataset_name: str, confidences: np.ndarray, labels: np.ndarray) -> MLPVerifier:\n",
        "    \"\"\"\n",
        "    Train MLP verifier for a dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        confidences: Array of confidence scores\n",
        "        labels: Array of binary labels (1 if LLM better, 0 if SLM better)\n",
        "    \n",
        "    Returns:\n",
        "        Trained MLP model\n",
        "    \"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        confidences, labels, test_size=1-TRAIN_RATIO, random_state=42, stratify=labels\n",
        "    )\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = ConfidenceDataset(X_train, y_train)\n",
        "    test_dataset = ConfidenceDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = MLPVerifier(input_dim=1).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    \n",
        "    # Training loop\n",
        "    best_test_acc = 0.0\n",
        "    patience = 20  # Increased patience to allow more training\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for conf, lab in train_loader:\n",
        "            conf, lab = conf.to(device), lab.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(conf)\n",
        "            loss = criterion(outputs, lab)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for conf, lab in test_loader:\n",
        "                conf, lab = conf.to(device), lab.to(device)\n",
        "                outputs = model(conf)\n",
        "                loss = criterion(outputs, lab)\n",
        "                test_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += lab.size(0)\n",
        "                correct += (predicted == lab).sum().item()\n",
        "        \n",
        "        test_acc = correct / total\n",
        "        \n",
        "        # Print every epoch for first 20 epochs, then every 10\n",
        "        if (epoch + 1) <= 20 or (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
        "                  f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_acc:.4f}, \"\n",
        "                  f\"Best: {best_test_acc:.4f}, Patience: {patience_counter}/{patience}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            patience_counter = 0\n",
        "            if (epoch + 1) <= 20 or (epoch + 1) % 10 == 0:\n",
        "                print(f\"  -> New best test accuracy!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
        "                break\n",
        "    \n",
        "    print(f\"\\nTraining completed. Best test accuracy: {best_test_acc:.4f}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_delta_ibc(p_slm: float, p_llm: float, p_m: float, c_m: float, \n",
        "                        c_slm: float = COST_LM1, c_llm: float = COST_LM2) -> float:\n",
        "    \"\"\"\n",
        "    Calculate delta IBC.\n",
        "    \n",
        "    IBC_M = (P_M - P_SLM) / (C_M - C_SLM)\n",
        "    IBC_BASE = (P_LLM - P_SLM) / (C_LLM - C_SLM)\n",
        "    delta_IBC = ((IBC_M - IBC_BASE) / IBC_BASE) * 100\n",
        "    \"\"\"\n",
        "    if c_m <= c_slm:\n",
        "        return -100.0\n",
        "    \n",
        "    ibc_m = (p_m - p_slm) / (c_m - c_slm)\n",
        "    ibc_base = (p_llm - p_slm) / (c_llm - c_slm)\n",
        "    \n",
        "    if ibc_base == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    delta_ibc = ((ibc_m - ibc_base) / ibc_base) * 100\n",
        "    return delta_ibc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each dataset\n",
        "all_results = {}\n",
        "\n",
        "for dataset_name in DATASETS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load router data\n",
        "    router_data_path = os.path.join(ROUTER_DATA_DIR, f\"router_data_{dataset_name}.jsonl\")\n",
        "    if not os.path.exists(router_data_path):\n",
        "        print(f\"Warning: {router_data_path} not found, skipping...\")\n",
        "        continue\n",
        "    \n",
        "    df = pd.read_json(router_data_path, lines=True, orient=\"records\")\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    \n",
        "    # Create labels: 1 if LLM better, 0 if SLM better\n",
        "    labels = (df[\"perf_llm\"] > df[\"perf_slm\"]).astype(int).values\n",
        "    confidences = df[\"slm_confidence\"].values\n",
        "    \n",
        "    print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "    print(f\"Confidence range: [{confidences.min():.2f}, {confidences.max():.2f}]\")\n",
        "    \n",
        "    # Train MLP verifier\n",
        "    print(\"\\nTraining MLP verifier...\")\n",
        "    model = train_mlp_verifier(dataset_name, confidences, labels)\n",
        "    \n",
        "    # Save model\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, f\"mlp_verifier_{dataset_name}.pth\")\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to: {model_save_path}\")\n",
        "    \n",
        "    # Evaluate on all data with different logits thresholds\n",
        "    model.eval()\n",
        "    confidences_tensor = torch.FloatTensor(confidences).unsqueeze(1).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        mlp_predictions = model(confidences_tensor).cpu().numpy().flatten()\n",
        "    \n",
        "    # Test different logits thresholds\n",
        "    threshold_results = []\n",
        "    \n",
        "    for threshold in LOGITS_THRESHOLDS:\n",
        "        # Combine MLP prediction with threshold\n",
        "        # If MLP predicts 1 (LLM better) OR confidence >= threshold, use LLM\n",
        "        # Otherwise use SLM\n",
        "        use_llm = (mlp_predictions > 0.5) | (confidences >= threshold)\n",
        "        \n",
        "        # Get final predictions and performance\n",
        "        final_perfs = np.where(use_llm, df[\"perf_llm\"].values, df[\"perf_slm\"].values)\n",
        "        final_costs = np.where(use_llm, COST_LM2, COST_LM1)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        avg_perf = final_perfs.mean()\n",
        "        avg_cost = final_costs.mean()\n",
        "        lm1_count = (~use_llm).sum()\n",
        "        lm2_count = use_llm.sum()\n",
        "        \n",
        "        # Calculate delta IBC\n",
        "        p_slm = df[\"perf_slm\"].mean()\n",
        "        p_llm = df[\"perf_llm\"].mean()\n",
        "        delta_ibc = calculate_delta_ibc(p_slm, p_llm, avg_perf, avg_cost)\n",
        "        \n",
        "        threshold_results.append({\n",
        "            \"threshold\": float(threshold),\n",
        "            \"avg_perf\": float(avg_perf),\n",
        "            \"avg_cost\": float(avg_cost),\n",
        "            \"lm1_count\": int(lm1_count),\n",
        "            \"lm2_count\": int(lm2_count),\n",
        "            \"lm1_ratio\": float(lm1_count / len(df)),\n",
        "            \"lm2_ratio\": float(lm2_count / len(df)),\n",
        "            \"delta_ibc\": float(delta_ibc),\n",
        "            \"p_slm\": float(p_slm),\n",
        "            \"p_llm\": float(p_llm)\n",
        "        })\n",
        "    \n",
        "    all_results[dataset_name] = threshold_results\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Results Summary for {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    results_df = pd.DataFrame(threshold_results)\n",
        "    print(results_df[[\"threshold\", \"avg_perf\", \"avg_cost\", \"lm1_ratio\", \"lm2_ratio\", \"delta_ibc\"]].to_string(index=False))\n",
        "    \n",
        "    # Save results\n",
        "    results_path = os.path.join(RESULTS_DIR, f\"mlp_verifier_results_{dataset_name}.json\")\n",
        "    with open(results_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(threshold_results, f, indent=2)\n",
        "    print(f\"\\nResults saved to: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate plots for each dataset\n",
        "for dataset_name in DATASETS:\n",
        "    if dataset_name not in all_results:\n",
        "        continue\n",
        "    \n",
        "    results = all_results[dataset_name]\n",
        "    thresholds = [r[\"threshold\"] for r in results]\n",
        "    performances = [r[\"avg_perf\"] for r in results]\n",
        "    delta_ibcs = [r[\"delta_ibc\"] for r in results]\n",
        "    \n",
        "    # Create figure with dual y-axes (wider to accommodate all x-axis labels)\n",
        "    fig, ax1 = plt.subplots(figsize=(max(12, len(thresholds) * 1.2), 6))\n",
        "    \n",
        "    # Left y-axis: Performance\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel('Logits Threshold', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Performance (Accuracy/F1)', color=color1, fontsize=12, fontweight='bold')\n",
        "    line1 = ax1.plot(thresholds, performances, color=color1, marker='o', linewidth=2, \n",
        "                     markersize=6, label='Performance')\n",
        "    ax1.tick_params(axis='y', labelcolor=color1)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Set x-axis ticks to show all threshold values\n",
        "    ax1.set_xticks(thresholds)\n",
        "    ax1.set_xticklabels([f'{t:.1f}' for t in thresholds], rotation=45, ha='right', fontsize=9)\n",
        "    \n",
        "    # Right y-axis: Delta IBC\n",
        "    ax2 = ax1.twinx()\n",
        "    color2 = 'tab:red'\n",
        "    ax2.set_ylabel('Delta IBC (%)', color=color2, fontsize=12, fontweight='bold')\n",
        "    line2 = ax2.plot(thresholds, delta_ibcs, color=color2, marker='s', linewidth=2, \n",
        "                     markersize=6, label='Delta IBC', linestyle='--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color2)\n",
        "    \n",
        "    # Add title\n",
        "    plt.title(f'{dataset_name.upper()} - MLP Verifier Results\\nPerformance vs Delta IBC', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add legend\n",
        "    lines = line1 + line2\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax1.legend(lines, labels, loc='best', fontsize=10)\n",
        "    \n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    \n",
        "    # Save plot (both PDF and PNG)\n",
        "    plot_path_pdf = os.path.join(RESULTS_DIR, f'{dataset_name}_mlp_verifier_plot.pdf')\n",
        "    plt.savefig(plot_path_pdf, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Plot saved to: {plot_path_pdf}\")\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table across all datasets\n",
        "summary_data = []\n",
        "for dataset_name in DATASETS:\n",
        "    if dataset_name not in all_results:\n",
        "        continue\n",
        "    \n",
        "    results = all_results[dataset_name]\n",
        "    for r in results:\n",
        "        summary_data.append({\n",
        "            \"dataset\": dataset_name,\n",
        "            \"threshold\": r[\"threshold\"],\n",
        "            \"avg_perf\": r[\"avg_perf\"],\n",
        "            \"avg_cost\": r[\"avg_cost\"],\n",
        "            \"delta_ibc\": r[\"delta_ibc\"],\n",
        "            \"lm1_ratio\": r[\"lm1_ratio\"],\n",
        "            \"lm2_ratio\": r[\"lm2_ratio\"]\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Save summary\n",
        "summary_path = os.path.join(RESULTS_DIR, \"mlp_verifier_summary.csv\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"Summary saved to: {summary_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Summary Table\")\n",
        "print(\"=\"*60)\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "10601-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
