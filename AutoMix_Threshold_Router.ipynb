{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoMix Threshold-Based Router\n",
        "\n",
        "This notebook implements a simple threshold-based router for the AutoMix model.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The router makes routing decisions based on the verifier confidence probability `v` from LM1:\n",
        "- If `v ≥ threshold t`: Keep LM1's answer\n",
        "- If `v < threshold t`: Route to LM2\n",
        "\n",
        "The threshold `t` can be varied to analyze the cost-performance trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Import required libraries\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the dataset name and other parameters here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: qasper_short\n",
            "Router data path: router_data/router_data_qasper_short.jsonl\n",
            "Output directory: threshold_router_outputs\n",
            "\n",
            "Cost Parameters:\n",
            "  C_SLM: 1.0\n",
            "  C_LLM: 20.0\n",
            "  C_ver: 4.0\n",
            "  Cost when keeping LM1: 5.0\n",
            "  Cost when routing to LM2: 25.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Configuration\n",
        "# ============================================================\n",
        "\n",
        "# Dataset name: cnli_short / coqa_short / narrativeqa_short / qasper_short\n",
        "DATASET_NAME = \"qasper_short\"\n",
        "\n",
        "# Router data path\n",
        "ROUTER_DATA_PATH = os.path.join(\"router_data\", f\"router_data_{DATASET_NAME}.jsonl\")\n",
        "\n",
        "# Output directory for results\n",
        "OUTPUT_DIR = \"threshold_router_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Cost parameters (for cost-performance analysis)\n",
        "# Cost model: C = C_SLM + C_ver + w * C_LLM\n",
        "# where C_ver = C_SLM (verification is performed by SLM)\n",
        "C_SLM = 1.0  # Cost of small language model (LM1)\n",
        "C_LLM = 20.0  # Cost of large language model (LM2)\n",
        "C_ver = 4 * C_SLM  # Cost of verification (performed by SLM)\n",
        "\n",
        "# Cost when keeping LM1: C = C_SLM + C_ver = 2 * C_SLM\n",
        "# Cost when routing to LM2: C = C_SLM + C_ver + C_LLM = 2 * C_SLM + C_LLM\n",
        "COST_LM1 = C_SLM + C_ver\n",
        "COST_LM2 = C_SLM + C_ver + C_LLM\n",
        "\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Router data path: {ROUTER_DATA_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"\\nCost Parameters:\")\n",
        "print(f\"  C_SLM: {C_SLM}\")\n",
        "print(f\"  C_LLM: {C_LLM}\")\n",
        "print(f\"  C_ver: {C_ver}\")\n",
        "print(f\"  Cost when keeping LM1: {COST_LM1}\")\n",
        "print(f\"  Cost when routing to LM2: {COST_LM2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Router Data\n",
        "\n",
        "Load the router data that contains:\n",
        "- `slm_pred`: LM1's prediction\n",
        "- `llm_pred`: LM2's prediction\n",
        "- `slm_confidence`: Verifier confidence probability `v`\n",
        "- `perf_slm`: Performance of LM1's answer\n",
        "- `perf_llm`: Performance of LM2's answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading router data from router_data/router_data_qasper_short.jsonl\n",
            "[INFO] Loaded 1000 samples\n",
            "[INFO] Columns: ['id', 'dataset', 'gold_output', 'slm_pred', 'llm_pred', 'slm_confidence', 'perf_slm', 'perf_llm']\n",
            "\n",
            "[INFO] Confidence value distribution:\n",
            "0.00    225\n",
            "0.25     16\n",
            "0.50     14\n",
            "0.75    745\n",
            "Name: slm_confidence, dtype: int64\n",
            "\n",
            "[INFO] Sample data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dataset</th>\n",
              "      <th>gold_output</th>\n",
              "      <th>slm_pred</th>\n",
              "      <th>llm_pred</th>\n",
              "      <th>slm_confidence</th>\n",
              "      <th>perf_slm</th>\n",
              "      <th>perf_llm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>753990d0b621d390ed58f20c4d9e4f065f0dc672</td>\n",
              "      <td>qasper_short</td>\n",
              "      <td>a vocabulary of positive and negative predicat...</td>\n",
              "      <td>A seed lexicon consisting of 15 positive words...</td>\n",
              "      <td>a small list of 30 Japanese emotion predicates...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.256410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>753990d0b621d390ed58f20c4d9e4f065f0dc672</td>\n",
              "      <td>qasper_short</td>\n",
              "      <td>seed lexicon consists of positive and negative...</td>\n",
              "      <td>A seed lexicon consisting of 15 positive words...</td>\n",
              "      <td>a list of 15 positive and 15 negative Japanese...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9d578ddccc27dd849244d632dd0f6bf27348ad81</td>\n",
              "      <td>qasper_short</td>\n",
              "      <td>Using all data to train: AL -- BiGRU achieved ...</td>\n",
              "      <td>The proposed method performed well, even with ...</td>\n",
              "      <td>BiGRU trained with AL+CA+CO achieved the highe...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.090090</td>\n",
              "      <td>0.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>02e4bf719b1a504e385c35c6186742e720bcb281</td>\n",
              "      <td>qasper_short</td>\n",
              "      <td>based on the relation between events, the sugg...</td>\n",
              "      <td>The answer is through the exploitation of disc...</td>\n",
              "      <td>By exploiting discourse relations: for Cause p...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.318182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>02e4bf719b1a504e385c35c6186742e720bcb281</td>\n",
              "      <td>qasper_short</td>\n",
              "      <td>cause relation: both events in the relation sh...</td>\n",
              "      <td>The relations are used to propagate polarity t...</td>\n",
              "      <td>By using discourse relations between event pai...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.355556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id       dataset  \\\n",
              "0  753990d0b621d390ed58f20c4d9e4f065f0dc672  qasper_short   \n",
              "1  753990d0b621d390ed58f20c4d9e4f065f0dc672  qasper_short   \n",
              "2  9d578ddccc27dd849244d632dd0f6bf27348ad81  qasper_short   \n",
              "3  02e4bf719b1a504e385c35c6186742e720bcb281  qasper_short   \n",
              "4  02e4bf719b1a504e385c35c6186742e720bcb281  qasper_short   \n",
              "\n",
              "                                         gold_output  \\\n",
              "0  a vocabulary of positive and negative predicat...   \n",
              "1  seed lexicon consists of positive and negative...   \n",
              "2  Using all data to train: AL -- BiGRU achieved ...   \n",
              "3  based on the relation between events, the sugg...   \n",
              "4  cause relation: both events in the relation sh...   \n",
              "\n",
              "                                            slm_pred  \\\n",
              "0  A seed lexicon consisting of 15 positive words...   \n",
              "1  A seed lexicon consisting of 15 positive words...   \n",
              "2  The proposed method performed well, even with ...   \n",
              "3  The answer is through the exploitation of disc...   \n",
              "4  The relations are used to propagate polarity t...   \n",
              "\n",
              "                                            llm_pred  slm_confidence  \\\n",
              "0  a small list of 30 Japanese emotion predicates...            0.75   \n",
              "1  a list of 15 positive and 15 negative Japanese...            0.75   \n",
              "2  BiGRU trained with AL+CA+CO achieved the highe...            0.75   \n",
              "3  By exploiting discourse relations: for Cause p...            0.75   \n",
              "4  By using discourse relations between event pai...            0.75   \n",
              "\n",
              "   perf_slm  perf_llm  \n",
              "0  0.357143  0.256410  \n",
              "1  0.600000  0.357143  \n",
              "2  0.090090  0.105263  \n",
              "3  0.176471  0.318182  \n",
              "4  0.222222  0.355556  "
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Load router data\n",
        "# ============================================================\n",
        "\n",
        "if not os.path.exists(ROUTER_DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Router data not found: {ROUTER_DATA_PATH}\")\n",
        "\n",
        "print(f\"[INFO] Loading router data from {ROUTER_DATA_PATH}\")\n",
        "df = pd.read_json(ROUTER_DATA_PATH, lines=True, orient=\"records\")\n",
        "\n",
        "print(f\"[INFO] Loaded {len(df)} samples\")\n",
        "print(f\"[INFO] Columns: {list(df.columns)}\")\n",
        "print(f\"\\n[INFO] Confidence value distribution:\")\n",
        "print(df[\"slm_confidence\"].value_counts().sort_index())\n",
        "print(f\"\\n[INFO] Sample data:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold-Based Router Function\n",
        "\n",
        "Implement the simple threshold-based routing logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Threshold-based router function\n",
        "# ============================================================\n",
        "\n",
        "def apply_threshold_router(df: pd.DataFrame, threshold: float) -> Tuple[pd.DataFrame, Dict]:\n",
        "    \"\"\"\n",
        "    Apply threshold-based router to route queries between LM1 and LM2.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame containing router data\n",
        "        threshold: Threshold value t. If verifier confidence v >= t, use LM1, else route to LM2.\n",
        "    \n",
        "    Returns:\n",
        "        results_df: DataFrame with routing results\n",
        "        stats: Dictionary with summary statistics\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    lm1_count = 0\n",
        "    lm2_count = 0\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        v = float(row[\"slm_confidence\"])  # Verifier confidence\n",
        "        slm_pred = row[\"slm_pred\"]  # LM1's answer\n",
        "        llm_pred = row[\"llm_pred\"]  # LM2's answer\n",
        "        \n",
        "        # Routing decision: if v >= threshold, use LM1, else route to LM2\n",
        "        # Cost model: C = C_SLM + C_ver + w * C_LLM\n",
        "        if v >= threshold:\n",
        "            final_ans = slm_pred\n",
        "            action = \"keep\"  # Keep LM1's answer\n",
        "            model_used = \"lm1\"\n",
        "            lm1_count += 1\n",
        "            cost = COST_LM1  # C_SLM + C_ver = 2 * C_SLM\n",
        "        else:\n",
        "            final_ans = llm_pred\n",
        "            action = \"route\"  # Route to LM2\n",
        "            model_used = \"lm2\"\n",
        "            lm2_count += 1\n",
        "            cost = COST_LM2  # C_SLM + C_ver + C_LLM = 2 * C_SLM + C_LLM\n",
        "        \n",
        "        # Compute performance for the chosen answer\n",
        "        perf_chosen = row[\"perf_slm\"] if model_used == \"lm1\" else row[\"perf_llm\"]\n",
        "        \n",
        "        results.append({\n",
        "            \"id\": row.get(\"id\", i),\n",
        "            \"dataset\": row.get(\"dataset\", DATASET_NAME),\n",
        "            \"slm_confidence\": v,\n",
        "            \"threshold\": threshold,\n",
        "            \"action\": action,\n",
        "            \"model_used\": model_used,\n",
        "            \"gold_answer\": row.get(\"gold_output\"),\n",
        "            \"final_answer\": final_ans,\n",
        "            \"slm_pred\": slm_pred,\n",
        "            \"llm_pred\": llm_pred,\n",
        "            \"perf_chosen\": perf_chosen,\n",
        "            \"perf_slm\": row.get(\"perf_slm\"),\n",
        "            \"perf_llm\": row.get(\"perf_llm\"),\n",
        "            \"cost\": cost,\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Compute summary statistics\n",
        "    total = len(results_df)\n",
        "    lm1_ratio = lm1_count / total\n",
        "    lm2_ratio = lm2_count / total\n",
        "    avg_perf = results_df[\"perf_chosen\"].mean()\n",
        "    avg_perf_slm = results_df[\"perf_slm\"].mean()\n",
        "    avg_perf_llm = results_df[\"perf_llm\"].mean()\n",
        "    avg_cost = results_df[\"cost\"].mean()\n",
        "    total_cost = results_df[\"cost\"].sum()\n",
        "    \n",
        "    stats = {\n",
        "        \"threshold\": threshold,\n",
        "        \"total\": total,\n",
        "        \"lm1_count\": lm1_count,\n",
        "        \"lm2_count\": lm2_count,\n",
        "        \"lm1_ratio\": lm1_ratio,\n",
        "        \"lm2_ratio\": lm2_ratio,\n",
        "        \"avg_perf\": avg_perf,\n",
        "        \"avg_perf_slm\": avg_perf_slm,\n",
        "        \"avg_perf_llm\": avg_perf_llm,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"total_cost\": total_cost,\n",
        "    }\n",
        "    \n",
        "    return results_df, stats\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Testing threshold = 0.5\n",
            "\n",
            "============================================================\n",
            "Threshold Router Results for coqa_short\n",
            "============================================================\n",
            "Threshold: 0.500\n",
            "Total samples: 1000\n",
            "LM1 (keep): 668 (66.80%)\n",
            "LM2 (route): 332 (33.20%)\n",
            "\n",
            "Performance:\n",
            "  Average performance (routed): 0.5256\n",
            "  Average performance (LM1 only): 0.4626\n",
            "  Average performance (LM2 only): 0.5939\n",
            "\n",
            "Cost:\n",
            "  Average cost per sample: 8.64\n",
            "  Total cost: 8640.00\n",
            "============================================================\n",
            "\n",
            "Sample results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slm_confidence</th>\n",
              "      <th>threshold</th>\n",
              "      <th>action</th>\n",
              "      <th>model_used</th>\n",
              "      <th>perf_chosen</th>\n",
              "      <th>cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>keep</td>\n",
              "      <td>lm1</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   slm_confidence  threshold action model_used  perf_chosen  cost\n",
              "0            0.75        0.5   keep        lm1     1.000000   2.0\n",
              "1            0.75        0.5   keep        lm1     0.400000   2.0\n",
              "2            0.75        0.5   keep        lm1     0.142857   2.0\n",
              "3            0.75        0.5   keep        lm1     0.833333   2.0\n",
              "4            0.75        0.5   keep        lm1     0.500000   2.0\n",
              "5            0.75        0.5   keep        lm1     0.000000   2.0\n",
              "6            0.75        0.5   keep        lm1     0.500000   2.0\n",
              "7            0.75        0.5   keep        lm1     0.400000   2.0\n",
              "8            0.75        0.5   keep        lm1     1.000000   2.0\n",
              "9            0.75        0.5   keep        lm1     0.888889   2.0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Test with a single threshold\n",
        "# ============================================================\n",
        "\n",
        "THRESHOLD = 0.5  # Adjust this threshold value\n",
        "\n",
        "print(f\"[INFO] Testing threshold = {THRESHOLD}\")\n",
        "results_df, stats = apply_threshold_router(df, THRESHOLD)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Threshold Router Results for {DATASET_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Threshold: {stats['threshold']:.3f}\")\n",
        "print(f\"Total samples: {stats['total']}\")\n",
        "print(f\"LM1 (keep): {stats['lm1_count']} ({stats['lm1_ratio']:.2%})\")\n",
        "print(f\"LM2 (route): {stats['lm2_count']} ({stats['lm2_ratio']:.2%})\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Average performance (routed): {stats['avg_perf']:.4f}\")\n",
        "print(f\"  Average performance (LM1 only): {stats['avg_perf_slm']:.4f}\")\n",
        "print(f\"  Average performance (LM2 only): {stats['avg_perf_llm']:.4f}\")\n",
        "print(f\"\\nCost:\")\n",
        "print(f\"  Average cost per sample: {stats['avg_cost']:.2f}\")\n",
        "print(f\"  Total cost: {stats['total_cost']:.2f}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Show sample results\n",
        "print(\"Sample results:\")\n",
        "results_df[[\"slm_confidence\", \"threshold\", \"action\", \"model_used\", \"perf_chosen\", \"cost\"]].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: threshold_router_outputs/threshold_0.50_coqa_short.json\n"
          ]
        }
      ],
      "source": [
        "# Save results for single threshold\n",
        "output_filename = f\"threshold_{THRESHOLD:.2f}_{DATASET_NAME}.json\"\n",
        "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "results_dict = results_df.to_dict(orient=\"records\")\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Sweep Analysis\n",
        "\n",
        "Analyze the cost-performance trade-off by testing multiple threshold values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IBC (Incremental Benefit Per Cost) Metric\n",
        "\n",
        "The IBC metric quantifies the cost-effectiveness of routing methods relative to the SLM baseline.\n",
        "\n",
        "**Cost Model:**\n",
        "- Cost when keeping LM1: `C = C_SLM + C_ver = 2 * C_SLM`\n",
        "- Cost when routing to LM2: `C = C_SLM + C_ver + C_LLM = 2 * C_SLM + C_LLM`\n",
        "- Where `C_ver = C_SLM` (verification is performed by SLM)\n",
        "\n",
        "**IBC Formulas:**\n",
        "- **IBC_M** = (P_M - P_SLM) / (C_M - C_SLM) - Incremental benefit per cost for method M\n",
        "- **IBC_BASE** = (P_LLM - P_SLM) / (C_LLM - C_SLM) - Baseline (always using LLM)\n",
        "- **Δ_IBC(M)** = ((IBC_M - IBC_BASE) / IBC_BASE) * 100 - Percentage improvement over baseline\n",
        "\n",
        "**Interpretation:**\n",
        "- Positive Δ_IBC: Method is more cost-effective than always using LLM\n",
        "- Negative Δ_IBC: Method is less cost-effective than always using LLM\n",
        "- Higher IBC values indicate better cost-effectiveness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Analyzing threshold sweep for qasper_short\n",
            "[INFO] Testing 3 thresholds: [0.25, 0.5, 0.75]\n",
            "\n",
            "Processing threshold = 0.250...\n",
            "  LM1: 77.50%, LM2: 22.50%, Perf: 0.3359, Avg Cost: 9.50\n",
            "Processing threshold = 0.500...\n",
            "  LM1: 75.90%, LM2: 24.10%, Perf: 0.3399, Avg Cost: 9.82\n",
            "Processing threshold = 0.750...\n",
            "  LM1: 74.50%, LM2: 25.50%, Perf: 0.3421, Avg Cost: 10.10\n",
            "\n",
            "============================================================\n",
            "Threshold Sweep Summary for qasper_short\n",
            "============================================================\n",
            " threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost  total_cost\n",
            "      0.25      0.775      0.225  0.335881      9.50      9500.0\n",
            "      0.50      0.759      0.241  0.339856      9.82      9820.0\n",
            "      0.75      0.745      0.255  0.342115     10.10     10100.0\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Threshold sweep: test multiple thresholds\n",
        "# ============================================================\n",
        "\n",
        "# Define thresholds to test\n",
        "# Common values: 0.0, 0.25, 0.5, 0.75, 1.0\n",
        "THRESHOLDS = [0.25, 0.5, 0.75]\n",
        "\n",
        "# Or create a finer grid\n",
        "# THRESHOLDS = np.arange(0.0, 1.01, 0.1).tolist()\n",
        "\n",
        "print(f\"[INFO] Analyzing threshold sweep for {DATASET_NAME}\")\n",
        "print(f\"[INFO] Testing {len(THRESHOLDS)} thresholds: {THRESHOLDS}\\n\")\n",
        "\n",
        "sweep_results = []\n",
        "all_results = []\n",
        "\n",
        "for threshold in THRESHOLDS:\n",
        "    print(f\"Processing threshold = {threshold:.3f}...\")\n",
        "    results_df, stats = apply_threshold_router(df, threshold)\n",
        "    sweep_results.append(stats)\n",
        "    all_results.append(results_df)\n",
        "    \n",
        "    # Save results for single threshold\n",
        "    output_filename = f\"threshold_{threshold:.2f}_{DATASET_NAME}.json\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "    results_dict = results_df.to_dict(orient=\"records\")\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Print quick summary\n",
        "    print(f\"  LM1: {stats['lm1_ratio']:.2%}, LM2: {stats['lm2_ratio']:.2%}, \"\n",
        "          f\"Perf: {stats['avg_perf']:.4f}, Avg Cost: {stats['avg_cost']:.2f}\")\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "sweep_df = pd.DataFrame(sweep_results)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Threshold Sweep Summary for {DATASET_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(sweep_df[[\"threshold\", \"lm1_ratio\", \"lm2_ratio\", \"avg_perf\", \"avg_cost\", \"total_cost\"]].to_string(index=False))\n",
        "print(f\"{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "IBC (Incremental Benefit Per Cost) Baseline\n",
            "============================================================\n",
            "P_SLM (SLM performance): 0.3019\n",
            "P_LLM (LLM performance): 0.3853\n",
            "C_SLM (SLM cost): 5.00\n",
            "C_LLM (LLM cost): 25.00\n",
            "IBC_BASE: 0.004166\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "Threshold Sweep Summary with IBC Metrics for qasper_short\n",
            "============================================================\n",
            " threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost      IBC  delta_IBC\n",
            "      0.25      0.775      0.225  0.335881      9.50 0.007543  81.035751\n",
            "      0.50      0.759      0.241  0.339856      9.82 0.007867  88.810684\n",
            "      0.75      0.745      0.255  0.342115     10.10 0.007878  89.072782\n",
            "============================================================\n",
            "\n",
            "IBC Metric Interpretation:\n",
            "  IBC_BASE (baseline): 0.004166 - Benefit of always using LLM over SLM\n",
            "  Positive Δ_IBC: Method is more cost-effective than baseline\n",
            "  Negative Δ_IBC: Method is less cost-effective than baseline\n",
            "\n",
            "Sweep summary saved to: threshold_router_outputs/threshold_sweep_summary_qasper_short.json\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>total</th>\n",
              "      <th>lm1_count</th>\n",
              "      <th>lm2_count</th>\n",
              "      <th>lm1_ratio</th>\n",
              "      <th>lm2_ratio</th>\n",
              "      <th>avg_perf</th>\n",
              "      <th>avg_perf_slm</th>\n",
              "      <th>avg_perf_llm</th>\n",
              "      <th>avg_cost</th>\n",
              "      <th>total_cost</th>\n",
              "      <th>IBC</th>\n",
              "      <th>delta_IBC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.25</td>\n",
              "      <td>1000</td>\n",
              "      <td>775</td>\n",
              "      <td>225</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.335881</td>\n",
              "      <td>0.301938</td>\n",
              "      <td>0.385268</td>\n",
              "      <td>9.50</td>\n",
              "      <td>9500.0</td>\n",
              "      <td>0.007543</td>\n",
              "      <td>81.035751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>1000</td>\n",
              "      <td>759</td>\n",
              "      <td>241</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.339856</td>\n",
              "      <td>0.301938</td>\n",
              "      <td>0.385268</td>\n",
              "      <td>9.82</td>\n",
              "      <td>9820.0</td>\n",
              "      <td>0.007867</td>\n",
              "      <td>88.810684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>1000</td>\n",
              "      <td>745</td>\n",
              "      <td>255</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.342115</td>\n",
              "      <td>0.301938</td>\n",
              "      <td>0.385268</td>\n",
              "      <td>10.10</td>\n",
              "      <td>10100.0</td>\n",
              "      <td>0.007878</td>\n",
              "      <td>89.072782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   threshold  total  lm1_count  lm2_count  lm1_ratio  lm2_ratio  avg_perf  \\\n",
              "0       0.25   1000        775        225      0.775      0.225  0.335881   \n",
              "1       0.50   1000        759        241      0.759      0.241  0.339856   \n",
              "2       0.75   1000        745        255      0.745      0.255  0.342115   \n",
              "\n",
              "   avg_perf_slm  avg_perf_llm  avg_cost  total_cost       IBC  delta_IBC  \n",
              "0      0.301938      0.385268      9.50      9500.0  0.007543  81.035751  \n",
              "1      0.301938      0.385268      9.82      9820.0  0.007867  88.810684  \n",
              "2      0.301938      0.385268     10.10     10100.0  0.007878  89.072782  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Calculate IBC (Incremental Benefit Per Cost) metrics\n",
        "# ============================================================\n",
        "# Based on the IBC metric defined in the paper:\n",
        "# IBC_M = (P_M - P_SLM) / (C_M - C_SLM)\n",
        "# IBC_BASE = (P_LLM - P_SLM) / (C_LLM - C_SLM)\n",
        "# Δ_IBC(M) = ((IBC_M - IBC_BASE) / IBC_BASE) * 100\n",
        "\n",
        "# Get baseline performance values\n",
        "P_SLM = sweep_df[\"avg_perf_slm\"].iloc[0]  # Average performance of SLM (with verification)\n",
        "P_LLM = sweep_df[\"avg_perf_llm\"].iloc[0]  # Average performance of LLM (with verification)\n",
        "\n",
        "# Baseline costs\n",
        "# C_SLM = C_SLM + C_ver = 2 * C_SLM (SLM inference + verification)\n",
        "# C_LLM = C_SLM + C_ver + C_LLM = 2 * C_SLM + C_LLM (SLM inference + verification + LLM inference)\n",
        "C_SLM_baseline = COST_LM1  # Cost of SLM method: 2 * C_SLM\n",
        "C_LLM_baseline = COST_LM2  # Cost of LLM method: 2 * C_SLM + C_LLM\n",
        "\n",
        "# Calculate IBC_BASE: Baseline incremental benefit per cost\n",
        "# IBC_BASE represents the benefit of always using LLM over SLM\n",
        "# IBC_BASE = (P_LLM - P_SLM) / (C_LLM - C_SLM)\n",
        "IBC_BASE = (P_LLM - P_SLM) / (C_LLM_baseline - C_SLM_baseline)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"IBC (Incremental Benefit Per Cost) Baseline\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"P_SLM (SLM performance): {P_SLM:.4f}\")\n",
        "print(f\"P_LLM (LLM performance): {P_LLM:.4f}\")\n",
        "print(f\"C_SLM (SLM cost): {C_SLM_baseline:.2f}\")\n",
        "print(f\"C_LLM (LLM cost): {C_LLM_baseline:.2f}\")\n",
        "print(f\"IBC_BASE: {IBC_BASE:.6f}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Calculate IBC for each threshold method M\n",
        "# IBC_M = (P_M - P_SLM) / (C_M - C_SLM)\n",
        "# where P_M is the performance of method M, C_M is the cost of method M\n",
        "sweep_df[\"IBC\"] = (sweep_df[\"avg_perf\"] - P_SLM) / (sweep_df[\"avg_cost\"] - C_SLM_baseline)\n",
        "\n",
        "# Calculate Δ_IBC: Percentage lift of IBC\n",
        "# Δ_IBC(M) = ((IBC_M - IBC_BASE) / IBC_BASE) * 100\n",
        "# Positive values indicate the method achieves performance increments more cost-effectively than baseline\n",
        "# Negative values indicate reduced efficiency compared to baseline\n",
        "sweep_df[\"delta_IBC\"] = ((sweep_df[\"IBC\"] - IBC_BASE) / IBC_BASE) * 100\n",
        "\n",
        "\n",
        "\n",
        "# Display the sweep results with IBC metrics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Threshold Sweep Summary with IBC Metrics for {DATASET_NAME}\")\n",
        "print(f\"{'='*60}\")\n",
        "display_cols = [\"threshold\", \"lm1_ratio\", \"lm2_ratio\", \"avg_perf\", \"avg_cost\", \"IBC\", \"delta_IBC\"]\n",
        "print(sweep_df[display_cols].round(6).to_string(index=False))\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Display interpretation\n",
        "print(\"IBC Metric Interpretation:\")\n",
        "print(f\"  IBC_BASE (baseline): {IBC_BASE:.6f} - Benefit of always using LLM over SLM\")\n",
        "print(f\"  Positive Δ_IBC: Method is more cost-effective than baseline\")\n",
        "print(f\"  Negative Δ_IBC: Method is less cost-effective than baseline\")\n",
        "print()\n",
        "\n",
        "# Save sweep summary\n",
        "sweep_summary_path = os.path.join(OUTPUT_DIR, f\"threshold_sweep_summary_{DATASET_NAME}.json\")\n",
        "sweep_summary_with_ibc = sweep_df.to_dict(orient=\"records\")\n",
        "with open(sweep_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(sweep_summary_with_ibc, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Sweep summary saved to: {sweep_summary_path}\")\n",
        "\n",
        "sweep_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IBC-Based Optimal Threshold Analysis\n",
        "\n",
        "Additional analysis focused on IBC (Incremental Benefit Per Cost) metrics for identifying the most cost-effective thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "IBC-Based Optimal Threshold Analysis\n",
            "============================================================\n",
            "\n",
            "1. Maximum IBC (Best Cost-Effectiveness vs SLM):\n",
            "   Threshold: 0.750\n",
            "   IBC: 0.007878\n",
            "   Δ_IBC: 89.07% (vs baseline)\n",
            "   Performance: 0.3421\n",
            "   Cost: 10.10\n",
            "\n",
            "2. Maximum Δ_IBC (Best Improvement over Baseline LLM):\n",
            "   Threshold: 0.750\n",
            "   Δ_IBC: 89.07%\n",
            "   IBC: 0.007878\n",
            "   Performance: 0.3421\n",
            "   Cost: 10.10\n",
            "\n",
            "3. Methods with Positive Δ_IBC (More Cost-Effective than Baseline):\n",
            "   Count: 3 out of 3\n",
            "   Best threshold: 0.750\n",
            "   Best Δ_IBC: 89.07%\n",
            "\n",
            "============================================================\n",
            "\n",
            "Summary Table with IBC Metrics:\n",
            " threshold  avg_perf  avg_cost      IBC  delta_IBC\n",
            "      0.25  0.335881      9.50 0.007543  81.035751\n",
            "      0.50  0.339856      9.82 0.007867  88.810684\n",
            "      0.75  0.342115     10.10 0.007878  89.072782\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IBC-based optimal threshold analysis\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"IBC-Based Optimal Threshold Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Maximum IBC (Best cost-effectiveness relative to SLM baseline)\n",
        "max_ibc_idx = sweep_df[\"IBC\"].idxmax()\n",
        "max_ibc_threshold = sweep_df.loc[max_ibc_idx, \"threshold\"]\n",
        "max_ibc_value = sweep_df.loc[max_ibc_idx, \"IBC\"]\n",
        "max_ibc_delta = sweep_df.loc[max_ibc_idx, \"delta_IBC\"]\n",
        "max_ibc_perf = sweep_df.loc[max_ibc_idx, \"avg_perf\"]\n",
        "max_ibc_cost = sweep_df.loc[max_ibc_idx, \"avg_cost\"]\n",
        "print(f\"\\n1. Maximum IBC (Best Cost-Effectiveness vs SLM):\")\n",
        "print(f\"   Threshold: {max_ibc_threshold:.3f}\")\n",
        "print(f\"   IBC: {max_ibc_value:.6f}\")\n",
        "print(f\"   Δ_IBC: {max_ibc_delta:.2f}% (vs baseline)\")\n",
        "print(f\"   Performance: {max_ibc_perf:.4f}\")\n",
        "print(f\"   Cost: {max_ibc_cost:.2f}\")\n",
        "\n",
        "# 2. Maximum Δ_IBC (Best improvement over baseline LLM)\n",
        "max_delta_ibc_idx = sweep_df[\"delta_IBC\"].idxmax()\n",
        "max_delta_ibc_threshold = sweep_df.loc[max_delta_ibc_idx, \"threshold\"]\n",
        "max_delta_ibc_value = sweep_df.loc[max_delta_ibc_idx, \"delta_IBC\"]\n",
        "max_delta_ibc_ibc = sweep_df.loc[max_delta_ibc_idx, \"IBC\"]\n",
        "max_delta_ibc_perf = sweep_df.loc[max_delta_ibc_idx, \"avg_perf\"]\n",
        "max_delta_ibc_cost = sweep_df.loc[max_delta_ibc_idx, \"avg_cost\"]\n",
        "print(f\"\\n2. Maximum Δ_IBC (Best Improvement over Baseline LLM):\")\n",
        "print(f\"   Threshold: {max_delta_ibc_threshold:.3f}\")\n",
        "print(f\"   Δ_IBC: {max_delta_ibc_value:.2f}%\")\n",
        "print(f\"   IBC: {max_delta_ibc_ibc:.6f}\")\n",
        "print(f\"   Performance: {max_delta_ibc_perf:.4f}\")\n",
        "print(f\"   Cost: {max_delta_ibc_cost:.2f}\")\n",
        "\n",
        "# 3. Positive Δ_IBC methods (more cost-effective than baseline)\n",
        "positive_delta = sweep_df[sweep_df[\"delta_IBC\"] > 0]\n",
        "if len(positive_delta) > 0:\n",
        "    print(f\"\\n3. Methods with Positive Δ_IBC (More Cost-Effective than Baseline):\")\n",
        "    print(f\"   Count: {len(positive_delta)} out of {len(sweep_df)}\")\n",
        "    print(f\"   Best threshold: {positive_delta.loc[positive_delta['delta_IBC'].idxmax(), 'threshold']:.3f}\")\n",
        "    print(f\"   Best Δ_IBC: {positive_delta['delta_IBC'].max():.2f}%\")\n",
        "else:\n",
        "    print(f\"\\n3. Methods with Positive Δ_IBC:\")\n",
        "    print(f\"   None - All methods are less cost-effective than baseline\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Display summary table with IBC metrics\n",
        "print(\"\\nSummary Table with IBC Metrics:\")\n",
        "summary_cols = [\"threshold\", \"avg_perf\", \"avg_cost\", \"IBC\", \"delta_IBC\"]\n",
        "print(sweep_df[summary_cols].round(6).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "10601-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
