{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c7f61f",
   "metadata": {},
   "source": [
    "AutoMix Threshold Router on Log Probability and Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a260da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rquired libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39113388",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Set the dataset name and other parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78d931ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: qasper_short\n",
      "Router data path: logits_entropy/qasper_short_answerlogits.jsonl\n",
      "Output directory: logits_entropy_threshold_output\n",
      "\n",
      "Cost Parameters:\n",
      "  C_SLM: 1.0\n",
      "  C_LLM: 20.0\n",
      "  C_ver: 4.0\n",
      "  Cost when keeping LM1: 5.0\n",
      "  Cost when routing to LM2: 25.0\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "# Dataset name: cnli_short / coqa_short / narrativeqa_short / qasper_short\n",
    "DATASET_NAME = \"qasper_short\"\n",
    "\n",
    "# Router data path\n",
    "LOGIT_DATA_PATH = os.path.join(\"logits_entropy\", f\"{DATASET_NAME}_answerlogits.jsonl\")\n",
    "\n",
    "# Output directory for results\n",
    "DIR = f\"{DATASET_NAME}_outputs\"\n",
    "OUTPUT_DIR = \"logits_entropy_threshold_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cost parameters (for cost-performance analysis)\n",
    "# Cost model: C = C_SLM + C_ver + w * C_LLM\n",
    "# where C_ver = C_SLM (verification is performed by SLM)\n",
    "C_SLM = 1.0  # Cost of small language model (LM1)\n",
    "C_LLM = 20.0  # Cost of large language model (LM2)\n",
    "C_ver = 4 * C_SLM  # Cost of verification (performed by SLM)\n",
    "\n",
    "# Cost when keeping LM1: C = C_SLM + C_ver = 2 * C_SLM\n",
    "# Cost when routing to LM2: C = C_SLM + C_ver + C_LLM = 2 * C_SLM + C_LLM\n",
    "COST_LM1 = C_SLM + C_ver\n",
    "COST_LM2 = C_SLM + C_ver + C_LLM\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Router data path: {LOGIT_DATA_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nCost Parameters:\")\n",
    "print(f\"  C_SLM: {C_SLM}\")\n",
    "print(f\"  C_LLM: {C_LLM}\")\n",
    "print(f\"  C_ver: {C_ver}\")\n",
    "print(f\"  Cost when keeping LM1: {COST_LM1}\")\n",
    "print(f\"  Cost when routing to LM2: {COST_LM2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ecc7b",
   "metadata": {},
   "source": [
    "# Load Logits and Entropy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fcd5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Logits data from logits_entropy/qasper_short_answerlogits.jsonl\n",
      "[INFO] Loaded 1000 samples\n",
      "[INFO] Columns: ['generated_text', 'avg_logp', 'avg_entropy_nats', 'avg_entropy_bits']\n",
      "\n",
      "[INFO] Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_text</th>\n",
       "      <th>avg_logp</th>\n",
       "      <th>avg_entropy_nats</th>\n",
       "      <th>avg_entropy_bits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very small lexicon consisting of 15 positive...</td>\n",
       "      <td>-0.157852</td>\n",
       "      <td>0.323955</td>\n",
       "      <td>0.467368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A very small lexicon consisting of 15 positive...</td>\n",
       "      <td>-0.157852</td>\n",
       "      <td>0.323955</td>\n",
       "      <td>0.467368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The proposed method performed well even with a...</td>\n",
       "      <td>-0.152505</td>\n",
       "      <td>0.370588</td>\n",
       "      <td>0.534645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The relations of Cause and Concession are used...</td>\n",
       "      <td>-0.226092</td>\n",
       "      <td>0.522570</td>\n",
       "      <td>0.753909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The relations of Cause and Concession are used...</td>\n",
       "      <td>-0.226092</td>\n",
       "      <td>0.522570</td>\n",
       "      <td>0.753909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      generated_text  avg_logp  \\\n",
       "0  A very small lexicon consisting of 15 positive... -0.157852   \n",
       "1  A very small lexicon consisting of 15 positive... -0.157852   \n",
       "2  The proposed method performed well even with a... -0.152505   \n",
       "3  The relations of Cause and Concession are used... -0.226092   \n",
       "4  The relations of Cause and Concession are used... -0.226092   \n",
       "\n",
       "   avg_entropy_nats  avg_entropy_bits  \n",
       "0          0.323955          0.467368  \n",
       "1          0.323955          0.467368  \n",
       "2          0.370588          0.534645  \n",
       "3          0.522570          0.753909  \n",
       "4          0.522570          0.753909  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load logit data\n",
    "\n",
    "if not os.path.exists(LOGIT_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Logits data not found: {LOGIT_DATA_PATH}\")\n",
    "\n",
    "print(f\"[INFO] Loading Logits data from {LOGIT_DATA_PATH}\")\n",
    "df = pd.read_json(LOGIT_DATA_PATH, lines=True, orient=\"records\")\n",
    "\n",
    "print(f\"[INFO] Loaded {len(df)} samples\")\n",
    "print(f\"[INFO] Columns: {list(df.columns)}\")\n",
    "print(f\"\\n[INFO] Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c526f91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading router data from router_data/router_data_qasper_short.jsonl\n",
      "[INFO] Loaded 1000 router samples\n",
      "[INFO] Router columns: ['id', 'dataset', 'gold_output', 'slm_pred', 'llm_pred', 'slm_confidence', 'perf_slm', 'perf_llm']\n",
      "\n",
      "[INFO] Merged data shape: (1000, 10)\n",
      "[INFO] Merged columns: ['id', 'dataset', 'gold_output', 'slm_pred', 'llm_pred', 'slm_confidence', 'perf_slm', 'perf_llm', 'avg_logp', 'avg_entropy_nats']\n",
      "\n",
      "[INFO] Sample merged data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>gold_output</th>\n",
       "      <th>slm_pred</th>\n",
       "      <th>llm_pred</th>\n",
       "      <th>slm_confidence</th>\n",
       "      <th>perf_slm</th>\n",
       "      <th>perf_llm</th>\n",
       "      <th>avg_logp</th>\n",
       "      <th>avg_entropy_nats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753990d0b621d390ed58f20c4d9e4f065f0dc672</td>\n",
       "      <td>qasper_short</td>\n",
       "      <td>a vocabulary of positive and negative predicat...</td>\n",
       "      <td>A seed lexicon consisting of 15 positive words...</td>\n",
       "      <td>a small list of 30 Japanese emotion predicates...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>-0.157852</td>\n",
       "      <td>0.323955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>753990d0b621d390ed58f20c4d9e4f065f0dc672</td>\n",
       "      <td>qasper_short</td>\n",
       "      <td>seed lexicon consists of positive and negative...</td>\n",
       "      <td>A seed lexicon consisting of 15 positive words...</td>\n",
       "      <td>a list of 15 positive and 15 negative Japanese...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>-0.157852</td>\n",
       "      <td>0.323955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9d578ddccc27dd849244d632dd0f6bf27348ad81</td>\n",
       "      <td>qasper_short</td>\n",
       "      <td>Using all data to train: AL -- BiGRU achieved ...</td>\n",
       "      <td>The proposed method performed well, even with ...</td>\n",
       "      <td>BiGRU trained with AL+CA+CO achieved the highe...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-0.152505</td>\n",
       "      <td>0.370588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02e4bf719b1a504e385c35c6186742e720bcb281</td>\n",
       "      <td>qasper_short</td>\n",
       "      <td>based on the relation between events, the sugg...</td>\n",
       "      <td>The answer is through the exploitation of disc...</td>\n",
       "      <td>By exploiting discourse relations: for Cause p...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>-0.226092</td>\n",
       "      <td>0.522570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02e4bf719b1a504e385c35c6186742e720bcb281</td>\n",
       "      <td>qasper_short</td>\n",
       "      <td>cause relation: both events in the relation sh...</td>\n",
       "      <td>The relations are used to propagate polarity t...</td>\n",
       "      <td>By using discourse relations between event pai...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>-0.226092</td>\n",
       "      <td>0.522570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id       dataset  \\\n",
       "0  753990d0b621d390ed58f20c4d9e4f065f0dc672  qasper_short   \n",
       "1  753990d0b621d390ed58f20c4d9e4f065f0dc672  qasper_short   \n",
       "2  9d578ddccc27dd849244d632dd0f6bf27348ad81  qasper_short   \n",
       "3  02e4bf719b1a504e385c35c6186742e720bcb281  qasper_short   \n",
       "4  02e4bf719b1a504e385c35c6186742e720bcb281  qasper_short   \n",
       "\n",
       "                                         gold_output  \\\n",
       "0  a vocabulary of positive and negative predicat...   \n",
       "1  seed lexicon consists of positive and negative...   \n",
       "2  Using all data to train: AL -- BiGRU achieved ...   \n",
       "3  based on the relation between events, the sugg...   \n",
       "4  cause relation: both events in the relation sh...   \n",
       "\n",
       "                                            slm_pred  \\\n",
       "0  A seed lexicon consisting of 15 positive words...   \n",
       "1  A seed lexicon consisting of 15 positive words...   \n",
       "2  The proposed method performed well, even with ...   \n",
       "3  The answer is through the exploitation of disc...   \n",
       "4  The relations are used to propagate polarity t...   \n",
       "\n",
       "                                            llm_pred  slm_confidence  \\\n",
       "0  a small list of 30 Japanese emotion predicates...            0.75   \n",
       "1  a list of 15 positive and 15 negative Japanese...            0.75   \n",
       "2  BiGRU trained with AL+CA+CO achieved the highe...            0.75   \n",
       "3  By exploiting discourse relations: for Cause p...            0.75   \n",
       "4  By using discourse relations between event pai...            0.75   \n",
       "\n",
       "   perf_slm  perf_llm  avg_logp  avg_entropy_nats  \n",
       "0  0.357143  0.256410 -0.157852          0.323955  \n",
       "1  0.600000  0.357143 -0.157852          0.323955  \n",
       "2  0.090090  0.105263 -0.152505          0.370588  \n",
       "3  0.176471  0.318182 -0.226092          0.522570  \n",
       "4  0.222222  0.355556 -0.226092          0.522570  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with router data to get performance metrics\n",
    "ROUTER_DATA_PATH_FULL = os.path.join(\"router_data\", f\"router_data_{DATASET_NAME}.jsonl\")\n",
    "\n",
    "if not os.path.exists(ROUTER_DATA_PATH_FULL):\n",
    "    raise FileNotFoundError(f\"Router data not found: {ROUTER_DATA_PATH_FULL}\")\n",
    "\n",
    "print(f\"[INFO] Loading router data from {ROUTER_DATA_PATH_FULL}\")\n",
    "router_df = pd.read_json(ROUTER_DATA_PATH_FULL, lines=True, orient=\"records\")\n",
    "\n",
    "print(f\"[INFO] Loaded {len(router_df)} router samples\")\n",
    "print(f\"[INFO] Router columns: {list(router_df.columns)}\")\n",
    "\n",
    "# Merge logits/entropy data with router data\n",
    "# Assuming they are in the same order (same number of samples)\n",
    "assert len(df) == len(router_df), f\"Data length mismatch: {len(df)} vs {len(router_df)}\"\n",
    "\n",
    "# Merge the dataframes\n",
    "df_merged = router_df.copy()\n",
    "df_merged[\"avg_logp\"] = df[\"avg_logp\"].values\n",
    "df_merged[\"avg_entropy_nats\"] = df[\"avg_entropy_nats\"].values\n",
    "\n",
    "print(f\"\\n[INFO] Merged data shape: {df_merged.shape}\")\n",
    "print(f\"[INFO] Merged columns: {list(df_merged.columns)}\")\n",
    "print(f\"\\n[INFO] Sample merged data:\")\n",
    "df_merged.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b97a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logits thresholds (9 bins, excluding min and max):\n",
      "[-0.97362576 -0.86590608 -0.7581864  -0.65046671 -0.54274703 -0.43502735\n",
      " -0.32730767 -0.21958799 -0.11186831]\n",
      "\n",
      "Entropy thresholds (9 bins, excluding min and max):\n",
      "[0.19565922 0.37014548 0.54463174 0.719118   0.89360426 1.06809052\n",
      " 1.24257678 1.41706304 1.5915493 ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the threshold for the logits and entropy based on the max and min values\n",
    "# partition into 10 equal-sized bins\n",
    "\n",
    "# Get min and max values for logits and entropy\n",
    "logits_min = df_merged[\"avg_logp\"].min()\n",
    "logits_max = df_merged[\"avg_logp\"].max()\n",
    "entropy_min = df_merged[\"avg_entropy_nats\"].min()\n",
    "entropy_max = df_merged[\"avg_entropy_nats\"].max()\n",
    "\n",
    "# Create 12 evenly spaced thresholds (including min and max), then remove first and last\n",
    "# to get 10 thresholds between min and max\n",
    "logits_thresholds = np.linspace(logits_min, logits_max, 11)[1:-1]\n",
    "entropy_thresholds = np.linspace(entropy_min, entropy_max, 11)[1:-1]\n",
    "\n",
    "print(f\"\\nLogits thresholds (9 bins, excluding min and max):\")\n",
    "print(logits_thresholds)\n",
    "print(f\"\\nEntropy thresholds (9 bins, excluding min and max):\")\n",
    "print(entropy_thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9baa3",
   "metadata": {},
   "source": [
    "# Threshold Router Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6088c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold-based router function using logits and entropy\n",
    "\n",
    "def apply_logits_entropy_router(df: pd.DataFrame, logits_threshold: float = None, entropy_threshold: float = None) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Apply threshold-based router using logits and/or entropy to route queries between LM1 and LM2.\n",
    "    \n",
    "    Routing logic:\n",
    "    - If avg_logp >= logits_threshold OR avg_entropy_nats <= entropy_threshold: Keep LM1\n",
    "    - Otherwise: Route to LM2\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    lm1_count = 0\n",
    "    lm2_count = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        avg_logp = float(row[\"avg_logp\"])\n",
    "        avg_entropy = float(row[\"avg_entropy_nats\"])\n",
    "        slm_pred = row[\"slm_pred\"]  # LM1's answer\n",
    "        llm_pred = row[\"llm_pred\"]  # LM2's answer\n",
    "        \n",
    "        # Routing decision based on logits and/or entropy\n",
    "        # High logits (>= threshold) AND low entropy (<= threshold) = keep LM1\n",
    "        keep_lm1 = True\n",
    "        \n",
    "        if logits_threshold is not None:\n",
    "            keep_lm1 = keep_lm1 and (avg_logp >= logits_threshold)\n",
    "        \n",
    "        if entropy_threshold is not None:\n",
    "            keep_lm1 = keep_lm1 and (avg_entropy <= entropy_threshold)\n",
    "        \n",
    "        if keep_lm1:\n",
    "            final_ans = slm_pred\n",
    "            action = \"keep\"  # Keep LM1's answer\n",
    "            model_used = \"lm1\"\n",
    "            lm1_count += 1\n",
    "            cost = COST_LM1  # C_SLM + C_ver = 5.0\n",
    "        else:\n",
    "            final_ans = llm_pred\n",
    "            action = \"route\"  # Route to LM2\n",
    "            model_used = \"lm2\"\n",
    "            lm2_count += 1\n",
    "            cost = COST_LM2  # C_SLM + C_ver + C_LLM = 25.0\n",
    "        \n",
    "        # Compute performance for the chosen answer\n",
    "        perf_chosen = row[\"perf_slm\"] if model_used == \"lm1\" else row[\"perf_llm\"]\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": row.get(\"id\", i),\n",
    "            \"dataset\": row.get(\"dataset\", DATASET_NAME),\n",
    "            \"avg_logp\": avg_logp,\n",
    "            \"avg_entropy_nats\": avg_entropy,\n",
    "            \"logits_threshold\": logits_threshold,\n",
    "            \"entropy_threshold\": entropy_threshold,\n",
    "            \"action\": action,\n",
    "            \"model_used\": model_used,\n",
    "            \"gold_answer\": row.get(\"gold_output\"),\n",
    "            \"final_answer\": final_ans,\n",
    "            \"slm_pred\": slm_pred,\n",
    "            \"llm_pred\": llm_pred,\n",
    "            \"perf_chosen\": perf_chosen,\n",
    "            \"perf_slm\": row.get(\"perf_slm\"),\n",
    "            \"perf_llm\": row.get(\"perf_llm\"),\n",
    "            \"cost\": cost,\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    total = len(results_df)\n",
    "    lm1_ratio = lm1_count / total\n",
    "    lm2_ratio = lm2_count / total\n",
    "    avg_perf = results_df[\"perf_chosen\"].mean()\n",
    "    avg_perf_slm = results_df[\"perf_slm\"].mean()\n",
    "    avg_perf_llm = results_df[\"perf_llm\"].mean()\n",
    "    avg_cost = results_df[\"cost\"].mean()\n",
    "    total_cost = results_df[\"cost\"].sum()\n",
    "    \n",
    "    stats = {\n",
    "        \"logits_threshold\": logits_threshold,\n",
    "        \"entropy_threshold\": entropy_threshold,\n",
    "        \"total\": total,\n",
    "        \"lm1_count\": lm1_count,\n",
    "        \"lm2_count\": lm2_count,\n",
    "        \"lm1_ratio\": lm1_ratio,\n",
    "        \"lm2_ratio\": lm2_ratio,\n",
    "        \"avg_perf\": avg_perf,\n",
    "        \"avg_perf_slm\": avg_perf_slm,\n",
    "        \"avg_perf_llm\": avg_perf_llm,\n",
    "        \"avg_cost\": avg_cost,\n",
    "        \"total_cost\": total_cost,\n",
    "    }\n",
    "    \n",
    "    return results_df, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5969aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Test 1: Using ONLY logits threshold\n",
      "============================================================\n",
      "Logits threshold: -0.4350\n",
      "Entropy threshold: None\n",
      "LM1 (keep): 907 (90.70%)\n",
      "LM2 (route): 93 (9.30%)\n",
      "Average performance: 0.3208\n",
      "Average cost: 6.86\n",
      "\n",
      "============================================================\n",
      "Test 2: Using ONLY entropy threshold\n",
      "============================================================\n",
      "Logits threshold: None\n",
      "Entropy threshold: 1.0681\n",
      "LM1 (keep): 949 (94.90%)\n",
      "LM2 (route): 51 (5.10%)\n",
      "Average performance: 0.3108\n",
      "Average cost: 6.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with single thresholds (one at a time)\n",
    "\n",
    "\n",
    "# Test 1: Only logits threshold\n",
    "print(\"=\"*60)\n",
    "print(\"Test 1: Using ONLY logits threshold\")\n",
    "print(\"=\"*60)\n",
    "TEST_LOGITS_THRESHOLD = logits_thresholds[5]  # Middle threshold\n",
    "\n",
    "results_df_logits, stats_logits = apply_logits_entropy_router(\n",
    "    df_merged, \n",
    "    logits_threshold=TEST_LOGITS_THRESHOLD,\n",
    "    entropy_threshold=None  # Only using logits\n",
    ")\n",
    "\n",
    "print(f\"Logits threshold: {stats_logits['logits_threshold']:.4f}\")\n",
    "print(f\"Entropy threshold: {stats_logits['entropy_threshold']}\")\n",
    "print(f\"LM1 (keep): {stats_logits['lm1_count']} ({stats_logits['lm1_ratio']:.2%})\")\n",
    "print(f\"LM2 (route): {stats_logits['lm2_count']} ({stats_logits['lm2_ratio']:.2%})\")\n",
    "print(f\"Average performance: {stats_logits['avg_perf']:.4f}\")\n",
    "print(f\"Average cost: {stats_logits['avg_cost']:.2f}\\n\")\n",
    "\n",
    "# Test 2: Only entropy threshold\n",
    "print(\"=\"*60)\n",
    "print(\"Test 2: Using ONLY entropy threshold\")\n",
    "print(\"=\"*60)\n",
    "TEST_ENTROPY_THRESHOLD = entropy_thresholds[5]  # Middle threshold\n",
    "\n",
    "results_df_entropy, stats_entropy = apply_logits_entropy_router(\n",
    "    df_merged, \n",
    "    logits_threshold=None,  # Only using entropy\n",
    "    entropy_threshold=TEST_ENTROPY_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"Logits threshold: {stats_entropy['logits_threshold']}\")\n",
    "print(f\"Entropy threshold: {stats_entropy['entropy_threshold']:.4f}\")\n",
    "print(f\"LM1 (keep): {stats_entropy['lm1_count']} ({stats_entropy['lm1_ratio']:.2%})\")\n",
    "print(f\"LM2 (route): {stats_entropy['lm2_count']} ({stats_entropy['lm2_ratio']:.2%})\")\n",
    "print(f\"Average performance: {stats_entropy['avg_perf']:.4f}\")\n",
    "print(f\"Average cost: {stats_entropy['avg_cost']:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f039a",
   "metadata": {},
   "source": [
    "# Threshold Sweep for Logits and Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Analyzing threshold sweep for qasper_short\n",
      "\n",
      "============================================================\n",
      "Threshold Sweep Summary for qasper_short\n",
      "============================================================\n",
      "\n",
      "Logits-only results:\n",
      " logits_threshold entropy_threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost  total_cost\n",
      "        -0.973626              None      0.998      0.002  0.302848      5.04      5040.0\n",
      "        -0.865906              None      0.994      0.006  0.306242      5.12      5120.0\n",
      "        -0.758186              None      0.988      0.012  0.307260      5.24      5240.0\n",
      "        -0.650467              None      0.975      0.025  0.308617      5.50      5500.0\n",
      "        -0.542747              None      0.954      0.046  0.309875      5.92      5920.0\n",
      "        -0.435027              None      0.907      0.093  0.320810      6.86      6860.0\n",
      "        -0.327308              None      0.797      0.203  0.337952      9.06      9060.0\n",
      "        -0.219588              None      0.532      0.468  0.365406     14.36     14360.0\n",
      "        -0.111868              None      0.195      0.805  0.383237     21.10     21100.0\n",
      "\n",
      "============================================================\n",
      "\n",
      "Entropy-only results:\n",
      "logits_threshold  entropy_threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost  total_cost\n",
      "            None           0.195659      0.096      0.904  0.382145     23.08     23080.0\n",
      "            None           0.370145      0.324      0.676  0.388769     18.52     18520.0\n",
      "            None           0.544632      0.591      0.409  0.363000     13.18     13180.0\n",
      "            None           0.719118      0.790      0.210  0.342287      9.20      9200.0\n",
      "            None           0.893604      0.892      0.108  0.324049      7.16      7160.0\n",
      "            None           1.068091      0.949      0.051  0.310848      6.02      6020.0\n",
      "            None           1.242577      0.975      0.025  0.309063      5.50      5500.0\n",
      "            None           1.417063      0.990      0.010  0.305257      5.20      5200.0\n",
      "            None           1.591549      0.994      0.006  0.305462      5.12      5120.0\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/pkp8kns52zd14207btyv1s7h0000gn/T/ipykernel_19045/1593122087.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sweep_df = pd.concat([logits_sweep_df, entropy_sweep_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Threshold sweep: test logits and entropy thresholds separately\n",
    "\n",
    "\n",
    "print(f\"[INFO] Analyzing threshold sweep for {DATASET_NAME}\")\n",
    "\n",
    "# Separate result variables for logits and entropy\n",
    "logits_results = []\n",
    "logits_all_results = []\n",
    "entropy_results = []\n",
    "entropy_all_results = []\n",
    "\n",
    "# Test 1: Logits thresholds only (entropy_threshold = None)\n",
    "for logits_thresh in logits_thresholds:\n",
    "    results_df, stats = apply_logits_entropy_router(\n",
    "        df_merged,\n",
    "        logits_threshold=logits_thresh,\n",
    "        entropy_threshold=None  # Only using logits\n",
    "    )\n",
    "    logits_results.append(stats)\n",
    "    logits_all_results.append(results_df)\n",
    "    \n",
    "\n",
    "# Test 2: Entropy thresholds only (logits_threshold = None)\n",
    "for entropy_thresh in entropy_thresholds:\n",
    "    results_df, stats = apply_logits_entropy_router(\n",
    "        df_merged,\n",
    "        logits_threshold=None,  # Only using entropy\n",
    "        entropy_threshold=entropy_thresh\n",
    "    )\n",
    "    entropy_results.append(stats)\n",
    "    entropy_all_results.append(results_df)\n",
    "    \n",
    "\n",
    "# Convert to separate DataFrames for easier analysis\n",
    "logits_sweep_df = pd.DataFrame(logits_results)\n",
    "entropy_sweep_df = pd.DataFrame(entropy_results)\n",
    "\n",
    "# Also create a combined DataFrame for backward compatibility\n",
    "sweep_df = pd.concat([logits_sweep_df, entropy_sweep_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Threshold Sweep Summary for {DATASET_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nLogits-only results:\")\n",
    "print(logits_sweep_df[[\"logits_threshold\", \"entropy_threshold\", \"lm1_ratio\", \"lm2_ratio\", \n",
    "                       \"avg_perf\", \"avg_cost\", \"total_cost\"]].to_string(index=False))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\nEntropy-only results:\")\n",
    "print(entropy_sweep_df[[\"logits_threshold\", \"entropy_threshold\", \"lm1_ratio\", \"lm2_ratio\", \n",
    "                        \"avg_perf\", \"avg_cost\", \"total_cost\"]].to_string(index=False))\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IBC (Incremental Benefit Per Cost) Baseline\n",
      "============================================================\n",
      "P_SLM (SLM average performance): 0.3019\n",
      "P_LLM (LLM average performance): 0.3853\n",
      "C_SLM (SLM cost): 5.00\n",
      "C_LLM (LLM cost): 25.00\n",
      "IBC_BASE: 0.004166\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Logits-Only Threshold Sweep with IBC Metrics for qasper_short\n",
      "============================================================\n",
      " logits_threshold entropy_threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost      IBC  delta_IBC\n",
      "        -0.973626              None      0.998      0.002  0.302848      5.04 0.022727 445.478226\n",
      "        -0.865906              None      0.994      0.006  0.306242      5.12 0.035865 760.802920\n",
      "        -0.758186              None      0.988      0.012  0.307260      5.24 0.022175 432.226056\n",
      "        -0.650467              None      0.975      0.025  0.308617      5.50 0.013358 220.601522\n",
      "        -0.542747              None      0.954      0.046  0.309875      5.92 0.008626 107.043919\n",
      "        -0.435027              None      0.907      0.093  0.320810      6.86 0.010146 143.510314\n",
      "        -0.327308              None      0.797      0.203  0.337952      9.06 0.008870 112.899552\n",
      "        -0.219588              None      0.532      0.468  0.365406     14.36 0.006781  62.744305\n",
      "        -0.111868              None      0.195      0.805  0.383237     21.10 0.005050  21.195006\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Entropy-Only Threshold Sweep with IBC Metrics for qasper_short\n",
      "============================================================\n",
      "logits_threshold  entropy_threshold  lm1_ratio  lm2_ratio  avg_perf  avg_cost      IBC  delta_IBC\n",
      "            None           0.195659      0.096      0.904  0.382145     23.08 0.004436   6.473413\n",
      "            None           0.370145      0.324      0.676  0.388769     18.52 0.006422  54.144184\n",
      "            None           0.544632      0.591      0.409  0.363000     13.18 0.007465  79.161780\n",
      "            None           0.719118      0.790      0.210  0.342287      9.20 0.009607 130.574442\n",
      "            None           0.893604      0.892      0.108  0.324049      7.16 0.010236 145.685149\n",
      "            None           1.068091      0.949      0.051  0.310848      6.02 0.008734 109.636672\n",
      "            None           1.242577      0.975      0.025  0.309063      5.50 0.014249 241.980619\n",
      "            None           1.417063      0.990      0.010  0.305257      5.20 0.016592 298.233851\n",
      "            None           1.591549      0.994      0.006  0.305462      5.12 0.029359 604.655790\n",
      "============================================================\n",
      "\n",
      "IBC Metric Interpretation:\n",
      "  IBC_BASE (baseline): 0.004166 - Benefit of always using LLM over SLM\n",
      "  Positive Δ_IBC: Method is more cost-effective than baseline\n",
      "  Negative Δ_IBC: Method is less cost-effective than baseline\n",
      "\n",
      "Logits-only summary saved to: logits_entropy_threshold_output/logits_only_sweep_summary_qasper_short.json\n",
      "Entropy-only summary saved to: logits_entropy_threshold_output/entropy_only_sweep_summary_qasper_short.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/pkp8kns52zd14207btyv1s7h0000gn/T/ipykernel_19045/3588706870.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sweep_df = pd.concat([logits_sweep_df, entropy_sweep_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate IBC (Incremental Benefit Per Cost) metrics\n",
    "\n",
    "# Based on the IBC metric defined in the paper:\n",
    "# IBC_M = (P_M - P_SLM) / (C_M - C_SLM)\n",
    "# IBC_BASE = (P_LLM - P_SLM) / (C_LLM - C_SLM)\n",
    "# Δ_IBC(M) = ((IBC_M - IBC_BASE) / IBC_BASE) * 100\n",
    "\n",
    "# Get baseline performance values (same for both methods)\n",
    "P_SLM = logits_sweep_df[\"avg_perf_slm\"].iloc[0]  # Average performance of SLM (with verification)\n",
    "P_LLM = logits_sweep_df[\"avg_perf_llm\"].iloc[0]  # Average performance of LLM (with verification)\n",
    "\n",
    "# Baseline costs\n",
    "C_SLM_baseline = COST_LM1  # Cost of SLM method: 5.0\n",
    "C_LLM_baseline = COST_LM2  # Cost of LLM method: 25.0\n",
    "\n",
    "# Calculate IBC_BASE: Baseline incremental benefit per cost\n",
    "IBC_BASE = (P_LLM - P_SLM) / (C_LLM_baseline - C_SLM_baseline)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"IBC (Incremental Benefit Per Cost) Baseline\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"P_SLM (SLM average performance): {P_SLM:.4f}\")\n",
    "print(f\"P_LLM (LLM average performance): {P_LLM:.4f}\")\n",
    "print(f\"C_SLM (SLM cost): {C_SLM_baseline:.2f}\")\n",
    "print(f\"C_LLM (LLM cost): {C_LLM_baseline:.2f}\")\n",
    "print(f\"IBC_BASE: {IBC_BASE:.6f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Calculate IBC for logits thresholds\n",
    "logits_sweep_df[\"IBC\"] = (logits_sweep_df[\"avg_perf\"] - P_SLM) / (logits_sweep_df[\"avg_cost\"] - C_SLM_baseline)\n",
    "logits_sweep_df[\"delta_IBC\"] = ((logits_sweep_df[\"IBC\"] - IBC_BASE) / IBC_BASE) * 100\n",
    "\n",
    "# Calculate IBC for entropy thresholds\n",
    "entropy_sweep_df[\"IBC\"] = (entropy_sweep_df[\"avg_perf\"] - P_SLM) / (entropy_sweep_df[\"avg_cost\"] - C_SLM_baseline)\n",
    "entropy_sweep_df[\"delta_IBC\"] = ((entropy_sweep_df[\"IBC\"] - IBC_BASE) / IBC_BASE) * 100\n",
    "\n",
    "# Update combined DataFrame\n",
    "sweep_df = pd.concat([logits_sweep_df, entropy_sweep_df], ignore_index=True)\n",
    "\n",
    "# Display the sweep results with IBC metrics - Logits only\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Logits-Only Threshold Sweep with IBC Metrics for {DATASET_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "display_cols = [\"logits_threshold\", \"entropy_threshold\", \"lm1_ratio\", \"lm2_ratio\", \n",
    "                \"avg_perf\", \"avg_cost\", \"IBC\", \"delta_IBC\"]\n",
    "print(logits_sweep_df[display_cols].round(6).to_string(index=False))\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Display the sweep results with IBC metrics - Entropy only\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Entropy-Only Threshold Sweep with IBC Metrics for {DATASET_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(entropy_sweep_df[display_cols].round(6).to_string(index=False))\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Display interpretation\n",
    "print(\"IBC Metric Interpretation:\")\n",
    "print(f\"  IBC_BASE (baseline): {IBC_BASE:.6f} - Benefit of always using LLM over SLM\")\n",
    "print(f\"  Positive Δ_IBC: Method is more cost-effective than baseline\")\n",
    "print(f\"  Negative Δ_IBC: Method is less cost-effective than baseline\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Save separate summaries\n",
    "logits_summary_path = os.path.join(OUTPUT_DIR, f\"logits_only_sweep_summary_{DATASET_NAME}.json\")\n",
    "logits_summary = logits_sweep_df.to_dict(orient=\"records\")\n",
    "with open(logits_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(logits_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "entropy_summary_path = os.path.join(OUTPUT_DIR, f\"entropy_only_sweep_summary_{DATASET_NAME}.json\")\n",
    "entropy_summary = entropy_sweep_df.to_dict(orient=\"records\")\n",
    "with open(entropy_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(entropy_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Logits-only summary saved to: {logits_summary_path}\")\n",
    "print(f\"Entropy-only summary saved to: {entropy_summary_path}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10601-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
